{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cole Turner and Ethan Seal**\n",
    "\n",
    "Fall 2019\n",
    "\n",
    "CS343: Neural Networks\n",
    "\n",
    "Project 3: Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(['seaborn-colorblind', 'seaborn-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=7)\n",
    "\n",
    "# Automatically reload your external source code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global note: Make sure any debug printouts do not appear if `verbose=False`!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4) Implement weight optimizers for gradient descent\n",
    "\n",
    "To change the weights during training, we need an optimization algorithm to have our loss decrease over epochs as we learn the structure of the input patterns. Until now, we used **Stochastic gradient descent (SGD)**, which is the simplest algorithm. We will implement 3 popular algorithms:\n",
    "\n",
    "- `SGD` (stochastic gradient descent)\n",
    "- `SGD_Momentum` (stochastic gradient descent with momentum)\n",
    "- `Adam` (Adaptive Moment Estimation)\n",
    "\n",
    "Implement each of these according to the update equations (in `optimizer.py::update_weights` in each subclass). Let's use $w_t$ in the math below to represent the weights in a layer at time step $t$, $dw$ to represent the gradient of the weights in a layer, and $\\eta$ represent the learning rate. We use vectorized notation below (update applies to all weights element-wise). Then:\n",
    "\n",
    "**SGD**: \n",
    "\n",
    "$w_{t} = w_{t-1} - \\eta \\times dw$\n",
    "\n",
    "**SGD (momentum)**:\n",
    "\n",
    "$v_{t} = m \\times v_{t-1} - \\eta \\times dw$\n",
    "\n",
    "$w_{t} = w_{t-1} + v_t$\n",
    "\n",
    "where $v_t$ is called the `velocity` at time $t$. At the first time step (0), velocity should be set to all zeros and have the same shape as $w$. $m$ is a constant that determines how much of the gradient obtained on the previous time step should factor into the weight update for the current time step.\n",
    "\n",
    "\n",
    "**Adam**:\n",
    "\n",
    "$m_{t} = \\beta_1 \\times m_{t-1} + (1 - \\beta_1)\\times dw$\n",
    "\n",
    "$v_{t} = \\beta_2 \\times v_{t-1} + (1 - \\beta_2)\\times dw^2$\n",
    "\n",
    "$n = m_{t} / \\left (1-(\\beta_1^t) \\right )$\n",
    "\n",
    "$u = v_{t} / \\left (1-(\\beta_2^t) \\right )$\n",
    "\n",
    "$w_{t} = w_{t-1} - \\left ( \\eta \\times n \\right ) / \\left ( \\sqrt(u) + \\epsilon \\right ) $\n",
    "\n",
    "\n",
    "Like SGD (momentum), Adam records momentum terms $m$ and $v$. At time step 0, you should initialize them to zeros in an array equal in size to the weights. $n$ and $u$ are variables computed on each time step. The remaining quantities are constants. Note that $t$ keeps track of the integer time step, and needs to be incremented on each update. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimizer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Test SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD: Wts after 1 iter [-3.1764052 -2.0400157 -1.0978738 -0.2240893  0.8132442  2.0977278]\n",
      "SGD: Wts after 2 iter [-3.3528105 -2.0800314 -1.1957476 -0.4481786  0.6264884  2.1954556]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "wts = np.arange(-3, 3, dtype=np.float64)\n",
    "d_wts = np.random.randn(len(wts))\n",
    "\n",
    "optimizer = SGD()\n",
    "optimizer.prepare(wts, d_wts)\n",
    "\n",
    "new_wts_1 = optimizer.update_weights()\n",
    "new_wts_2 = optimizer.update_weights()\n",
    "\n",
    "print(f'SGD: Wts after 1 iter {new_wts_1}')\n",
    "print(f'SGD: Wts after 2 iter {new_wts_2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should be:\n",
    "\n",
    "    SGD: Wts after 1 iter [-3.1764052 -2.0400157 -1.0978738 -0.2240893  0.8132442  2.0977278]\n",
    "    SGD: Wts after 2 iter [-3.3528105 -2.0800314 -1.1957476 -0.4481786  0.6264884  2.1954556]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Test SGD_Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD M: Wts after 1 iter\n",
      "[[ 1.6879486  0.3879897  0.9343517  2.2075258]\n",
      " [ 1.7181501 -0.9567621  0.9187816 -0.0659476]\n",
      " [ 0.1520801  0.3452366  0.0576     1.52849  ]]\n",
      "SGD M: Wts after 2 iter\n",
      "[[ 1.5661825  0.3685217  0.8633335  2.1541379]\n",
      " [ 1.4790974 -0.9239367  0.8686908  0.0707077]\n",
      " [ 0.5605585  0.2406577 -0.0807098  1.6472364]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "wts = np.random.randn(3, 4)\n",
    "d_wts = np.random.randn(3, 4)\n",
    "\n",
    "optimizer = SGD_Momentum(lr=0.1, m=0.6)\n",
    "optimizer.prepare(wts, d_wts)\n",
    "\n",
    "new_wts_1 = optimizer.update_weights()\n",
    "new_wts_2 = optimizer.update_weights()\n",
    "\n",
    "print(f'SGD M: Wts after 1 iter\\n{new_wts_1}')\n",
    "print(f'SGD M: Wts after 2 iter\\n{new_wts_2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should be:\n",
    "\n",
    "    SGD M: Wts after 1 iter\n",
    "    [[ 1.6879486  0.3879897  0.9343517  2.2075258]\n",
    "     [ 1.7181501 -0.9567621  0.9187816 -0.0659476]\n",
    "     [ 0.1520801  0.3452366  0.0576     1.52849  ]]\n",
    "    SGD M: Wts after 2 iter\n",
    "    [[ 1.5661825  0.3685217  0.8633335  2.1541379]\n",
    "     [ 1.4790974 -0.9239367  0.8686908  0.0707077]\n",
    "     [ 0.5605585  0.2406577 -0.0807098  1.6472364]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Test Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam: Wts after 1 iter\n",
      "[[ 1.6640523  0.3001572  0.878738   2.1408932]\n",
      " [ 1.767558  -0.8772779  0.8500884 -0.0513572]\n",
      " [-0.0032189  0.3105985  0.0440436  1.5542735]]\n",
      "Adam: Wts after 2 iter\n",
      "[[ 1.5640523  0.2001572  0.778738   2.0408932]\n",
      " [ 1.667558  -0.7772779  0.7500884  0.0486428]\n",
      " [ 0.0967811  0.2105985 -0.0559564  1.6542735]]\n",
      "Adam: Wts after 3 iter\n",
      "[[ 1.4640523  0.1001572  0.678738   1.9408932]\n",
      " [ 1.567558  -0.6772779  0.6500884  0.1486428]\n",
      " [ 0.1967811  0.1105985 -0.1559564  1.7542735]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "wts = np.random.randn(3, 4)\n",
    "d_wts = np.random.randn(3, 4)\n",
    "\n",
    "optimizer = Adam(lr=0.1)\n",
    "optimizer.prepare(wts, d_wts)\n",
    "\n",
    "new_wts_1 = optimizer.update_weights()\n",
    "new_wts_2 = optimizer.update_weights()\n",
    "new_wts_3 = optimizer.update_weights()\n",
    "\n",
    "print(f'Adam: Wts after 1 iter\\n{new_wts_1}')\n",
    "print(f'Adam: Wts after 2 iter\\n{new_wts_2}')\n",
    "print(f'Adam: Wts after 3 iter\\n{new_wts_3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should be:\n",
    "\n",
    "    Adam: Wts after 1 iter\n",
    "    [[ 1.6640523  0.3001572  0.878738   2.1408932]\n",
    "     [ 1.767558  -0.8772779  0.8500884 -0.0513572]\n",
    "     [-0.0032189  0.3105985  0.0440436  1.5542735]]\n",
    "    Adam: Wts after 2 iter\n",
    "    [[ 1.5640523  0.2001572  0.778738   2.0408932]\n",
    "     [ 1.667558  -0.7772779  0.7500884  0.0486428]\n",
    "     [ 0.0967811  0.2105985 -0.0559564  1.6542735]]\n",
    "    Adam: Wts after 3 iter\n",
    "    [[ 1.4640523  0.1001572  0.678738   1.9408932]\n",
    "     [ 1.567558  -0.6772779  0.6500884  0.1486428]\n",
    "     [ 0.1967811  0.1105985 -0.1559564  1.7542735]]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5) Write network training methods\n",
    "\n",
    "Implement methods in `network.py` to actually train the network, using all the building blocks that you have created. The methods to implement are:\n",
    "\n",
    "- `predict`\n",
    "- `fit`. Add an optional parameter `print_every=1` that controls the frequency (in iterations) with which to wait before printing out the loss and iteration number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6) Overfitting a convolutional neural network\n",
    "\n",
    "Usually we try to prevent overfitting, but we can use it as a valuable debugging tool to test out a complex backprop-style neural network. Assuming everything is working, it is almost always the case that we should be able to overfit a tiny dataset with a huge model with tons of parameters (i.e. your CNN). You will use this strategy to verify that your network is working.\n",
    "\n",
    "Let's use a small amount of real data from STL-10. If everything is working properly, the network should overfit and you should see a significant drop in the loss from its starting value of ~2.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6a) Move your `preprocess_data.py` from the MLP project\n",
    "\n",
    "Make the one following change:\n",
    "\n",
    "- Re-arrange dimensions of `imgs` so that when it is returned, `shape=(Num imgs, RGB color chans, height, width)` (No longer flatten non-batch dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_stl10_dataset\n",
    "import preprocess_data\n",
    "from network import ConvNet4\n",
    "import optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b) Load in STL-10 at 16x16 resolution\n",
    "\n",
    "If you don't want to wait for STL-10 to download from the internet and resize, copy over your data and numpy folders from your MLP project.\n",
    "\n",
    "**Notes:**\n",
    "- You will need to download the new version of `load_stl10_dataset`.\n",
    "- The different train/test split here won't work if you hard coded the proportions in your `create_splits` implementation! *This isn't catastrophic, it just means that it will take longer to compute accuracy on the validation set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images are: (5000, 96, 96, 3)\n",
      "Labels are: (5000,)\n",
      "Resizing 5000 images to 16x16...Done!\n",
      "Saving Numpy arrays the images and labels to ./numpy...Done!\n",
      "imgs.shape (5000, 16, 16, 3)\n",
      "data.shape (5000, 768)\n",
      "Train data shape:  (4548, 768)\n",
      "Train labels shape:  (4548,)\n",
      "Test data shape:  (400, 768)\n",
      "Test labels shape:  (400,)\n",
      "Validation data shape:  (2, 768)\n",
      "Validation labels shape:  (2,)\n",
      "dev data shape:  (50, 768)\n",
      "dev labels shape:  (50,)\n"
     ]
    }
   ],
   "source": [
    "# Download the STL-10 dataset from the internet, convert it to Numpy ndarray, resize to 16x16\n",
    "# cache it locally on your computer for faster loading next time.\n",
    "load_stl10_dataset.purge_cached_dataset()\n",
    "stl_imgs, stl_labels = load_stl10_dataset.load(scale_fact=6)\n",
    "# preprocess\n",
    "stl_imgs, stl_labels = preprocess_data.preprocess_stl(stl_imgs, stl_labels)\n",
    "# create splits\n",
    "x_train, y_train, x_test, y_test, x_val, y_val, x_dev, y_dev = preprocess_data.create_splits(\n",
    "    stl_imgs, stl_labels, n_train_samps=4548, n_test_samps=400, n_valid_samps=2, n_dev_samps=50)\n",
    "\n",
    "print ('Train data shape: ', x_train.shape)\n",
    "print ('Train labels shape: ', y_train.shape)\n",
    "print ('Test data shape: ', x_test.shape)\n",
    "print ('Test labels shape: ', y_test.shape)\n",
    "print ('Validation data shape: ', x_val.shape)\n",
    "print ('Validation labels shape: ', y_val.shape)\n",
    "print ('dev data shape: ', x_dev.shape)\n",
    "print ('dev labels shape: ', y_dev.shape)\n",
    "\n",
    "classes = np.loadtxt(os.path.join('data', 'stl10_binary', 'class_names.txt'), dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6c) Train and overfit the network on a small STL-10 sample with each optimizer\n",
    "\n",
    "**Goal:** If your network works, you should see a drop in loss over epochs to 0.\n",
    "\n",
    "In 3 seperate cells below\n",
    "\n",
    "- Create 3 different `ConvNet4` networks.\n",
    "- Compile each with a different optimizer (each net uses a different optimizer).\n",
    "- Train each on the **dev** set and validate on the tiny validation set (we dont care about out-of-training-set performance here).\n",
    "\n",
    "You will be making plots demonstrating the overfitting for each optimizer below. **You should train the nets with the same number of epochs such that at least 2/3 of them clearly show loss convergence to a small value; one optimizer may not converge yet, and that's ok**. Cut off the simulations based on the 2/3 that do converge.\n",
    "\n",
    "Guidelines:\n",
    "\n",
    "- Weight scales and learning rates of `1e-2` should work well.\n",
    "- Start by testing the Adam optimizer.\n",
    "- Remember that the input shape is (3, 16, 16). You need to specify this to the network constructor.\n",
    "- The hyperparameters are up to you, though I wouldn't recommend a batch size that is too small (close to 1), otherwise it may be tricky to see whether the loss is actually decreasing on average.\n",
    "- Decreasing `acc_freq` will make the `fit` function evaluate the training and validation accuracy more often. This is a computationally intensive process, so small values come with an increase in training time. On the other hand, checking the accuracy too infrequently means you won't know whether the network is trending toward overfitting the training data, which is what you're checking for.\n",
    "- Each training session takes ~30 mins on my laptop.\n",
    "\n",
    "**Caveat emptor:** Training convolutional networks is notoriously computationally intensive. If you experiment with hyperparameters, each training session may take several hours. Use the loss/accuracy print outs to quickly gauge whether your hyperparameter choices are getting your network to decrease in loss. Monitor print outs and interrupt the Jupyter kernel if things are not trending in the right direction. Consider using the Davis 102 iMacs if this is running too slow on your laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_scale = 1e-2\n",
    "lr = 1e-2\n",
    "input_shape = (3, 16, 16)\n",
    "mini_batch_sz = 10\n",
    "n_epochs = 150\n",
    "\n",
    "#preprocessing flattened it when we actually wanted it not flattened.\n",
    "x_dev = x_dev.reshape(x_dev.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "x_val = x_val.reshape(x_val.shape[0], input_shape[0], input_shape[1], input_shape[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train...\n",
      "750 iterations. 5 iter/epoch.\n",
      "Iteration: 1/750.\n",
      "Time taken for iteration 0: 1.6660547256469727\n",
      "Estimated time to complete: 1249.5410442352295\n",
      "Iteration: 2/750.\n",
      "Iteration: 3/750.\n",
      "Iteration: 4/750.\n",
      "Iteration: 5/750.\n",
      "Iteration: 6/750.\n",
      "Iteration: 7/750.\n",
      "Iteration: 8/750.\n",
      "Iteration: 9/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [2.2845796024885723, 2.2924845991086413, 2.2430353643670484]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.16, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 10/750.\n",
      "Iteration: 11/750.\n",
      "Iteration: 12/750.\n",
      "Iteration: 13/750.\n",
      "Iteration: 14/750.\n",
      "Iteration: 15/750.\n",
      "Iteration: 16/750.\n",
      "Iteration: 17/750.\n",
      "Iteration: 18/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [2.196307338513974, 2.1725589737331377, 1.8555820840431605]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.24, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 19/750.\n",
      "Iteration: 20/750.\n",
      "Iteration: 21/750.\n",
      "Iteration: 22/750.\n",
      "Iteration: 23/750.\n",
      "Iteration: 24/750.\n",
      "Iteration: 25/750.\n",
      "Iteration: 26/750.\n",
      "Iteration: 27/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [2.312345181935326, 2.2428537670038167, 1.8153889792853417]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 28/750.\n",
      "Iteration: 29/750.\n",
      "Iteration: 30/750.\n",
      "Iteration: 31/750.\n",
      "Iteration: 32/750.\n",
      "Iteration: 33/750.\n",
      "Iteration: 34/750.\n",
      "Iteration: 35/750.\n",
      "Iteration: 36/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.7621807182453688, 1.833807183850169, 2.0549246832422936]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.24, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 37/750.\n",
      "Iteration: 38/750.\n",
      "Iteration: 39/750.\n",
      "Iteration: 40/750.\n",
      "Iteration: 41/750.\n",
      "Iteration: 42/750.\n",
      "Iteration: 43/750.\n",
      "Iteration: 44/750.\n",
      "Iteration: 45/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [2.1230847051097714, 2.00511916232943, 1.9732593373540852]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.26, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 46/750.\n",
      "Iteration: 47/750.\n",
      "Iteration: 48/750.\n",
      "Iteration: 49/750.\n",
      "Iteration: 50/750.\n",
      "Iteration: 51/750.\n",
      "Iteration: 52/750.\n",
      "Iteration: 53/750.\n",
      "Iteration: 54/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.8132958306345657, 2.0563964202738827, 1.6435401383740862]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.42, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 55/750.\n",
      "Iteration: 56/750.\n",
      "Iteration: 57/750.\n",
      "Iteration: 58/750.\n",
      "Iteration: 59/750.\n",
      "Iteration: 60/750.\n",
      "Iteration: 61/750.\n",
      "Iteration: 62/750.\n",
      "Iteration: 63/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [2.1429283671619177, 1.3842210419573906, 1.8503751364566963]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.42, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 64/750.\n",
      "Iteration: 65/750.\n",
      "Iteration: 66/750.\n",
      "Iteration: 67/750.\n",
      "Iteration: 68/750.\n",
      "Iteration: 69/750.\n",
      "Iteration: 70/750.\n",
      "Iteration: 71/750.\n",
      "Iteration: 72/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.4492693059040154, 1.8337719840693965, 1.788756308516624]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.42, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 73/750.\n",
      "Iteration: 74/750.\n",
      "Iteration: 75/750.\n",
      "Iteration: 76/750.\n",
      "Iteration: 77/750.\n",
      "Iteration: 78/750.\n",
      "Iteration: 79/750.\n",
      "Iteration: 80/750.\n",
      "Iteration: 81/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.4360661578335743, 1.5099931642285551, 1.4284829401566097]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.46, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 82/750.\n",
      "Iteration: 83/750.\n",
      "Iteration: 84/750.\n",
      "Iteration: 85/750.\n",
      "Iteration: 86/750.\n",
      "Iteration: 87/750.\n",
      "Iteration: 88/750.\n",
      "Iteration: 89/750.\n",
      "Iteration: 90/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.3285889474860497, 1.476922730116448, 1.0004611806216006]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.46, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 91/750.\n",
      "Iteration: 92/750.\n",
      "Iteration: 93/750.\n",
      "Iteration: 94/750.\n",
      "Iteration: 95/750.\n",
      "Iteration: 96/750.\n",
      "Iteration: 97/750.\n",
      "Iteration: 98/750.\n",
      "Iteration: 99/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.4980733151569927, 1.2347177336803679, 1.2437637714007388]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.56, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 100/750.\n",
      "Iteration: 101/750.\n",
      "Iteration: 102/750.\n",
      "Iteration: 103/750.\n",
      "Iteration: 104/750.\n",
      "Iteration: 105/750.\n",
      "Iteration: 106/750.\n",
      "Iteration: 107/750.\n",
      "Iteration: 108/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.1590602893676074, 1.31185675119782, 0.7630824680897652]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.54, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 109/750.\n",
      "Iteration: 110/750.\n",
      "Iteration: 111/750.\n",
      "Iteration: 112/750.\n",
      "Iteration: 113/750.\n",
      "Iteration: 114/750.\n",
      "Iteration: 115/750.\n",
      "Iteration: 116/750.\n",
      "Iteration: 117/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.1463413255311268, 0.8585563678333387, 1.010251243340975]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.62, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 118/750.\n",
      "Iteration: 119/750.\n",
      "Iteration: 120/750.\n",
      "Iteration: 121/750.\n",
      "Iteration: 122/750.\n",
      "Iteration: 123/750.\n",
      "Iteration: 124/750.\n",
      "Iteration: 125/750.\n",
      "Iteration: 126/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.9948293101744844, 0.6375138809069608, 0.9847764571024111]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.68, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 127/750.\n",
      "Iteration: 128/750.\n",
      "Iteration: 129/750.\n",
      "Iteration: 130/750.\n",
      "Iteration: 131/750.\n",
      "Iteration: 132/750.\n",
      "Iteration: 133/750.\n",
      "Iteration: 134/750.\n",
      "Iteration: 135/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.0188620864883335, 1.5110819357261827, 0.5282107176868567]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.72, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 136/750.\n",
      "Iteration: 137/750.\n",
      "Iteration: 138/750.\n",
      "Iteration: 139/750.\n",
      "Iteration: 140/750.\n",
      "Iteration: 141/750.\n",
      "Iteration: 142/750.\n",
      "Iteration: 143/750.\n",
      "Iteration: 144/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.0612828686712, 0.7893206818157843, 0.7050813317119973]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.88, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 145/750.\n",
      "Iteration: 146/750.\n",
      "Iteration: 147/750.\n",
      "Iteration: 148/750.\n",
      "Iteration: 149/750.\n",
      "Iteration: 150/750.\n",
      "Iteration: 151/750.\n",
      "Iteration: 152/750.\n",
      "Iteration: 153/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.7095368860314037, 0.7967863091393386, 0.34622555323144777]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.88, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 154/750.\n",
      "Iteration: 155/750.\n",
      "Iteration: 156/750.\n",
      "Iteration: 157/750.\n",
      "Iteration: 158/750.\n",
      "Iteration: 159/750.\n",
      "Iteration: 160/750.\n",
      "Iteration: 161/750.\n",
      "Iteration: 162/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.4147177801834292, 0.1976633903218503, 0.6149993686508091]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.88, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 163/750.\n",
      "Iteration: 164/750.\n",
      "Iteration: 165/750.\n",
      "Iteration: 166/750.\n",
      "Iteration: 167/750.\n",
      "Iteration: 168/750.\n",
      "Iteration: 169/750.\n",
      "Iteration: 170/750.\n",
      "Iteration: 171/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.5417135652175434, 0.5127860907579702, 0.4423694055698868]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.9, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 172/750.\n",
      "Iteration: 173/750.\n",
      "Iteration: 174/750.\n",
      "Iteration: 175/750.\n",
      "Iteration: 176/750.\n",
      "Iteration: 177/750.\n",
      "Iteration: 178/750.\n",
      "Iteration: 179/750.\n",
      "Iteration: 180/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.3257512884451925, 0.23361025284867254, 0.4208937622866595]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.9, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 181/750.\n",
      "Iteration: 182/750.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 183/750.\n",
      "Iteration: 184/750.\n",
      "Iteration: 185/750.\n",
      "Iteration: 186/750.\n",
      "Iteration: 187/750.\n",
      "Iteration: 188/750.\n",
      "Iteration: 189/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.2526039181014987, 0.2661376588799025, 0.29333143778269355]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.96, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 190/750.\n",
      "Iteration: 191/750.\n",
      "Iteration: 192/750.\n",
      "Iteration: 193/750.\n",
      "Iteration: 194/750.\n",
      "Iteration: 195/750.\n",
      "Iteration: 196/750.\n",
      "Iteration: 197/750.\n",
      "Iteration: 198/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.1112629225244871, 0.07248974144177063, 0.31392346579773295]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 199/750.\n",
      "Iteration: 200/750.\n",
      "Iteration: 201/750.\n",
      "Iteration: 202/750.\n",
      "Iteration: 203/750.\n",
      "Iteration: 204/750.\n",
      "Iteration: 205/750.\n",
      "Iteration: 206/750.\n",
      "Iteration: 207/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.10167808297320831, 0.07616059586943538, 0.17105394612346236]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 208/750.\n",
      "Iteration: 209/750.\n",
      "Iteration: 210/750.\n",
      "Iteration: 211/750.\n",
      "Iteration: 212/750.\n",
      "Iteration: 213/750.\n",
      "Iteration: 214/750.\n",
      "Iteration: 215/750.\n",
      "Iteration: 216/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.04604406308275605, 0.2700110150812488, 0.1730756502564659]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.98, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 217/750.\n",
      "Iteration: 218/750.\n",
      "Iteration: 219/750.\n",
      "Iteration: 220/750.\n",
      "Iteration: 221/750.\n",
      "Iteration: 222/750.\n",
      "Iteration: 223/750.\n",
      "Iteration: 224/750.\n",
      "Iteration: 225/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.198977290108442, 0.14312686463978938, 0.03261416902984667]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 226/750.\n",
      "Iteration: 227/750.\n",
      "Iteration: 228/750.\n",
      "Iteration: 229/750.\n",
      "Iteration: 230/750.\n",
      "Iteration: 231/750.\n",
      "Iteration: 232/750.\n",
      "Iteration: 233/750.\n",
      "Iteration: 234/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.12942711737335458, 0.07879379011385884, 0.062299998236475564]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 235/750.\n",
      "Iteration: 236/750.\n",
      "Iteration: 237/750.\n",
      "Iteration: 238/750.\n",
      "Iteration: 239/750.\n",
      "Iteration: 240/750.\n",
      "Iteration: 241/750.\n",
      "Iteration: 242/750.\n",
      "Iteration: 243/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0771401766704466, 0.061047090270442084, 0.07818698688785436]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 244/750.\n",
      "Iteration: 245/750.\n",
      "Iteration: 246/750.\n",
      "Iteration: 247/750.\n",
      "Iteration: 248/750.\n",
      "Iteration: 249/750.\n",
      "Iteration: 250/750.\n",
      "Iteration: 251/750.\n",
      "Iteration: 252/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.02739438570280992, 0.049484310869164085, 0.03895022424867601]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 253/750.\n",
      "Iteration: 254/750.\n",
      "Iteration: 255/750.\n",
      "Iteration: 256/750.\n",
      "Iteration: 257/750.\n",
      "Iteration: 258/750.\n",
      "Iteration: 259/750.\n",
      "Iteration: 260/750.\n",
      "Iteration: 261/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.05045440734712168, 0.032420542822017895, 0.06781557374825271]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 262/750.\n",
      "Iteration: 263/750.\n",
      "Iteration: 264/750.\n",
      "Iteration: 265/750.\n",
      "Iteration: 266/750.\n",
      "Iteration: 267/750.\n",
      "Iteration: 268/750.\n",
      "Iteration: 269/750.\n",
      "Iteration: 270/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.03012681363097762, 0.04319413731979136, 0.02004262906877935]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 271/750.\n",
      "Iteration: 272/750.\n",
      "Iteration: 273/750.\n",
      "Iteration: 274/750.\n",
      "Iteration: 275/750.\n",
      "Iteration: 276/750.\n",
      "Iteration: 277/750.\n",
      "Iteration: 278/750.\n",
      "Iteration: 279/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.03676389344140057, 0.06299340377472308, 0.03454798975959152]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 280/750.\n",
      "Iteration: 281/750.\n",
      "Iteration: 282/750.\n",
      "Iteration: 283/750.\n",
      "Iteration: 284/750.\n",
      "Iteration: 285/750.\n",
      "Iteration: 286/750.\n",
      "Iteration: 287/750.\n",
      "Iteration: 288/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.021970949590423193, 0.008230711804036533, 0.04246734441958538]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 289/750.\n",
      "Iteration: 290/750.\n",
      "Iteration: 291/750.\n",
      "Iteration: 292/750.\n",
      "Iteration: 293/750.\n",
      "Iteration: 294/750.\n",
      "Iteration: 295/750.\n",
      "Iteration: 296/750.\n",
      "Iteration: 297/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.024177658392888598, 0.017464338244271845, 0.022840108982812742]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 298/750.\n",
      "Iteration: 299/750.\n",
      "Iteration: 300/750.\n",
      "Iteration: 301/750.\n",
      "Iteration: 302/750.\n",
      "Iteration: 303/750.\n",
      "Iteration: 304/750.\n",
      "Iteration: 305/750.\n",
      "Iteration: 306/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.025275973834323114, 0.030583171943476435, 0.00984498326368509]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 307/750.\n",
      "Iteration: 308/750.\n",
      "Iteration: 309/750.\n",
      "Iteration: 310/750.\n",
      "Iteration: 311/750.\n",
      "Iteration: 312/750.\n",
      "Iteration: 313/750.\n",
      "Iteration: 314/750.\n",
      "Iteration: 315/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0203630879049658, 0.01757106751657544, 0.010262528385192541]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 316/750.\n",
      "Iteration: 317/750.\n",
      "Iteration: 318/750.\n",
      "Iteration: 319/750.\n",
      "Iteration: 320/750.\n",
      "Iteration: 321/750.\n",
      "Iteration: 322/750.\n",
      "Iteration: 323/750.\n",
      "Iteration: 324/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.02262841694298828, 0.01758433643867625, 0.012217754002525399]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 325/750.\n",
      "Iteration: 326/750.\n",
      "Iteration: 327/750.\n",
      "Iteration: 328/750.\n",
      "Iteration: 329/750.\n",
      "Iteration: 330/750.\n",
      "Iteration: 331/750.\n",
      "Iteration: 332/750.\n",
      "Iteration: 333/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.015909905810328133, 0.01982560712916763, 0.013691125194401766]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 334/750.\n",
      "Iteration: 335/750.\n",
      "Iteration: 336/750.\n",
      "Iteration: 337/750.\n",
      "Iteration: 338/750.\n",
      "Iteration: 339/750.\n",
      "Iteration: 340/750.\n",
      "Iteration: 341/750.\n",
      "Iteration: 342/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.015110213725853711, 0.009527124064834098, 0.009334541094691867]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 343/750.\n",
      "Iteration: 344/750.\n",
      "Iteration: 345/750.\n",
      "Iteration: 346/750.\n",
      "Iteration: 347/750.\n",
      "Iteration: 348/750.\n",
      "Iteration: 349/750.\n",
      "Iteration: 350/750.\n",
      "Iteration: 351/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.01383921247847794, 0.017888971550242515, 0.012172627766161605]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 352/750.\n",
      "Iteration: 353/750.\n",
      "Iteration: 354/750.\n",
      "Iteration: 355/750.\n",
      "Iteration: 356/750.\n",
      "Iteration: 357/750.\n",
      "Iteration: 358/750.\n",
      "Iteration: 359/750.\n",
      "Iteration: 360/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.007692151459628377, 0.00860071117621646, 0.01337183452254036]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 361/750.\n",
      "Iteration: 362/750.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 363/750.\n",
      "Iteration: 364/750.\n",
      "Iteration: 365/750.\n",
      "Iteration: 366/750.\n",
      "Iteration: 367/750.\n",
      "Iteration: 368/750.\n",
      "Iteration: 369/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.016828954981043625, 0.008532854220705712, 0.014868278238738564]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 370/750.\n",
      "Iteration: 371/750.\n",
      "Iteration: 372/750.\n",
      "Iteration: 373/750.\n",
      "Iteration: 374/750.\n",
      "Iteration: 375/750.\n",
      "Iteration: 376/750.\n",
      "Iteration: 377/750.\n",
      "Iteration: 378/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.009769343830962701, 0.014373982427621002, 0.00897708285199268]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 379/750.\n",
      "Iteration: 380/750.\n",
      "Iteration: 381/750.\n",
      "Iteration: 382/750.\n",
      "Iteration: 383/750.\n",
      "Iteration: 384/750.\n",
      "Iteration: 385/750.\n",
      "Iteration: 386/750.\n",
      "Iteration: 387/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0061601486759959275, 0.011447400788466693, 0.010592205644013353]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 388/750.\n",
      "Iteration: 389/750.\n",
      "Iteration: 390/750.\n",
      "Iteration: 391/750.\n",
      "Iteration: 392/750.\n",
      "Iteration: 393/750.\n",
      "Iteration: 394/750.\n",
      "Iteration: 395/750.\n",
      "Iteration: 396/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.01006021025842885, 0.007103090284885151, 0.008748900552573213]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 397/750.\n",
      "Iteration: 398/750.\n",
      "Iteration: 399/750.\n",
      "Iteration: 400/750.\n",
      "Iteration: 401/750.\n",
      "Iteration: 402/750.\n",
      "Iteration: 403/750.\n",
      "Iteration: 404/750.\n",
      "Iteration: 405/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.006573226155617962, 0.008800037481439274, 0.007748006161016941]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 406/750.\n",
      "Iteration: 407/750.\n",
      "Iteration: 408/750.\n",
      "Iteration: 409/750.\n",
      "Iteration: 410/750.\n",
      "Iteration: 411/750.\n",
      "Iteration: 412/750.\n",
      "Iteration: 413/750.\n",
      "Iteration: 414/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.00853616274950926, 0.010528373331639172, 0.007736132999175462]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 415/750.\n",
      "Iteration: 416/750.\n",
      "Iteration: 417/750.\n",
      "Iteration: 418/750.\n",
      "Iteration: 419/750.\n",
      "Iteration: 420/750.\n",
      "Iteration: 421/750.\n",
      "Iteration: 422/750.\n",
      "Iteration: 423/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.007007899223572968, 0.00644505755233859, 0.006488757529163384]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 424/750.\n",
      "Iteration: 425/750.\n",
      "Iteration: 426/750.\n",
      "Iteration: 427/750.\n",
      "Iteration: 428/750.\n",
      "Iteration: 429/750.\n",
      "Iteration: 430/750.\n",
      "Iteration: 431/750.\n",
      "Iteration: 432/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.006053104493819026, 0.006276737155629471, 0.004831008743959072]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 433/750.\n",
      "Iteration: 434/750.\n",
      "Iteration: 435/750.\n",
      "Iteration: 436/750.\n",
      "Iteration: 437/750.\n",
      "Iteration: 438/750.\n",
      "Iteration: 439/750.\n",
      "Iteration: 440/750.\n",
      "Iteration: 441/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0052529349195933744, 0.00732885029553593, 0.003807727914067893]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 442/750.\n",
      "Iteration: 443/750.\n",
      "Iteration: 444/750.\n",
      "Iteration: 445/750.\n",
      "Iteration: 446/750.\n",
      "Iteration: 447/750.\n",
      "Iteration: 448/750.\n",
      "Iteration: 449/750.\n",
      "Iteration: 450/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.002921542476523221, 0.003654111207226949, 0.0032846278323096566]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 451/750.\n",
      "Iteration: 452/750.\n",
      "Iteration: 453/750.\n",
      "Iteration: 454/750.\n",
      "Iteration: 455/750.\n",
      "Iteration: 456/750.\n",
      "Iteration: 457/750.\n",
      "Iteration: 458/750.\n",
      "Iteration: 459/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0037279718019347834, 0.010223271739306093, 0.005651504630691279]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 460/750.\n",
      "Iteration: 461/750.\n",
      "Iteration: 462/750.\n",
      "Iteration: 463/750.\n",
      "Iteration: 464/750.\n",
      "Iteration: 465/750.\n",
      "Iteration: 466/750.\n",
      "Iteration: 467/750.\n",
      "Iteration: 468/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.006245607454844204, 0.005093410584849283, 0.0015902946597379669]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 469/750.\n",
      "Iteration: 470/750.\n",
      "Iteration: 471/750.\n",
      "Iteration: 472/750.\n",
      "Iteration: 473/750.\n",
      "Iteration: 474/750.\n",
      "Iteration: 475/750.\n",
      "Iteration: 476/750.\n",
      "Iteration: 477/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.00439410620243234, 0.006613928781317931, 0.003303326800842834]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 478/750.\n",
      "Iteration: 479/750.\n",
      "Iteration: 480/750.\n",
      "Iteration: 481/750.\n",
      "Iteration: 482/750.\n",
      "Iteration: 483/750.\n",
      "Iteration: 484/750.\n",
      "Iteration: 485/750.\n",
      "Iteration: 486/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0031054786424553732, 0.004483647626745672, 0.005548467720664309]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 487/750.\n",
      "Iteration: 488/750.\n",
      "Iteration: 489/750.\n",
      "Iteration: 490/750.\n",
      "Iteration: 491/750.\n",
      "Iteration: 492/750.\n",
      "Iteration: 493/750.\n",
      "Iteration: 494/750.\n",
      "Iteration: 495/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.00414603240714461, 0.009087762031101948, 0.0044056132546797895]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 496/750.\n",
      "Iteration: 497/750.\n",
      "Iteration: 498/750.\n",
      "Iteration: 499/750.\n",
      "Iteration: 500/750.\n",
      "Iteration: 501/750.\n",
      "Iteration: 502/750.\n",
      "Iteration: 503/750.\n",
      "Iteration: 504/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.002901846718032038, 0.004217244810202035, 0.005485786030073221]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 505/750.\n",
      "Iteration: 506/750.\n",
      "Iteration: 507/750.\n",
      "Iteration: 508/750.\n",
      "Iteration: 509/750.\n",
      "Iteration: 510/750.\n",
      "Iteration: 511/750.\n",
      "Iteration: 512/750.\n",
      "Iteration: 513/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0026945877464181088, 0.003658771101894766, 0.006712447580500221]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 514/750.\n",
      "Iteration: 515/750.\n",
      "Iteration: 516/750.\n",
      "Iteration: 517/750.\n",
      "Iteration: 518/750.\n",
      "Iteration: 519/750.\n",
      "Iteration: 520/750.\n",
      "Iteration: 521/750.\n",
      "Iteration: 522/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.004699095964409763, 0.003251637175180632, 0.003271414183918524]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 523/750.\n",
      "Iteration: 524/750.\n",
      "Iteration: 525/750.\n",
      "Iteration: 526/750.\n",
      "Iteration: 527/750.\n",
      "Iteration: 528/750.\n",
      "Iteration: 529/750.\n",
      "Iteration: 530/750.\n",
      "Iteration: 531/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0042051060893134274, 0.0038884924191254335, 0.002962506320237182]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 532/750.\n",
      "Iteration: 533/750.\n",
      "Iteration: 534/750.\n",
      "Iteration: 535/750.\n",
      "Iteration: 536/750.\n",
      "Iteration: 537/750.\n",
      "Iteration: 538/750.\n",
      "Iteration: 539/750.\n",
      "Iteration: 540/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.00298678405030627, 0.0027571970161227623, 0.003610517848250861]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 541/750.\n",
      "Iteration: 542/750.\n",
      "Iteration: 543/750.\n",
      "Iteration: 544/750.\n",
      "Iteration: 545/750.\n",
      "Iteration: 546/750.\n",
      "Iteration: 547/750.\n",
      "Iteration: 548/750.\n",
      "Iteration: 549/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.002019017040326986, 0.003892469532275133, 0.0035536293901333015]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 550/750.\n",
      "Iteration: 551/750.\n",
      "Iteration: 552/750.\n",
      "Iteration: 553/750.\n",
      "Iteration: 554/750.\n",
      "Iteration: 555/750.\n",
      "Iteration: 556/750.\n",
      "Iteration: 557/750.\n",
      "Iteration: 558/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0033645742777648154, 0.0025017081009596756, 0.003921704589888853]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 559/750.\n",
      "Iteration: 560/750.\n",
      "Iteration: 561/750.\n",
      "Iteration: 562/750.\n",
      "Iteration: 563/750.\n",
      "Iteration: 564/750.\n",
      "Iteration: 565/750.\n",
      "Iteration: 566/750.\n",
      "Iteration: 567/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.002981434723113973, 0.0013819217487384989, 0.0032949836318237656]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 568/750.\n",
      "Iteration: 569/750.\n",
      "Iteration: 570/750.\n",
      "Iteration: 571/750.\n",
      "Iteration: 572/750.\n",
      "Iteration: 573/750.\n",
      "Iteration: 574/750.\n",
      "Iteration: 575/750.\n",
      "Iteration: 576/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.002842557068409717, 0.0024130196333980534, 0.003387120788490034]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 577/750.\n",
      "Iteration: 578/750.\n",
      "Iteration: 579/750.\n",
      "Iteration: 580/750.\n",
      "Iteration: 581/750.\n",
      "Iteration: 582/750.\n",
      "Iteration: 583/750.\n",
      "Iteration: 584/750.\n",
      "Iteration: 585/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.002065377079902587, 0.002318131956749458, 0.0026479241884973977]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 586/750.\n",
      "Iteration: 587/750.\n",
      "Iteration: 588/750.\n",
      "Iteration: 589/750.\n",
      "Iteration: 590/750.\n",
      "Iteration: 591/750.\n",
      "Iteration: 592/750.\n",
      "Iteration: 593/750.\n",
      "Iteration: 594/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0028781183348701484, 0.005051741188932144, 0.0037435946121089254]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 595/750.\n",
      "Iteration: 596/750.\n",
      "Iteration: 597/750.\n",
      "Iteration: 598/750.\n",
      "Iteration: 599/750.\n",
      "Iteration: 600/750.\n",
      "Iteration: 601/750.\n",
      "Iteration: 602/750.\n",
      "Iteration: 603/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.00299654626254093, 0.003778291377038163, 0.0020178715544124266]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 604/750.\n",
      "Iteration: 605/750.\n",
      "Iteration: 606/750.\n",
      "Iteration: 607/750.\n",
      "Iteration: 608/750.\n",
      "Iteration: 609/750.\n",
      "Iteration: 610/750.\n",
      "Iteration: 611/750.\n",
      "Iteration: 612/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0035613454363410397, 0.0020711168632821953, 0.0017361541606328416]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 613/750.\n",
      "Iteration: 614/750.\n",
      "Iteration: 615/750.\n",
      "Iteration: 616/750.\n",
      "Iteration: 617/750.\n",
      "Iteration: 618/750.\n",
      "Iteration: 619/750.\n",
      "Iteration: 620/750.\n",
      "Iteration: 621/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0023794135897436224, 0.002434979975880696, 0.002614170525187537]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 622/750.\n",
      "Iteration: 623/750.\n",
      "Iteration: 624/750.\n",
      "Iteration: 625/750.\n",
      "Iteration: 626/750.\n",
      "Iteration: 627/750.\n",
      "Iteration: 628/750.\n",
      "Iteration: 629/750.\n",
      "Iteration: 630/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.002221729193596045, 0.0031591962917994717, 0.0026405344122758093]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 631/750.\n",
      "Iteration: 632/750.\n",
      "Iteration: 633/750.\n",
      "Iteration: 634/750.\n",
      "Iteration: 635/750.\n",
      "Iteration: 636/750.\n",
      "Iteration: 637/750.\n",
      "Iteration: 638/750.\n",
      "Iteration: 639/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0017053509258779108, 0.002381196224160104, 0.0024068952505818864]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 640/750.\n",
      "Iteration: 641/750.\n",
      "Iteration: 642/750.\n",
      "Iteration: 643/750.\n",
      "Iteration: 644/750.\n",
      "Iteration: 645/750.\n",
      "Iteration: 646/750.\n",
      "Iteration: 647/750.\n",
      "Iteration: 648/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0020553921174259224, 0.0025360110659460853, 0.0015094830603940579]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 649/750.\n",
      "Iteration: 650/750.\n",
      "Iteration: 651/750.\n",
      "Iteration: 652/750.\n",
      "Iteration: 653/750.\n",
      "Iteration: 654/750.\n",
      "Iteration: 655/750.\n",
      "Iteration: 656/750.\n",
      "Iteration: 657/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0015051251377963782, 0.0017464836483383286, 0.0011076337311476472]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 658/750.\n",
      "Iteration: 659/750.\n",
      "Iteration: 660/750.\n",
      "Iteration: 661/750.\n",
      "Iteration: 662/750.\n",
      "Iteration: 663/750.\n",
      "Iteration: 664/750.\n",
      "Iteration: 665/750.\n",
      "Iteration: 666/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0017189487269323558, 0.0017419271948256071, 0.001517056077212622]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 667/750.\n",
      "Iteration: 668/750.\n",
      "Iteration: 669/750.\n",
      "Iteration: 670/750.\n",
      "Iteration: 671/750.\n",
      "Iteration: 672/750.\n",
      "Iteration: 673/750.\n",
      "Iteration: 674/750.\n",
      "Iteration: 675/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.002583472775782791, 0.003131088204767879, 0.0017153648706021698]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 676/750.\n",
      "Iteration: 677/750.\n",
      "Iteration: 678/750.\n",
      "Iteration: 679/750.\n",
      "Iteration: 680/750.\n",
      "Iteration: 681/750.\n",
      "Iteration: 682/750.\n",
      "Iteration: 683/750.\n",
      "Iteration: 684/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0015867622306953801, 0.0011906861619485998, 0.00190017199205851]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 685/750.\n",
      "Iteration: 686/750.\n",
      "Iteration: 687/750.\n",
      "Iteration: 688/750.\n",
      "Iteration: 689/750.\n",
      "Iteration: 690/750.\n",
      "Iteration: 691/750.\n",
      "Iteration: 692/750.\n",
      "Iteration: 693/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0019919869100415086, 0.0022330823709953945, 0.0027735989679547437]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 694/750.\n",
      "Iteration: 695/750.\n",
      "Iteration: 696/750.\n",
      "Iteration: 697/750.\n",
      "Iteration: 698/750.\n",
      "Iteration: 699/750.\n",
      "Iteration: 700/750.\n",
      "Iteration: 701/750.\n",
      "Iteration: 702/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0019377714647102334, 0.0021468643512156747, 0.001613401638587184]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 703/750.\n",
      "Iteration: 704/750.\n",
      "Iteration: 705/750.\n",
      "Iteration: 706/750.\n",
      "Iteration: 707/750.\n",
      "Iteration: 708/750.\n",
      "Iteration: 709/750.\n",
      "Iteration: 710/750.\n",
      "Iteration: 711/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.00086116675978332, 0.001959338149785799, 0.0013139268230490256]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 712/750.\n",
      "Iteration: 713/750.\n",
      "Iteration: 714/750.\n",
      "Iteration: 715/750.\n",
      "Iteration: 716/750.\n",
      "Iteration: 717/750.\n",
      "Iteration: 718/750.\n",
      "Iteration: 719/750.\n",
      "Iteration: 720/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.000922927035018237, 0.0017368995073886746, 0.0009850095962978867]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 721/750.\n",
      "Iteration: 722/750.\n",
      "Iteration: 723/750.\n",
      "Iteration: 724/750.\n",
      "Iteration: 725/750.\n",
      "Iteration: 726/750.\n",
      "Iteration: 727/750.\n",
      "Iteration: 728/750.\n",
      "Iteration: 729/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.00160438701564368, 0.0015889317417497155, 0.0020780690656655564]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 730/750.\n",
      "Iteration: 731/750.\n",
      "Iteration: 732/750.\n",
      "Iteration: 733/750.\n",
      "Iteration: 734/750.\n",
      "Iteration: 735/750.\n",
      "Iteration: 736/750.\n",
      "Iteration: 737/750.\n",
      "Iteration: 738/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0010388349009595177, 0.0013960343425225312, 0.0021584754233142122]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 739/750.\n",
      "Iteration: 740/750.\n",
      "Iteration: 741/750.\n",
      "Iteration: 742/750.\n",
      "Iteration: 743/750.\n",
      "Iteration: 744/750.\n",
      "Iteration: 745/750.\n",
      "Iteration: 746/750.\n",
      "Iteration: 747/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.001567330547213729, 0.0015021132826861594, 0.0016439893587461441]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 748/750.\n",
      "Iteration: 749/750.\n",
      "Iteration: 750/750.\n",
      "\n",
      "\n",
      "----------------FINAL OUTPUT----------------\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "Loss history: [2.3055575589923514, 2.300406899071696, 2.3029133221585023, 2.294747648840793, 2.288576523611883, 2.290180646017032, 2.2845796024885723, 2.2924845991086413, 2.2430353643670484, 2.206360550015477, 2.1353708524936774, 2.3061608483970777, 2.2316064461131218, 2.271748997353242, 2.13538884465873, 2.196307338513974, 2.1725589737331377, 1.8555820840431605, 2.2766782177253577, 1.934898385421089, 1.871502819358501, 1.9138400595387086, 1.7624269599259677, 2.4884701186691163, 2.312345181935326, 2.2428537670038167, 1.8153889792853417, 2.042472231906978, 2.0274354973933386, 2.064275502731321, 1.9516374136257122, 1.897432557211994, 1.6579292576131857, 1.7621807182453688, 1.833807183850169, 2.0549246832422936, 1.8723331847903255, 1.6442609969283204, 1.9184862164623917, 1.9046591136566846, 1.8139214361928027, 2.005325645143333, 2.1230847051097714, 2.00511916232943, 1.9732593373540852, 2.0184364213823445, 2.6499883370507757, 1.5295603987287203, 2.14307588096561, 1.6253938407019612, 1.8354510726197937, 1.8132958306345657, 2.0563964202738827, 1.6435401383740862, 1.8748368238786222, 1.828618938846007, 1.7541986087968249, 1.806783549190694, 2.1103791353341728, 1.8129516954502638, 2.1429283671619177, 1.3842210419573906, 1.8503751364566963, 1.6541310471876487, 1.7109102547436128, 1.5414166184470133, 2.0695831223441785, 1.8010124632203686, 1.7868103714244592, 1.4492693059040154, 1.8337719840693965, 1.788756308516624, 1.5214341048054703, 1.3312439993016554, 1.5178424544515476, 1.382287926451269, 1.6661837300331095, 1.511687800988327, 1.4360661578335743, 1.5099931642285551, 1.4284829401566097, 1.875279531052607, 1.3028650293485664, 2.0328953693539904, 1.1246929247966073, 1.0400213378264602, 1.0522843412446876, 1.3285889474860497, 1.476922730116448, 1.0004611806216006, 1.140158729501649, 1.219787349219302, 1.4518095314654245, 1.6059248107977853, 1.1818203407602288, 1.315681706105867, 1.4980733151569927, 1.2347177336803679, 1.2437637714007388, 0.9512570234349385, 1.2619418598021444, 1.1709905748167786, 1.2123945793386361, 0.9025242810217403, 1.2810415819546384, 1.1590602893676074, 1.31185675119782, 0.7630824680897652, 1.2410263152957888, 1.1463289555778127, 1.3242519901328729, 0.8602249197955484, 1.2629893396069092, 1.0838840137850903, 1.1463413255311268, 0.8585563678333387, 1.010251243340975, 0.9108188527342874, 1.1657749006634384, 0.8501433736954743, 1.158100675622405, 1.2207810975212778, 0.6518704700850245, 0.9948293101744844, 0.6375138809069608, 0.9847764571024111, 0.6513278452428268, 1.0415511171954228, 1.032705407100247, 0.6644553302503602, 0.9375853799071927, 1.0291258904932936, 1.0188620864883335, 1.5110819357261827, 0.5282107176868567, 0.7851462760323777, 0.6624157430401119, 0.6488930768763339, 0.4787559503043197, 0.5610481598343083, 0.8771298906666285, 1.0612828686712, 0.7893206818157843, 0.7050813317119973, 0.8077446501275436, 0.7209304969752148, 0.48393176482096134, 0.5696522016964861, 0.3705807523842548, 0.44725253533085835, 0.7095368860314037, 0.7967863091393386, 0.34622555323144777, 0.7874294421665962, 0.28001171706999456, 0.6417807190258413, 0.503012086297786, 0.40403112139353486, 0.7426523787714432, 0.4147177801834292, 0.1976633903218503, 0.6149993686508091, 0.33555014104080705, 0.4207246377423761, 0.7233705795063519, 0.5423455286862345, 0.2921541497299826, 0.395845769945578, 0.5417135652175434, 0.5127860907579702, 0.4423694055698868, 0.3236602867531084, 0.14447520925983015, 0.4420298535896451, 0.4239328888498195, 0.21793086374474135, 0.49037664023562694, 0.3257512884451925, 0.23361025284867254, 0.4208937622866595, 0.2746972798672402, 0.4300013296525805, 0.36844921168523465, 0.23540087771781135, 0.41544197284494255, 0.2823062465263835, 0.2526039181014987, 0.2661376588799025, 0.29333143778269355, 0.3487392255354627, 0.20258932667943813, 0.43918308371232995, 0.30036378963684185, 0.22148967050890503, 0.11776599062723939, 0.1112629225244871, 0.07248974144177063, 0.31392346579773295, 0.11642156820684864, 0.23277828595846, 0.27315102825750665, 0.10417861245265847, 0.19039342095246847, 0.1679942273827797, 0.10167808297320831, 0.07616059586943538, 0.17105394612346236, 0.12500151358681744, 0.23919722520178413, 0.07845111215034989, 0.1245876885694504, 0.08742772534386235, 0.09124494133054097, 0.04604406308275605, 0.2700110150812488, 0.1730756502564659, 0.1568864321992886, 0.301464783995552, 0.08661943850772261, 0.17291598426721763, 0.07844323760715582, 0.1762749192416052, 0.198977290108442, 0.14312686463978938, 0.03261416902984667, 0.03054111706547881, 0.037543319868447673, 0.08079612114761033, 0.08678542311860352, 0.060218295902046116, 0.11474317879204149, 0.12942711737335458, 0.07879379011385884, 0.062299998236475564, 0.03503384923598232, 0.05560824559122778, 0.1030194030756865, 0.0990760726210339, 0.13145477710517545, 0.01610179754280706, 0.0771401766704466, 0.061047090270442084, 0.07818698688785436, 0.0918949199397228, 0.09319227375868416, 0.08811378867269333, 0.0734461336023442, 0.03174724699505884, 0.06212261310142785, 0.02739438570280992, 0.049484310869164085, 0.03895022424867601, 0.04981987358064388, 0.05074515396988789, 0.03983638177250487, 0.08492574436301814, 0.05200150962079785, 0.030332530426612006, 0.05045440734712168, 0.032420542822017895, 0.06781557374825271, 0.07558561610917186, 0.027929277068080117, 0.05190464068235282, 0.012895986292345257, 0.03725704383043042, 0.02313330775136576, 0.03012681363097762, 0.04319413731979136, 0.02004262906877935, 0.02229894280837623, 0.02751672654121899, 0.035268494043214145, 0.039428610645982264, 0.03204125921046961, 0.02661427731158411, 0.03676389344140057, 0.06299340377472308, 0.03454798975959152, 0.030682535393103484, 0.04194412309936003, 0.015105433473034708, 0.027204625626327257, 0.040608940778716955, 0.023246911952024316, 0.021970949590423193, 0.008230711804036533, 0.04246734441958538, 0.030043344573878424, 0.01763246479153635, 0.0284837154134865, 0.03091560022319131, 0.016077278588038684, 0.026727421826212808, 0.024177658392888598, 0.017464338244271845, 0.022840108982812742, 0.017288535993820235, 0.02148621491454814, 0.028915874613482787, 0.011973472052999645, 0.021289459056199462, 0.012232264444989205, 0.025275973834323114, 0.030583171943476435, 0.00984498326368509, 0.028018216705707557, 0.023579674426418645, 0.02469447606856087, 0.014756764283264868, 0.029010128608519467, 0.0195356637239778, 0.0203630879049658, 0.01757106751657544, 0.010262528385192541, 0.013262062860265728, 0.021245177339018713, 0.021721729017228963, 0.03689103708585828, 0.01351651153150445, 0.009935061205575439, 0.02262841694298828, 0.01758433643867625, 0.012217754002525399, 0.01932559752139203, 0.014958402214521075, 0.027567374480082448, 0.018025596817403688, 0.010517139914541018, 0.017699444398169662, 0.015909905810328133, 0.01982560712916763, 0.013691125194401766, 0.016550432755662575, 0.009078229764578996, 0.007944549330278787, 0.01326805701525457, 0.025850371833740872, 0.013628392763211048, 0.015110213725853711, 0.009527124064834098, 0.009334541094691867, 0.006085630296447723, 0.017934603923836508, 0.009869102393588697, 0.01877156124176056, 0.010938960103062646, 0.009361092362202691, 0.01383921247847794, 0.017888971550242515, 0.012172627766161605, 0.012892033167378431, 0.008206894884965657, 0.00905059199327114, 0.017796027177664306, 0.009163779352044385, 0.009489457224419108, 0.007692151459628377, 0.00860071117621646, 0.01337183452254036, 0.01293385673719119, 0.008399997383587529, 0.01264211171765953, 0.013858170752131613, 0.004903633772470935, 0.009291095305653377, 0.016828954981043625, 0.008532854220705712, 0.014868278238738564, 0.009907119201207511, 0.015346446124348127, 0.011719231907154463, 0.004210578679004389, 0.014362194165312063, 0.013576512265913249, 0.009769343830962701, 0.014373982427621002, 0.00897708285199268, 0.007879795891827508, 0.009862970582605868, 0.007378127775919613, 0.007813068998589977, 0.006245027504013833, 0.008922686984725212, 0.0061601486759959275, 0.011447400788466693, 0.010592205644013353, 0.008779988807601034, 0.007384445939282525, 0.005231718442068179, 0.005374112996673978, 0.007218698803703233, 0.009120530892147403, 0.01006021025842885, 0.007103090284885151, 0.008748900552573213, 0.0035180300370962697, 0.007995185752619328, 0.006997362700971801, 0.009063252542229545, 0.00764816543458346, 0.010021398502396002, 0.006573226155617962, 0.008800037481439274, 0.007748006161016941, 0.010937734736951477, 0.006072712121079891, 0.00538165310937602, 0.007465023465125563, 0.004909194654504705, 0.0058572163199658075, 0.00853616274950926, 0.010528373331639172, 0.007736132999175462, 0.00718902707290607, 0.0029909235888185715, 0.010247816293518714, 0.006803988991259701, 0.0063724690003217335, 0.002905587914533317, 0.007007899223572968, 0.00644505755233859, 0.006488757529163384, 0.002572594387541828, 0.0044282391344211405, 0.006817876694734597, 0.004539010813511677, 0.004361713562243303, 0.0064384843265000965, 0.006053104493819026, 0.006276737155629471, 0.004831008743959072, 0.010958469366237057, 0.0060074596681144, 0.0029252938844696874, 0.0030119681989681257, 0.006741203732546441, 0.005249409183349996, 0.0052529349195933744, 0.00732885029553593, 0.003807727914067893, 0.0040358186911486175, 0.004833614462360483, 0.0047207717788715015, 0.003263436409534696, 0.00383955053305814, 0.009459019838161612, 0.002921542476523221, 0.003654111207226949, 0.0032846278323096566, 0.004885562824869446, 0.00508569049506098, 0.006373271890193562, 0.0050280810405093435, 0.006200431003390502, 0.0035311075063430976, 0.0037279718019347834, 0.010223271739306093, 0.005651504630691279, 0.005008483937049452, 0.006027997653171636, 0.006361605154960152, 0.0034867165884126454, 0.0029331952938917664, 0.004020571156771747, 0.006245607454844204, 0.005093410584849283, 0.0015902946597379669, 0.004266418964347722, 0.005104216601482122, 0.0033153953966085464, 0.002263779852399839, 0.007160398650149055, 0.006600048804114131, 0.00439410620243234, 0.006613928781317931, 0.003303326800842834, 0.004055481427072928, 0.006521513929952537, 0.004458408414755986, 0.005556058564968627, 0.0032512623862984338, 0.0029908740123645145, 0.0031054786424553732, 0.004483647626745672, 0.005548467720664309, 0.006621132659978442, 0.004664450628537602, 0.005383152499169567, 0.0044664641921742234, 0.006147528404749515, 0.004589144306922832, 0.00414603240714461, 0.009087762031101948, 0.0044056132546797895, 0.003909099621062447, 0.002374078828294372, 0.004199905211404633, 0.006057050601898557, 0.0032105797489554015, 0.0031396686273902057, 0.002901846718032038, 0.004217244810202035, 0.005485786030073221, 0.00456064989139799, 0.004663277704692785, 0.003895897854008306, 0.0035728899014997257, 0.0027536640593061232, 0.004219869830662639, 0.0026945877464181088, 0.003658771101894766, 0.006712447580500221, 0.0034696071602221797, 0.003184528785590471, 0.006798037894825511, 0.0049662754478161, 0.0027370586846505564, 0.004805179326626523, 0.004699095964409763, 0.003251637175180632, 0.003271414183918524, 0.005589838323135825, 0.0032441389877758903, 0.0067858601622701, 0.006118630688254335, 0.004131691877312017, 0.003082671607921422, 0.0042051060893134274, 0.0038884924191254335, 0.002962506320237182, 0.002076157311928311, 0.0020558838118918516, 0.003356738651684689, 0.0028620378255369678, 0.0029686248846644923, 0.00349765788185177, 0.00298678405030627, 0.0027571970161227623, 0.003610517848250861, 0.004466379683559644, 0.003580558601558644, 0.0022491600988507276, 0.0038862574565933887, 0.0020585755986470416, 0.003350119130455706, 0.002019017040326986, 0.003892469532275133, 0.0035536293901333015, 0.0032306450342034334, 0.002800186661059094, 0.003231323578908408, 0.002385128178917209, 0.0031244951981927393, 0.004250466780830907, 0.0033645742777648154, 0.0025017081009596756, 0.003921704589888853, 0.004116295634103122, 0.003620619447717498, 0.0017603315439187269, 0.002469395053677466, 0.0029986673784294093, 0.0032258431313620115, 0.002981434723113973, 0.0013819217487384989, 0.0032949836318237656, 0.004161248977183076, 0.0025140866251171495, 0.002230685022040397, 0.003939352093486844, 0.00295376376826937, 0.0030149312382889482, 0.002842557068409717, 0.0024130196333980534, 0.003387120788490034, 0.002919653903554604, 0.004610478841244241, 0.004830742754672676, 0.004849249306296692, 0.004092267674917679, 0.00323413799897149, 0.002065377079902587, 0.002318131956749458, 0.0026479241884973977, 0.002350889617063655, 0.002304473890369692, 0.004179157886665444, 0.00311171460734571, 0.002343209887573981, 0.0036532783388440442, 0.0028781183348701484, 0.005051741188932144, 0.0037435946121089254, 0.004010236480847776, 0.0017777984537796284, 0.0018778375051706848, 0.0035253957550480178, 0.004959555307564778, 0.001887589324937939, 0.00299654626254093, 0.003778291377038163, 0.0020178715544124266, 0.0030543333680973016, 0.002629996715882432, 0.00205269095445219, 0.0020654795626069322, 0.002139058456069485, 0.002084374852789543, 0.0035613454363410397, 0.0020711168632821953, 0.0017361541606328416, 0.002791846028621271, 0.0015494622708039484, 0.0026310909319199483, 0.0017459589478171595, 0.003877744793507751, 0.0026055125449147, 0.0023794135897436224, 0.002434979975880696, 0.002614170525187537, 0.002936776859233544, 0.001795591592682955, 0.002159181356940997, 0.0024298340813780532, 0.0022545333375030376, 0.0020952799545892044, 0.002221729193596045, 0.0031591962917994717, 0.0026405344122758093, 0.002253969437781386, 0.0020810698389622123, 0.002098951589179424, 0.0023879481220214368, 0.0024659611953409643, 0.0012442477664569135, 0.0017053509258779108, 0.002381196224160104, 0.0024068952505818864, 0.001942113174759474, 0.002249302904875942, 0.001991766694607963, 0.0021797384551068, 0.0024118015747329564, 0.0027592926374591935, 0.0020553921174259224, 0.0025360110659460853, 0.0015094830603940579, 0.0018396449015827107, 0.0019136425148368588, 0.0035232864151260052, 0.0037251412669029194, 0.002507815100855471, 0.0023615706936043353, 0.0015051251377963782, 0.0017464836483383286, 0.0011076337311476472, 0.0025276373927947977, 0.0019052993614125083, 0.0019418524240472032, 0.0017731584356773104, 0.0021402632026583635, 0.0022181837621589057, 0.0017189487269323558, 0.0017419271948256071, 0.001517056077212622, 0.0036580975340142043, 0.0021318460724613457, 0.0022641845365276372, 0.001735248554264855, 0.0023377994092444844, 0.0013196547302400724, 0.002583472775782791, 0.003131088204767879, 0.0017153648706021698, 0.0014351853360627023, 0.001400209395550599, 0.0024686255004100384, 0.0025826632567593215, 0.0022213754367474653, 0.0033098318179195334, 0.0015867622306953801, 0.0011906861619485998, 0.00190017199205851, 0.002267275949970542, 0.0017973148529912772, 0.0017866884705282319, 0.0015852220841670378, 0.002458232138383762, 0.0014244905431833356, 0.0019919869100415086, 0.0022330823709953945, 0.0027735989679547437, 0.0026790056440027204, 0.0016648337697080765, 0.0020633290652193647, 0.0018820694134797256, 0.0023719812136731443, 0.0019624419874155175, 0.0019377714647102334, 0.0021468643512156747, 0.001613401638587184, 0.0027210783105111533, 0.0013589544920751294, 0.000832850823144387, 0.0012942071864269781, 0.002052153088747626, 0.0018600081981792272, 0.00086116675978332, 0.001959338149785799, 0.0013139268230490256, 0.0014306314232718851, 0.0016959028832292259, 0.0017518964170656918, 0.0017331592446227701, 0.0015918302126712758, 0.0011052993354616284, 0.000922927035018237, 0.0017368995073886746, 0.0009850095962978867, 0.0011681268652901126, 0.0006304484831412962, 0.001487507723879396, 0.0026474813828503124, 0.001394117871138313, 0.0008204897749794848, 0.00160438701564368, 0.0015889317417497155, 0.0020780690656655564, 0.0010636636904031375, 0.0016175576179632877, 0.0008621424073317424, 0.0017633454725117536, 0.0019517233887101733, 0.0016230313448372285, 0.0010388349009595177, 0.0013960343425225312, 0.0021584754233142122, 0.0015085948267311207, 0.0015261316094420914, 0.0014562299200879358, 0.0021428721253259246, 0.0015409984000469296, 0.001926323120287074, 0.001567330547213729, 0.0015021132826861594, 0.0016439893587461441, 0.0016136400756901178, 0.0007363023758899155, 0.0013633331881789958]\n",
      "Accuracy history: [0.16, 0.24, 0.22, 0.24, 0.26, 0.42, 0.42, 0.42, 0.46, 0.46, 0.56, 0.54, 0.62, 0.68, 0.72, 0.88, 0.88, 0.88, 0.9, 0.9, 0.96, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Adam\n",
    "\n",
    "adam = ConvNet4(input_shape=input_shape, wt_scale=wt_scale, verbose=False)\n",
    "adam.compile('adam')\n",
    "adam.fit(x_dev, y_dev, x_val, y_val, mini_batch_sz=mini_batch_sz, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train...\n",
      "750 iterations. 5 iter/epoch.\n",
      "Iteration: 1/750.\n",
      "Time taken for iteration 0: 1.6611967086791992\n",
      "Estimated time to complete: 1245.8975315093994\n",
      "Iteration: 2/750.\n",
      "Iteration: 3/750.\n",
      "Iteration: 4/750.\n",
      "Iteration: 5/750.\n",
      "Iteration: 6/750.\n",
      "Iteration: 7/750.\n",
      "Iteration: 8/750.\n",
      "Iteration: 9/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3083244218204695, 2.305725695413475, 2.297431716772699]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.16, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 10/750.\n",
      "Iteration: 11/750.\n",
      "Iteration: 12/750.\n",
      "Iteration: 13/750.\n",
      "Iteration: 14/750.\n",
      "Iteration: 15/750.\n",
      "Iteration: 16/750.\n",
      "Iteration: 17/750.\n",
      "Iteration: 18/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3018450824131884, 2.3036695764710173, 2.3017066047698584]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.16, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 19/750.\n",
      "Iteration: 20/750.\n",
      "Iteration: 21/750.\n",
      "Iteration: 22/750.\n",
      "Iteration: 23/750.\n",
      "Iteration: 24/750.\n",
      "Iteration: 25/750.\n",
      "Iteration: 26/750.\n",
      "Iteration: 27/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.296826694035708, 2.2988054741544746, 2.298301305372676]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.16, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 28/750.\n",
      "Iteration: 29/750.\n",
      "Iteration: 30/750.\n",
      "Iteration: 31/750.\n",
      "Iteration: 32/750.\n",
      "Iteration: 33/750.\n",
      "Iteration: 34/750.\n",
      "Iteration: 35/750.\n",
      "Iteration: 36/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2966168260260056, 2.299502510069381, 2.297408434557621]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.16, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 37/750.\n",
      "Iteration: 38/750.\n",
      "Iteration: 39/750.\n",
      "Iteration: 40/750.\n",
      "Iteration: 41/750.\n",
      "Iteration: 42/750.\n",
      "Iteration: 43/750.\n",
      "Iteration: 44/750.\n",
      "Iteration: 45/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.298535173218746, 2.2969774199484214, 2.287743382443448]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.16, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 46/750.\n",
      "Iteration: 47/750.\n",
      "Iteration: 48/750.\n",
      "Iteration: 49/750.\n",
      "Iteration: 50/750.\n",
      "Iteration: 51/750.\n",
      "Iteration: 52/750.\n",
      "Iteration: 53/750.\n",
      "Iteration: 54/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2839536623842185, 2.2938137555028617, 2.302438358369893]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 55/750.\n",
      "Iteration: 56/750.\n",
      "Iteration: 57/750.\n",
      "Iteration: 58/750.\n",
      "Iteration: 59/750.\n",
      "Iteration: 60/750.\n",
      "Iteration: 61/750.\n",
      "Iteration: 62/750.\n",
      "Iteration: 63/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3025548231332373, 2.2845544381527216, 2.281798459737148]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 64/750.\n",
      "Iteration: 65/750.\n",
      "Iteration: 66/750.\n",
      "Iteration: 67/750.\n",
      "Iteration: 68/750.\n",
      "Iteration: 69/750.\n",
      "Iteration: 70/750.\n",
      "Iteration: 71/750.\n",
      "Iteration: 72/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2835935731873747, 2.2789792517783667, 2.2761709594171795]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 73/750.\n",
      "Iteration: 74/750.\n",
      "Iteration: 75/750.\n",
      "Iteration: 76/750.\n",
      "Iteration: 77/750.\n",
      "Iteration: 78/750.\n",
      "Iteration: 79/750.\n",
      "Iteration: 80/750.\n",
      "Iteration: 81/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.274863026115056, 2.294948740664123, 2.2861885214757716]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 82/750.\n",
      "Iteration: 83/750.\n",
      "Iteration: 84/750.\n",
      "Iteration: 85/750.\n",
      "Iteration: 86/750.\n",
      "Iteration: 87/750.\n",
      "Iteration: 88/750.\n",
      "Iteration: 89/750.\n",
      "Iteration: 90/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2580556523144146, 2.2770504059269747, 2.2811090608307416]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 91/750.\n",
      "Iteration: 92/750.\n",
      "Iteration: 93/750.\n",
      "Iteration: 94/750.\n",
      "Iteration: 95/750.\n",
      "Iteration: 96/750.\n",
      "Iteration: 97/750.\n",
      "Iteration: 98/750.\n",
      "Iteration: 99/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2767012191425837, 2.261673609481268, 2.272762194269741]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 100/750.\n",
      "Iteration: 101/750.\n",
      "Iteration: 102/750.\n",
      "Iteration: 103/750.\n",
      "Iteration: 104/750.\n",
      "Iteration: 105/750.\n",
      "Iteration: 106/750.\n",
      "Iteration: 107/750.\n",
      "Iteration: 108/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2664487825242845, 2.286639542318909, 2.247916495356978]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 109/750.\n",
      "Iteration: 110/750.\n",
      "Iteration: 111/750.\n",
      "Iteration: 112/750.\n",
      "Iteration: 113/750.\n",
      "Iteration: 114/750.\n",
      "Iteration: 115/750.\n",
      "Iteration: 116/750.\n",
      "Iteration: 117/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2729363999356873, 2.273953395587147, 2.2786906412783487]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 118/750.\n",
      "Iteration: 119/750.\n",
      "Iteration: 120/750.\n",
      "Iteration: 121/750.\n",
      "Iteration: 122/750.\n",
      "Iteration: 123/750.\n",
      "Iteration: 124/750.\n",
      "Iteration: 125/750.\n",
      "Iteration: 126/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2537882165991663, 2.278439164214905, 2.2486419643059565]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 127/750.\n",
      "Iteration: 128/750.\n",
      "Iteration: 129/750.\n",
      "Iteration: 130/750.\n",
      "Iteration: 131/750.\n",
      "Iteration: 132/750.\n",
      "Iteration: 133/750.\n",
      "Iteration: 134/750.\n",
      "Iteration: 135/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.32051304490723, 2.3152721759448656, 2.2505928687197856]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 136/750.\n",
      "Iteration: 137/750.\n",
      "Iteration: 138/750.\n",
      "Iteration: 139/750.\n",
      "Iteration: 140/750.\n",
      "Iteration: 141/750.\n",
      "Iteration: 142/750.\n",
      "Iteration: 143/750.\n",
      "Iteration: 144/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2918283532741106, 2.230395258419411, 2.2907653948807876]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 145/750.\n",
      "Iteration: 146/750.\n",
      "Iteration: 147/750.\n",
      "Iteration: 148/750.\n",
      "Iteration: 149/750.\n",
      "Iteration: 150/750.\n",
      "Iteration: 151/750.\n",
      "Iteration: 152/750.\n",
      "Iteration: 153/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.250193863582836, 2.2973584825330042, 2.268586391634377]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 154/750.\n",
      "Iteration: 155/750.\n",
      "Iteration: 156/750.\n",
      "Iteration: 157/750.\n",
      "Iteration: 158/750.\n",
      "Iteration: 159/750.\n",
      "Iteration: 160/750.\n",
      "Iteration: 161/750.\n",
      "Iteration: 162/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2824466859916486, 2.244650987574829, 2.2543351594952203]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 163/750.\n",
      "Iteration: 164/750.\n",
      "Iteration: 165/750.\n",
      "Iteration: 166/750.\n",
      "Iteration: 167/750.\n",
      "Iteration: 168/750.\n",
      "Iteration: 169/750.\n",
      "Iteration: 170/750.\n",
      "Iteration: 171/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.232717186500342, 2.2397897020179474, 2.2738842139177944]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 172/750.\n",
      "Iteration: 173/750.\n",
      "Iteration: 174/750.\n",
      "Iteration: 175/750.\n",
      "Iteration: 176/750.\n",
      "Iteration: 177/750.\n",
      "Iteration: 178/750.\n",
      "Iteration: 179/750.\n",
      "Iteration: 180/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2679793280689706, 2.28689228759865, 2.276174764319612]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 181/750.\n",
      "Iteration: 182/750.\n",
      "Iteration: 183/750.\n",
      "Iteration: 184/750.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 185/750.\n",
      "Iteration: 186/750.\n",
      "Iteration: 187/750.\n",
      "Iteration: 188/750.\n",
      "Iteration: 189/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2809092996558507, 2.2789688282647362, 2.28805322376837]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 190/750.\n",
      "Iteration: 191/750.\n",
      "Iteration: 192/750.\n",
      "Iteration: 193/750.\n",
      "Iteration: 194/750.\n",
      "Iteration: 195/750.\n",
      "Iteration: 196/750.\n",
      "Iteration: 197/750.\n",
      "Iteration: 198/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2551803876097742, 2.268825191182065, 2.2279083463692317]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 199/750.\n",
      "Iteration: 200/750.\n",
      "Iteration: 201/750.\n",
      "Iteration: 202/750.\n",
      "Iteration: 203/750.\n",
      "Iteration: 204/750.\n",
      "Iteration: 205/750.\n",
      "Iteration: 206/750.\n",
      "Iteration: 207/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.308294898786341, 2.2447185877161426, 2.2561030174177947]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 208/750.\n",
      "Iteration: 209/750.\n",
      "Iteration: 210/750.\n",
      "Iteration: 211/750.\n",
      "Iteration: 212/750.\n",
      "Iteration: 213/750.\n",
      "Iteration: 214/750.\n",
      "Iteration: 215/750.\n",
      "Iteration: 216/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.274031923422989, 2.292507221173354, 2.2214233061154625]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 217/750.\n",
      "Iteration: 218/750.\n",
      "Iteration: 219/750.\n",
      "Iteration: 220/750.\n",
      "Iteration: 221/750.\n",
      "Iteration: 222/750.\n",
      "Iteration: 223/750.\n",
      "Iteration: 224/750.\n",
      "Iteration: 225/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3173336547347247, 2.267883190300659, 2.216650036147166]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 226/750.\n",
      "Iteration: 227/750.\n",
      "Iteration: 228/750.\n",
      "Iteration: 229/750.\n",
      "Iteration: 230/750.\n",
      "Iteration: 231/750.\n",
      "Iteration: 232/750.\n",
      "Iteration: 233/750.\n",
      "Iteration: 234/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3015755984933626, 2.238681765961866, 2.2016660365529783]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 235/750.\n",
      "Iteration: 236/750.\n",
      "Iteration: 237/750.\n",
      "Iteration: 238/750.\n",
      "Iteration: 239/750.\n",
      "Iteration: 240/750.\n",
      "Iteration: 241/750.\n",
      "Iteration: 242/750.\n",
      "Iteration: 243/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3058068408717873, 2.2569811975613874, 2.269927435171348]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 244/750.\n",
      "Iteration: 245/750.\n",
      "Iteration: 246/750.\n",
      "Iteration: 247/750.\n",
      "Iteration: 248/750.\n",
      "Iteration: 249/750.\n",
      "Iteration: 250/750.\n",
      "Iteration: 251/750.\n",
      "Iteration: 252/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2473843431840854, 2.1699650093380507, 2.330035628675186]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 253/750.\n",
      "Iteration: 254/750.\n",
      "Iteration: 255/750.\n",
      "Iteration: 256/750.\n",
      "Iteration: 257/750.\n",
      "Iteration: 258/750.\n",
      "Iteration: 259/750.\n",
      "Iteration: 260/750.\n",
      "Iteration: 261/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.271419112296858, 2.201561180448635, 2.2923443997167285]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 262/750.\n",
      "Iteration: 263/750.\n",
      "Iteration: 264/750.\n",
      "Iteration: 265/750.\n",
      "Iteration: 266/750.\n",
      "Iteration: 267/750.\n",
      "Iteration: 268/750.\n",
      "Iteration: 269/750.\n",
      "Iteration: 270/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3423970038143236, 2.2769262981458995, 2.3062019756584777]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 271/750.\n",
      "Iteration: 272/750.\n",
      "Iteration: 273/750.\n",
      "Iteration: 274/750.\n",
      "Iteration: 275/750.\n",
      "Iteration: 276/750.\n",
      "Iteration: 277/750.\n",
      "Iteration: 278/750.\n",
      "Iteration: 279/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2407741533860976, 2.172899416933032, 2.1984333670970226]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 280/750.\n",
      "Iteration: 281/750.\n",
      "Iteration: 282/750.\n",
      "Iteration: 283/750.\n",
      "Iteration: 284/750.\n",
      "Iteration: 285/750.\n",
      "Iteration: 286/750.\n",
      "Iteration: 287/750.\n",
      "Iteration: 288/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2981574744501674, 2.2600462418147753, 2.2166681453444483]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 289/750.\n",
      "Iteration: 290/750.\n",
      "Iteration: 291/750.\n",
      "Iteration: 292/750.\n",
      "Iteration: 293/750.\n",
      "Iteration: 294/750.\n",
      "Iteration: 295/750.\n",
      "Iteration: 296/750.\n",
      "Iteration: 297/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2155338896642607, 2.197460097265643, 2.318388907515723]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 298/750.\n",
      "Iteration: 299/750.\n",
      "Iteration: 300/750.\n",
      "Iteration: 301/750.\n",
      "Iteration: 302/750.\n",
      "Iteration: 303/750.\n",
      "Iteration: 304/750.\n",
      "Iteration: 305/750.\n",
      "Iteration: 306/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.183530894514179, 2.1687370910024586, 2.280636857520292]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 307/750.\n",
      "Iteration: 308/750.\n",
      "Iteration: 309/750.\n",
      "Iteration: 310/750.\n",
      "Iteration: 311/750.\n",
      "Iteration: 312/750.\n",
      "Iteration: 313/750.\n",
      "Iteration: 314/750.\n",
      "Iteration: 315/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3083214739652798, 2.2727792010817764, 2.2652212535838214]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 316/750.\n",
      "Iteration: 317/750.\n",
      "Iteration: 318/750.\n",
      "Iteration: 319/750.\n",
      "Iteration: 320/750.\n",
      "Iteration: 321/750.\n",
      "Iteration: 322/750.\n",
      "Iteration: 323/750.\n",
      "Iteration: 324/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2743787708138794, 2.205430793999642, 2.216025726528513]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 325/750.\n",
      "Iteration: 326/750.\n",
      "Iteration: 327/750.\n",
      "Iteration: 328/750.\n",
      "Iteration: 329/750.\n",
      "Iteration: 330/750.\n",
      "Iteration: 331/750.\n",
      "Iteration: 332/750.\n",
      "Iteration: 333/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2523932586141244, 2.2818711031090086, 2.2729374546977894]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 334/750.\n",
      "Iteration: 335/750.\n",
      "Iteration: 336/750.\n",
      "Iteration: 337/750.\n",
      "Iteration: 338/750.\n",
      "Iteration: 339/750.\n",
      "Iteration: 340/750.\n",
      "Iteration: 341/750.\n",
      "Iteration: 342/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3870551622869107, 2.227611855833517, 2.2965227388282172]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 343/750.\n",
      "Iteration: 344/750.\n",
      "Iteration: 345/750.\n",
      "Iteration: 346/750.\n",
      "Iteration: 347/750.\n",
      "Iteration: 348/750.\n",
      "Iteration: 349/750.\n",
      "Iteration: 350/750.\n",
      "Iteration: 351/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1972767659635544, 2.1985326184695064, 2.2349970739214924]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 352/750.\n",
      "Iteration: 353/750.\n",
      "Iteration: 354/750.\n",
      "Iteration: 355/750.\n",
      "Iteration: 356/750.\n",
      "Iteration: 357/750.\n",
      "Iteration: 358/750.\n",
      "Iteration: 359/750.\n",
      "Iteration: 360/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1742901250718467, 2.0799756410980557, 2.2220841861419194]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 361/750.\n",
      "Iteration: 362/750.\n",
      "Iteration: 363/750.\n",
      "Iteration: 364/750.\n",
      "Iteration: 365/750.\n",
      "Iteration: 366/750.\n",
      "Iteration: 367/750.\n",
      "Iteration: 368/750.\n",
      "Iteration: 369/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1819677069929813, 2.2273059778822533, 2.1947998105505557]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 370/750.\n",
      "Iteration: 371/750.\n",
      "Iteration: 372/750.\n",
      "Iteration: 373/750.\n",
      "Iteration: 374/750.\n",
      "Iteration: 375/750.\n",
      "Iteration: 376/750.\n",
      "Iteration: 377/750.\n",
      "Iteration: 378/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.24355648076423, 2.176654193386759, 2.343633754517829]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 379/750.\n",
      "Iteration: 380/750.\n",
      "Iteration: 381/750.\n",
      "Iteration: 382/750.\n",
      "Iteration: 383/750.\n",
      "Iteration: 384/750.\n",
      "Iteration: 385/750.\n",
      "Iteration: 386/750.\n",
      "Iteration: 387/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.258344127286521, 2.287439493420776, 2.2160262565649242]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 388/750.\n",
      "Iteration: 389/750.\n",
      "Iteration: 390/750.\n",
      "Iteration: 391/750.\n",
      "Iteration: 392/750.\n",
      "Iteration: 393/750.\n",
      "Iteration: 394/750.\n",
      "Iteration: 395/750.\n",
      "Iteration: 396/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2432384498691653, 2.247009227711138, 2.3254272749122347]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 397/750.\n",
      "Iteration: 398/750.\n",
      "Iteration: 399/750.\n",
      "Iteration: 400/750.\n",
      "Iteration: 401/750.\n",
      "Iteration: 402/750.\n",
      "Iteration: 403/750.\n",
      "Iteration: 404/750.\n",
      "Iteration: 405/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.241971823851108, 2.1875700843869, 2.158207424004244]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 406/750.\n",
      "Iteration: 407/750.\n",
      "Iteration: 408/750.\n",
      "Iteration: 409/750.\n",
      "Iteration: 410/750.\n",
      "Iteration: 411/750.\n",
      "Iteration: 412/750.\n",
      "Iteration: 413/750.\n",
      "Iteration: 414/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2484347859408387, 2.3060932894119572, 2.2624999998559]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 415/750.\n",
      "Iteration: 416/750.\n",
      "Iteration: 417/750.\n",
      "Iteration: 418/750.\n",
      "Iteration: 419/750.\n",
      "Iteration: 420/750.\n",
      "Iteration: 421/750.\n",
      "Iteration: 422/750.\n",
      "Iteration: 423/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1886142866691873, 2.128478050810751, 2.311385551409236]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 424/750.\n",
      "Iteration: 425/750.\n",
      "Iteration: 426/750.\n",
      "Iteration: 427/750.\n",
      "Iteration: 428/750.\n",
      "Iteration: 429/750.\n",
      "Iteration: 430/750.\n",
      "Iteration: 431/750.\n",
      "Iteration: 432/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.194539654885159, 2.2900531022302424, 2.2121713336740445]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 433/750.\n",
      "Iteration: 434/750.\n",
      "Iteration: 435/750.\n",
      "Iteration: 436/750.\n",
      "Iteration: 437/750.\n",
      "Iteration: 438/750.\n",
      "Iteration: 439/750.\n",
      "Iteration: 440/750.\n",
      "Iteration: 441/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.134445198940538, 2.2699183887282217, 2.1507394401438193]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 442/750.\n",
      "Iteration: 443/750.\n",
      "Iteration: 444/750.\n",
      "Iteration: 445/750.\n",
      "Iteration: 446/750.\n",
      "Iteration: 447/750.\n",
      "Iteration: 448/750.\n",
      "Iteration: 449/750.\n",
      "Iteration: 450/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1917973532475976, 2.2950102348914974, 2.100521391053965]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 451/750.\n",
      "Iteration: 452/750.\n",
      "Iteration: 453/750.\n",
      "Iteration: 454/750.\n",
      "Iteration: 455/750.\n",
      "Iteration: 456/750.\n",
      "Iteration: 457/750.\n",
      "Iteration: 458/750.\n",
      "Iteration: 459/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.208949986017907, 2.302902253085499, 2.2244442228505825]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 460/750.\n",
      "Iteration: 461/750.\n",
      "Iteration: 462/750.\n",
      "Iteration: 463/750.\n",
      "Iteration: 464/750.\n",
      "Iteration: 465/750.\n",
      "Iteration: 466/750.\n",
      "Iteration: 467/750.\n",
      "Iteration: 468/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.311667349891113, 2.2451032878593913, 2.114113198046391]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 469/750.\n",
      "Iteration: 470/750.\n",
      "Iteration: 471/750.\n",
      "Iteration: 472/750.\n",
      "Iteration: 473/750.\n",
      "Iteration: 474/750.\n",
      "Iteration: 475/750.\n",
      "Iteration: 476/750.\n",
      "Iteration: 477/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.362584979184528, 2.1842563367266337, 2.2175075899941072]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 478/750.\n",
      "Iteration: 479/750.\n",
      "Iteration: 480/750.\n",
      "Iteration: 481/750.\n",
      "Iteration: 482/750.\n",
      "Iteration: 483/750.\n",
      "Iteration: 484/750.\n",
      "Iteration: 485/750.\n",
      "Iteration: 486/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.274318562637799, 2.065706641955925, 2.1479803585598147]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 487/750.\n",
      "Iteration: 488/750.\n",
      "Iteration: 489/750.\n",
      "Iteration: 490/750.\n",
      "Iteration: 491/750.\n",
      "Iteration: 492/750.\n",
      "Iteration: 493/750.\n",
      "Iteration: 494/750.\n",
      "Iteration: 495/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2761224118898045, 2.1852569399565933, 2.208590116865381]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 496/750.\n",
      "Iteration: 497/750.\n",
      "Iteration: 498/750.\n",
      "Iteration: 499/750.\n",
      "Iteration: 500/750.\n",
      "Iteration: 501/750.\n",
      "Iteration: 502/750.\n",
      "Iteration: 503/750.\n",
      "Iteration: 504/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.116283809246591, 2.2028335737626565, 2.2326783799474863]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 505/750.\n",
      "Iteration: 506/750.\n",
      "Iteration: 507/750.\n",
      "Iteration: 508/750.\n",
      "Iteration: 509/750.\n",
      "Iteration: 510/750.\n",
      "Iteration: 511/750.\n",
      "Iteration: 512/750.\n",
      "Iteration: 513/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1968076409861594, 2.274975264968471, 1.9855868075389775]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 514/750.\n",
      "Iteration: 515/750.\n",
      "Iteration: 516/750.\n",
      "Iteration: 517/750.\n",
      "Iteration: 518/750.\n",
      "Iteration: 519/750.\n",
      "Iteration: 520/750.\n",
      "Iteration: 521/750.\n",
      "Iteration: 522/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1953933392272265, 2.215828703238852, 2.3100137441845474]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 523/750.\n",
      "Iteration: 524/750.\n",
      "Iteration: 525/750.\n",
      "Iteration: 526/750.\n",
      "Iteration: 527/750.\n",
      "Iteration: 528/750.\n",
      "Iteration: 529/750.\n",
      "Iteration: 530/750.\n",
      "Iteration: 531/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.146009311380914, 2.2716558721394833, 2.2301757314982242]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 532/750.\n",
      "Iteration: 533/750.\n",
      "Iteration: 534/750.\n",
      "Iteration: 535/750.\n",
      "Iteration: 536/750.\n",
      "Iteration: 537/750.\n",
      "Iteration: 538/750.\n",
      "Iteration: 539/750.\n",
      "Iteration: 540/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.332063398600958, 2.28230492085212, 1.9736515415982288]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 541/750.\n",
      "Iteration: 542/750.\n",
      "Iteration: 543/750.\n",
      "Iteration: 544/750.\n",
      "Iteration: 545/750.\n",
      "Iteration: 546/750.\n",
      "Iteration: 547/750.\n",
      "Iteration: 548/750.\n",
      "Iteration: 549/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1829420813707525, 2.2437436638194823, 2.286485280603498]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 550/750.\n",
      "Iteration: 551/750.\n",
      "Iteration: 552/750.\n",
      "Iteration: 553/750.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 554/750.\n",
      "Iteration: 555/750.\n",
      "Iteration: 556/750.\n",
      "Iteration: 557/750.\n",
      "Iteration: 558/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1614771451312933, 2.3313536288551155, 2.166155356616962]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 559/750.\n",
      "Iteration: 560/750.\n",
      "Iteration: 561/750.\n",
      "Iteration: 562/750.\n",
      "Iteration: 563/750.\n",
      "Iteration: 564/750.\n",
      "Iteration: 565/750.\n",
      "Iteration: 566/750.\n",
      "Iteration: 567/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1527029769325354, 2.256560466136464, 2.298755900566209]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 568/750.\n",
      "Iteration: 569/750.\n",
      "Iteration: 570/750.\n",
      "Iteration: 571/750.\n",
      "Iteration: 572/750.\n",
      "Iteration: 573/750.\n",
      "Iteration: 574/750.\n",
      "Iteration: 575/750.\n",
      "Iteration: 576/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3348348069425224, 2.2370508707565415, 2.1151456857666564]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 577/750.\n",
      "Iteration: 578/750.\n",
      "Iteration: 579/750.\n",
      "Iteration: 580/750.\n",
      "Iteration: 581/750.\n",
      "Iteration: 582/750.\n",
      "Iteration: 583/750.\n",
      "Iteration: 584/750.\n",
      "Iteration: 585/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.250810597110293, 2.293818748703291, 2.020107784339772]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 586/750.\n",
      "Iteration: 587/750.\n",
      "Iteration: 588/750.\n",
      "Iteration: 589/750.\n",
      "Iteration: 590/750.\n",
      "Iteration: 591/750.\n",
      "Iteration: 592/750.\n",
      "Iteration: 593/750.\n",
      "Iteration: 594/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.206940417505602, 2.2418594893978243, 2.435203323547574]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 595/750.\n",
      "Iteration: 596/750.\n",
      "Iteration: 597/750.\n",
      "Iteration: 598/750.\n",
      "Iteration: 599/750.\n",
      "Iteration: 600/750.\n",
      "Iteration: 601/750.\n",
      "Iteration: 602/750.\n",
      "Iteration: 603/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2084709801104907, 2.303666300630048, 2.2445135191978163]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 604/750.\n",
      "Iteration: 605/750.\n",
      "Iteration: 606/750.\n",
      "Iteration: 607/750.\n",
      "Iteration: 608/750.\n",
      "Iteration: 609/750.\n",
      "Iteration: 610/750.\n",
      "Iteration: 611/750.\n",
      "Iteration: 612/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1552491626113706, 2.1625910894636866, 2.2808288405473975]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 613/750.\n",
      "Iteration: 614/750.\n",
      "Iteration: 615/750.\n",
      "Iteration: 616/750.\n",
      "Iteration: 617/750.\n",
      "Iteration: 618/750.\n",
      "Iteration: 619/750.\n",
      "Iteration: 620/750.\n",
      "Iteration: 621/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.221663604770886, 2.1665319540090957, 2.1961497029878596]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 622/750.\n",
      "Iteration: 623/750.\n",
      "Iteration: 624/750.\n",
      "Iteration: 625/750.\n",
      "Iteration: 626/750.\n",
      "Iteration: 627/750.\n",
      "Iteration: 628/750.\n",
      "Iteration: 629/750.\n",
      "Iteration: 630/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2191093408749887, 2.396593898677007, 2.171257568036445]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 631/750.\n",
      "Iteration: 632/750.\n",
      "Iteration: 633/750.\n",
      "Iteration: 634/750.\n",
      "Iteration: 635/750.\n",
      "Iteration: 636/750.\n",
      "Iteration: 637/750.\n",
      "Iteration: 638/750.\n",
      "Iteration: 639/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2136993033344794, 2.1609558943243043, 2.252445795017344]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 640/750.\n",
      "Iteration: 641/750.\n",
      "Iteration: 642/750.\n",
      "Iteration: 643/750.\n",
      "Iteration: 644/750.\n",
      "Iteration: 645/750.\n",
      "Iteration: 646/750.\n",
      "Iteration: 647/750.\n",
      "Iteration: 648/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.270517546564173, 2.077304121289031, 2.317812946959583]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 649/750.\n",
      "Iteration: 650/750.\n",
      "Iteration: 651/750.\n",
      "Iteration: 652/750.\n",
      "Iteration: 653/750.\n",
      "Iteration: 654/750.\n",
      "Iteration: 655/750.\n",
      "Iteration: 656/750.\n",
      "Iteration: 657/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.156857904148581, 2.1572597766780865, 2.3406381410100545]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 658/750.\n",
      "Iteration: 659/750.\n",
      "Iteration: 660/750.\n",
      "Iteration: 661/750.\n",
      "Iteration: 662/750.\n",
      "Iteration: 663/750.\n",
      "Iteration: 664/750.\n",
      "Iteration: 665/750.\n",
      "Iteration: 666/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3015316106955814, 2.286232444163045, 2.162189343749455]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 667/750.\n",
      "Iteration: 668/750.\n",
      "Iteration: 669/750.\n",
      "Iteration: 670/750.\n",
      "Iteration: 671/750.\n",
      "Iteration: 672/750.\n",
      "Iteration: 673/750.\n",
      "Iteration: 674/750.\n",
      "Iteration: 675/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2639214328186092, 2.2495302165354953, 2.3265115638659126]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 676/750.\n",
      "Iteration: 677/750.\n",
      "Iteration: 678/750.\n",
      "Iteration: 679/750.\n",
      "Iteration: 680/750.\n",
      "Iteration: 681/750.\n",
      "Iteration: 682/750.\n",
      "Iteration: 683/750.\n",
      "Iteration: 684/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.05322340943015, 2.2531749255380404, 2.166140649838978]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 685/750.\n",
      "Iteration: 686/750.\n",
      "Iteration: 687/750.\n",
      "Iteration: 688/750.\n",
      "Iteration: 689/750.\n",
      "Iteration: 690/750.\n",
      "Iteration: 691/750.\n",
      "Iteration: 692/750.\n",
      "Iteration: 693/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1525457102214616, 2.2363613751447224, 2.1949898306646323]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 694/750.\n",
      "Iteration: 695/750.\n",
      "Iteration: 696/750.\n",
      "Iteration: 697/750.\n",
      "Iteration: 698/750.\n",
      "Iteration: 699/750.\n",
      "Iteration: 700/750.\n",
      "Iteration: 701/750.\n",
      "Iteration: 702/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2611339353388433, 2.3471530584952562, 2.148841912322166]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 703/750.\n",
      "Iteration: 704/750.\n",
      "Iteration: 705/750.\n",
      "Iteration: 706/750.\n",
      "Iteration: 707/750.\n",
      "Iteration: 708/750.\n",
      "Iteration: 709/750.\n",
      "Iteration: 710/750.\n",
      "Iteration: 711/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.279490786046512, 2.3235368702024597, 2.1051195390993214]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 712/750.\n",
      "Iteration: 713/750.\n",
      "Iteration: 714/750.\n",
      "Iteration: 715/750.\n",
      "Iteration: 716/750.\n",
      "Iteration: 717/750.\n",
      "Iteration: 718/750.\n",
      "Iteration: 719/750.\n",
      "Iteration: 720/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2942693326887547, 2.2811890204616265, 2.1792211232871224]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 721/750.\n",
      "Iteration: 722/750.\n",
      "Iteration: 723/750.\n",
      "Iteration: 724/750.\n",
      "Iteration: 725/750.\n",
      "Iteration: 726/750.\n",
      "Iteration: 727/750.\n",
      "Iteration: 728/750.\n",
      "Iteration: 729/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.004881074775065, 2.165659208624099, 2.3316752411036696]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 730/750.\n",
      "Iteration: 731/750.\n",
      "Iteration: 732/750.\n",
      "Iteration: 733/750.\n",
      "Iteration: 734/750.\n",
      "Iteration: 735/750.\n",
      "Iteration: 736/750.\n",
      "Iteration: 737/750.\n",
      "Iteration: 738/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.254104787989307, 2.237855966679909, 2.157645520877249]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 739/750.\n",
      "Iteration: 740/750.\n",
      "Iteration: 741/750.\n",
      "Iteration: 742/750.\n",
      "Iteration: 743/750.\n",
      "Iteration: 744/750.\n",
      "Iteration: 745/750.\n",
      "Iteration: 746/750.\n",
      "Iteration: 747/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3353460865512017, 2.3626829171958854, 2.1615144716245003]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 748/750.\n",
      "Iteration: 749/750.\n",
      "Iteration: 750/750.\n",
      "\n",
      "\n",
      "----------------FINAL OUTPUT----------------\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "Loss history: [2.309333752799659, 2.3100475210286935, 2.302734681556495, 2.303403251768486, 2.3008919111426778, 2.3030688070130423, 2.3083244218204695, 2.305725695413475, 2.297431716772699, 2.2971185583164577, 2.295047037977388, 2.3075030230866163, 2.2950493262693032, 2.302557765275875, 2.302703579304868, 2.3018450824131884, 2.3036695764710173, 2.3017066047698584, 2.3026332858630787, 2.3012372917903128, 2.3057936665043886, 2.300111243151785, 2.304657471487298, 2.294923033034066, 2.296826694035708, 2.2988054741544746, 2.298301305372676, 2.308902546197689, 2.293720393659111, 2.3003195881098004, 2.289276288094972, 2.3027816559633787, 2.2924948880619533, 2.2966168260260056, 2.299502510069381, 2.297408434557621, 2.2952866492919974, 2.2881488785226263, 2.303345064379366, 2.2877893250629198, 2.3013919178983135, 2.2975664705523684, 2.298535173218746, 2.2969774199484214, 2.287743382443448, 2.280444956983327, 2.2919499877728744, 2.298898649446118, 2.3153907594305907, 2.3021109911026945, 2.3012461177238124, 2.2839536623842185, 2.2938137555028617, 2.302438358369893, 2.2965300061329255, 2.294813767132136, 2.280370548936137, 2.2904263157753526, 2.286187019459692, 2.2942040775522634, 2.3025548231332373, 2.2845544381527216, 2.281798459737148, 2.297068291329227, 2.2617424650288287, 2.2809299499204694, 2.2538198629755417, 2.271909689382298, 2.2776021913092346, 2.2835935731873747, 2.2789792517783667, 2.2761709594171795, 2.3143959334197306, 2.3177621604066103, 2.2884189660660303, 2.311392910466611, 2.275393185958308, 2.286765087652806, 2.274863026115056, 2.294948740664123, 2.2861885214757716, 2.2871390892288956, 2.2743610652958326, 2.263448886273185, 2.289666590899042, 2.2975086874459687, 2.2790189825112743, 2.2580556523144146, 2.2770504059269747, 2.2811090608307416, 2.2757719949998716, 2.2933836279835105, 2.290496385895961, 2.2614085040767975, 2.28383061215381, 2.277935653130013, 2.2767012191425837, 2.261673609481268, 2.272762194269741, 2.2714647010266664, 2.290487473501098, 2.2658902911532883, 2.2773112089049175, 2.2770148055491357, 2.2772368045511544, 2.2664487825242845, 2.286639542318909, 2.247916495356978, 2.305542273864441, 2.2512646507169696, 2.2485440855344465, 2.272688105860993, 2.260213811437478, 2.2630233761375496, 2.2729363999356873, 2.273953395587147, 2.2786906412783487, 2.3192567603996523, 2.281512004316321, 2.262797660909111, 2.2803547558436756, 2.3100277732334074, 2.2617148656766113, 2.2537882165991663, 2.278439164214905, 2.2486419643059565, 2.289976849054879, 2.2591118473935636, 2.246622518184403, 2.2485539756996356, 2.2714022353054437, 2.270946773985217, 2.32051304490723, 2.3152721759448656, 2.2505928687197856, 2.271279483717185, 2.2719340757255693, 2.2735338698687357, 2.2513331669232333, 2.258728522823861, 2.31674771521857, 2.2918283532741106, 2.230395258419411, 2.2907653948807876, 2.3037061892965354, 2.28888440081966, 2.277688608511672, 2.288715704807792, 2.2466543833451955, 2.2653392763452334, 2.250193863582836, 2.2973584825330042, 2.268586391634377, 2.222452286381358, 2.3213260910671365, 2.2621099588300413, 2.2481418825953106, 2.261055779220247, 2.269432519652733, 2.2824466859916486, 2.244650987574829, 2.2543351594952203, 2.292507476194037, 2.2723139494422697, 2.2869218409553285, 2.279710771930238, 2.266694714661016, 2.2587020252489243, 2.232717186500342, 2.2397897020179474, 2.2738842139177944, 2.3130731927395285, 2.2755739239371615, 2.277945426675915, 2.243454052034584, 2.2645562990858603, 2.2133727421426292, 2.2679793280689706, 2.28689228759865, 2.276174764319612, 2.2236739566899186, 2.298363742985818, 2.2729309430404605, 2.2563693135874368, 2.240465637311098, 2.2955739515634828, 2.2809092996558507, 2.2789688282647362, 2.28805322376837, 2.3166855884055906, 2.2527713544474053, 2.2731325887768667, 2.2651127106971516, 2.2173477328947846, 2.2098539807486257, 2.2551803876097742, 2.268825191182065, 2.2279083463692317, 2.301795306412694, 2.276189958551281, 2.3712431846473616, 2.2868328038979064, 2.225494902506178, 2.2219202034810803, 2.308294898786341, 2.2447185877161426, 2.2561030174177947, 2.2817544299604884, 2.3094376965185885, 2.3225717067902347, 2.2474790805137927, 2.226260243316582, 2.2742641512258435, 2.274031923422989, 2.292507221173354, 2.2214233061154625, 2.3038294789458624, 2.2006161740230277, 2.3114010439995507, 2.2498249385676092, 2.2499307261201498, 2.26822350700755, 2.3173336547347247, 2.267883190300659, 2.216650036147166, 2.242851204365984, 2.308774772880274, 2.208584330916785, 2.251177796565325, 2.2929429254504283, 2.2742531109980155, 2.3015755984933626, 2.238681765961866, 2.2016660365529783, 2.2414976699522264, 2.2339527682828093, 2.2419668631047154, 2.22561029152744, 2.2696078286368095, 2.289551143034933, 2.3058068408717873, 2.2569811975613874, 2.269927435171348, 2.26324520377211, 2.1625316569152084, 2.2589574526575413, 2.2686326588972605, 2.277235929152427, 2.316094084563435, 2.2473843431840854, 2.1699650093380507, 2.330035628675186, 2.236761831921097, 2.2167136729817942, 2.1725469404099202, 2.3115513746023284, 2.2496455469082437, 2.2959816020938013, 2.271419112296858, 2.201561180448635, 2.2923443997167285, 2.2735742312378213, 2.25518484706304, 2.2075910110023926, 2.245772882471312, 2.2168554586432294, 2.1799703079039072, 2.3423970038143236, 2.2769262981458995, 2.3062019756584777, 2.179473352098399, 2.3465312122995425, 2.2788156964679245, 2.2823152490006624, 2.291660751892977, 2.273853471694125, 2.2407741533860976, 2.172899416933032, 2.1984333670970226, 2.178768841035081, 2.260190263650482, 2.194242320129877, 2.301754317479586, 2.1379310461165866, 2.1671659270788104, 2.2981574744501674, 2.2600462418147753, 2.2166681453444483, 2.3116682133374518, 2.27308235347993, 2.315415519813724, 2.2454148717035594, 2.2420022123738086, 2.2460575891304244, 2.2155338896642607, 2.197460097265643, 2.318388907515723, 2.3021859078807085, 2.286161200886601, 2.339669902771893, 2.197040245797655, 2.282705282487249, 2.1695848944874228, 2.183530894514179, 2.1687370910024586, 2.280636857520292, 2.176898355230328, 2.1658383758779904, 2.2287497642505523, 2.184245077130056, 2.254430533214847, 2.296480705564703, 2.3083214739652798, 2.2727792010817764, 2.2652212535838214, 2.3004397483350063, 2.2647615090690967, 2.1270535971968676, 2.234923844557901, 2.3556544580008727, 2.320327028296227, 2.2743787708138794, 2.205430793999642, 2.216025726528513, 2.2339827155221434, 2.1774274007415815, 2.2233050871556332, 2.2044080679554936, 2.2527007936336836, 2.2318030781141664, 2.2523932586141244, 2.2818711031090086, 2.2729374546977894, 2.2574072904957827, 2.3021979157163655, 2.2759473621417485, 2.204111729271655, 2.189252558882519, 2.237886234675441, 2.3870551622869107, 2.227611855833517, 2.2965227388282172, 2.174574405759914, 2.2223644710891475, 2.39034188002874, 2.1858808523300532, 2.296386752140095, 2.2378239480935656, 2.1972767659635544, 2.1985326184695064, 2.2349970739214924, 2.1900170750618506, 2.1709146497469116, 2.1477515443108333, 2.179970896350563, 2.24916708009886, 2.2343122848263763, 2.1742901250718467, 2.0799756410980557, 2.2220841861419194, 2.247956174372676, 2.1803141868710143, 2.1836113036816145, 2.1633272251408706, 2.225359362065844, 2.2165446854092736, 2.1819677069929813, 2.2273059778822533, 2.1947998105505557, 2.2708157561509386, 2.157979293178159, 2.174449023672463, 2.139562600685668, 2.235704364427927, 2.156121229015864, 2.24355648076423, 2.176654193386759, 2.343633754517829, 2.292389334541857, 2.276347535490325, 2.211671708181607, 2.2094181486726217, 2.185462330048158, 2.205960619547817, 2.258344127286521, 2.287439493420776, 2.2160262565649242, 2.203038506014433, 2.171591728644529, 2.251956833505868, 2.2446382834380154, 2.3595602557073363, 2.2099139511800328, 2.2432384498691653, 2.247009227711138, 2.3254272749122347, 2.301744621986177, 2.2252482918131613, 2.2291867398624254, 2.113158742224389, 2.189558115279206, 2.2351086865998013, 2.241971823851108, 2.1875700843869, 2.158207424004244, 2.236749938926907, 2.226051608823242, 2.2771237789620407, 2.27284293806041, 2.2980481552943437, 2.176719451395235, 2.2484347859408387, 2.3060932894119572, 2.2624999998559, 2.1618495127116626, 2.3237064027617147, 2.240239675948818, 2.2147104791774455, 2.2982272673545237, 2.3358278030850026, 2.1886142866691873, 2.128478050810751, 2.311385551409236, 2.2407913353908095, 2.2632245758216527, 2.2672079876550835, 2.252928202675109, 2.220774177698773, 2.2748848137560786, 2.194539654885159, 2.2900531022302424, 2.2121713336740445, 2.0913593308298855, 2.2703914639860456, 2.1523936948973397, 2.2416866824376345, 2.21617490475356, 2.246419970940201, 2.134445198940538, 2.2699183887282217, 2.1507394401438193, 2.261806802927795, 2.2308286221768205, 2.281423296231043, 2.31822669922193, 2.144160823199504, 2.1513400802171367, 2.1917973532475976, 2.2950102348914974, 2.100521391053965, 2.2267112330326637, 2.1420273375967542, 2.239265776719974, 2.240059615437333, 2.1599768407343682, 2.2681748784584044, 2.208949986017907, 2.302902253085499, 2.2244442228505825, 2.263264839591967, 2.1429665401663955, 2.1882899043981214, 2.246000540413417, 2.334004475609229, 2.2200255863545557, 2.311667349891113, 2.2451032878593913, 2.114113198046391, 2.1379800774131756, 2.220794836257581, 2.254915990121502, 2.1857252690471274, 2.1547434933653054, 2.3005650631533237, 2.362584979184528, 2.1842563367266337, 2.2175075899941072, 2.3064659369084306, 2.2739830313540934, 2.2070774735280523, 2.263569936190832, 2.257398307509753, 2.2027435879602897, 2.274318562637799, 2.065706641955925, 2.1479803585598147, 2.116617104891958, 2.190731908778447, 2.1471997685634467, 2.072579382669971, 2.3231734245159856, 2.23225608264937, 2.2761224118898045, 2.1852569399565933, 2.208590116865381, 2.2565222860804144, 2.0983197982911714, 2.3377358893142426, 2.182478165500148, 2.1482805444065542, 2.31464717537705, 2.116283809246591, 2.2028335737626565, 2.2326783799474863, 2.265263127103594, 2.2437697487925976, 2.167442523067767, 2.2645192117642092, 2.216527928137511, 2.187732284931041, 2.1968076409861594, 2.274975264968471, 1.9855868075389775, 2.2597717458141573, 2.2821159178218937, 2.198963079030357, 2.1891945104577792, 2.190512050266871, 2.244300651790686, 2.1953933392272265, 2.215828703238852, 2.3100137441845474, 2.3159477531534285, 2.266643430772907, 2.3032593686739, 2.394652653544321, 2.254032433202731, 2.1311157260927445, 2.146009311380914, 2.2716558721394833, 2.2301757314982242, 2.2678814937265264, 2.213838422499859, 2.1760596121838343, 2.273506105418976, 2.251507362888061, 2.34512189411497, 2.332063398600958, 2.28230492085212, 1.9736515415982288, 2.1189227626523808, 2.114206787754031, 2.151698611244652, 2.2705814786836984, 2.177990638800233, 2.149726026830303, 2.1829420813707525, 2.2437436638194823, 2.286485280603498, 2.24980885955483, 2.1938046741035966, 2.166350240360741, 2.2258846532623258, 2.34285458114567, 2.1929663692256596, 2.1614771451312933, 2.3313536288551155, 2.166155356616962, 2.307011222268517, 2.205694221413372, 2.2193724314442504, 2.2778575348161674, 2.179588781404833, 2.0986308429095795, 2.1527029769325354, 2.256560466136464, 2.298755900566209, 2.1344338590413137, 2.2271608822114453, 2.284828067580046, 2.2062007773753085, 2.2073675067406073, 2.2657247281164135, 2.3348348069425224, 2.2370508707565415, 2.1151456857666564, 2.2414551117609363, 2.242918937566582, 2.0946004292917, 2.011953260774737, 2.3388613365973963, 2.2385748500313736, 2.250810597110293, 2.293818748703291, 2.020107784339772, 2.28195275115603, 2.255223923374266, 2.14869507768359, 2.2096173142820246, 2.12365362130284, 2.165925894978741, 2.206940417505602, 2.2418594893978243, 2.435203323547574, 2.20321458119135, 2.070248470128996, 2.2187936744119288, 2.161224616410725, 2.10497447919098, 2.3018433928622675, 2.2084709801104907, 2.303666300630048, 2.2445135191978163, 2.14649496794392, 2.0165073561594746, 2.2256396388575146, 2.169012236912601, 2.2624673734805483, 2.2824335920666723, 2.1552491626113706, 2.1625910894636866, 2.2808288405473975, 2.2439545751133325, 2.2382588261264305, 2.0898953426194247, 2.281819890727656, 2.1287797113418496, 2.052971964456318, 2.221663604770886, 2.1665319540090957, 2.1961497029878596, 2.243790478326919, 2.293736071871257, 2.0943813936038187, 2.1252794685292478, 2.196465547035758, 2.134140944633871, 2.2191093408749887, 2.396593898677007, 2.171257568036445, 2.155883916476155, 2.1104135783744593, 2.0524650095148735, 2.2488262598015436, 2.0561069788442414, 2.1472249905974543, 2.2136993033344794, 2.1609558943243043, 2.252445795017344, 2.2339208167457225, 2.2423719595775986, 2.1862903605124084, 2.2134832077203015, 2.174377667422002, 2.1115971157864943, 2.270517546564173, 2.077304121289031, 2.317812946959583, 2.268672642916388, 2.3044974800034406, 2.239222797193361, 2.0344313210390585, 2.2635853161527493, 2.328787323928576, 2.156857904148581, 2.1572597766780865, 2.3406381410100545, 2.2613898090021998, 2.220838742755532, 2.127641741219276, 2.2885573407925426, 2.3954004390646886, 2.1131574528161963, 2.3015316106955814, 2.286232444163045, 2.162189343749455, 2.2110244404878454, 2.282304150695663, 2.2319069197630688, 2.0955823281878074, 2.2538220790481094, 2.309866398808077, 2.2639214328186092, 2.2495302165354953, 2.3265115638659126, 2.2928396386392604, 2.1773725694758985, 2.2069998234792862, 2.230035014498944, 2.2812789349642024, 2.2869453871015626, 2.05322340943015, 2.2531749255380404, 2.166140649838978, 2.160660281549361, 2.129177684204914, 2.1192023458260607, 2.2709323561132626, 2.1739322421048892, 2.3058492710722223, 2.1525457102214616, 2.2363613751447224, 2.1949898306646323, 2.2208228374884667, 2.1805919721518943, 2.2467683982408158, 2.01945280802785, 2.324196217749735, 2.219503405308274, 2.2611339353388433, 2.3471530584952562, 2.148841912322166, 2.2105145700352185, 2.10299896001295, 2.2452306411883876, 2.2659922834657737, 2.0824698025658726, 2.296669271511598, 2.279490786046512, 2.3235368702024597, 2.1051195390993214, 2.1919482176571354, 2.3903194487425594, 2.274765857341968, 2.319324972765244, 2.1576878470952674, 2.2717888821075, 2.2942693326887547, 2.2811890204616265, 2.1792211232871224, 2.345779364711508, 2.0234519079395117, 2.1855523912559702, 2.241816207815938, 2.352432464466387, 2.2271404793424554, 2.004881074775065, 2.165659208624099, 2.3316752411036696, 2.1963508768128817, 2.0938752581510442, 2.37773310826539, 2.224772979726787, 2.2509349442974904, 2.10176376249588, 2.254104787989307, 2.237855966679909, 2.157645520877249, 2.090897439914133, 2.0999701055851396, 2.1815854494495666, 2.0745644609802456, 2.21998773174819, 2.0882375961335344, 2.3353460865512017, 2.3626829171958854, 2.1615144716245003, 2.0949775769564627, 2.1786421785573125, 2.2810184967981635]\n",
      "Accuracy history: [0.16, 0.16, 0.16, 0.16, 0.16, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22]\n"
     ]
    }
   ],
   "source": [
    "# SGD-M\n",
    "sgd_m = ConvNet4(input_shape=input_shape, wt_scale=wt_scale, verbose=False)\n",
    "sgd_m.compile('sgd_momentum')\n",
    "sgd_m.fit(x_dev, y_dev, x_val, y_val, mini_batch_sz=mini_batch_sz, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train...\n",
      "750 iterations. 5 iter/epoch.\n",
      "Iteration: 1/750.\n",
      "Time taken for iteration 0: 1.6547000408172607\n",
      "Estimated time to complete: 1241.0250306129456\n",
      "Iteration: 2/750.\n",
      "Iteration: 3/750.\n",
      "Iteration: 4/750.\n",
      "Iteration: 5/750.\n",
      "Iteration: 6/750.\n",
      "Iteration: 7/750.\n",
      "Iteration: 8/750.\n",
      "Iteration: 9/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.2667797754626706, 2.275256568294465, 2.2556253335188514]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 10/750.\n",
      "Iteration: 11/750.\n",
      "Iteration: 12/750.\n",
      "Iteration: 13/750.\n",
      "Iteration: 14/750.\n",
      "Iteration: 15/750.\n",
      "Iteration: 16/750.\n",
      "Iteration: 17/750.\n",
      "Iteration: 18/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.2727310367417854, 2.204103899788421, 2.258343837284645]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 19/750.\n",
      "Iteration: 20/750.\n",
      "Iteration: 21/750.\n",
      "Iteration: 22/750.\n",
      "Iteration: 23/750.\n",
      "Iteration: 24/750.\n",
      "Iteration: 25/750.\n",
      "Iteration: 26/750.\n",
      "Iteration: 27/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.1944675031511447, 2.2390716805564996, 2.281966984895512]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 28/750.\n",
      "Iteration: 29/750.\n",
      "Iteration: 30/750.\n",
      "Iteration: 31/750.\n",
      "Iteration: 32/750.\n",
      "Iteration: 33/750.\n",
      "Iteration: 34/750.\n",
      "Iteration: 35/750.\n",
      "Iteration: 36/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.2104394749579153, 2.186158200387568, 2.314671231617274]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 37/750.\n",
      "Iteration: 38/750.\n",
      "Iteration: 39/750.\n",
      "Iteration: 40/750.\n",
      "Iteration: 41/750.\n",
      "Iteration: 42/750.\n",
      "Iteration: 43/750.\n",
      "Iteration: 44/750.\n",
      "Iteration: 45/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.262181205808585, 2.1475236738810057, 2.1927888638495125]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 46/750.\n",
      "Iteration: 47/750.\n",
      "Iteration: 48/750.\n",
      "Iteration: 49/750.\n",
      "Iteration: 50/750.\n",
      "Iteration: 51/750.\n",
      "Iteration: 52/750.\n",
      "Iteration: 53/750.\n",
      "Iteration: 54/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.1454005350969774, 2.3007085399330136, 2.254623388762617]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 55/750.\n",
      "Iteration: 56/750.\n",
      "Iteration: 57/750.\n",
      "Iteration: 58/750.\n",
      "Iteration: 59/750.\n",
      "Iteration: 60/750.\n",
      "Iteration: 61/750.\n",
      "Iteration: 62/750.\n",
      "Iteration: 63/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.2224804656011212, 2.132319431106546, 2.160046661484646]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 64/750.\n",
      "Iteration: 65/750.\n",
      "Iteration: 66/750.\n",
      "Iteration: 67/750.\n",
      "Iteration: 68/750.\n",
      "Iteration: 69/750.\n",
      "Iteration: 70/750.\n",
      "Iteration: 71/750.\n",
      "Iteration: 72/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.062876644831836, 2.1102064822282625, 2.342895680648545]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 73/750.\n",
      "Iteration: 74/750.\n",
      "Iteration: 75/750.\n",
      "Iteration: 76/750.\n",
      "Iteration: 77/750.\n",
      "Iteration: 78/750.\n",
      "Iteration: 79/750.\n",
      "Iteration: 80/750.\n",
      "Iteration: 81/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.245438425118237, 2.1422264296376468, 2.328747695698904]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 82/750.\n",
      "Iteration: 83/750.\n",
      "Iteration: 84/750.\n",
      "Iteration: 85/750.\n",
      "Iteration: 86/750.\n",
      "Iteration: 87/750.\n",
      "Iteration: 88/750.\n",
      "Iteration: 89/750.\n",
      "Iteration: 90/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.2147544396521504, 2.182436801416794, 2.3272995062125292]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 91/750.\n",
      "Iteration: 92/750.\n",
      "Iteration: 93/750.\n",
      "Iteration: 94/750.\n",
      "Iteration: 95/750.\n",
      "Iteration: 96/750.\n",
      "Iteration: 97/750.\n",
      "Iteration: 98/750.\n",
      "Iteration: 99/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.141941328348959, 2.112567311006279, 2.2184563897284577]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 100/750.\n",
      "Iteration: 101/750.\n",
      "Iteration: 102/750.\n",
      "Iteration: 103/750.\n",
      "Iteration: 104/750.\n",
      "Iteration: 105/750.\n",
      "Iteration: 106/750.\n",
      "Iteration: 107/750.\n",
      "Iteration: 108/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.280583389083929, 2.0969955815327004, 1.8145079639294601]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 109/750.\n",
      "Iteration: 110/750.\n",
      "Iteration: 111/750.\n",
      "Iteration: 112/750.\n",
      "Iteration: 113/750.\n",
      "Iteration: 114/750.\n",
      "Iteration: 115/750.\n",
      "Iteration: 116/750.\n",
      "Iteration: 117/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.9084026833757428, 2.1167140453380213, 2.3740182895421293]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 118/750.\n",
      "Iteration: 119/750.\n",
      "Iteration: 120/750.\n",
      "Iteration: 121/750.\n",
      "Iteration: 122/750.\n",
      "Iteration: 123/750.\n",
      "Iteration: 124/750.\n",
      "Iteration: 125/750.\n",
      "Iteration: 126/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.346263581167927, 1.8869907552364524, 2.461087123313006]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.24, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 127/750.\n",
      "Iteration: 128/750.\n",
      "Iteration: 129/750.\n",
      "Iteration: 130/750.\n",
      "Iteration: 131/750.\n",
      "Iteration: 132/750.\n",
      "Iteration: 133/750.\n",
      "Iteration: 134/750.\n",
      "Iteration: 135/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.810131401373492, 2.1380569646358207, 2.21698438382967]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.24, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 136/750.\n",
      "Iteration: 137/750.\n",
      "Iteration: 138/750.\n",
      "Iteration: 139/750.\n",
      "Iteration: 140/750.\n",
      "Iteration: 141/750.\n",
      "Iteration: 142/750.\n",
      "Iteration: 143/750.\n",
      "Iteration: 144/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.8719002668156919, 2.0312890920937923, 2.0585284689321237]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.28, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 145/750.\n",
      "Iteration: 146/750.\n",
      "Iteration: 147/750.\n",
      "Iteration: 148/750.\n",
      "Iteration: 149/750.\n",
      "Iteration: 150/750.\n",
      "Iteration: 151/750.\n",
      "Iteration: 152/750.\n",
      "Iteration: 153/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.7564821912496176, 1.9865067476393694, 1.943536227476239]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.32, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 154/750.\n",
      "Iteration: 155/750.\n",
      "Iteration: 156/750.\n",
      "Iteration: 157/750.\n",
      "Iteration: 158/750.\n",
      "Iteration: 159/750.\n",
      "Iteration: 160/750.\n",
      "Iteration: 161/750.\n",
      "Iteration: 162/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.8245981558917728, 1.9881851896046179, 1.9763830868347005]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.36, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 163/750.\n",
      "Iteration: 164/750.\n",
      "Iteration: 165/750.\n",
      "Iteration: 166/750.\n",
      "Iteration: 167/750.\n",
      "Iteration: 168/750.\n",
      "Iteration: 169/750.\n",
      "Iteration: 170/750.\n",
      "Iteration: 171/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.080556145782496, 1.624516182196767, 1.592757630272275]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.32, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 172/750.\n",
      "Iteration: 173/750.\n",
      "Iteration: 174/750.\n",
      "Iteration: 175/750.\n",
      "Iteration: 176/750.\n",
      "Iteration: 177/750.\n",
      "Iteration: 178/750.\n",
      "Iteration: 179/750.\n",
      "Iteration: 180/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.69090942977038, 1.9665592430540089, 1.8076948352084192]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.38, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 181/750.\n",
      "Iteration: 182/750.\n",
      "Iteration: 183/750.\n",
      "Iteration: 184/750.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 185/750.\n",
      "Iteration: 186/750.\n",
      "Iteration: 187/750.\n",
      "Iteration: 188/750.\n",
      "Iteration: 189/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.7687937906780133, 1.5499046858392924, 1.7933401927001027]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.44, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 190/750.\n",
      "Iteration: 191/750.\n",
      "Iteration: 192/750.\n",
      "Iteration: 193/750.\n",
      "Iteration: 194/750.\n",
      "Iteration: 195/750.\n",
      "Iteration: 196/750.\n",
      "Iteration: 197/750.\n",
      "Iteration: 198/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.8762247744837355, 1.8434306913313045, 1.646268527799408]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.4, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 199/750.\n",
      "Iteration: 200/750.\n",
      "Iteration: 201/750.\n",
      "Iteration: 202/750.\n",
      "Iteration: 203/750.\n",
      "Iteration: 204/750.\n",
      "Iteration: 205/750.\n",
      "Iteration: 206/750.\n",
      "Iteration: 207/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.5854351235733253, 1.6089370888972951, 2.058576125229221]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.36, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 208/750.\n",
      "Iteration: 209/750.\n",
      "Iteration: 210/750.\n",
      "Iteration: 211/750.\n",
      "Iteration: 212/750.\n",
      "Iteration: 213/750.\n",
      "Iteration: 214/750.\n",
      "Iteration: 215/750.\n",
      "Iteration: 216/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.784510600880739, 1.7362789713859994, 1.446772620741509]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.46, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 217/750.\n",
      "Iteration: 218/750.\n",
      "Iteration: 219/750.\n",
      "Iteration: 220/750.\n",
      "Iteration: 221/750.\n",
      "Iteration: 222/750.\n",
      "Iteration: 223/750.\n",
      "Iteration: 224/750.\n",
      "Iteration: 225/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.367081370829135, 1.7940175672363345, 1.0815554925170499]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.32, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 226/750.\n",
      "Iteration: 227/750.\n",
      "Iteration: 228/750.\n",
      "Iteration: 229/750.\n",
      "Iteration: 230/750.\n",
      "Iteration: 231/750.\n",
      "Iteration: 232/750.\n",
      "Iteration: 233/750.\n",
      "Iteration: 234/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.5370650534071886, 1.4994189511311506, 1.2382007588936237]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.34, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 235/750.\n",
      "Iteration: 236/750.\n",
      "Iteration: 237/750.\n",
      "Iteration: 238/750.\n",
      "Iteration: 239/750.\n",
      "Iteration: 240/750.\n",
      "Iteration: 241/750.\n",
      "Iteration: 242/750.\n",
      "Iteration: 243/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.2365533918378877, 1.4200319681257227, 1.8429169176312614]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.52, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 244/750.\n",
      "Iteration: 245/750.\n",
      "Iteration: 246/750.\n",
      "Iteration: 247/750.\n",
      "Iteration: 248/750.\n",
      "Iteration: 249/750.\n",
      "Iteration: 250/750.\n",
      "Iteration: 251/750.\n",
      "Iteration: 252/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.565128906395955, 0.9596664751092816, 1.229862366192645]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.58, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 253/750.\n",
      "Iteration: 254/750.\n",
      "Iteration: 255/750.\n",
      "Iteration: 256/750.\n",
      "Iteration: 257/750.\n",
      "Iteration: 258/750.\n",
      "Iteration: 259/750.\n",
      "Iteration: 260/750.\n",
      "Iteration: 261/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.9498037380592687, 1.0349733756213138, 1.866582896709912]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.62, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 262/750.\n",
      "Iteration: 263/750.\n",
      "Iteration: 264/750.\n",
      "Iteration: 265/750.\n",
      "Iteration: 266/750.\n",
      "Iteration: 267/750.\n",
      "Iteration: 268/750.\n",
      "Iteration: 269/750.\n",
      "Iteration: 270/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.1245383727738474, 1.0784979611253103, 1.2490428991528333]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.56, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 271/750.\n",
      "Iteration: 272/750.\n",
      "Iteration: 273/750.\n",
      "Iteration: 274/750.\n",
      "Iteration: 275/750.\n",
      "Iteration: 276/750.\n",
      "Iteration: 277/750.\n",
      "Iteration: 278/750.\n",
      "Iteration: 279/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.6562065181910253, 0.6959926880204463, 0.3588537720135193]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.62, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 280/750.\n",
      "Iteration: 281/750.\n",
      "Iteration: 282/750.\n",
      "Iteration: 283/750.\n",
      "Iteration: 284/750.\n",
      "Iteration: 285/750.\n",
      "Iteration: 286/750.\n",
      "Iteration: 287/750.\n",
      "Iteration: 288/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.7398576115325761, 0.9311622601760052, 1.2553016971047823]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.7, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 289/750.\n",
      "Iteration: 290/750.\n",
      "Iteration: 291/750.\n",
      "Iteration: 292/750.\n",
      "Iteration: 293/750.\n",
      "Iteration: 294/750.\n",
      "Iteration: 295/750.\n",
      "Iteration: 296/750.\n",
      "Iteration: 297/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.6652714303068812, 0.8960706763006234, 0.9986283064194643]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.7, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 298/750.\n",
      "Iteration: 299/750.\n",
      "Iteration: 300/750.\n",
      "Iteration: 301/750.\n",
      "Iteration: 302/750.\n",
      "Iteration: 303/750.\n",
      "Iteration: 304/750.\n",
      "Iteration: 305/750.\n",
      "Iteration: 306/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.7121118251675107, 0.7926670437404754, 0.42816989121324867]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.8, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 307/750.\n",
      "Iteration: 308/750.\n",
      "Iteration: 309/750.\n",
      "Iteration: 310/750.\n",
      "Iteration: 311/750.\n",
      "Iteration: 312/750.\n",
      "Iteration: 313/750.\n",
      "Iteration: 314/750.\n",
      "Iteration: 315/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.5999820458657846, 0.5932785082964959, 0.726994351994561]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.7, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 316/750.\n",
      "Iteration: 317/750.\n",
      "Iteration: 318/750.\n",
      "Iteration: 319/750.\n",
      "Iteration: 320/750.\n",
      "Iteration: 321/750.\n",
      "Iteration: 322/750.\n",
      "Iteration: 323/750.\n",
      "Iteration: 324/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.5185312490896163, 0.6175317543712656, 0.8038231033579081]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.84, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 325/750.\n",
      "Iteration: 326/750.\n",
      "Iteration: 327/750.\n",
      "Iteration: 328/750.\n",
      "Iteration: 329/750.\n",
      "Iteration: 330/750.\n",
      "Iteration: 331/750.\n",
      "Iteration: 332/750.\n",
      "Iteration: 333/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.3721801906432122, 0.48718576388629914, 0.7962355987553126]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.76, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 334/750.\n",
      "Iteration: 335/750.\n",
      "Iteration: 336/750.\n",
      "Iteration: 337/750.\n",
      "Iteration: 338/750.\n",
      "Iteration: 339/750.\n",
      "Iteration: 340/750.\n",
      "Iteration: 341/750.\n",
      "Iteration: 342/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.45703582118824787, 0.4455654487389473, 0.6131551995768093]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.86, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 343/750.\n",
      "Iteration: 344/750.\n",
      "Iteration: 345/750.\n",
      "Iteration: 346/750.\n",
      "Iteration: 347/750.\n",
      "Iteration: 348/750.\n",
      "Iteration: 349/750.\n",
      "Iteration: 350/750.\n",
      "Iteration: 351/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.3600096173785579, 0.24084408281141323, 0.44647209122066905]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.92, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 352/750.\n",
      "Iteration: 353/750.\n",
      "Iteration: 354/750.\n",
      "Iteration: 355/750.\n",
      "Iteration: 356/750.\n",
      "Iteration: 357/750.\n",
      "Iteration: 358/750.\n",
      "Iteration: 359/750.\n",
      "Iteration: 360/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.21452174186284392, 0.7726704370448986, 0.45634574429319485]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.88, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 361/750.\n",
      "Iteration: 362/750.\n",
      "Iteration: 363/750.\n",
      "Iteration: 364/750.\n",
      "Iteration: 365/750.\n",
      "Iteration: 366/750.\n",
      "Iteration: 367/750.\n",
      "Iteration: 368/750.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 369/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.3295559392454376, 0.5024100581564189, 0.4312812816763762]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.92, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 370/750.\n",
      "Iteration: 371/750.\n",
      "Iteration: 372/750.\n",
      "Iteration: 373/750.\n",
      "Iteration: 374/750.\n",
      "Iteration: 375/750.\n",
      "Iteration: 376/750.\n",
      "Iteration: 377/750.\n",
      "Iteration: 378/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.17926499542018026, 0.08146063120636833, 0.14776578401052776]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.96, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 379/750.\n",
      "Iteration: 380/750.\n",
      "Iteration: 381/750.\n",
      "Iteration: 382/750.\n",
      "Iteration: 383/750.\n",
      "Iteration: 384/750.\n",
      "Iteration: 385/750.\n",
      "Iteration: 386/750.\n",
      "Iteration: 387/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.12690574396394314, 0.14626360697760457, 0.27095333847671027]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.96, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 388/750.\n",
      "Iteration: 389/750.\n",
      "Iteration: 390/750.\n",
      "Iteration: 391/750.\n",
      "Iteration: 392/750.\n",
      "Iteration: 393/750.\n",
      "Iteration: 394/750.\n",
      "Iteration: 395/750.\n",
      "Iteration: 396/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.05127357695541305, 0.03077601974795683, 0.09978465409029391]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.94, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 397/750.\n",
      "Iteration: 398/750.\n",
      "Iteration: 399/750.\n",
      "Iteration: 400/750.\n",
      "Iteration: 401/750.\n",
      "Iteration: 402/750.\n",
      "Iteration: 403/750.\n",
      "Iteration: 404/750.\n",
      "Iteration: 405/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.7968578466671137, 0.12050352961368434, 0.13951782523844286]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.86, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 406/750.\n",
      "Iteration: 407/750.\n",
      "Iteration: 408/750.\n",
      "Iteration: 409/750.\n",
      "Iteration: 410/750.\n",
      "Iteration: 411/750.\n",
      "Iteration: 412/750.\n",
      "Iteration: 413/750.\n",
      "Iteration: 414/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.0725913969922685, 0.05298564169811315, 0.11045412407002203]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.94, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 415/750.\n",
      "Iteration: 416/750.\n",
      "Iteration: 417/750.\n",
      "Iteration: 418/750.\n",
      "Iteration: 419/750.\n",
      "Iteration: 420/750.\n",
      "Iteration: 421/750.\n",
      "Iteration: 422/750.\n",
      "Iteration: 423/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.4215869937130474, 0.10524540448488609, 0.06936716339445756]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.98, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 424/750.\n",
      "Iteration: 425/750.\n",
      "Iteration: 426/750.\n",
      "Iteration: 427/750.\n",
      "Iteration: 428/750.\n",
      "Iteration: 429/750.\n",
      "Iteration: 430/750.\n",
      "Iteration: 431/750.\n",
      "Iteration: 432/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.12616707832464547, 0.045277642345445346, 0.057425751483152626]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 433/750.\n",
      "Iteration: 434/750.\n",
      "Iteration: 435/750.\n",
      "Iteration: 436/750.\n",
      "Iteration: 437/750.\n",
      "Iteration: 438/750.\n",
      "Iteration: 439/750.\n",
      "Iteration: 440/750.\n",
      "Iteration: 441/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.06250334432634279, 0.06565315499960024, 0.09353956286963783]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 442/750.\n",
      "Iteration: 443/750.\n",
      "Iteration: 444/750.\n",
      "Iteration: 445/750.\n",
      "Iteration: 446/750.\n",
      "Iteration: 447/750.\n",
      "Iteration: 448/750.\n",
      "Iteration: 449/750.\n",
      "Iteration: 450/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.04103252661290321, 0.023090117515521657, 0.02139307379917506]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 451/750.\n",
      "Iteration: 452/750.\n",
      "Iteration: 453/750.\n",
      "Iteration: 454/750.\n",
      "Iteration: 455/750.\n",
      "Iteration: 456/750.\n",
      "Iteration: 457/750.\n",
      "Iteration: 458/750.\n",
      "Iteration: 459/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.061574340908845206, 0.03061612113091562, 0.0361981308188108]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 460/750.\n",
      "Iteration: 461/750.\n",
      "Iteration: 462/750.\n",
      "Iteration: 463/750.\n",
      "Iteration: 464/750.\n",
      "Iteration: 465/750.\n",
      "Iteration: 466/750.\n",
      "Iteration: 467/750.\n",
      "Iteration: 468/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.04146044511708326, 0.04634376549811827, 0.02264088468215882]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 469/750.\n",
      "Iteration: 470/750.\n",
      "Iteration: 471/750.\n",
      "Iteration: 472/750.\n",
      "Iteration: 473/750.\n",
      "Iteration: 474/750.\n",
      "Iteration: 475/750.\n",
      "Iteration: 476/750.\n",
      "Iteration: 477/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.0529215937297416, 0.015122620732716927, 0.016567539291492402]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 478/750.\n",
      "Iteration: 479/750.\n",
      "Iteration: 480/750.\n",
      "Iteration: 481/750.\n",
      "Iteration: 482/750.\n",
      "Iteration: 483/750.\n",
      "Iteration: 484/750.\n",
      "Iteration: 485/750.\n",
      "Iteration: 486/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.024942579891443245, 0.014512102097624636, 0.017145682969261618]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 487/750.\n",
      "Iteration: 488/750.\n",
      "Iteration: 489/750.\n",
      "Iteration: 490/750.\n",
      "Iteration: 491/750.\n",
      "Iteration: 492/750.\n",
      "Iteration: 493/750.\n",
      "Iteration: 494/750.\n",
      "Iteration: 495/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.01651922213586791, 0.0232326344336274, 0.012857308652123273]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 496/750.\n",
      "Iteration: 497/750.\n",
      "Iteration: 498/750.\n",
      "Iteration: 499/750.\n",
      "Iteration: 500/750.\n",
      "Iteration: 501/750.\n",
      "Iteration: 502/750.\n",
      "Iteration: 503/750.\n",
      "Iteration: 504/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.01592147211275496, 0.009630283114641458, 0.019034746972734173]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 505/750.\n",
      "Iteration: 506/750.\n",
      "Iteration: 507/750.\n",
      "Iteration: 508/750.\n",
      "Iteration: 509/750.\n",
      "Iteration: 510/750.\n",
      "Iteration: 511/750.\n",
      "Iteration: 512/750.\n",
      "Iteration: 513/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.01378188759272413, 0.015363995120761626, 0.015868681066423168]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 514/750.\n",
      "Iteration: 515/750.\n",
      "Iteration: 516/750.\n",
      "Iteration: 517/750.\n",
      "Iteration: 518/750.\n",
      "Iteration: 519/750.\n",
      "Iteration: 520/750.\n",
      "Iteration: 521/750.\n",
      "Iteration: 522/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.009422600680863272, 0.012448477097153542, 0.011722024897707557]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 523/750.\n",
      "Iteration: 524/750.\n",
      "Iteration: 525/750.\n",
      "Iteration: 526/750.\n",
      "Iteration: 527/750.\n",
      "Iteration: 528/750.\n",
      "Iteration: 529/750.\n",
      "Iteration: 530/750.\n",
      "Iteration: 531/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.014234979332353945, 0.01259274166389674, 0.010110019908980701]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 532/750.\n",
      "Iteration: 533/750.\n",
      "Iteration: 534/750.\n",
      "Iteration: 535/750.\n",
      "Iteration: 536/750.\n",
      "Iteration: 537/750.\n",
      "Iteration: 538/750.\n",
      "Iteration: 539/750.\n",
      "Iteration: 540/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.011304716332335658, 0.01568272955105841, 0.009243550549294243]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 541/750.\n",
      "Iteration: 542/750.\n",
      "Iteration: 543/750.\n",
      "Iteration: 544/750.\n",
      "Iteration: 545/750.\n",
      "Iteration: 546/750.\n",
      "Iteration: 547/750.\n",
      "Iteration: 548/750.\n",
      "Iteration: 549/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.009461286540340778, 0.015398113683632223, 0.007788799428446233]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 550/750.\n",
      "Iteration: 551/750.\n",
      "Iteration: 552/750.\n",
      "Iteration: 553/750.\n",
      "Iteration: 554/750.\n",
      "Iteration: 555/750.\n",
      "Iteration: 556/750.\n",
      "Iteration: 557/750.\n",
      "Iteration: 558/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.023462035444168056, 0.0072876130475124055, 0.007217936771467708]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 559/750.\n",
      "Iteration: 560/750.\n",
      "Iteration: 561/750.\n",
      "Iteration: 562/750.\n",
      "Iteration: 563/750.\n",
      "Iteration: 564/750.\n",
      "Iteration: 565/750.\n",
      "Iteration: 566/750.\n",
      "Iteration: 567/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.011486809869905357, 0.014080469611488039, 0.006042325705134973]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 568/750.\n",
      "Iteration: 569/750.\n",
      "Iteration: 570/750.\n",
      "Iteration: 571/750.\n",
      "Iteration: 572/750.\n",
      "Iteration: 573/750.\n",
      "Iteration: 574/750.\n",
      "Iteration: 575/750.\n",
      "Iteration: 576/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.00531625625296299, 0.008090467558170616, 0.007376739239527481]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 577/750.\n",
      "Iteration: 578/750.\n",
      "Iteration: 579/750.\n",
      "Iteration: 580/750.\n",
      "Iteration: 581/750.\n",
      "Iteration: 582/750.\n",
      "Iteration: 583/750.\n",
      "Iteration: 584/750.\n",
      "Iteration: 585/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.011827042260566472, 0.004132437648233549, 0.00841132813573615]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 586/750.\n",
      "Iteration: 587/750.\n",
      "Iteration: 588/750.\n",
      "Iteration: 589/750.\n",
      "Iteration: 590/750.\n",
      "Iteration: 591/750.\n",
      "Iteration: 592/750.\n",
      "Iteration: 593/750.\n",
      "Iteration: 594/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.012874743552425738, 0.006268778404724124, 0.008973090645707517]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 595/750.\n",
      "Iteration: 596/750.\n",
      "Iteration: 597/750.\n",
      "Iteration: 598/750.\n",
      "Iteration: 599/750.\n",
      "Iteration: 600/750.\n",
      "Iteration: 601/750.\n",
      "Iteration: 602/750.\n",
      "Iteration: 603/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.01861776886689462, 0.00892701187826333, 0.0037573490446317624]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 604/750.\n",
      "Iteration: 605/750.\n",
      "Iteration: 606/750.\n",
      "Iteration: 607/750.\n",
      "Iteration: 608/750.\n",
      "Iteration: 609/750.\n",
      "Iteration: 610/750.\n",
      "Iteration: 611/750.\n",
      "Iteration: 612/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.01449660654662139, 0.01239944670797838, 0.019480864669444573]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 613/750.\n",
      "Iteration: 614/750.\n",
      "Iteration: 615/750.\n",
      "Iteration: 616/750.\n",
      "Iteration: 617/750.\n",
      "Iteration: 618/750.\n",
      "Iteration: 619/750.\n",
      "Iteration: 620/750.\n",
      "Iteration: 621/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.007348764169813485, 0.005429214354160296, 0.009889597384216563]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 622/750.\n",
      "Iteration: 623/750.\n",
      "Iteration: 624/750.\n",
      "Iteration: 625/750.\n",
      "Iteration: 626/750.\n",
      "Iteration: 627/750.\n",
      "Iteration: 628/750.\n",
      "Iteration: 629/750.\n",
      "Iteration: 630/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.007102762083355663, 0.006198899235130445, 0.006963071672403594]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 631/750.\n",
      "Iteration: 632/750.\n",
      "Iteration: 633/750.\n",
      "Iteration: 634/750.\n",
      "Iteration: 635/750.\n",
      "Iteration: 636/750.\n",
      "Iteration: 637/750.\n",
      "Iteration: 638/750.\n",
      "Iteration: 639/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.010189974498328942, 0.006265418445138659, 0.009006211923601624]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 640/750.\n",
      "Iteration: 641/750.\n",
      "Iteration: 642/750.\n",
      "Iteration: 643/750.\n",
      "Iteration: 644/750.\n",
      "Iteration: 645/750.\n",
      "Iteration: 646/750.\n",
      "Iteration: 647/750.\n",
      "Iteration: 648/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.009768225769362357, 0.004369052790291455, 0.009330586000170749]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 649/750.\n",
      "Iteration: 650/750.\n",
      "Iteration: 651/750.\n",
      "Iteration: 652/750.\n",
      "Iteration: 653/750.\n",
      "Iteration: 654/750.\n",
      "Iteration: 655/750.\n",
      "Iteration: 656/750.\n",
      "Iteration: 657/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.006903764936367092, 0.008378244942623849, 0.006026925378267827]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 658/750.\n",
      "Iteration: 659/750.\n",
      "Iteration: 660/750.\n",
      "Iteration: 661/750.\n",
      "Iteration: 662/750.\n",
      "Iteration: 663/750.\n",
      "Iteration: 664/750.\n",
      "Iteration: 665/750.\n",
      "Iteration: 666/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.005647861855906821, 0.006373796991816481, 0.0036909966583682525]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 667/750.\n",
      "Iteration: 668/750.\n",
      "Iteration: 669/750.\n",
      "Iteration: 670/750.\n",
      "Iteration: 671/750.\n",
      "Iteration: 672/750.\n",
      "Iteration: 673/750.\n",
      "Iteration: 674/750.\n",
      "Iteration: 675/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.0035338558313725097, 0.004852368511403443, 0.006987997809365887]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 676/750.\n",
      "Iteration: 677/750.\n",
      "Iteration: 678/750.\n",
      "Iteration: 679/750.\n",
      "Iteration: 680/750.\n",
      "Iteration: 681/750.\n",
      "Iteration: 682/750.\n",
      "Iteration: 683/750.\n",
      "Iteration: 684/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.004503677118881455, 0.004347344683964638, 0.005801303881045078]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 685/750.\n",
      "Iteration: 686/750.\n",
      "Iteration: 687/750.\n",
      "Iteration: 688/750.\n",
      "Iteration: 689/750.\n",
      "Iteration: 690/750.\n",
      "Iteration: 691/750.\n",
      "Iteration: 692/750.\n",
      "Iteration: 693/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.008569560094122864, 0.004063908132902382, 0.00772795803548363]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 694/750.\n",
      "Iteration: 695/750.\n",
      "Iteration: 696/750.\n",
      "Iteration: 697/750.\n",
      "Iteration: 698/750.\n",
      "Iteration: 699/750.\n",
      "Iteration: 700/750.\n",
      "Iteration: 701/750.\n",
      "Iteration: 702/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.004720701108118153, 0.004657498544578502, 0.006138915366123302]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 703/750.\n",
      "Iteration: 704/750.\n",
      "Iteration: 705/750.\n",
      "Iteration: 706/750.\n",
      "Iteration: 707/750.\n",
      "Iteration: 708/750.\n",
      "Iteration: 709/750.\n",
      "Iteration: 710/750.\n",
      "Iteration: 711/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.003572075049107347, 0.0035199080476802816, 0.004912580903612035]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 712/750.\n",
      "Iteration: 713/750.\n",
      "Iteration: 714/750.\n",
      "Iteration: 715/750.\n",
      "Iteration: 716/750.\n",
      "Iteration: 717/750.\n",
      "Iteration: 718/750.\n",
      "Iteration: 719/750.\n",
      "Iteration: 720/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.010649138571592715, 0.0077419436390885115, 0.007040307072026977]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 721/750.\n",
      "Iteration: 722/750.\n",
      "Iteration: 723/750.\n",
      "Iteration: 724/750.\n",
      "Iteration: 725/750.\n",
      "Iteration: 726/750.\n",
      "Iteration: 727/750.\n",
      "Iteration: 728/750.\n",
      "Iteration: 729/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.005056200145948542, 0.0028941767692891984, 0.0044539151251743285]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 730/750.\n",
      "Iteration: 731/750.\n",
      "Iteration: 732/750.\n",
      "Iteration: 733/750.\n",
      "Iteration: 734/750.\n",
      "Iteration: 735/750.\n",
      "Iteration: 736/750.\n",
      "Iteration: 737/750.\n",
      "Iteration: 738/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.005925159649311415, 0.005571045170012228, 0.002954864165883129]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 739/750.\n",
      "Iteration: 740/750.\n",
      "Iteration: 741/750.\n",
      "Iteration: 742/750.\n",
      "Iteration: 743/750.\n",
      "Iteration: 744/750.\n",
      "Iteration: 745/750.\n",
      "Iteration: 746/750.\n",
      "Iteration: 747/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.00718462533908763, 0.004472624064811412, 0.005578008915382949]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 748/750.\n",
      "Iteration: 749/750.\n",
      "Iteration: 750/750.\n",
      "\n",
      "\n",
      "----------------FINAL OUTPUT----------------\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "Loss history: [2.304373733952622, 2.3001941262256884, 2.303508335898844, 2.2942834211171674, 2.302257539684362, 2.2846940314338897, 2.2667797754626706, 2.275256568294465, 2.2556253335188514, 2.255724245474301, 2.343866761998466, 2.273996881707418, 2.277199410984449, 2.2972349987580034, 2.237717600372464, 2.2727310367417854, 2.204103899788421, 2.258343837284645, 2.2590217321572386, 2.2410365036727793, 2.21648210922612, 2.257682756751057, 2.1638925675409975, 2.3051962858235155, 2.1944675031511447, 2.2390716805564996, 2.281966984895512, 2.2260888371900878, 2.3167035067377824, 2.1894941797773226, 2.2825784282113477, 2.3185600416673253, 2.2054036260114587, 2.2104394749579153, 2.186158200387568, 2.314671231617274, 2.1841281528840213, 2.2162186641313677, 2.2318659828023284, 2.215329853793867, 2.2706347464264036, 2.238357849701217, 2.262181205808585, 2.1475236738810057, 2.1927888638495125, 2.1374337344344143, 2.334798103876355, 2.314091974017603, 2.100504717556598, 2.310267870364869, 2.08967420692944, 2.1454005350969774, 2.3007085399330136, 2.254623388762617, 2.2115336061775284, 2.237354164274268, 2.2526442310975536, 2.1206935324215714, 2.091221586124789, 2.2756473590375514, 2.2224804656011212, 2.132319431106546, 2.160046661484646, 2.1065756443487866, 2.294840035022363, 2.304341258435041, 2.260400324063611, 2.1889218481103163, 2.1682860883507287, 2.062876644831836, 2.1102064822282625, 2.342895680648545, 2.272264401800523, 2.2724597301609637, 2.2953864638718358, 2.2955029132038853, 2.027709232618005, 2.335128325909235, 2.245438425118237, 2.1422264296376468, 2.328747695698904, 2.1241701530745263, 2.211319922999345, 2.404273097629641, 2.1776793791750038, 2.311223172066713, 2.253595040493041, 2.2147544396521504, 2.182436801416794, 2.3272995062125292, 2.14755198939707, 2.224311403857144, 2.193804558545494, 2.27463863108589, 1.983868980764864, 2.0552797263934353, 2.141941328348959, 2.112567311006279, 2.2184563897284577, 2.2253812604806753, 2.155493279442576, 2.04488320323522, 2.208044770196635, 2.2270848300181565, 2.0493456358370006, 2.280583389083929, 2.0969955815327004, 1.8145079639294601, 1.733432267376918, 2.361115924278763, 2.011099680766377, 2.1170719359633607, 2.021971444499357, 2.1079372889774723, 1.9084026833757428, 2.1167140453380213, 2.3740182895421293, 1.8632060517607802, 2.076402464514721, 2.215907711831237, 2.4446407943538606, 2.1757815782797603, 2.1450355930985054, 2.346263581167927, 1.8869907552364524, 2.461087123313006, 2.0273978330067357, 2.1504613281145346, 2.088402859836269, 2.198812654945001, 2.2531395007423036, 2.059688649164976, 1.810131401373492, 2.1380569646358207, 2.21698438382967, 2.4823956220819454, 2.1195679167362993, 1.971004143341482, 2.0001928766222687, 2.0518658433650567, 1.8909377547150426, 1.8719002668156919, 2.0312890920937923, 2.0585284689321237, 1.9562310562304412, 2.2622172964686587, 2.3135086353729846, 1.7294282976062227, 2.1329809755411775, 1.6555943980913044, 1.7564821912496176, 1.9865067476393694, 1.943536227476239, 1.978874337334997, 2.2721219696167485, 1.3500510895461262, 1.945111821369137, 2.128804782670874, 2.0435319259775824, 1.8245981558917728, 1.9881851896046179, 1.9763830868347005, 1.410153978585237, 1.9877138408190764, 1.5592077802792934, 2.1382530196473235, 1.6794143710999903, 1.7629319662927774, 2.080556145782496, 1.624516182196767, 1.592757630272275, 1.6688359778036816, 2.1488719483329546, 1.749666852250499, 1.8956478752597332, 1.9571766940492932, 1.8259836749279672, 1.69090942977038, 1.9665592430540089, 1.8076948352084192, 1.7684841448798214, 1.9426014940154408, 1.7010945896422616, 1.7521388139652931, 1.6375481266407115, 1.6952798468192982, 1.7687937906780133, 1.5499046858392924, 1.7933401927001027, 1.6386323965508027, 1.6667082364842258, 1.5691874815949882, 1.6214483501900927, 1.6512104077564604, 1.3213007010113835, 1.8762247744837355, 1.8434306913313045, 1.646268527799408, 1.9997997873481772, 1.6309667060129955, 1.1701267890877338, 1.4891833294811843, 1.70317424237858, 1.5404069149122528, 1.5854351235733253, 1.6089370888972951, 2.058576125229221, 1.4337536442625005, 1.6372614896313955, 1.8904958727682826, 1.607664259219223, 1.7799635780008747, 1.7339694812403939, 1.784510600880739, 1.7362789713859994, 1.446772620741509, 1.7854874993092509, 1.7322336368741515, 1.4470217991003047, 1.287346335620474, 1.589470404148197, 1.4931802740816207, 1.367081370829135, 1.7940175672363345, 1.0815554925170499, 1.7961263543101298, 1.6367526511272754, 1.5904461673912726, 1.2824299904658127, 1.4529797833257037, 1.8568912108204836, 1.5370650534071886, 1.4994189511311506, 1.2382007588936237, 1.8971348537323285, 1.6130488323359, 1.1399157682988508, 1.6915364333186673, 1.1268907625369757, 1.7847238692913756, 1.2365533918378877, 1.4200319681257227, 1.8429169176312614, 1.3573146631657107, 1.3613831185322685, 1.4558154294226406, 1.2780111208324212, 0.7886692002199776, 1.2513134697770254, 1.565128906395955, 0.9596664751092816, 1.229862366192645, 1.081924466891789, 1.111507314381988, 0.9787962615669147, 0.6116717571822116, 0.9510060655520807, 1.1903071320327847, 0.9498037380592687, 1.0349733756213138, 1.866582896709912, 1.0865466015454803, 0.6267400342107526, 0.9140208774985443, 0.9798408473607394, 1.1648825935370135, 1.2159846140092239, 1.1245383727738474, 1.0784979611253103, 1.2490428991528333, 0.6407967725801771, 1.068668324933873, 0.6870142273109042, 0.927031672449401, 0.4454871539636873, 0.5097538075567264, 0.6562065181910253, 0.6959926880204463, 0.3588537720135193, 0.6224377220658721, 1.7307132444984323, 2.3000145835268153, 1.327206831417534, 0.9257733388814313, 1.2153523303143385, 0.7398576115325761, 0.9311622601760052, 1.2553016971047823, 1.133802533321011, 0.5773086413340248, 0.8756150802856881, 1.0928458184275962, 1.0155751885345552, 0.8983073758017677, 0.6652714303068812, 0.8960706763006234, 0.9986283064194643, 0.44583114642604027, 1.0500690080684956, 0.8119711476721613, 1.0797259989022492, 1.1591392809881758, 0.5589498619539535, 0.7121118251675107, 0.7926670437404754, 0.42816989121324867, 1.3000331944944883, 0.7871258827220289, 0.6142199270606485, 0.7738938012672615, 0.7036560755282376, 0.7959690693617248, 0.5999820458657846, 0.5932785082964959, 0.726994351994561, 0.3667169119883454, 0.40103360430521057, 1.1649978002926777, 0.6092126977255794, 0.35891871619390314, 0.4949443402085074, 0.5185312490896163, 0.6175317543712656, 0.8038231033579081, 0.2553670226039733, 0.3339115350747408, 0.5362680466660616, 0.39771775162361667, 0.5402554749542119, 0.3149396882057357, 0.3721801906432122, 0.48718576388629914, 0.7962355987553126, 0.4918489195598988, 0.25226505624543255, 0.15026628045480792, 0.7376997211704017, 0.7590797499739401, 0.38793019759373837, 0.45703582118824787, 0.4455654487389473, 0.6131551995768093, 0.7531816428423523, 0.41108880104674034, 0.29370583260422584, 0.5912291931437593, 0.21482833435145687, 0.3093758810872063, 0.3600096173785579, 0.24084408281141323, 0.44647209122066905, 0.19964547106471958, 0.29215732521324317, 0.3130263662296953, 0.24575520068006115, 0.13825717262552809, 0.36514773550099505, 0.21452174186284392, 0.7726704370448986, 0.45634574429319485, 0.3122368050781844, 0.05776697602024016, 0.1854773946131284, 0.5503131811487881, 0.28743680936224414, 0.3771210398268423, 0.3295559392454376, 0.5024100581564189, 0.4312812816763762, 0.16620777183144497, 0.3508810148044679, 0.16407267229376166, 0.24960096171718807, 0.1099984918803749, 0.25971806004965553, 0.17926499542018026, 0.08146063120636833, 0.14776578401052776, 0.21537143605947007, 0.2795480336964628, 0.09655567892633884, 0.12237956574301555, 0.16769773791632528, 0.2053869851110839, 0.12690574396394314, 0.14626360697760457, 0.27095333847671027, 0.08534698567233853, 0.11189195969800379, 0.04948403915152692, 0.04686323923690525, 0.025367597061111982, 0.09769466920014397, 0.05127357695541305, 0.03077601974795683, 0.09978465409029391, 0.08285593039943791, 0.08336358672151922, 0.296778078865661, 0.17747115931833968, 0.1652346467031926, 0.9328940050886166, 0.7968578466671137, 0.12050352961368434, 0.13951782523844286, 1.5305899356410402, 0.5357295783816367, 0.25324114847254797, 0.20073981006510697, 0.2233964466265701, 0.24295233857099802, 0.0725913969922685, 0.05298564169811315, 0.11045412407002203, 0.4968842232084023, 0.4371184149349949, 1.4942854079244765, 0.14652136624419634, 0.15948199664138601, 0.31005253162862445, 0.4215869937130474, 0.10524540448488609, 0.06936716339445756, 0.028366359470039727, 0.06622668476741339, 0.05638199354172865, 0.09400144409042956, 0.07466870217471178, 0.06626108132385396, 0.12616707832464547, 0.045277642345445346, 0.057425751483152626, 0.04994955847014086, 0.0357417098933736, 0.05729720134239927, 0.0380212057132703, 0.022473503359104828, 0.10765829459673788, 0.06250334432634279, 0.06565315499960024, 0.09353956286963783, 0.06560212981643271, 0.08354932861172695, 0.04462981999921014, 0.035550513887743794, 0.04394705129940501, 0.02369816594578828, 0.04103252661290321, 0.023090117515521657, 0.02139307379917506, 0.027697651054030034, 0.047769814246892944, 0.030182028669400757, 0.0102742852100776, 0.059926868062292596, 0.04037808543071978, 0.061574340908845206, 0.03061612113091562, 0.0361981308188108, 0.11871876214480453, 0.02711514343001913, 0.030976357804806915, 0.02748553665434398, 0.021510635515774104, 0.019245058776419233, 0.04146044511708326, 0.04634376549811827, 0.02264088468215882, 0.0392049201174265, 0.006836150385772681, 0.020894789802302166, 0.007139786195501827, 0.02259755622753597, 0.027050669788486522, 0.0529215937297416, 0.015122620732716927, 0.016567539291492402, 0.02348200079721816, 0.047905095082202344, 0.039325259914641225, 0.024334853338468125, 0.021740722869688995, 0.023134379727173526, 0.024942579891443245, 0.014512102097624636, 0.017145682969261618, 0.014833737849948887, 0.030325040465723686, 0.025440286233534438, 0.013319141922429036, 0.034172089968108166, 0.017870379722329913, 0.01651922213586791, 0.0232326344336274, 0.012857308652123273, 0.015452327329549577, 0.011569529579973335, 0.01606499371361354, 0.0254439479702484, 0.005483964579962343, 0.03755204699226703, 0.01592147211275496, 0.009630283114641458, 0.019034746972734173, 0.026398567195622516, 0.03537987162931496, 0.027986562536791812, 0.02010668388338037, 0.016697632867202675, 0.009339512873053887, 0.01378188759272413, 0.015363995120761626, 0.015868681066423168, 0.01925600705433446, 0.0162695367607405, 0.011625233886545794, 0.01253275354471131, 0.011914555606381896, 0.01619025255379878, 0.009422600680863272, 0.012448477097153542, 0.011722024897707557, 0.010980889587049568, 0.019698072511830034, 0.008106453248236514, 0.024261969313031276, 0.01664952295875847, 0.02209273164248876, 0.014234979332353945, 0.01259274166389674, 0.010110019908980701, 0.01836947418802377, 0.004577332326274213, 0.015563620312384625, 0.010464749807813395, 0.031001429885089412, 0.013934688045500736, 0.011304716332335658, 0.01568272955105841, 0.009243550549294243, 0.015496693887549928, 0.009004373868964697, 0.012918905489206253, 0.014236148726334837, 0.007616032784086871, 0.011923181484998463, 0.009461286540340778, 0.015398113683632223, 0.007788799428446233, 0.015113599583182646, 0.021553783004299915, 0.01639392292162116, 0.008440645614175579, 0.012806029385153225, 0.00874927098586356, 0.023462035444168056, 0.0072876130475124055, 0.007217936771467708, 0.01782194198233033, 0.01207155237968858, 0.011752139959465968, 0.0062419636601818256, 0.013935065942190003, 0.004752039193712535, 0.011486809869905357, 0.014080469611488039, 0.006042325705134973, 0.011840332992193676, 0.007672160077540881, 0.014259665348858825, 0.015814291732097942, 0.00996384980651853, 0.009753477038422397, 0.00531625625296299, 0.008090467558170616, 0.007376739239527481, 0.007561630080063654, 0.0048067482718969325, 0.01455321646510775, 0.009534030105478314, 0.006823118986963241, 0.012258925685638085, 0.011827042260566472, 0.004132437648233549, 0.00841132813573615, 0.01163982120993098, 0.008515131479418538, 0.01037824653241514, 0.011728255494906156, 0.008709818568441973, 0.009080710544627063, 0.012874743552425738, 0.006268778404724124, 0.008973090645707517, 0.00789153802921903, 0.010576438959029565, 0.01370747626669494, 0.010576139975443058, 0.008087506534542226, 0.008060868983640241, 0.01861776886689462, 0.00892701187826333, 0.0037573490446317624, 0.016570516146758602, 0.01005173608680383, 0.0073664631730625445, 0.011563938003768235, 0.005498498875273184, 0.007866180621534696, 0.01449660654662139, 0.01239944670797838, 0.019480864669444573, 0.008557679950982921, 0.008240917611021543, 0.007697114025691263, 0.004959484933526304, 0.0076770077535636624, 0.005585870863360915, 0.007348764169813485, 0.005429214354160296, 0.009889597384216563, 0.005898542378264898, 0.006844108812315968, 0.008954046744797191, 0.006692399897707854, 0.0068495914899186355, 0.0038659909177255446, 0.007102762083355663, 0.006198899235130445, 0.006963071672403594, 0.009938492067863744, 0.005734729090470414, 0.010965489489377032, 0.00696197330801953, 0.009014750912451984, 0.005234961588205492, 0.010189974498328942, 0.006265418445138659, 0.009006211923601624, 0.0069112952326092275, 0.006952217059568428, 0.0060525169198034105, 0.004624616378595277, 0.006300349436876483, 0.006327778553716645, 0.009768225769362357, 0.004369052790291455, 0.009330586000170749, 0.011916091975691092, 0.009587720872694959, 0.009000985560248923, 0.009036224419451375, 0.008854067363251455, 0.0062079453437354085, 0.006903764936367092, 0.008378244942623849, 0.006026925378267827, 0.0070854818870063315, 0.003817874057583822, 0.006060431005049817, 0.006294697868466508, 0.004406882469618212, 0.00513493240171714, 0.005647861855906821, 0.006373796991816481, 0.0036909966583682525, 0.003103439946741614, 0.007847740087956221, 0.0050946230776867584, 0.00548313360983361, 0.0066137318919213045, 0.008360881951082057, 0.0035338558313725097, 0.004852368511403443, 0.006987997809365887, 0.006896849101987068, 0.002169801300467622, 0.003442951170971006, 0.009987380424426477, 0.005124334851919167, 0.009748315281390522, 0.004503677118881455, 0.004347344683964638, 0.005801303881045078, 0.0054503464532295424, 0.00651982963991027, 0.006015332376606099, 0.005741210110396739, 0.004278929573893216, 0.0029834098257500016, 0.008569560094122864, 0.004063908132902382, 0.00772795803548363, 0.0061756214695268055, 0.00395160928194057, 0.011982435101879504, 0.010200359248933318, 0.006216088906124246, 0.004598530518563445, 0.004720701108118153, 0.004657498544578502, 0.006138915366123302, 0.0013014428460153586, 0.004593745286225871, 0.007867314830581736, 0.00706103713156039, 0.0047291814880302055, 0.004435165978522119, 0.003572075049107347, 0.0035199080476802816, 0.004912580903612035, 0.010671017101633795, 0.005025775275609579, 0.006725925348585075, 0.004400996638160431, 0.0057950247019024625, 0.003918046208421628, 0.010649138571592715, 0.0077419436390885115, 0.007040307072026977, 0.004485089963919566, 0.0025554286710430685, 0.004692542015776534, 0.004412933727764303, 0.0028848586911630805, 0.0043098954352949245, 0.005056200145948542, 0.0028941767692891984, 0.0044539151251743285, 0.0033704169158858266, 0.004387385607255327, 0.005898412790377338, 0.0033261656118733825, 0.005787752550088934, 0.003084066630380062, 0.005925159649311415, 0.005571045170012228, 0.002954864165883129, 0.0047262716933875164, 0.005992546982146556, 0.006148414847011039, 0.002562720094109994, 0.007561353489565804, 0.004224458399807254, 0.00718462533908763, 0.004472624064811412, 0.005578008915382949, 0.0051878711302764564, 0.007220312662910829, 0.004471873677363599]\n",
      "Accuracy history: [0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.24, 0.24, 0.28, 0.32, 0.36, 0.32, 0.38, 0.44, 0.4, 0.36, 0.46, 0.32, 0.34, 0.52, 0.58, 0.62, 0.56, 0.62, 0.7, 0.7, 0.8, 0.7, 0.84, 0.76, 0.86, 0.92, 0.88, 0.92, 0.96, 0.96, 0.94, 0.86, 0.94, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# SGD\n",
    "sgd = ConvNet4(input_shape=input_shape, wt_scale=wt_scale, verbose=False)\n",
    "sgd.compile('sgd')\n",
    "sgd.fit(x_dev, y_dev, x_val, y_val, mini_batch_sz=mini_batch_sz, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**: Why does decreasing the mini-batch size make the loss print-outs more erratic?\n",
    "\n",
    "Answer: Decreasing the mini-batch size makes the loss print-outs more erratic because there are fewer samples that have been able to propagate back through the network, so each sample has much more of an impact on what the loss value will be. There is an averaging over all samples that takes place, so if all of the samples in a very small mini-batch are predicted successfully, loss will be low, but it will be high if it performs poorly on the mini-batch. The smaller the mini-batch, the greater the chance that there is a skewed distribution of correct and incorrect predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6d) Evaluate the different optimizers\n",
    "\n",
    "Make 2 \"high quality\" plots showing the following\n",
    "\n",
    "- Plot the accuracy (y axis) for the three optimizers as a function of training epoch (x axis).\n",
    "- Plot the loss (y axis) for the three optimizers as a function of training iteration (x axis).\n",
    "\n",
    "A high quality plot consists of:\n",
    "- A useful title\n",
    "- X and Y axis labels\n",
    "- A legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get accuracy values\n",
    "adam_acc = adam.train_acc_history\n",
    "sgd_m_acc = sgd_m.train_acc_history\n",
    "sgd_acc = sgd.train_acc_history\n",
    "\n",
    "#plot\n",
    "plt.plot(adam_acc, label=\"adam\")\n",
    "plt.plot(sgd_m_acc, label=\"sgd_m\")\n",
    "plt.plot(sgd_acc, label=\"sgd\")\n",
    "\n",
    "#make words show up\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy Over Iterations\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Training Iterations\")\n",
    "plt.show()\n",
    "\n",
    "#get loss values\n",
    "adam_loss = adam.loss_history\n",
    "sgd_m_loss = sgd_m.loss_history\n",
    "sgd_loss = sgd.loss_history\n",
    "\n",
    "#plot\n",
    "plt.plot(adam_loss, label=\"adam\")\n",
    "plt.plot(sgd_m_loss, label=\"sgd_m\")\n",
    "plt.plot(sgd_loss, label=\"sgd\")\n",
    "\n",
    "#make words show up\n",
    "plt.legend()\n",
    "plt.title(\"Loss Over Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Training Iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**: Which optimizer works best and why do think it is best?\n",
    "\n",
    "The adam optimizer works the best. It does so because it dynamically updates learning rate.\n",
    "\n",
    "**Question 5**: What is happening with the training set accuracy and why?\n",
    "\n",
    "The training set accuracy is overfitting. It is basically memorizing the features about each image, or memorizing the dataset. Therefore, it has perfect accuracy while looking at its own training set, but once it has a non-training image, it has horrible accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Training convolutional neural network on STL-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7a) Load in STL-10 at 32x32 resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images are: (5000, 96, 96, 3)\n",
      "Labels are: (5000,)\n",
      "Resizing 5000 images to 32x32...Done!\n",
      "Saving Numpy arrays the images and labels to ./numpy...Done!\n",
      "imgs.shape (5000, 32, 32, 3)\n",
      "data.shape (5000, 3072)\n",
      "Train data shape:  (4548, 3072)\n",
      "Train labels shape:  (4548,)\n",
      "Test data shape:  (400, 3072)\n",
      "Test labels shape:  (400,)\n",
      "Validation data shape:  (2, 3072)\n",
      "Validation labels shape:  (2,)\n",
      "dev data shape:  (50, 3072)\n",
      "dev labels shape:  (50,)\n"
     ]
    }
   ],
   "source": [
    "# Download the STL-10 dataset from the internet, convert it to Numpy ndarray, resize to 32x32\n",
    "# cache it locally on your computer for faster loading next time.\n",
    "load_stl10_dataset.purge_cached_dataset()\n",
    "stl_imgs, stl_labels = load_stl10_dataset.load(scale_fact=3)\n",
    "# preprocess\n",
    "stl_imgs, stl_labels = preprocess_data.preprocess_stl(stl_imgs, stl_labels)\n",
    "# create splits\n",
    "x_train, y_train, x_test, y_test, x_val, y_val, x_dev, y_dev = preprocess_data.create_splits(\n",
    "    stl_imgs, stl_labels, n_train_samps=4548, n_test_samps=400, n_valid_samps=2, n_dev_samps=50)\n",
    "\n",
    "print ('Train data shape: ', x_train.shape)\n",
    "print ('Train labels shape: ', y_train.shape)\n",
    "print ('Test data shape: ', x_test.shape)\n",
    "print ('Test labels shape: ', y_test.shape)\n",
    "print ('Validation data shape: ', x_val.shape)\n",
    "print ('Validation labels shape: ', y_val.shape)\n",
    "print ('dev data shape: ', x_dev.shape)\n",
    "print ('dev labels shape: ', y_dev.shape)\n",
    "\n",
    "classes = np.loadtxt(os.path.join('data', 'stl10_binary', 'class_names.txt'), dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7b) Set up accelerated convolution and max pooling layers\n",
    "\n",
    "As you may have noticed, we had to downsize STL-10 to 16x16 resolution to train the network on the dev set (N=50) in a reasonable amount of time. The training set is N=4000, how will we ever manage to process that amount of data!?\n",
    "\n",
    "On one hand, this is an unfortunate inevitable reality of working with large (\"big\") datasets: you can easily find a dataset that is too time consuming to process for any computer, despite how fast/many CPU/GPUs it has.\n",
    "\n",
    "On the other hand, we can do better for this project and STL-10 :) If you were to time (profile) different parts of the training process, you'd notice that largest bottleneck is convolution and max pooling operations (both forward/backward). You implemented those operations intuitively, which does not always yield the best performance. **By swapping out forward/backward convolution and maxpooling for implementations that use different algorithms (im2col, reshaping) that are compiled to C code, we will speed up training up by several orders of magnitude**.\n",
    "\n",
    "Follow these steps to subsitute in the \"accelerated\" convolution and max pooling layers.\n",
    "\n",
    "- Install the `cython` python package: `pip3 install cython` (or `pip3 install cython --user` if working in Davis 102)\n",
    "- Dowload files `im2col_cython.pyx`, `accelerated_layer.py`, `setup.py` from the project website. Put them in your base project folder.\n",
    "- Open terminal, `cd` to Project directory.\n",
    "- Compile the im2col functions: `python3 setup.py build_ext --inplace`. A `.c` and `.so` file should have appeared in your project folder.\n",
    "- Restart Jupyter Notebook kernel\n",
    "- Create a class called `Conv4NetAccel` in `network.py` by copy-pasting the contents of `Conv4Net`. Import `accelerated_layer` at the top and replace the `Conv2D` and `MaxPool2D` layers with `Conv2DAccel` and `MaxPool2DAccel`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7c) Training convolutional neural network on STL-10\n",
    "\n",
    "You are now ready to train on the entire training set.\n",
    "\n",
    "- Create a `Conv4NetAccel` object with hyperparameters of your choice.\n",
    "- Your goal is to achieve 45% accuracy on the test and/or validation set.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- I suggest using your intuition about hyperparameters and over/underfitting to guide your choice, rather than a grid search. This should not be overly challenging.\n",
    "- Use the best / most efficient optimizer based on your prior analysis.\n",
    "- It should take on the order of 1 sec per training iteration. If that's way off, seek help as something could be wrong with running the acclerated code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network import ConvNet4Accel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4548, 3, 32, 32)\n",
      "(5000, 3, 32, 32)\n",
      "Starting to train...\n",
      "1485 iterations. 45 iter/epoch.\n",
      "Iteration: 1/1485.\n",
      "Time taken for iteration 0: 0.5223598480224609\n",
      "Estimated time to complete: 775.7043743133545\n",
      "Iteration: 2/1485.\n",
      "Iteration: 3/1485.\n",
      "Iteration: 4/1485.\n",
      "Iteration: 5/1485.\n",
      "Iteration: 6/1485.\n",
      "Iteration: 7/1485.\n",
      "Iteration: 8/1485.\n",
      "Iteration: 9/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [2.294601778763012, 2.282298125364221, 2.294678281032607]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.11, Val acc: 0.14\n",
      "\n",
      "\n",
      "Iteration: 10/1485.\n",
      "Iteration: 11/1485.\n",
      "Iteration: 12/1485.\n",
      "Iteration: 13/1485.\n",
      "Iteration: 14/1485.\n",
      "Iteration: 15/1485.\n",
      "Iteration: 16/1485.\n",
      "Iteration: 17/1485.\n",
      "Iteration: 18/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [2.2727747578789135, 2.208595316937772, 2.195159558508625]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.176, Val acc: 0.1\n",
      "\n",
      "\n",
      "Iteration: 19/1485.\n",
      "Iteration: 20/1485.\n",
      "Iteration: 21/1485.\n",
      "Iteration: 22/1485.\n",
      "Iteration: 23/1485.\n",
      "Iteration: 24/1485.\n",
      "Iteration: 25/1485.\n",
      "Iteration: 26/1485.\n",
      "Iteration: 27/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [2.036537010202001, 2.091745432015573, 2.021473753538662]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.186, Val acc: 0.18\n",
      "\n",
      "\n",
      "Iteration: 28/1485.\n",
      "Iteration: 29/1485.\n",
      "Iteration: 30/1485.\n",
      "Iteration: 31/1485.\n",
      "Iteration: 32/1485.\n",
      "Iteration: 33/1485.\n",
      "Iteration: 34/1485.\n",
      "Iteration: 35/1485.\n",
      "Iteration: 36/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [2.078980722412962, 1.9485219295705958, 2.049360068545789]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.248, Val acc: 0.2\n",
      "\n",
      "\n",
      "Iteration: 37/1485.\n",
      "Iteration: 38/1485.\n",
      "Iteration: 39/1485.\n",
      "Iteration: 40/1485.\n",
      "Iteration: 41/1485.\n",
      "Iteration: 42/1485.\n",
      "Iteration: 43/1485.\n",
      "Iteration: 44/1485.\n",
      "Iteration: 45/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.988761037492356, 2.031935958870427, 1.8523434062750006]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.276, Val acc: 0.26\n",
      "\n",
      "\n",
      "Iteration: 46/1485.\n",
      "Iteration: 47/1485.\n",
      "Iteration: 48/1485.\n",
      "Iteration: 49/1485.\n",
      "Iteration: 50/1485.\n",
      "Iteration: 51/1485.\n",
      "Iteration: 52/1485.\n",
      "Iteration: 53/1485.\n",
      "Iteration: 54/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.8364560193205208, 1.9734739488772242, 1.9625493846936863]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.3, Val acc: 0.2\n",
      "\n",
      "\n",
      "Iteration: 55/1485.\n",
      "Iteration: 56/1485.\n",
      "Iteration: 57/1485.\n",
      "Iteration: 58/1485.\n",
      "Iteration: 59/1485.\n",
      "Iteration: 60/1485.\n",
      "Iteration: 61/1485.\n",
      "Iteration: 62/1485.\n",
      "Iteration: 63/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.9208829070215894, 1.9324107130440562, 1.8977416964108889]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.272, Val acc: 0.22\n",
      "\n",
      "\n",
      "Iteration: 64/1485.\n",
      "Iteration: 65/1485.\n",
      "Iteration: 66/1485.\n",
      "Iteration: 67/1485.\n",
      "Iteration: 68/1485.\n",
      "Iteration: 69/1485.\n",
      "Iteration: 70/1485.\n",
      "Iteration: 71/1485.\n",
      "Iteration: 72/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.7734441252840918, 1.6897121893166205, 2.0254373521006257]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.27, Val acc: 0.24\n",
      "\n",
      "\n",
      "Iteration: 73/1485.\n",
      "Iteration: 74/1485.\n",
      "Iteration: 75/1485.\n",
      "Iteration: 76/1485.\n",
      "Iteration: 77/1485.\n",
      "Iteration: 78/1485.\n",
      "Iteration: 79/1485.\n",
      "Iteration: 80/1485.\n",
      "Iteration: 81/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.725106592817923, 1.8648550240998365, 1.8314207932096107]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.308, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 82/1485.\n",
      "Iteration: 83/1485.\n",
      "Iteration: 84/1485.\n",
      "Iteration: 85/1485.\n",
      "Iteration: 86/1485.\n",
      "Iteration: 87/1485.\n",
      "Iteration: 88/1485.\n",
      "Iteration: 89/1485.\n",
      "Iteration: 90/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.9079038279546436, 1.866891083330692, 1.78745841032149]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.318, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 91/1485.\n",
      "Iteration: 92/1485.\n",
      "Iteration: 93/1485.\n",
      "Iteration: 94/1485.\n",
      "Iteration: 95/1485.\n",
      "Iteration: 96/1485.\n",
      "Iteration: 97/1485.\n",
      "Iteration: 98/1485.\n",
      "Iteration: 99/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.8792102253888772, 1.7485673100092367, 1.7914777533688002]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.334, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 100/1485.\n",
      "Iteration: 101/1485.\n",
      "Iteration: 102/1485.\n",
      "Iteration: 103/1485.\n",
      "Iteration: 104/1485.\n",
      "Iteration: 105/1485.\n",
      "Iteration: 106/1485.\n",
      "Iteration: 107/1485.\n",
      "Iteration: 108/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.8118806953246038, 1.7770203074056874, 1.6941629844668398]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.316, Val acc: 0.26\n",
      "\n",
      "\n",
      "Iteration: 109/1485.\n",
      "Iteration: 110/1485.\n",
      "Iteration: 111/1485.\n",
      "Iteration: 112/1485.\n",
      "Iteration: 113/1485.\n",
      "Iteration: 114/1485.\n",
      "Iteration: 115/1485.\n",
      "Iteration: 116/1485.\n",
      "Iteration: 117/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.6748192438056284, 1.782027155739778, 1.6906995638863358]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.342, Val acc: 0.26\n",
      "\n",
      "\n",
      "Iteration: 118/1485.\n",
      "Iteration: 119/1485.\n",
      "Iteration: 120/1485.\n",
      "Iteration: 121/1485.\n",
      "Iteration: 122/1485.\n",
      "Iteration: 123/1485.\n",
      "Iteration: 124/1485.\n",
      "Iteration: 125/1485.\n",
      "Iteration: 126/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.6716590727300586, 1.6586521703069834, 1.6094946504820653]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.336, Val acc: 0.26\n",
      "\n",
      "\n",
      "Iteration: 127/1485.\n",
      "Iteration: 128/1485.\n",
      "Iteration: 129/1485.\n",
      "Iteration: 130/1485.\n",
      "Iteration: 131/1485.\n",
      "Iteration: 132/1485.\n",
      "Iteration: 133/1485.\n",
      "Iteration: 134/1485.\n",
      "Iteration: 135/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.7760282474902465, 1.6653346202934263, 1.6733084779435268]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.38, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 136/1485.\n",
      "Iteration: 137/1485.\n",
      "Iteration: 138/1485.\n",
      "Iteration: 139/1485.\n",
      "Iteration: 140/1485.\n",
      "Iteration: 141/1485.\n",
      "Iteration: 142/1485.\n",
      "Iteration: 143/1485.\n",
      "Iteration: 144/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.558530729591002, 1.733866480403747, 1.638029435073854]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.308, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 145/1485.\n",
      "Iteration: 146/1485.\n",
      "Iteration: 147/1485.\n",
      "Iteration: 148/1485.\n",
      "Iteration: 149/1485.\n",
      "Iteration: 150/1485.\n",
      "Iteration: 151/1485.\n",
      "Iteration: 152/1485.\n",
      "Iteration: 153/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.8913188746703082, 1.6025249658726595, 1.6827321239588784]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.374, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 154/1485.\n",
      "Iteration: 155/1485.\n",
      "Iteration: 156/1485.\n",
      "Iteration: 157/1485.\n",
      "Iteration: 158/1485.\n",
      "Iteration: 159/1485.\n",
      "Iteration: 160/1485.\n",
      "Iteration: 161/1485.\n",
      "Iteration: 162/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.6741812598587915, 1.538431527428379, 1.614734036898915]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.356, Val acc: 0.22\n",
      "\n",
      "\n",
      "Iteration: 163/1485.\n",
      "Iteration: 164/1485.\n",
      "Iteration: 165/1485.\n",
      "Iteration: 166/1485.\n",
      "Iteration: 167/1485.\n",
      "Iteration: 168/1485.\n",
      "Iteration: 169/1485.\n",
      "Iteration: 170/1485.\n",
      "Iteration: 171/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.6852584447805865, 1.5935864497824057, 1.5958805650194157]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.384, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 172/1485.\n",
      "Iteration: 173/1485.\n",
      "Iteration: 174/1485.\n",
      "Iteration: 175/1485.\n",
      "Iteration: 176/1485.\n",
      "Iteration: 177/1485.\n",
      "Iteration: 178/1485.\n",
      "Iteration: 179/1485.\n",
      "Iteration: 180/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.6576614504411677, 1.6117451512779213, 1.727666951775964]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.416, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 181/1485.\n",
      "Iteration: 182/1485.\n",
      "Iteration: 183/1485.\n",
      "Iteration: 184/1485.\n",
      "Iteration: 185/1485.\n",
      "Iteration: 186/1485.\n",
      "Iteration: 187/1485.\n",
      "Iteration: 188/1485.\n",
      "Iteration: 189/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.7052412024011703, 1.5583973973197582, 1.5287946956227074]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.352, Val acc: 0.26\n",
      "\n",
      "\n",
      "Iteration: 190/1485.\n",
      "Iteration: 191/1485.\n",
      "Iteration: 192/1485.\n",
      "Iteration: 193/1485.\n",
      "Iteration: 194/1485.\n",
      "Iteration: 195/1485.\n",
      "Iteration: 196/1485.\n",
      "Iteration: 197/1485.\n",
      "Iteration: 198/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.6715366159467617, 1.6284367671244246, 1.662069364162937]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.38, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 199/1485.\n",
      "Iteration: 200/1485.\n",
      "Iteration: 201/1485.\n",
      "Iteration: 202/1485.\n",
      "Iteration: 203/1485.\n",
      "Iteration: 204/1485.\n",
      "Iteration: 205/1485.\n",
      "Iteration: 206/1485.\n",
      "Iteration: 207/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.4974565335626888, 1.6806144523572357, 1.5071321412505085]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.392, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 208/1485.\n",
      "Iteration: 209/1485.\n",
      "Iteration: 210/1485.\n",
      "Iteration: 211/1485.\n",
      "Iteration: 212/1485.\n",
      "Iteration: 213/1485.\n",
      "Iteration: 214/1485.\n",
      "Iteration: 215/1485.\n",
      "Iteration: 216/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.6864203429442537, 1.518072423329791, 1.6817557580596891]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.402, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 217/1485.\n",
      "Iteration: 218/1485.\n",
      "Iteration: 219/1485.\n",
      "Iteration: 220/1485.\n",
      "Iteration: 221/1485.\n",
      "Iteration: 222/1485.\n",
      "Iteration: 223/1485.\n",
      "Iteration: 224/1485.\n",
      "Iteration: 225/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.7217386862556463, 1.6568159388196477, 1.5954444919452675]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.408, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 226/1485.\n",
      "Iteration: 227/1485.\n",
      "Iteration: 228/1485.\n",
      "Iteration: 229/1485.\n",
      "Iteration: 230/1485.\n",
      "Iteration: 231/1485.\n",
      "Iteration: 232/1485.\n",
      "Iteration: 233/1485.\n",
      "Iteration: 234/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.4639175711176848, 1.5707056502699643, 1.4183476973081792]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.42, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 235/1485.\n",
      "Iteration: 236/1485.\n",
      "Iteration: 237/1485.\n",
      "Iteration: 238/1485.\n",
      "Iteration: 239/1485.\n",
      "Iteration: 240/1485.\n",
      "Iteration: 241/1485.\n",
      "Iteration: 242/1485.\n",
      "Iteration: 243/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.713870277790241, 1.4701614401273264, 1.4624129887189514]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.392, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 244/1485.\n",
      "Iteration: 245/1485.\n",
      "Iteration: 246/1485.\n",
      "Iteration: 247/1485.\n",
      "Iteration: 248/1485.\n",
      "Iteration: 249/1485.\n",
      "Iteration: 250/1485.\n",
      "Iteration: 251/1485.\n",
      "Iteration: 252/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.369001262510948, 1.611257136523804, 1.5442046364514561]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.434, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 253/1485.\n",
      "Iteration: 254/1485.\n",
      "Iteration: 255/1485.\n",
      "Iteration: 256/1485.\n",
      "Iteration: 257/1485.\n",
      "Iteration: 258/1485.\n",
      "Iteration: 259/1485.\n",
      "Iteration: 260/1485.\n",
      "Iteration: 261/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.5756427882273463, 1.534034849424308, 1.4805494931512428]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.414, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 262/1485.\n",
      "Iteration: 263/1485.\n",
      "Iteration: 264/1485.\n",
      "Iteration: 265/1485.\n",
      "Iteration: 266/1485.\n",
      "Iteration: 267/1485.\n",
      "Iteration: 268/1485.\n",
      "Iteration: 269/1485.\n",
      "Iteration: 270/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.5351978040730634, 1.5623354303771613, 1.4954703295743275]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.422, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 271/1485.\n",
      "Iteration: 272/1485.\n",
      "Iteration: 273/1485.\n",
      "Iteration: 274/1485.\n",
      "Iteration: 275/1485.\n",
      "Iteration: 276/1485.\n",
      "Iteration: 277/1485.\n",
      "Iteration: 278/1485.\n",
      "Iteration: 279/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.517447720557368, 1.3559487809916986, 1.2652920473139881]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.37, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 280/1485.\n",
      "Iteration: 281/1485.\n",
      "Iteration: 282/1485.\n",
      "Iteration: 283/1485.\n",
      "Iteration: 284/1485.\n",
      "Iteration: 285/1485.\n",
      "Iteration: 286/1485.\n",
      "Iteration: 287/1485.\n",
      "Iteration: 288/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.5523204796342613, 1.3959333169228825, 1.5991618325573287]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.414, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 289/1485.\n",
      "Iteration: 290/1485.\n",
      "Iteration: 291/1485.\n",
      "Iteration: 292/1485.\n",
      "Iteration: 293/1485.\n",
      "Iteration: 294/1485.\n",
      "Iteration: 295/1485.\n",
      "Iteration: 296/1485.\n",
      "Iteration: 297/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.4576171119246581, 1.6049197199584118, 1.4192445720314644]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.498, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 298/1485.\n",
      "Iteration: 299/1485.\n",
      "Iteration: 300/1485.\n",
      "Iteration: 301/1485.\n",
      "Iteration: 302/1485.\n",
      "Iteration: 303/1485.\n",
      "Iteration: 304/1485.\n",
      "Iteration: 305/1485.\n",
      "Iteration: 306/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.5192132439799582, 1.441139109794347, 1.2996101344242623]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.496, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 307/1485.\n",
      "Iteration: 308/1485.\n",
      "Iteration: 309/1485.\n",
      "Iteration: 310/1485.\n",
      "Iteration: 311/1485.\n",
      "Iteration: 312/1485.\n",
      "Iteration: 313/1485.\n",
      "Iteration: 314/1485.\n",
      "Iteration: 315/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.500734193525169, 1.5008320160276265, 1.3706860501506994]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.478, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 316/1485.\n",
      "Iteration: 317/1485.\n",
      "Iteration: 318/1485.\n",
      "Iteration: 319/1485.\n",
      "Iteration: 320/1485.\n",
      "Iteration: 321/1485.\n",
      "Iteration: 322/1485.\n",
      "Iteration: 323/1485.\n",
      "Iteration: 324/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.4505800250494365, 1.384620844821697, 1.3745433744504902]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.516, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 325/1485.\n",
      "Iteration: 326/1485.\n",
      "Iteration: 327/1485.\n",
      "Iteration: 328/1485.\n",
      "Iteration: 329/1485.\n",
      "Iteration: 330/1485.\n",
      "Iteration: 331/1485.\n",
      "Iteration: 332/1485.\n",
      "Iteration: 333/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.499825428502901, 1.366913828092043, 1.4319303579457392]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.424, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 334/1485.\n",
      "Iteration: 335/1485.\n",
      "Iteration: 336/1485.\n",
      "Iteration: 337/1485.\n",
      "Iteration: 338/1485.\n",
      "Iteration: 339/1485.\n",
      "Iteration: 340/1485.\n",
      "Iteration: 341/1485.\n",
      "Iteration: 342/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.5049027859043822, 1.2978613263056236, 1.5206620994458124]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.458, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 343/1485.\n",
      "Iteration: 344/1485.\n",
      "Iteration: 345/1485.\n",
      "Iteration: 346/1485.\n",
      "Iteration: 347/1485.\n",
      "Iteration: 348/1485.\n",
      "Iteration: 349/1485.\n",
      "Iteration: 350/1485.\n",
      "Iteration: 351/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.4399159378627704, 1.5479422195023025, 1.2091039250152205]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.522, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 352/1485.\n",
      "Iteration: 353/1485.\n",
      "Iteration: 354/1485.\n",
      "Iteration: 355/1485.\n",
      "Iteration: 356/1485.\n",
      "Iteration: 357/1485.\n",
      "Iteration: 358/1485.\n",
      "Iteration: 359/1485.\n",
      "Iteration: 360/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.3888701703717248, 1.498290293747371, 1.359160416237354]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.45, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 361/1485.\n",
      "Iteration: 362/1485.\n",
      "Iteration: 363/1485.\n",
      "Iteration: 364/1485.\n",
      "Iteration: 365/1485.\n",
      "Iteration: 366/1485.\n",
      "Iteration: 367/1485.\n",
      "Iteration: 368/1485.\n",
      "Iteration: 369/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.5338307849926287, 1.361879661409273, 1.5885320795924807]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.498, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 370/1485.\n",
      "Iteration: 371/1485.\n",
      "Iteration: 372/1485.\n",
      "Iteration: 373/1485.\n",
      "Iteration: 374/1485.\n",
      "Iteration: 375/1485.\n",
      "Iteration: 376/1485.\n",
      "Iteration: 377/1485.\n",
      "Iteration: 378/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.316386525137998, 1.397954810225495, 1.3219124169596603]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.478, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 379/1485.\n",
      "Iteration: 380/1485.\n",
      "Iteration: 381/1485.\n",
      "Iteration: 382/1485.\n",
      "Iteration: 383/1485.\n",
      "Iteration: 384/1485.\n",
      "Iteration: 385/1485.\n",
      "Iteration: 386/1485.\n",
      "Iteration: 387/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.225029912626833, 1.336402453082213, 1.3757955811978204]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.486, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 388/1485.\n",
      "Iteration: 389/1485.\n",
      "Iteration: 390/1485.\n",
      "Iteration: 391/1485.\n",
      "Iteration: 392/1485.\n",
      "Iteration: 393/1485.\n",
      "Iteration: 394/1485.\n",
      "Iteration: 395/1485.\n",
      "Iteration: 396/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.2673657428777017, 1.249724814297281, 1.332538219335769]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.48, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 397/1485.\n",
      "Iteration: 398/1485.\n",
      "Iteration: 399/1485.\n",
      "Iteration: 400/1485.\n",
      "Iteration: 401/1485.\n",
      "Iteration: 402/1485.\n",
      "Iteration: 403/1485.\n",
      "Iteration: 404/1485.\n",
      "Iteration: 405/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.4122616369983731, 1.4103063290544262, 1.2997217711172337]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.532, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 406/1485.\n",
      "Iteration: 407/1485.\n",
      "Iteration: 408/1485.\n",
      "Iteration: 409/1485.\n",
      "Iteration: 410/1485.\n",
      "Iteration: 411/1485.\n",
      "Iteration: 412/1485.\n",
      "Iteration: 413/1485.\n",
      "Iteration: 414/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.4091845539286316, 1.3783203653425085, 1.1725908290733218]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.524, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 415/1485.\n",
      "Iteration: 416/1485.\n",
      "Iteration: 417/1485.\n",
      "Iteration: 418/1485.\n",
      "Iteration: 419/1485.\n",
      "Iteration: 420/1485.\n",
      "Iteration: 421/1485.\n",
      "Iteration: 422/1485.\n",
      "Iteration: 423/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.422164337266596, 1.2812341535401488, 1.2396840964991949]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.542, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 424/1485.\n",
      "Iteration: 425/1485.\n",
      "Iteration: 426/1485.\n",
      "Iteration: 427/1485.\n",
      "Iteration: 428/1485.\n",
      "Iteration: 429/1485.\n",
      "Iteration: 430/1485.\n",
      "Iteration: 431/1485.\n",
      "Iteration: 432/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.2763063586405818, 1.1385915556111492, 1.3512381621162037]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.528, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 433/1485.\n",
      "Iteration: 434/1485.\n",
      "Iteration: 435/1485.\n",
      "Iteration: 436/1485.\n",
      "Iteration: 437/1485.\n",
      "Iteration: 438/1485.\n",
      "Iteration: 439/1485.\n",
      "Iteration: 440/1485.\n",
      "Iteration: 441/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.1688046845481652, 1.2900704825762586, 1.3376455695014986]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.546, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 442/1485.\n",
      "Iteration: 443/1485.\n",
      "Iteration: 444/1485.\n",
      "Iteration: 445/1485.\n",
      "Iteration: 446/1485.\n",
      "Iteration: 447/1485.\n",
      "Iteration: 448/1485.\n",
      "Iteration: 449/1485.\n",
      "Iteration: 450/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.2542576934087644, 1.1969583705721079, 1.189299563997143]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.572, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 451/1485.\n",
      "Iteration: 452/1485.\n",
      "Iteration: 453/1485.\n",
      "Iteration: 454/1485.\n",
      "Iteration: 455/1485.\n",
      "Iteration: 456/1485.\n",
      "Iteration: 457/1485.\n",
      "Iteration: 458/1485.\n",
      "Iteration: 459/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.2520860034762245, 1.1155807495840413, 1.3361102439983783]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.538, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 460/1485.\n",
      "Iteration: 461/1485.\n",
      "Iteration: 462/1485.\n",
      "Iteration: 463/1485.\n",
      "Iteration: 464/1485.\n",
      "Iteration: 465/1485.\n",
      "Iteration: 466/1485.\n",
      "Iteration: 467/1485.\n",
      "Iteration: 468/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.2036559811450511, 1.0934480822989994, 1.0871709764601343]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.572, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 469/1485.\n",
      "Iteration: 470/1485.\n",
      "Iteration: 471/1485.\n",
      "Iteration: 472/1485.\n",
      "Iteration: 473/1485.\n",
      "Iteration: 474/1485.\n",
      "Iteration: 475/1485.\n",
      "Iteration: 476/1485.\n",
      "Iteration: 477/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.2591174841141648, 1.0998386933168924, 1.1160539874745676]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.552, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 478/1485.\n",
      "Iteration: 479/1485.\n",
      "Iteration: 480/1485.\n",
      "Iteration: 481/1485.\n",
      "Iteration: 482/1485.\n",
      "Iteration: 483/1485.\n",
      "Iteration: 484/1485.\n",
      "Iteration: 485/1485.\n",
      "Iteration: 486/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.2527783460986965, 1.2447736361730204, 1.2639933063039341]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.562, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 487/1485.\n",
      "Iteration: 488/1485.\n",
      "Iteration: 489/1485.\n",
      "Iteration: 490/1485.\n",
      "Iteration: 491/1485.\n",
      "Iteration: 492/1485.\n",
      "Iteration: 493/1485.\n",
      "Iteration: 494/1485.\n",
      "Iteration: 495/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.3629379684907017, 1.1710059778503306, 1.2146260876776507]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.524, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 496/1485.\n",
      "Iteration: 497/1485.\n",
      "Iteration: 498/1485.\n",
      "Iteration: 499/1485.\n",
      "Iteration: 500/1485.\n",
      "Iteration: 501/1485.\n",
      "Iteration: 502/1485.\n",
      "Iteration: 503/1485.\n",
      "Iteration: 504/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.2274143109961835, 1.2417072090102192, 1.220968580102546]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.548, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 505/1485.\n",
      "Iteration: 506/1485.\n",
      "Iteration: 507/1485.\n",
      "Iteration: 508/1485.\n",
      "Iteration: 509/1485.\n",
      "Iteration: 510/1485.\n",
      "Iteration: 511/1485.\n",
      "Iteration: 512/1485.\n",
      "Iteration: 513/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.1499145954033458, 1.214782327489712, 1.0945067407968947]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.588, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 514/1485.\n",
      "Iteration: 515/1485.\n",
      "Iteration: 516/1485.\n",
      "Iteration: 517/1485.\n",
      "Iteration: 518/1485.\n",
      "Iteration: 519/1485.\n",
      "Iteration: 520/1485.\n",
      "Iteration: 521/1485.\n",
      "Iteration: 522/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.1675970689792596, 1.1390529488509593, 1.3665914578894347]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.572, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 523/1485.\n",
      "Iteration: 524/1485.\n",
      "Iteration: 525/1485.\n",
      "Iteration: 526/1485.\n",
      "Iteration: 527/1485.\n",
      "Iteration: 528/1485.\n",
      "Iteration: 529/1485.\n",
      "Iteration: 530/1485.\n",
      "Iteration: 531/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.179496513242717, 1.2632434001970871, 1.11901222585501]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.608, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 532/1485.\n",
      "Iteration: 533/1485.\n",
      "Iteration: 534/1485.\n",
      "Iteration: 535/1485.\n",
      "Iteration: 536/1485.\n",
      "Iteration: 537/1485.\n",
      "Iteration: 538/1485.\n",
      "Iteration: 539/1485.\n",
      "Iteration: 540/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.9958110417089128, 1.028260792751332, 1.0087140157158425]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.57, Val acc: 0.28\n",
      "\n",
      "\n",
      "Iteration: 541/1485.\n",
      "Iteration: 542/1485.\n",
      "Iteration: 543/1485.\n",
      "Iteration: 544/1485.\n",
      "Iteration: 545/1485.\n",
      "Iteration: 546/1485.\n",
      "Iteration: 547/1485.\n",
      "Iteration: 548/1485.\n",
      "Iteration: 549/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.1267160992730096, 1.1400958842693691, 1.13074444555759]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.556, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 550/1485.\n",
      "Iteration: 551/1485.\n",
      "Iteration: 552/1485.\n",
      "Iteration: 553/1485.\n",
      "Iteration: 554/1485.\n",
      "Iteration: 555/1485.\n",
      "Iteration: 556/1485.\n",
      "Iteration: 557/1485.\n",
      "Iteration: 558/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.2341733710029956, 1.2046994343587067, 0.9662468652539375]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.604, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 559/1485.\n",
      "Iteration: 560/1485.\n",
      "Iteration: 561/1485.\n",
      "Iteration: 562/1485.\n",
      "Iteration: 563/1485.\n",
      "Iteration: 564/1485.\n",
      "Iteration: 565/1485.\n",
      "Iteration: 566/1485.\n",
      "Iteration: 567/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.249748282710797, 1.135897880090176, 1.2412037127888058]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.558, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 568/1485.\n",
      "Iteration: 569/1485.\n",
      "Iteration: 570/1485.\n",
      "Iteration: 571/1485.\n",
      "Iteration: 572/1485.\n",
      "Iteration: 573/1485.\n",
      "Iteration: 574/1485.\n",
      "Iteration: 575/1485.\n",
      "Iteration: 576/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.211678408672611, 1.222878851103086, 1.0023812068772149]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.558, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 577/1485.\n",
      "Iteration: 578/1485.\n",
      "Iteration: 579/1485.\n",
      "Iteration: 580/1485.\n",
      "Iteration: 581/1485.\n",
      "Iteration: 582/1485.\n",
      "Iteration: 583/1485.\n",
      "Iteration: 584/1485.\n",
      "Iteration: 585/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.1037405729217498, 1.111138922557626, 0.9240662245308588]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.62, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 586/1485.\n",
      "Iteration: 587/1485.\n",
      "Iteration: 588/1485.\n",
      "Iteration: 589/1485.\n",
      "Iteration: 590/1485.\n",
      "Iteration: 591/1485.\n",
      "Iteration: 592/1485.\n",
      "Iteration: 593/1485.\n",
      "Iteration: 594/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.146010718921043, 1.2925813604142158, 1.1863202021753256]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.596, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 595/1485.\n",
      "Iteration: 596/1485.\n",
      "Iteration: 597/1485.\n",
      "Iteration: 598/1485.\n",
      "Iteration: 599/1485.\n",
      "Iteration: 600/1485.\n",
      "Iteration: 601/1485.\n",
      "Iteration: 602/1485.\n",
      "Iteration: 603/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.0949086888801685, 1.0589493301228727, 1.070757476971296]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.62, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 604/1485.\n",
      "Iteration: 605/1485.\n",
      "Iteration: 606/1485.\n",
      "Iteration: 607/1485.\n",
      "Iteration: 608/1485.\n",
      "Iteration: 609/1485.\n",
      "Iteration: 610/1485.\n",
      "Iteration: 611/1485.\n",
      "Iteration: 612/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.1191296061491045, 1.0267861785901202, 0.9947321823064975]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.632, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 613/1485.\n",
      "Iteration: 614/1485.\n",
      "Iteration: 615/1485.\n",
      "Iteration: 616/1485.\n",
      "Iteration: 617/1485.\n",
      "Iteration: 618/1485.\n",
      "Iteration: 619/1485.\n",
      "Iteration: 620/1485.\n",
      "Iteration: 621/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.0948730742691757, 1.2185792158650868, 1.130258144098088]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.612, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 622/1485.\n",
      "Iteration: 623/1485.\n",
      "Iteration: 624/1485.\n",
      "Iteration: 625/1485.\n",
      "Iteration: 626/1485.\n",
      "Iteration: 627/1485.\n",
      "Iteration: 628/1485.\n",
      "Iteration: 629/1485.\n",
      "Iteration: 630/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.9943180699578246, 0.9976551737402173, 1.1172626037381088]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.618, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 631/1485.\n",
      "Iteration: 632/1485.\n",
      "Iteration: 633/1485.\n",
      "Iteration: 634/1485.\n",
      "Iteration: 635/1485.\n",
      "Iteration: 636/1485.\n",
      "Iteration: 637/1485.\n",
      "Iteration: 638/1485.\n",
      "Iteration: 639/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.9485078017150249, 1.0208657647224164, 1.0676613539598063]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.618, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 640/1485.\n",
      "Iteration: 641/1485.\n",
      "Iteration: 642/1485.\n",
      "Iteration: 643/1485.\n",
      "Iteration: 644/1485.\n",
      "Iteration: 645/1485.\n",
      "Iteration: 646/1485.\n",
      "Iteration: 647/1485.\n",
      "Iteration: 648/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.0656228636513907, 1.042965519410831, 1.0315839401391078]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.656, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 649/1485.\n",
      "Iteration: 650/1485.\n",
      "Iteration: 651/1485.\n",
      "Iteration: 652/1485.\n",
      "Iteration: 653/1485.\n",
      "Iteration: 654/1485.\n",
      "Iteration: 655/1485.\n",
      "Iteration: 656/1485.\n",
      "Iteration: 657/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.9604448640423945, 1.0449662846154, 1.0357843909524826]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.702, Val acc: 0.48\n",
      "\n",
      "\n",
      "Iteration: 658/1485.\n",
      "Iteration: 659/1485.\n",
      "Iteration: 660/1485.\n",
      "Iteration: 661/1485.\n",
      "Iteration: 662/1485.\n",
      "Iteration: 663/1485.\n",
      "Iteration: 664/1485.\n",
      "Iteration: 665/1485.\n",
      "Iteration: 666/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.9803840686984097, 0.8857838314317129, 1.060857478205775]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.656, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 667/1485.\n",
      "Iteration: 668/1485.\n",
      "Iteration: 669/1485.\n",
      "Iteration: 670/1485.\n",
      "Iteration: 671/1485.\n",
      "Iteration: 672/1485.\n",
      "Iteration: 673/1485.\n",
      "Iteration: 674/1485.\n",
      "Iteration: 675/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.1468687175588475, 1.003544743586781, 1.0083159208740018]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.628, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 676/1485.\n",
      "Iteration: 677/1485.\n",
      "Iteration: 678/1485.\n",
      "Iteration: 679/1485.\n",
      "Iteration: 680/1485.\n",
      "Iteration: 681/1485.\n",
      "Iteration: 682/1485.\n",
      "Iteration: 683/1485.\n",
      "Iteration: 684/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.0656151421251203, 1.012316699052396, 0.8984497947239687]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.66, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 685/1485.\n",
      "Iteration: 686/1485.\n",
      "Iteration: 687/1485.\n",
      "Iteration: 688/1485.\n",
      "Iteration: 689/1485.\n",
      "Iteration: 690/1485.\n",
      "Iteration: 691/1485.\n",
      "Iteration: 692/1485.\n",
      "Iteration: 693/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.9852211244469732, 1.0840101205909385, 0.9952848007191183]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.668, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 694/1485.\n",
      "Iteration: 695/1485.\n",
      "Iteration: 696/1485.\n",
      "Iteration: 697/1485.\n",
      "Iteration: 698/1485.\n",
      "Iteration: 699/1485.\n",
      "Iteration: 700/1485.\n",
      "Iteration: 701/1485.\n",
      "Iteration: 702/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.0365149105522087, 0.8806683279424102, 0.86528536021004]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.674, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 703/1485.\n",
      "Iteration: 704/1485.\n",
      "Iteration: 705/1485.\n",
      "Iteration: 706/1485.\n",
      "Iteration: 707/1485.\n",
      "Iteration: 708/1485.\n",
      "Iteration: 709/1485.\n",
      "Iteration: 710/1485.\n",
      "Iteration: 711/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.9134708602054028, 0.8236474112723545, 0.8205720086756348]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.66, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 712/1485.\n",
      "Iteration: 713/1485.\n",
      "Iteration: 714/1485.\n",
      "Iteration: 715/1485.\n",
      "Iteration: 716/1485.\n",
      "Iteration: 717/1485.\n",
      "Iteration: 718/1485.\n",
      "Iteration: 719/1485.\n",
      "Iteration: 720/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.7971025134122192, 1.0040300839879088, 0.8035392661642812]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.65, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 721/1485.\n",
      "Iteration: 722/1485.\n",
      "Iteration: 723/1485.\n",
      "Iteration: 724/1485.\n",
      "Iteration: 725/1485.\n",
      "Iteration: 726/1485.\n",
      "Iteration: 727/1485.\n",
      "Iteration: 728/1485.\n",
      "Iteration: 729/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [1.0024616618669513, 0.8472037326095506, 0.7963440125124153]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.682, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 730/1485.\n",
      "Iteration: 731/1485.\n",
      "Iteration: 732/1485.\n",
      "Iteration: 733/1485.\n",
      "Iteration: 734/1485.\n",
      "Iteration: 735/1485.\n",
      "Iteration: 736/1485.\n",
      "Iteration: 737/1485.\n",
      "Iteration: 738/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.990779673791687, 0.889940291537839, 0.9468826055481563]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.708, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 739/1485.\n",
      "Iteration: 740/1485.\n",
      "Iteration: 741/1485.\n",
      "Iteration: 742/1485.\n",
      "Iteration: 743/1485.\n",
      "Iteration: 744/1485.\n",
      "Iteration: 745/1485.\n",
      "Iteration: 746/1485.\n",
      "Iteration: 747/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.829405783235664, 0.7065788341701563, 0.9442489232172138]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.676, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 748/1485.\n",
      "Iteration: 749/1485.\n",
      "Iteration: 750/1485.\n",
      "Iteration: 751/1485.\n",
      "Iteration: 752/1485.\n",
      "Iteration: 753/1485.\n",
      "Iteration: 754/1485.\n",
      "Iteration: 755/1485.\n",
      "Iteration: 756/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.8385820631248292, 0.8951693171389116, 0.9090982848223982]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.724, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 757/1485.\n",
      "Iteration: 758/1485.\n",
      "Iteration: 759/1485.\n",
      "Iteration: 760/1485.\n",
      "Iteration: 761/1485.\n",
      "Iteration: 762/1485.\n",
      "Iteration: 763/1485.\n",
      "Iteration: 764/1485.\n",
      "Iteration: 765/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.8511707017642838, 0.8257658517229252, 0.8664482123777089]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.71, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 766/1485.\n",
      "Iteration: 767/1485.\n",
      "Iteration: 768/1485.\n",
      "Iteration: 769/1485.\n",
      "Iteration: 770/1485.\n",
      "Iteration: 771/1485.\n",
      "Iteration: 772/1485.\n",
      "Iteration: 773/1485.\n",
      "Iteration: 774/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.7691527285591147, 0.8234677866803672, 0.7716413118627038]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.698, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 775/1485.\n",
      "Iteration: 776/1485.\n",
      "Iteration: 777/1485.\n",
      "Iteration: 778/1485.\n",
      "Iteration: 779/1485.\n",
      "Iteration: 780/1485.\n",
      "Iteration: 781/1485.\n",
      "Iteration: 782/1485.\n",
      "Iteration: 783/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.8543068253900898, 0.7224571532478231, 0.781510954948968]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.668, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 784/1485.\n",
      "Iteration: 785/1485.\n",
      "Iteration: 786/1485.\n",
      "Iteration: 787/1485.\n",
      "Iteration: 788/1485.\n",
      "Iteration: 789/1485.\n",
      "Iteration: 790/1485.\n",
      "Iteration: 791/1485.\n",
      "Iteration: 792/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.8710878185601953, 0.8436627985084169, 0.8637733139766336]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.68, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 793/1485.\n",
      "Iteration: 794/1485.\n",
      "Iteration: 795/1485.\n",
      "Iteration: 796/1485.\n",
      "Iteration: 797/1485.\n",
      "Iteration: 798/1485.\n",
      "Iteration: 799/1485.\n",
      "Iteration: 800/1485.\n",
      "Iteration: 801/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.9483714416887594, 0.9745051087039444, 0.7793374158318217]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.674, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 802/1485.\n",
      "Iteration: 803/1485.\n",
      "Iteration: 804/1485.\n",
      "Iteration: 805/1485.\n",
      "Iteration: 806/1485.\n",
      "Iteration: 807/1485.\n",
      "Iteration: 808/1485.\n",
      "Iteration: 809/1485.\n",
      "Iteration: 810/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.8811780122646536, 0.7609828072439478, 0.9677336145208797]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.714, Val acc: 0.48\n",
      "\n",
      "\n",
      "Iteration: 811/1485.\n",
      "Iteration: 812/1485.\n",
      "Iteration: 813/1485.\n",
      "Iteration: 814/1485.\n",
      "Iteration: 815/1485.\n",
      "Iteration: 816/1485.\n",
      "Iteration: 817/1485.\n",
      "Iteration: 818/1485.\n",
      "Iteration: 819/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.7292284184958077, 0.7738706846897734, 0.8588605604335005]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.718, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 820/1485.\n",
      "Iteration: 821/1485.\n",
      "Iteration: 822/1485.\n",
      "Iteration: 823/1485.\n",
      "Iteration: 824/1485.\n",
      "Iteration: 825/1485.\n",
      "Iteration: 826/1485.\n",
      "Iteration: 827/1485.\n",
      "Iteration: 828/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.8277842121791058, 0.7494479977309823, 0.7232736811839745]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.732, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 829/1485.\n",
      "Iteration: 830/1485.\n",
      "Iteration: 831/1485.\n",
      "Iteration: 832/1485.\n",
      "Iteration: 833/1485.\n",
      "Iteration: 834/1485.\n",
      "Iteration: 835/1485.\n",
      "Iteration: 836/1485.\n",
      "Iteration: 837/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.7432138557866199, 0.7728785129303393, 0.8689219579448553]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.768, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 838/1485.\n",
      "Iteration: 839/1485.\n",
      "Iteration: 840/1485.\n",
      "Iteration: 841/1485.\n",
      "Iteration: 842/1485.\n",
      "Iteration: 843/1485.\n",
      "Iteration: 844/1485.\n",
      "Iteration: 845/1485.\n",
      "Iteration: 846/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.7166693174716485, 0.7848041580906941, 0.6597503447114026]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.776, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 847/1485.\n",
      "Iteration: 848/1485.\n",
      "Iteration: 849/1485.\n",
      "Iteration: 850/1485.\n",
      "Iteration: 851/1485.\n",
      "Iteration: 852/1485.\n",
      "Iteration: 853/1485.\n",
      "Iteration: 854/1485.\n",
      "Iteration: 855/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.7888092410834585, 0.730711494487034, 0.6309199694929973]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.78, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 856/1485.\n",
      "Iteration: 857/1485.\n",
      "Iteration: 858/1485.\n",
      "Iteration: 859/1485.\n",
      "Iteration: 860/1485.\n",
      "Iteration: 861/1485.\n",
      "Iteration: 862/1485.\n",
      "Iteration: 863/1485.\n",
      "Iteration: 864/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.7236738066904794, 0.8292914946517824, 0.48838993462884545]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.762, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 865/1485.\n",
      "Iteration: 866/1485.\n",
      "Iteration: 867/1485.\n",
      "Iteration: 868/1485.\n",
      "Iteration: 869/1485.\n",
      "Iteration: 870/1485.\n",
      "Iteration: 871/1485.\n",
      "Iteration: 872/1485.\n",
      "Iteration: 873/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.7330299587046812, 0.8122495976804784, 0.6113361789365914]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.704, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 874/1485.\n",
      "Iteration: 875/1485.\n",
      "Iteration: 876/1485.\n",
      "Iteration: 877/1485.\n",
      "Iteration: 878/1485.\n",
      "Iteration: 879/1485.\n",
      "Iteration: 880/1485.\n",
      "Iteration: 881/1485.\n",
      "Iteration: 882/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.5694894855946232, 0.6296244413273733, 0.551845772795903]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.768, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 883/1485.\n",
      "Iteration: 884/1485.\n",
      "Iteration: 885/1485.\n",
      "Iteration: 886/1485.\n",
      "Iteration: 887/1485.\n",
      "Iteration: 888/1485.\n",
      "Iteration: 889/1485.\n",
      "Iteration: 890/1485.\n",
      "Iteration: 891/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.7522883330446922, 0.63649906579984, 0.8495135688841582]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.708, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 892/1485.\n",
      "Iteration: 893/1485.\n",
      "Iteration: 894/1485.\n",
      "Iteration: 895/1485.\n",
      "Iteration: 896/1485.\n",
      "Iteration: 897/1485.\n",
      "Iteration: 898/1485.\n",
      "Iteration: 899/1485.\n",
      "Iteration: 900/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.5466604890058117, 0.6320458360641605, 0.6801663897710846]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.758, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 901/1485.\n",
      "Iteration: 902/1485.\n",
      "Iteration: 903/1485.\n",
      "Iteration: 904/1485.\n",
      "Iteration: 905/1485.\n",
      "Iteration: 906/1485.\n",
      "Iteration: 907/1485.\n",
      "Iteration: 908/1485.\n",
      "Iteration: 909/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.6166734651207453, 0.7679840834711869, 0.7371977535276794]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.786, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 910/1485.\n",
      "Iteration: 911/1485.\n",
      "Iteration: 912/1485.\n",
      "Iteration: 913/1485.\n",
      "Iteration: 914/1485.\n",
      "Iteration: 915/1485.\n",
      "Iteration: 916/1485.\n",
      "Iteration: 917/1485.\n",
      "Iteration: 918/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.7138587861163777, 0.5755839654921183, 0.6559265178234565]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.736, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 919/1485.\n",
      "Iteration: 920/1485.\n",
      "Iteration: 921/1485.\n",
      "Iteration: 922/1485.\n",
      "Iteration: 923/1485.\n",
      "Iteration: 924/1485.\n",
      "Iteration: 925/1485.\n",
      "Iteration: 926/1485.\n",
      "Iteration: 927/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.6393861017806073, 0.6612978372697538, 0.6606804470817629]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.8, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 928/1485.\n",
      "Iteration: 929/1485.\n",
      "Iteration: 930/1485.\n",
      "Iteration: 931/1485.\n",
      "Iteration: 932/1485.\n",
      "Iteration: 933/1485.\n",
      "Iteration: 934/1485.\n",
      "Iteration: 935/1485.\n",
      "Iteration: 936/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.5915629975582661, 0.6742430769185893, 0.63819188037015]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.794, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 937/1485.\n",
      "Iteration: 938/1485.\n",
      "Iteration: 939/1485.\n",
      "Iteration: 940/1485.\n",
      "Iteration: 941/1485.\n",
      "Iteration: 942/1485.\n",
      "Iteration: 943/1485.\n",
      "Iteration: 944/1485.\n",
      "Iteration: 945/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.6737509497364442, 0.6178428933844122, 0.5457645134567659]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.792, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 946/1485.\n",
      "Iteration: 947/1485.\n",
      "Iteration: 948/1485.\n",
      "Iteration: 949/1485.\n",
      "Iteration: 950/1485.\n",
      "Iteration: 951/1485.\n",
      "Iteration: 952/1485.\n",
      "Iteration: 953/1485.\n",
      "Iteration: 954/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.5626155936706434, 0.561765346193188, 0.5208326743925482]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.824, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 955/1485.\n",
      "Iteration: 956/1485.\n",
      "Iteration: 957/1485.\n",
      "Iteration: 958/1485.\n",
      "Iteration: 959/1485.\n",
      "Iteration: 960/1485.\n",
      "Iteration: 961/1485.\n",
      "Iteration: 962/1485.\n",
      "Iteration: 963/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.7275829442150231, 0.51904080328298, 0.6714247885649391]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.81, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 964/1485.\n",
      "Iteration: 965/1485.\n",
      "Iteration: 966/1485.\n",
      "Iteration: 967/1485.\n",
      "Iteration: 968/1485.\n",
      "Iteration: 969/1485.\n",
      "Iteration: 970/1485.\n",
      "Iteration: 971/1485.\n",
      "Iteration: 972/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.4398557859602768, 0.49315602466251685, 0.593904249558505]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.776, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 973/1485.\n",
      "Iteration: 974/1485.\n",
      "Iteration: 975/1485.\n",
      "Iteration: 976/1485.\n",
      "Iteration: 977/1485.\n",
      "Iteration: 978/1485.\n",
      "Iteration: 979/1485.\n",
      "Iteration: 980/1485.\n",
      "Iteration: 981/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.482198526224888, 0.5645935101824181, 0.5746595555020616]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.78, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 982/1485.\n",
      "Iteration: 983/1485.\n",
      "Iteration: 984/1485.\n",
      "Iteration: 985/1485.\n",
      "Iteration: 986/1485.\n",
      "Iteration: 987/1485.\n",
      "Iteration: 988/1485.\n",
      "Iteration: 989/1485.\n",
      "Iteration: 990/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.6719377461519429, 0.5849651506937414, 0.5695532254818724]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.858, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 991/1485.\n",
      "Iteration: 992/1485.\n",
      "Iteration: 993/1485.\n",
      "Iteration: 994/1485.\n",
      "Iteration: 995/1485.\n",
      "Iteration: 996/1485.\n",
      "Iteration: 997/1485.\n",
      "Iteration: 998/1485.\n",
      "Iteration: 999/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.5570296958220764, 0.48312723884789904, 0.4939931910704119]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.822, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 1000/1485.\n",
      "Iteration: 1001/1485.\n",
      "Iteration: 1002/1485.\n",
      "Iteration: 1003/1485.\n",
      "Iteration: 1004/1485.\n",
      "Iteration: 1005/1485.\n",
      "Iteration: 1006/1485.\n",
      "Iteration: 1007/1485.\n",
      "Iteration: 1008/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.45356711784474874, 0.5209350982920619, 0.4227131585657221]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.846, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 1009/1485.\n",
      "Iteration: 1010/1485.\n",
      "Iteration: 1011/1485.\n",
      "Iteration: 1012/1485.\n",
      "Iteration: 1013/1485.\n",
      "Iteration: 1014/1485.\n",
      "Iteration: 1015/1485.\n",
      "Iteration: 1016/1485.\n",
      "Iteration: 1017/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.5610060234508999, 0.4844804103380227, 0.5401701364923089]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.858, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 1018/1485.\n",
      "Iteration: 1019/1485.\n",
      "Iteration: 1020/1485.\n",
      "Iteration: 1021/1485.\n",
      "Iteration: 1022/1485.\n",
      "Iteration: 1023/1485.\n",
      "Iteration: 1024/1485.\n",
      "Iteration: 1025/1485.\n",
      "Iteration: 1026/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.44389933301199513, 0.5440972704218915, 0.5487533529744714]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.86, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 1027/1485.\n",
      "Iteration: 1028/1485.\n",
      "Iteration: 1029/1485.\n",
      "Iteration: 1030/1485.\n",
      "Iteration: 1031/1485.\n",
      "Iteration: 1032/1485.\n",
      "Iteration: 1033/1485.\n",
      "Iteration: 1034/1485.\n",
      "Iteration: 1035/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.509997457743605, 0.5720763693963832, 0.48403235737954625]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.85, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 1036/1485.\n",
      "Iteration: 1037/1485.\n",
      "Iteration: 1038/1485.\n",
      "Iteration: 1039/1485.\n",
      "Iteration: 1040/1485.\n",
      "Iteration: 1041/1485.\n",
      "Iteration: 1042/1485.\n",
      "Iteration: 1043/1485.\n",
      "Iteration: 1044/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.4609768063248101, 0.5886646347026596, 0.5373786533620366]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.834, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1045/1485.\n",
      "Iteration: 1046/1485.\n",
      "Iteration: 1047/1485.\n",
      "Iteration: 1048/1485.\n",
      "Iteration: 1049/1485.\n",
      "Iteration: 1050/1485.\n",
      "Iteration: 1051/1485.\n",
      "Iteration: 1052/1485.\n",
      "Iteration: 1053/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.5216550882109062, 0.4859431890545514, 0.3922449099932531]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.888, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1054/1485.\n",
      "Iteration: 1055/1485.\n",
      "Iteration: 1056/1485.\n",
      "Iteration: 1057/1485.\n",
      "Iteration: 1058/1485.\n",
      "Iteration: 1059/1485.\n",
      "Iteration: 1060/1485.\n",
      "Iteration: 1061/1485.\n",
      "Iteration: 1062/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.49524966628447803, 0.4455382045578092, 0.5977788157376872]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.854, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1063/1485.\n",
      "Iteration: 1064/1485.\n",
      "Iteration: 1065/1485.\n",
      "Iteration: 1066/1485.\n",
      "Iteration: 1067/1485.\n",
      "Iteration: 1068/1485.\n",
      "Iteration: 1069/1485.\n",
      "Iteration: 1070/1485.\n",
      "Iteration: 1071/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.5196229030456708, 0.6423168003131201, 0.4915452838574387]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.836, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 1072/1485.\n",
      "Iteration: 1073/1485.\n",
      "Iteration: 1074/1485.\n",
      "Iteration: 1075/1485.\n",
      "Iteration: 1076/1485.\n",
      "Iteration: 1077/1485.\n",
      "Iteration: 1078/1485.\n",
      "Iteration: 1079/1485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1080/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.597757804853578, 0.4454809013255588, 0.41749899743760815]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.856, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 1081/1485.\n",
      "Iteration: 1082/1485.\n",
      "Iteration: 1083/1485.\n",
      "Iteration: 1084/1485.\n",
      "Iteration: 1085/1485.\n",
      "Iteration: 1086/1485.\n",
      "Iteration: 1087/1485.\n",
      "Iteration: 1088/1485.\n",
      "Iteration: 1089/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.4894649258881327, 0.5545309315231652, 0.4379756457468198]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.852, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 1090/1485.\n",
      "Iteration: 1091/1485.\n",
      "Iteration: 1092/1485.\n",
      "Iteration: 1093/1485.\n",
      "Iteration: 1094/1485.\n",
      "Iteration: 1095/1485.\n",
      "Iteration: 1096/1485.\n",
      "Iteration: 1097/1485.\n",
      "Iteration: 1098/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.333048448181901, 0.4574057608879252, 0.4231762901254754]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.892, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1099/1485.\n",
      "Iteration: 1100/1485.\n",
      "Iteration: 1101/1485.\n",
      "Iteration: 1102/1485.\n",
      "Iteration: 1103/1485.\n",
      "Iteration: 1104/1485.\n",
      "Iteration: 1105/1485.\n",
      "Iteration: 1106/1485.\n",
      "Iteration: 1107/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.47466524482726075, 0.4666838039137128, 0.5033207276241456]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.878, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1108/1485.\n",
      "Iteration: 1109/1485.\n",
      "Iteration: 1110/1485.\n",
      "Iteration: 1111/1485.\n",
      "Iteration: 1112/1485.\n",
      "Iteration: 1113/1485.\n",
      "Iteration: 1114/1485.\n",
      "Iteration: 1115/1485.\n",
      "Iteration: 1116/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.4253343738881752, 0.39536935445058496, 0.37382956668603223]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.89, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1117/1485.\n",
      "Iteration: 1118/1485.\n",
      "Iteration: 1119/1485.\n",
      "Iteration: 1120/1485.\n",
      "Iteration: 1121/1485.\n",
      "Iteration: 1122/1485.\n",
      "Iteration: 1123/1485.\n",
      "Iteration: 1124/1485.\n",
      "Iteration: 1125/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.3296760458980847, 0.4663628840386837, 0.3756291266894616]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.862, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1126/1485.\n",
      "Iteration: 1127/1485.\n",
      "Iteration: 1128/1485.\n",
      "Iteration: 1129/1485.\n",
      "Iteration: 1130/1485.\n",
      "Iteration: 1131/1485.\n",
      "Iteration: 1132/1485.\n",
      "Iteration: 1133/1485.\n",
      "Iteration: 1134/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.3736068839955066, 0.435754340369972, 0.3520750940321704]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.87, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1135/1485.\n",
      "Iteration: 1136/1485.\n",
      "Iteration: 1137/1485.\n",
      "Iteration: 1138/1485.\n",
      "Iteration: 1139/1485.\n",
      "Iteration: 1140/1485.\n",
      "Iteration: 1141/1485.\n",
      "Iteration: 1142/1485.\n",
      "Iteration: 1143/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.2910387503155714, 0.3304147214549478, 0.43265186887370666]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.86, Val acc: 0.5\n",
      "\n",
      "\n",
      "Iteration: 1144/1485.\n",
      "Iteration: 1145/1485.\n",
      "Iteration: 1146/1485.\n",
      "Iteration: 1147/1485.\n",
      "Iteration: 1148/1485.\n",
      "Iteration: 1149/1485.\n",
      "Iteration: 1150/1485.\n",
      "Iteration: 1151/1485.\n",
      "Iteration: 1152/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.41019017782292366, 0.33773845645978173, 0.32981789697593145]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.866, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 1153/1485.\n",
      "Iteration: 1154/1485.\n",
      "Iteration: 1155/1485.\n",
      "Iteration: 1156/1485.\n",
      "Iteration: 1157/1485.\n",
      "Iteration: 1158/1485.\n",
      "Iteration: 1159/1485.\n",
      "Iteration: 1160/1485.\n",
      "Iteration: 1161/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.29565176099255375, 0.2535807739368085, 0.36847523584250214]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.894, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1162/1485.\n",
      "Iteration: 1163/1485.\n",
      "Iteration: 1164/1485.\n",
      "Iteration: 1165/1485.\n",
      "Iteration: 1166/1485.\n",
      "Iteration: 1167/1485.\n",
      "Iteration: 1168/1485.\n",
      "Iteration: 1169/1485.\n",
      "Iteration: 1170/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.39209149470317606, 0.38677939927708116, 0.3292832533809392]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.896, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1171/1485.\n",
      "Iteration: 1172/1485.\n",
      "Iteration: 1173/1485.\n",
      "Iteration: 1174/1485.\n",
      "Iteration: 1175/1485.\n",
      "Iteration: 1176/1485.\n",
      "Iteration: 1177/1485.\n",
      "Iteration: 1178/1485.\n",
      "Iteration: 1179/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.3529336319348774, 0.3414934953628074, 0.31860310869969843]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.876, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1180/1485.\n",
      "Iteration: 1181/1485.\n",
      "Iteration: 1182/1485.\n",
      "Iteration: 1183/1485.\n",
      "Iteration: 1184/1485.\n",
      "Iteration: 1185/1485.\n",
      "Iteration: 1186/1485.\n",
      "Iteration: 1187/1485.\n",
      "Iteration: 1188/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.35102480691640187, 0.3599553550546024, 0.31312864274137064]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.932, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 1189/1485.\n",
      "Iteration: 1190/1485.\n",
      "Iteration: 1191/1485.\n",
      "Iteration: 1192/1485.\n",
      "Iteration: 1193/1485.\n",
      "Iteration: 1194/1485.\n",
      "Iteration: 1195/1485.\n",
      "Iteration: 1196/1485.\n",
      "Iteration: 1197/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.2669387481090374, 0.2779661505830089, 0.3547996756841779]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.898, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 1198/1485.\n",
      "Iteration: 1199/1485.\n",
      "Iteration: 1200/1485.\n",
      "Iteration: 1201/1485.\n",
      "Iteration: 1202/1485.\n",
      "Iteration: 1203/1485.\n",
      "Iteration: 1204/1485.\n",
      "Iteration: 1205/1485.\n",
      "Iteration: 1206/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.3211161011964911, 0.31770172684457704, 0.37694802843127684]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.912, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 1207/1485.\n",
      "Iteration: 1208/1485.\n",
      "Iteration: 1209/1485.\n",
      "Iteration: 1210/1485.\n",
      "Iteration: 1211/1485.\n",
      "Iteration: 1212/1485.\n",
      "Iteration: 1213/1485.\n",
      "Iteration: 1214/1485.\n",
      "Iteration: 1215/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.25139493176941313, 0.2436330900591637, 0.28638393694982284]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.922, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 1216/1485.\n",
      "Iteration: 1217/1485.\n",
      "Iteration: 1218/1485.\n",
      "Iteration: 1219/1485.\n",
      "Iteration: 1220/1485.\n",
      "Iteration: 1221/1485.\n",
      "Iteration: 1222/1485.\n",
      "Iteration: 1223/1485.\n",
      "Iteration: 1224/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.2735090528055463, 0.3673915149922737, 0.38406558127216167]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.94, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 1225/1485.\n",
      "Iteration: 1226/1485.\n",
      "Iteration: 1227/1485.\n",
      "Iteration: 1228/1485.\n",
      "Iteration: 1229/1485.\n",
      "Iteration: 1230/1485.\n",
      "Iteration: 1231/1485.\n",
      "Iteration: 1232/1485.\n",
      "Iteration: 1233/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.38872444340650725, 0.24918925239729184, 0.28769010665060324]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.926, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 1234/1485.\n",
      "Iteration: 1235/1485.\n",
      "Iteration: 1236/1485.\n",
      "Iteration: 1237/1485.\n",
      "Iteration: 1238/1485.\n",
      "Iteration: 1239/1485.\n",
      "Iteration: 1240/1485.\n",
      "Iteration: 1241/1485.\n",
      "Iteration: 1242/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.3142220869441442, 0.3137979418136054, 0.2932627144611744]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.884, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 1243/1485.\n",
      "Iteration: 1244/1485.\n",
      "Iteration: 1245/1485.\n",
      "Iteration: 1246/1485.\n",
      "Iteration: 1247/1485.\n",
      "Iteration: 1248/1485.\n",
      "Iteration: 1249/1485.\n",
      "Iteration: 1250/1485.\n",
      "Iteration: 1251/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.20974535169951553, 0.3342446934781211, 0.36340595376568435]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.938, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 1252/1485.\n",
      "Iteration: 1253/1485.\n",
      "Iteration: 1254/1485.\n",
      "Iteration: 1255/1485.\n",
      "Iteration: 1256/1485.\n",
      "Iteration: 1257/1485.\n",
      "Iteration: 1258/1485.\n",
      "Iteration: 1259/1485.\n",
      "Iteration: 1260/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.2433959696227239, 0.22945371792208086, 0.2840396393143924]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.934, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 1261/1485.\n",
      "Iteration: 1262/1485.\n",
      "Iteration: 1263/1485.\n",
      "Iteration: 1264/1485.\n",
      "Iteration: 1265/1485.\n",
      "Iteration: 1266/1485.\n",
      "Iteration: 1267/1485.\n",
      "Iteration: 1268/1485.\n",
      "Iteration: 1269/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.25014798417855666, 0.2405539424082836, 0.23466927658014608]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.91, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 1270/1485.\n",
      "Iteration: 1271/1485.\n",
      "Iteration: 1272/1485.\n",
      "Iteration: 1273/1485.\n",
      "Iteration: 1274/1485.\n",
      "Iteration: 1275/1485.\n",
      "Iteration: 1276/1485.\n",
      "Iteration: 1277/1485.\n",
      "Iteration: 1278/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.27646271600579403, 0.26523497512158406, 0.24958951038605193]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.93, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 1279/1485.\n",
      "Iteration: 1280/1485.\n",
      "Iteration: 1281/1485.\n",
      "Iteration: 1282/1485.\n",
      "Iteration: 1283/1485.\n",
      "Iteration: 1284/1485.\n",
      "Iteration: 1285/1485.\n",
      "Iteration: 1286/1485.\n",
      "Iteration: 1287/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.1795963241404267, 0.3066462621580255, 0.27770284711335685]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.94, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 1288/1485.\n",
      "Iteration: 1289/1485.\n",
      "Iteration: 1290/1485.\n",
      "Iteration: 1291/1485.\n",
      "Iteration: 1292/1485.\n",
      "Iteration: 1293/1485.\n",
      "Iteration: 1294/1485.\n",
      "Iteration: 1295/1485.\n",
      "Iteration: 1296/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.21884151614520897, 0.17542710064226305, 0.17479159368838856]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.936, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 1297/1485.\n",
      "Iteration: 1298/1485.\n",
      "Iteration: 1299/1485.\n",
      "Iteration: 1300/1485.\n",
      "Iteration: 1301/1485.\n",
      "Iteration: 1302/1485.\n",
      "Iteration: 1303/1485.\n",
      "Iteration: 1304/1485.\n",
      "Iteration: 1305/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.27639663482162635, 0.21194269030269522, 0.2390874168278842]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.938, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 1306/1485.\n",
      "Iteration: 1307/1485.\n",
      "Iteration: 1308/1485.\n",
      "Iteration: 1309/1485.\n",
      "Iteration: 1310/1485.\n",
      "Iteration: 1311/1485.\n",
      "Iteration: 1312/1485.\n",
      "Iteration: 1313/1485.\n",
      "Iteration: 1314/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.21456424354189094, 0.1963914608953894, 0.24355807949539038]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.94, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1315/1485.\n",
      "Iteration: 1316/1485.\n",
      "Iteration: 1317/1485.\n",
      "Iteration: 1318/1485.\n",
      "Iteration: 1319/1485.\n",
      "Iteration: 1320/1485.\n",
      "Iteration: 1321/1485.\n",
      "Iteration: 1322/1485.\n",
      "Iteration: 1323/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.18689627793909835, 0.23428435529255529, 0.29107746722434324]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.898, Val acc: 0.48\n",
      "\n",
      "\n",
      "Iteration: 1324/1485.\n",
      "Iteration: 1325/1485.\n",
      "Iteration: 1326/1485.\n",
      "Iteration: 1327/1485.\n",
      "Iteration: 1328/1485.\n",
      "Iteration: 1329/1485.\n",
      "Iteration: 1330/1485.\n",
      "Iteration: 1331/1485.\n",
      "Iteration: 1332/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.22776680486540418, 0.30061751381247237, 0.14715863790208347]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.926, Val acc: 0.48\n",
      "\n",
      "\n",
      "Iteration: 1333/1485.\n",
      "Iteration: 1334/1485.\n",
      "Iteration: 1335/1485.\n",
      "Iteration: 1336/1485.\n",
      "Iteration: 1337/1485.\n",
      "Iteration: 1338/1485.\n",
      "Iteration: 1339/1485.\n",
      "Iteration: 1340/1485.\n",
      "Iteration: 1341/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.17633420188595852, 0.2210595298214066, 0.15192512970603034]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.926, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1342/1485.\n",
      "Iteration: 1343/1485.\n",
      "Iteration: 1344/1485.\n",
      "Iteration: 1345/1485.\n",
      "Iteration: 1346/1485.\n",
      "Iteration: 1347/1485.\n",
      "Iteration: 1348/1485.\n",
      "Iteration: 1349/1485.\n",
      "Iteration: 1350/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.24971543261120438, 0.24004524503978594, 0.23761647275597825]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.93, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 1351/1485.\n",
      "Iteration: 1352/1485.\n",
      "Iteration: 1353/1485.\n",
      "Iteration: 1354/1485.\n",
      "Iteration: 1355/1485.\n",
      "Iteration: 1356/1485.\n",
      "Iteration: 1357/1485.\n",
      "Iteration: 1358/1485.\n",
      "Iteration: 1359/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.22182278346416098, 0.23608128790237548, 0.12163474483409374]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.958, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1360/1485.\n",
      "Iteration: 1361/1485.\n",
      "Iteration: 1362/1485.\n",
      "Iteration: 1363/1485.\n",
      "Iteration: 1364/1485.\n",
      "Iteration: 1365/1485.\n",
      "Iteration: 1366/1485.\n",
      "Iteration: 1367/1485.\n",
      "Iteration: 1368/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.16709663136667988, 0.14834377135045662, 0.16731836518825513]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.962, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1369/1485.\n",
      "Iteration: 1370/1485.\n",
      "Iteration: 1371/1485.\n",
      "Iteration: 1372/1485.\n",
      "Iteration: 1373/1485.\n",
      "Iteration: 1374/1485.\n",
      "Iteration: 1375/1485.\n",
      "Iteration: 1376/1485.\n",
      "Iteration: 1377/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.1759887102827008, 0.2274115174350917, 0.2314553749946785]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.976, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 1378/1485.\n",
      "Iteration: 1379/1485.\n",
      "Iteration: 1380/1485.\n",
      "Iteration: 1381/1485.\n",
      "Iteration: 1382/1485.\n",
      "Iteration: 1383/1485.\n",
      "Iteration: 1384/1485.\n",
      "Iteration: 1385/1485.\n",
      "Iteration: 1386/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.1496050344799498, 0.21471330432559022, 0.17425624551166458]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.984, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1387/1485.\n",
      "Iteration: 1388/1485.\n",
      "Iteration: 1389/1485.\n",
      "Iteration: 1390/1485.\n",
      "Iteration: 1391/1485.\n",
      "Iteration: 1392/1485.\n",
      "Iteration: 1393/1485.\n",
      "Iteration: 1394/1485.\n",
      "Iteration: 1395/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.14204687547076653, 0.20944343910131583, 0.12245227621408326]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.96, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 1396/1485.\n",
      "Iteration: 1397/1485.\n",
      "Iteration: 1398/1485.\n",
      "Iteration: 1399/1485.\n",
      "Iteration: 1400/1485.\n",
      "Iteration: 1401/1485.\n",
      "Iteration: 1402/1485.\n",
      "Iteration: 1403/1485.\n",
      "Iteration: 1404/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.1304462188220309, 0.18771497966006512, 0.10331244927613216]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.968, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1405/1485.\n",
      "Iteration: 1406/1485.\n",
      "Iteration: 1407/1485.\n",
      "Iteration: 1408/1485.\n",
      "Iteration: 1409/1485.\n",
      "Iteration: 1410/1485.\n",
      "Iteration: 1411/1485.\n",
      "Iteration: 1412/1485.\n",
      "Iteration: 1413/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.17610269983342286, 0.15806332150156618, 0.14106476404492493]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.96, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1414/1485.\n",
      "Iteration: 1415/1485.\n",
      "Iteration: 1416/1485.\n",
      "Iteration: 1417/1485.\n",
      "Iteration: 1418/1485.\n",
      "Iteration: 1419/1485.\n",
      "Iteration: 1420/1485.\n",
      "Iteration: 1421/1485.\n",
      "Iteration: 1422/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.10922279692960447, 0.18756796403568599, 0.13367873207441552]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.968, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1423/1485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1424/1485.\n",
      "Iteration: 1425/1485.\n",
      "Iteration: 1426/1485.\n",
      "Iteration: 1427/1485.\n",
      "Iteration: 1428/1485.\n",
      "Iteration: 1429/1485.\n",
      "Iteration: 1430/1485.\n",
      "Iteration: 1431/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.27367523772900576, 0.12056459082339198, 0.16092063270132598]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.944, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 1432/1485.\n",
      "Iteration: 1433/1485.\n",
      "Iteration: 1434/1485.\n",
      "Iteration: 1435/1485.\n",
      "Iteration: 1436/1485.\n",
      "Iteration: 1437/1485.\n",
      "Iteration: 1438/1485.\n",
      "Iteration: 1439/1485.\n",
      "Iteration: 1440/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.18298421465719136, 0.1340633973249787, 0.1738682657175981]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.976, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1441/1485.\n",
      "Iteration: 1442/1485.\n",
      "Iteration: 1443/1485.\n",
      "Iteration: 1444/1485.\n",
      "Iteration: 1445/1485.\n",
      "Iteration: 1446/1485.\n",
      "Iteration: 1447/1485.\n",
      "Iteration: 1448/1485.\n",
      "Iteration: 1449/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.13901590690342516, 0.16542534226388966, 0.08987231358211759]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.98, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 1450/1485.\n",
      "Iteration: 1451/1485.\n",
      "Iteration: 1452/1485.\n",
      "Iteration: 1453/1485.\n",
      "Iteration: 1454/1485.\n",
      "Iteration: 1455/1485.\n",
      "Iteration: 1456/1485.\n",
      "Iteration: 1457/1485.\n",
      "Iteration: 1458/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.16646123637702434, 0.14314302059925063, 0.12866253592753393]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.978, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 1459/1485.\n",
      "Iteration: 1460/1485.\n",
      "Iteration: 1461/1485.\n",
      "Iteration: 1462/1485.\n",
      "Iteration: 1463/1485.\n",
      "Iteration: 1464/1485.\n",
      "Iteration: 1465/1485.\n",
      "Iteration: 1466/1485.\n",
      "Iteration: 1467/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.13228594983859981, 0.1278524799036681, 0.09407381262195433]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.986, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 1468/1485.\n",
      "Iteration: 1469/1485.\n",
      "Iteration: 1470/1485.\n",
      "Iteration: 1471/1485.\n",
      "Iteration: 1472/1485.\n",
      "Iteration: 1473/1485.\n",
      "Iteration: 1474/1485.\n",
      "Iteration: 1475/1485.\n",
      "Iteration: 1476/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.14353234932056835, 0.17885519609778489, 0.12005765373654745]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.982, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 1477/1485.\n",
      "Iteration: 1478/1485.\n",
      "Iteration: 1479/1485.\n",
      "Iteration: 1480/1485.\n",
      "Iteration: 1481/1485.\n",
      "Iteration: 1482/1485.\n",
      "Iteration: 1483/1485.\n",
      "Iteration: 1484/1485.\n",
      "Iteration: 1485/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024631185454285\n",
      "Loss latest three: [0.08672003502923524, 0.17163332584488075, 0.08293218862576267]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.99, Val acc: 0.44\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------FINAL OUTPUT----------------\n",
      "  Train acc: 0.994, Val acc: 0.44\n",
      "Loss history: [2.3024631185454285, 2.3028111606630963, 2.3023294295762717, 2.3022911663640775, 2.3016427413557845, 2.299137263367542, 2.294601778763012, 2.282298125364221, 2.294678281032607, 2.293058205473805, 2.268410379431634, 2.2564033543168507, 2.278018382367718, 2.2885237243409224, 2.1822662553941066, 2.2727747578789135, 2.208595316937772, 2.195159558508625, 2.217837417325816, 2.1549164241149517, 2.1361344564725395, 2.1217642213072976, 2.228152142311292, 2.1713082264323016, 2.036537010202001, 2.091745432015573, 2.021473753538662, 2.1490247834573126, 2.192274134291311, 2.0282800598089725, 2.1015890331160842, 2.0749310654071906, 2.089843800452035, 2.078980722412962, 1.9485219295705958, 2.049360068545789, 1.9055876011173676, 1.9601726408989886, 2.0233761545629485, 2.088262289171769, 1.9341564335468096, 1.9727862422961497, 1.988761037492356, 2.031935958870427, 1.8523434062750006, 1.8895471622431006, 1.9911876413369163, 1.970792775132136, 1.8203728781255109, 1.8928908359275947, 1.9784929456477651, 1.8364560193205208, 1.9734739488772242, 1.9625493846936863, 1.8468696394287576, 1.9514427070714029, 1.9656734465465753, 1.7988227936680847, 1.8937080098958665, 1.8029932743917212, 1.9208829070215894, 1.9324107130440562, 1.8977416964108889, 1.8877773777256996, 1.952422745694076, 1.7240230360535926, 1.7766748327317532, 1.831272839207164, 1.847726604804757, 1.7734441252840918, 1.6897121893166205, 2.0254373521006257, 1.7780763594886588, 1.8350092841198264, 1.7959136036601369, 1.7823941089799888, 1.8849582850459452, 2.0501478714339174, 1.725106592817923, 1.8648550240998365, 1.8314207932096107, 1.8556231643562981, 1.8890915519718352, 1.7911440008406712, 1.8424615484684426, 1.7201654309008103, 1.870611403236684, 1.9079038279546436, 1.866891083330692, 1.78745841032149, 1.7133705392220073, 1.8011033641078051, 1.784181709052459, 1.811924629343964, 1.7793789897533463, 1.9173278713435558, 1.8792102253888772, 1.7485673100092367, 1.7914777533688002, 1.7929379190938195, 1.7942244198746926, 1.7257361320280566, 1.8362413770220456, 1.8742876542546236, 1.7467304933254455, 1.8118806953246038, 1.7770203074056874, 1.6941629844668398, 1.697510978887131, 1.695146826750969, 1.7462033120948124, 1.7993994771797168, 1.7489410693674166, 1.5474131224465724, 1.6748192438056284, 1.782027155739778, 1.6906995638863358, 1.6855058878168927, 1.7336446857001315, 1.628548285550247, 1.8472116613903484, 1.7964311244339792, 1.7085324661346928, 1.6716590727300586, 1.6586521703069834, 1.6094946504820653, 1.7467836220384392, 1.6649383434947862, 1.7029158961032664, 1.7146766147388608, 1.7860601155133873, 1.7103504210527933, 1.7760282474902465, 1.6653346202934263, 1.6733084779435268, 1.7366663940112448, 1.6082075300581924, 1.6772384959114568, 1.5812898007098226, 1.8607717456623463, 1.5870734263296304, 1.558530729591002, 1.733866480403747, 1.638029435073854, 1.764664965411423, 1.8074454660383559, 1.6079364252035155, 1.7461727821155608, 1.7424809748510046, 1.750747854197374, 1.8913188746703082, 1.6025249658726595, 1.6827321239588784, 1.6949619108468985, 1.7313015876204947, 1.800935551316294, 1.6454878129228885, 1.6649521595515426, 1.7492625491888263, 1.6741812598587915, 1.538431527428379, 1.614734036898915, 1.6784965076895193, 1.6096084401306638, 1.7858282773480088, 1.685950900376659, 1.6184866667266686, 1.6344329372989972, 1.6852584447805865, 1.5935864497824057, 1.5958805650194157, 1.4498362852509945, 1.7048009742893635, 1.7152477914485302, 1.6164019557421967, 1.7028700637823266, 1.618694224801254, 1.6576614504411677, 1.6117451512779213, 1.727666951775964, 1.6370910025574639, 1.5800786193183303, 1.5826265748152852, 1.499867617123098, 1.6211373249728303, 1.7228154643719018, 1.7052412024011703, 1.5583973973197582, 1.5287946956227074, 1.7432561784790932, 1.4903449688216979, 1.6629917671664667, 1.5260601707998689, 1.4695534554608762, 1.5813920877576315, 1.6715366159467617, 1.6284367671244246, 1.662069364162937, 1.623816167287188, 1.591834302896006, 1.700158275714562, 1.5412947267968153, 1.4381205393597365, 1.747036954422787, 1.4974565335626888, 1.6806144523572357, 1.5071321412505085, 1.4878315307432681, 1.4371778252298986, 1.6432021533886054, 1.5658650614909786, 1.5779909974207684, 1.5804991119287477, 1.6864203429442537, 1.518072423329791, 1.6817557580596891, 1.5718559927299764, 1.5499707222202477, 1.434473979619686, 1.6704469625108505, 1.4348634834745824, 1.7101697803951204, 1.7217386862556463, 1.6568159388196477, 1.5954444919452675, 1.500241953846297, 1.6289638167735958, 1.622748589578759, 1.6007029261138594, 1.494663873618208, 1.3535326293958674, 1.4639175711176848, 1.5707056502699643, 1.4183476973081792, 1.5302120822252687, 1.553935428166436, 1.407535183845558, 1.6514540316409116, 1.6764619817504633, 1.5696659363126424, 1.713870277790241, 1.4701614401273264, 1.4624129887189514, 1.4498315928394245, 1.4688269822104087, 1.5203553213453165, 1.4112688736747958, 1.367732361461621, 1.514419610265202, 1.369001262510948, 1.611257136523804, 1.5442046364514561, 1.3878967149810078, 1.6032122609062647, 1.5890126093529164, 1.5391893487647472, 1.5692924492796168, 1.2747591542753343, 1.5756427882273463, 1.534034849424308, 1.4805494931512428, 1.3690297620465581, 1.4612941866853713, 1.4146545626041862, 1.3614163997833146, 1.5636386257386508, 1.3943126105386714, 1.5351978040730634, 1.5623354303771613, 1.4954703295743275, 1.3787823925737153, 1.5350276532160265, 1.4823295937628231, 1.3959040789099684, 1.6511709581363507, 1.4388149910325405, 1.517447720557368, 1.3559487809916986, 1.2652920473139881, 1.4654276187228092, 1.5623556013071414, 1.2352384997083699, 1.574644486339519, 1.5828737501628118, 1.5601528556125084, 1.5523204796342613, 1.3959333169228825, 1.5991618325573287, 1.489556184214177, 1.5112017264334179, 1.5922434207792047, 1.490289335508186, 1.6065465089242899, 1.4624513348472241, 1.4576171119246581, 1.6049197199584118, 1.4192445720314644, 1.405712652718559, 1.3056816241484939, 1.557838144521204, 1.6091815659845736, 1.4129849288084717, 1.3871403747070676, 1.5192132439799582, 1.441139109794347, 1.2996101344242623, 1.5356058312095295, 1.3968396614408625, 1.3549215279804394, 1.5149579319029771, 1.3310427125993787, 1.3531589590164361, 1.500734193525169, 1.5008320160276265, 1.3706860501506994, 1.3097091039266093, 1.4108557427735415, 1.3250125679989557, 1.656956121445445, 1.431057603128117, 1.30297192670006, 1.4505800250494365, 1.384620844821697, 1.3745433744504902, 1.3493257163334478, 1.4991632380625683, 1.4266379766278119, 1.4972411275801387, 1.3573237262985387, 1.4823094545749078, 1.499825428502901, 1.366913828092043, 1.4319303579457392, 1.3857323123800265, 1.315038090583976, 1.4511463230171529, 1.4531758630071097, 1.360132150757032, 1.3853442641761207, 1.5049027859043822, 1.2978613263056236, 1.5206620994458124, 1.3133297267969954, 1.4001251559501287, 1.4337854046860792, 1.2674511887485855, 1.4161835841042776, 1.4360723692970232, 1.4399159378627704, 1.5479422195023025, 1.2091039250152205, 1.531031424581913, 1.3762688576781807, 1.2232796254015088, 1.340757200871796, 1.311544259505815, 1.2668749309532665, 1.3888701703717248, 1.498290293747371, 1.359160416237354, 1.275583112804288, 1.47083827649941, 1.426284554529689, 1.2874630410753378, 1.2571282899191993, 1.1820105953033428, 1.5338307849926287, 1.361879661409273, 1.5885320795924807, 1.4947133852871415, 1.337841070727769, 1.4654267637102247, 1.3007060656672425, 1.394953701989022, 1.2779280292472583, 1.316386525137998, 1.397954810225495, 1.3219124169596603, 1.2576191375539811, 1.133671233400444, 1.4296986843591248, 1.4499059549242659, 1.51874995049805, 1.288350383760493, 1.225029912626833, 1.336402453082213, 1.3757955811978204, 1.30839359855941, 1.2300165742073104, 1.5071349483364744, 1.3682506113808557, 1.333852524432989, 1.5227132250752013, 1.2673657428777017, 1.249724814297281, 1.332538219335769, 1.225552239739446, 1.1693660314852365, 1.292119070561383, 1.206788014519907, 1.3653393960470945, 1.4493853549526443, 1.4122616369983731, 1.4103063290544262, 1.2997217711172337, 1.2385479065228462, 1.2636244003248616, 1.2274694368898258, 1.2648791187421333, 1.331271148002642, 1.2800636208575131, 1.4091845539286316, 1.3783203653425085, 1.1725908290733218, 1.2410883474045316, 1.233961319197256, 1.278790168407911, 1.3057510745951475, 1.4130972302407185, 1.4235853795049778, 1.422164337266596, 1.2812341535401488, 1.2396840964991949, 1.276213884820922, 1.4077117651709339, 1.3989469411503561, 1.2742361060524636, 1.3712632876537076, 1.157384246738578, 1.2763063586405818, 1.1385915556111492, 1.3512381621162037, 1.2073737619744536, 1.4032679264304748, 1.306998096347419, 1.2057626345719001, 1.3185727222654269, 1.2560397382866313, 1.1688046845481652, 1.2900704825762586, 1.3376455695014986, 1.1857420118156552, 1.1842897347863393, 1.2531615018804132, 1.281551936324797, 1.104714964878678, 1.2718881904811183, 1.2542576934087644, 1.1969583705721079, 1.189299563997143, 1.2779424522807945, 1.0922916145261055, 1.367585490745953, 1.1768756083441767, 1.3117430734919717, 1.2330397434649971, 1.2520860034762245, 1.1155807495840413, 1.3361102439983783, 1.2811161972106493, 1.232128757859919, 1.311262641184024, 1.139605164487873, 1.2186581834642807, 1.1906112626133232, 1.2036559811450511, 1.0934480822989994, 1.0871709764601343, 1.188772803753167, 1.2194052865362364, 1.129716908049831, 1.202697190301676, 1.33559027440832, 1.2934058074195098, 1.2591174841141648, 1.0998386933168924, 1.1160539874745676, 1.1580406403057164, 1.1986168479311108, 1.1648973287875417, 1.293075335144601, 1.291839229386997, 1.3457982412370968, 1.2527783460986965, 1.2447736361730204, 1.2639933063039341, 1.021931174574364, 1.3364845422648757, 1.1832917537393501, 1.1386871805255376, 1.3426155145203942, 1.2775931860543255, 1.3629379684907017, 1.1710059778503306, 1.2146260876776507, 1.0859902593882258, 1.3457218528154178, 1.3823511808432798, 1.139545686393897, 1.3000551335136905, 1.2970213989494146, 1.2274143109961835, 1.2417072090102192, 1.220968580102546, 1.204103068212198, 1.2802137105790206, 1.1951942108046893, 1.062235981535876, 1.0365519652781814, 1.2900025730433577, 1.1499145954033458, 1.214782327489712, 1.0945067407968947, 1.2017334948850837, 1.252181938623809, 1.277231754382874, 1.0220719873333068, 1.161587363637124, 1.2465649768771776, 1.1675970689792596, 1.1390529488509593, 1.3665914578894347, 1.4384431190652263, 1.1013511955987452, 1.1417539590609398, 1.1098340960926878, 1.143215736997586, 1.111497199680243, 1.179496513242717, 1.2632434001970871, 1.11901222585501, 1.17063409688929, 1.020117680980057, 1.2334149046603464, 1.271984852954215, 1.1076130388166807, 1.024200238300416, 0.9958110417089128, 1.028260792751332, 1.0087140157158425, 1.1120147765574526, 1.0930523934354965, 0.9997891133989744, 1.2405179953705507, 1.1503976405871614, 0.9449968442886508, 1.1267160992730096, 1.1400958842693691, 1.13074444555759, 1.3651602432033647, 1.2313993836765227, 1.1237347990433397, 1.1548564092272444, 1.0870745892866405, 1.0810359714990678, 1.2341733710029956, 1.2046994343587067, 0.9662468652539375, 1.1029436098146064, 1.1974409860578172, 1.1630829352095453, 1.374309500629837, 1.2857502821241173, 1.1736462130847085, 1.249748282710797, 1.135897880090176, 1.2412037127888058, 1.0049323522163405, 1.1692167399138556, 1.3219516768252297, 1.306037982804312, 1.1726541861741948, 1.3722259326161665, 1.211678408672611, 1.222878851103086, 1.0023812068772149, 1.0439723415411912, 1.118533824457948, 1.2145105072718663, 1.0195661870663835, 1.0900955535720447, 1.2031025643528979, 1.1037405729217498, 1.111138922557626, 0.9240662245308588, 1.1228073054842194, 1.2261046600995351, 1.261708915009766, 1.0617711292042713, 1.2570304559736138, 1.04386154988571, 1.146010718921043, 1.2925813604142158, 1.1863202021753256, 1.0562881461044038, 1.0204382395118476, 1.1981695675845394, 1.1896795336149661, 1.1346397872738414, 1.1185464841573198, 1.0949086888801685, 1.0589493301228727, 1.070757476971296, 1.1986456420677978, 0.990855093020905, 1.1128820691683718, 1.0582752499220969, 1.1918191958762117, 1.0740553908993258, 1.1191296061491045, 1.0267861785901202, 0.9947321823064975, 1.1424809782116008, 1.0577934158158977, 0.9188662465899827, 1.069868329931869, 1.0598400799716459, 0.9974114312014549, 1.0948730742691757, 1.2185792158650868, 1.130258144098088, 1.0228479781566235, 1.1106999027231101, 1.1269022460530247, 1.092605897905719, 1.185125071173907, 1.005558270219091, 0.9943180699578246, 0.9976551737402173, 1.1172626037381088, 1.0764195475824498, 1.0464419431996261, 1.1363298251612812, 1.0516957855963558, 1.0736520925184185, 0.9325247831581972, 0.9485078017150249, 1.0208657647224164, 1.0676613539598063, 0.8861480837010123, 1.033796398469806, 1.0039899852488983, 1.1308424742904044, 0.9553296777017596, 1.0392397288538708, 1.0656228636513907, 1.042965519410831, 1.0315839401391078, 1.1561374259826518, 0.8023740653954516, 1.0587567282452999, 1.1287740680987466, 0.98782630185897, 0.9728278938054409, 0.9604448640423945, 1.0449662846154, 1.0357843909524826, 0.8990221035021295, 0.8634994755403507, 1.0948247810500253, 0.9593559069095031, 0.9861566280466808, 1.087983530276047, 0.9803840686984097, 0.8857838314317129, 1.060857478205775, 1.0579729450005655, 0.9532773060646474, 1.0403600475015355, 0.9524298259786547, 0.9418390365236061, 1.030189030168033, 1.1468687175588475, 1.003544743586781, 1.0083159208740018, 1.0288214099976596, 1.06232253764817, 0.994645450225277, 0.9762145739366854, 0.9270487425217269, 0.9805266850452712, 1.0656151421251203, 1.012316699052396, 0.8984497947239687, 0.9061561755766772, 1.090098168192855, 0.9612084821594767, 0.9393879044358614, 0.9264299159729594, 1.100564915495216, 0.9852211244469732, 1.0840101205909385, 0.9952848007191183, 0.9494626789126898, 0.8354307392657531, 0.9363836014759559, 0.9111256295513382, 0.9250139650465083, 0.9180892973667429, 1.0365149105522087, 0.8806683279424102, 0.86528536021004, 0.9728183350415089, 1.0110110707027118, 0.8684537554688676, 0.9365630608792325, 0.9265820616312429, 1.0839199499410106, 0.9134708602054028, 0.8236474112723545, 0.8205720086756348, 0.9801624744928381, 0.943699607723797, 0.9212695865042992, 0.9005381172295465, 0.7871392655092188, 0.7789261768509792, 0.7971025134122192, 1.0040300839879088, 0.8035392661642812, 0.9788422777989293, 0.9651772416489443, 0.9682963257138356, 0.8550578030266754, 0.946664524463319, 0.8779827759153388, 1.0024616618669513, 0.8472037326095506, 0.7963440125124153, 0.9924532329811833, 1.0650075679430109, 0.9245472448414366, 0.9128074042472067, 1.0103417316785233, 0.8248661006627406, 0.990779673791687, 0.889940291537839, 0.9468826055481563, 0.7593448901040044, 0.8648190903047671, 0.9119751273674944, 0.8947901886213481, 1.024094832567866, 0.833112864639129, 0.829405783235664, 0.7065788341701563, 0.9442489232172138, 1.0450715504135237, 0.915669260382654, 0.9714460259535846, 0.8796776538683826, 0.8244001299608816, 0.8065306030292029, 0.8385820631248292, 0.8951693171389116, 0.9090982848223982, 0.7761265160447117, 0.7853292219777441, 0.9292217808460816, 1.002127078716151, 0.8199657848477502, 1.0127824506090515, 0.8511707017642838, 0.8257658517229252, 0.8664482123777089, 0.8127003940644943, 0.8034869079566452, 0.8718169340027626, 0.8135656360860533, 0.7991956741397729, 0.7613086831324201, 0.7691527285591147, 0.8234677866803672, 0.7716413118627038, 0.7467739758607321, 0.8955124533067086, 0.9144629636169265, 0.7919302867858191, 0.9348936267029853, 0.8442308930320878, 0.8543068253900898, 0.7224571532478231, 0.781510954948968, 0.9314881106410373, 0.8845631220305998, 0.7319082944233394, 0.8659162440454264, 0.9345294136979672, 0.9408402717766247, 0.8710878185601953, 0.8436627985084169, 0.8637733139766336, 0.8118807780684166, 0.9023630631378354, 0.7764685081469266, 0.779597584612187, 0.8764726972506114, 0.6898304319043898, 0.9483714416887594, 0.9745051087039444, 0.7793374158318217, 0.8829595793169014, 0.7734566547975327, 0.8084069002893256, 0.9804313743772254, 0.9168505215254429, 0.895619777646331, 0.8811780122646536, 0.7609828072439478, 0.9677336145208797, 0.7455131514107316, 0.841346922074565, 0.9298161449892334, 0.8731692065461302, 0.9236657088202553, 0.9645418560892479, 0.7292284184958077, 0.7738706846897734, 0.8588605604335005, 0.94439601311808, 0.932740312544174, 0.6868544729735052, 0.9407574655457762, 0.8462795581026491, 0.8329607333541788, 0.8277842121791058, 0.7494479977309823, 0.7232736811839745, 0.6657501850971355, 0.7726970549937524, 0.8158164624554006, 0.9854448703467544, 0.7235118962076496, 0.7845722027502419, 0.7432138557866199, 0.7728785129303393, 0.8689219579448553, 0.7719323121933751, 0.7039992950664287, 0.7453226822039755, 0.7212320756483512, 0.5761302033664792, 0.6764705433152338, 0.7166693174716485, 0.7848041580906941, 0.6597503447114026, 0.6220340116355088, 0.7278243557172867, 0.7712324638453312, 0.6559280111594994, 0.7674722144921464, 0.7724610645954695, 0.7888092410834585, 0.730711494487034, 0.6309199694929973, 0.7435709028585026, 0.6944723424836101, 0.8280259447993866, 0.6396838998141622, 0.6498527746878, 0.7025110683786718, 0.7236738066904794, 0.8292914946517824, 0.48838993462884545, 0.642117820688915, 0.8702368536968892, 0.7521801807764379, 0.7893014624149589, 0.6486253265139018, 0.7329087938105286, 0.7330299587046812, 0.8122495976804784, 0.6113361789365914, 0.7907270606630431, 0.6945153426255943, 0.7643638630545954, 0.7608562727295513, 0.6780382167483417, 0.6840339576008257, 0.5694894855946232, 0.6296244413273733, 0.551845772795903, 0.6732678453354273, 0.6408088359256846, 0.6261070329678281, 0.7754526148346125, 0.6666853379371448, 0.8717450676625812, 0.7522883330446922, 0.63649906579984, 0.8495135688841582, 0.7183301524905444, 0.5227122065977925, 0.6959795399433177, 0.780972043088486, 0.5324623403891657, 0.6493202010547732, 0.5466604890058117, 0.6320458360641605, 0.6801663897710846, 0.7523121482976893, 0.7491103036356616, 0.65608485849872, 0.6856227759892041, 0.6548441288623973, 0.6935389218718416, 0.6166734651207453, 0.7679840834711869, 0.7371977535276794, 0.7527193769212204, 0.619154534345698, 0.4910629580515602, 0.5327227270050698, 0.7170238191505307, 0.5318939356961127, 0.7138587861163777, 0.5755839654921183, 0.6559265178234565, 0.7144255731033597, 0.7319924205705304, 0.5686581307386659, 0.7365504431762266, 0.6494773864524608, 0.7394949885049557, 0.6393861017806073, 0.6612978372697538, 0.6606804470817629, 0.6262299697983373, 0.6833836794840601, 0.7005618520088024, 0.6063501183996249, 0.45532405523526903, 0.5994513189733407, 0.5915629975582661, 0.6742430769185893, 0.63819188037015, 0.689338135982142, 0.5935977926806053, 0.5916805794122051, 0.5947371677116818, 0.5551788977004788, 0.6620118445478334, 0.6737509497364442, 0.6178428933844122, 0.5457645134567659, 0.5547573356599904, 0.5512658958919853, 0.48117006795944306, 0.5952539994830125, 0.5033726470411832, 0.605131917326119, 0.5626155936706434, 0.561765346193188, 0.5208326743925482, 0.5052472155377573, 0.4899369355013534, 0.6810167571563508, 0.4769909352895589, 0.637132268489634, 0.6543007630478549, 0.7275829442150231, 0.51904080328298, 0.6714247885649391, 0.46838357495925004, 0.5671502738355089, 0.7040679778278703, 0.5625607125401445, 0.5603521111864329, 0.5500017752595051, 0.4398557859602768, 0.49315602466251685, 0.593904249558505, 0.6796335973447739, 0.5807172817074683, 0.5397787806240524, 0.5054774203742062, 0.7038400071464799, 0.5869200452381494, 0.482198526224888, 0.5645935101824181, 0.5746595555020616, 0.5246016584771812, 0.5396767563440463, 0.5114907856295303, 0.5944868671899877, 0.6050581527185345, 0.5063518921762172, 0.6719377461519429, 0.5849651506937414, 0.5695532254818724, 0.5957882754949674, 0.5324144426936025, 0.6117626183136838, 0.5483407921127194, 0.538870042701125, 0.5206944098461799, 0.5570296958220764, 0.48312723884789904, 0.4939931910704119, 0.6348727619981674, 0.5318977921932148, 0.5327316070373828, 0.4045010002280388, 0.5167283956126256, 0.5996856391210369, 0.45356711784474874, 0.5209350982920619, 0.4227131585657221, 0.582442489137752, 0.44737788963472624, 0.5754031627672085, 0.4925418521330969, 0.5020305734970915, 0.6238844028365156, 0.5610060234508999, 0.4844804103380227, 0.5401701364923089, 0.4837557817301385, 0.48410630630392143, 0.524833698346902, 0.5496410771315982, 0.5366956150837292, 0.5197289214718613, 0.44389933301199513, 0.5440972704218915, 0.5487533529744714, 0.4492835349503637, 0.42116620057258, 0.6099620900495543, 0.48605954140098223, 0.5468227127607341, 0.5044460974360261, 0.509997457743605, 0.5720763693963832, 0.48403235737954625, 0.6798430042599746, 0.49043695854573766, 0.45323981745004366, 0.43806218490931564, 0.43501426374541047, 0.5126704889150713, 0.4609768063248101, 0.5886646347026596, 0.5373786533620366, 0.36952081377142365, 0.4646797620423696, 0.5038005704531389, 0.6572856858509942, 0.5959228083680448, 0.43412156333968244, 0.5216550882109062, 0.4859431890545514, 0.3922449099932531, 0.6128189304105637, 0.4691358128559906, 0.3725308245397836, 0.6733239559116488, 0.3946328243072215, 0.4511905897090437, 0.49524966628447803, 0.4455382045578092, 0.5977788157376872, 0.44334968836459043, 0.386231240406038, 0.4738018205283527, 0.3557099961197412, 0.4751721683965115, 0.4012656194371313, 0.5196229030456708, 0.6423168003131201, 0.4915452838574387, 0.37902818445945874, 0.46576807947512566, 0.5252824384767469, 0.48104372910075105, 0.5171307277339188, 0.440125607226805, 0.597757804853578, 0.4454809013255588, 0.41749899743760815, 0.41776502049170783, 0.4616052371737163, 0.42775378453367524, 0.47625025997800813, 0.3886633194070662, 0.4586824108154726, 0.4894649258881327, 0.5545309315231652, 0.4379756457468198, 0.40205033221776304, 0.48567172763725175, 0.5239856063062637, 0.4490334807193492, 0.513668464573273, 0.40524236400982694, 0.333048448181901, 0.4574057608879252, 0.4231762901254754, 0.43624063991009626, 0.3959190294523974, 0.4843360405659005, 0.34292518705631897, 0.45845812950130466, 0.4888944681720754, 0.47466524482726075, 0.4666838039137128, 0.5033207276241456, 0.3887812554816786, 0.39954799386448414, 0.43773984795633625, 0.3652962793480276, 0.35794481516260995, 0.4632163426296138, 0.4253343738881752, 0.39536935445058496, 0.37382956668603223, 0.3302996808165434, 0.3619200848794912, 0.32105210838220843, 0.44036626228520026, 0.39386776929049994, 0.45117082480334114, 0.3296760458980847, 0.4663628840386837, 0.3756291266894616, 0.3423194944498508, 0.4299476708682161, 0.4494090486073904, 0.42336927520068046, 0.3233940792239385, 0.5010171351296819, 0.3736068839955066, 0.435754340369972, 0.3520750940321704, 0.4438724770782325, 0.3746219649476943, 0.4503028933109357, 0.42313665217385216, 0.35736520216517675, 0.49180136814471964, 0.2910387503155714, 0.3304147214549478, 0.43265186887370666, 0.3764127825799467, 0.3159750935202329, 0.35018421115753684, 0.50907111942546, 0.33023585310789244, 0.29248833127920787, 0.41019017782292366, 0.33773845645978173, 0.32981789697593145, 0.45721357812734426, 0.3121378038252175, 0.4465669388140832, 0.4180859736571653, 0.25779138985459477, 0.3594809305701498, 0.29565176099255375, 0.2535807739368085, 0.36847523584250214, 0.5137003814647422, 0.3074192873395897, 0.3557259212940518, 0.2916434197105286, 0.421311665211258, 0.2655631967067215, 0.39209149470317606, 0.38677939927708116, 0.3292832533809392, 0.4223749052660923, 0.3330617994073624, 0.40867388673573984, 0.3189972227872613, 0.35852954367514406, 0.3605322710444929, 0.3529336319348774, 0.3414934953628074, 0.31860310869969843, 0.30143613259230273, 0.5066529276887457, 0.3193635906716265, 0.30118170311802867, 0.3774394866909868, 0.2961048774504757, 0.35102480691640187, 0.3599553550546024, 0.31312864274137064, 0.3425843839106906, 0.2841357249979113, 0.4146906232065897, 0.33868566489970614, 0.39098827911817063, 0.33057252086628597, 0.2669387481090374, 0.2779661505830089, 0.3547996756841779, 0.40956096078151427, 0.36040523277266234, 0.3559543249728137, 0.31035328628595865, 0.34350774751957774, 0.30529816192397263, 0.3211161011964911, 0.31770172684457704, 0.37694802843127684, 0.325022567568886, 0.22831762467453653, 0.3308383767388313, 0.20381441151873975, 0.19829755220676382, 0.21671343859002484, 0.25139493176941313, 0.2436330900591637, 0.28638393694982284, 0.3232596788705835, 0.41777009435964224, 0.35189565351426433, 0.31313493320352603, 0.3119694504831278, 0.37295118110083036, 0.2735090528055463, 0.3673915149922737, 0.38406558127216167, 0.3493227322778261, 0.29465756051806297, 0.335705243318054, 0.3640702585972588, 0.32394629034490324, 0.3424332910697022, 0.38872444340650725, 0.24918925239729184, 0.28769010665060324, 0.3365692603337646, 0.2916387895857888, 0.2659767915481469, 0.2444623724011248, 0.24438397487947414, 0.20645830053925868, 0.3142220869441442, 0.3137979418136054, 0.2932627144611744, 0.26478275278128205, 0.2849963976529809, 0.4266816902322811, 0.17412539841042896, 0.32225292832081737, 0.30603363852832866, 0.20974535169951553, 0.3342446934781211, 0.36340595376568435, 0.29814455446341126, 0.36510748269998994, 0.2775237910863532, 0.28710515214988874, 0.29985991312473725, 0.2600576174846041, 0.2433959696227239, 0.22945371792208086, 0.2840396393143924, 0.19716673065374984, 0.24116025798728924, 0.1952716891648449, 0.23419077735681876, 0.1840897599865063, 0.34451809095932256, 0.25014798417855666, 0.2405539424082836, 0.23466927658014608, 0.2921713474401367, 0.3329727955105573, 0.3019482335941008, 0.2981560723498331, 0.17892207225111034, 0.23778189469647426, 0.27646271600579403, 0.26523497512158406, 0.24958951038605193, 0.315999325342607, 0.235458536362491, 0.2588809882527482, 0.24867901080300442, 0.22909653837671695, 0.28985925445253563, 0.1795963241404267, 0.3066462621580255, 0.27770284711335685, 0.26829717354537613, 0.36117896376409375, 0.19880244053965665, 0.2878180766127843, 0.18820346386793735, 0.2942898922779407, 0.21884151614520897, 0.17542710064226305, 0.17479159368838856, 0.2472684190360135, 0.21723380199039583, 0.20184069051362902, 0.2921717409174573, 0.28183424635750554, 0.2192937392829286, 0.27639663482162635, 0.21194269030269522, 0.2390874168278842, 0.25868718393025997, 0.260578608190811, 0.20129654294792654, 0.3498223218711926, 0.295970884641042, 0.21685529540776138, 0.21456424354189094, 0.1963914608953894, 0.24355807949539038, 0.2503836579435839, 0.2775311967690202, 0.16883898314152102, 0.209227623028818, 0.2621304200094313, 0.2381748602421314, 0.18689627793909835, 0.23428435529255529, 0.29107746722434324, 0.3549639268830122, 0.16483096484026682, 0.2461343574312182, 0.2386645288804267, 0.1731300988787735, 0.30634882639528405, 0.22776680486540418, 0.30061751381247237, 0.14715863790208347, 0.2172442213513155, 0.24182411180514343, 0.2630040635797004, 0.2398618431260729, 0.2739820931944827, 0.17284791559113608, 0.17633420188595852, 0.2210595298214066, 0.15192512970603034, 0.1431339881062473, 0.18468541710642364, 0.19821036966882383, 0.1945895456611836, 0.25724707913373374, 0.2566328393328372, 0.24971543261120438, 0.24004524503978594, 0.23761647275597825, 0.2762205948421926, 0.28413294567961794, 0.20052552892022096, 0.25148400639411406, 0.26349553749386256, 0.1920245624018647, 0.22182278346416098, 0.23608128790237548, 0.12163474483409374, 0.22958144443319853, 0.26487468509393997, 0.1499383556373418, 0.11961917677627966, 0.21509355729759247, 0.22404947392456098, 0.16709663136667988, 0.14834377135045662, 0.16731836518825513, 0.19292903612906767, 0.1544112208623475, 0.19640645608537227, 0.1555295052222125, 0.16234981407864638, 0.15410242316747666, 0.1759887102827008, 0.2274115174350917, 0.2314553749946785, 0.17135778138119154, 0.1502107636649252, 0.24660887228844622, 0.11396705664429085, 0.16728485984328167, 0.1334870704213886, 0.1496050344799498, 0.21471330432559022, 0.17425624551166458, 0.13422991327565695, 0.14422757463016034, 0.15808486186488233, 0.217172512309596, 0.1732378677964927, 0.20395939125790766, 0.14204687547076653, 0.20944343910131583, 0.12245227621408326, 0.18444567334155085, 0.10693035077271593, 0.12845381600641956, 0.18166710482143025, 0.11862967856308944, 0.1531828732601922, 0.1304462188220309, 0.18771497966006512, 0.10331244927613216, 0.16719601217660124, 0.11999762485881889, 0.15092761178113068, 0.1163785987152067, 0.10511924210301, 0.13735138075129388, 0.17610269983342286, 0.15806332150156618, 0.14106476404492493, 0.1291291498502988, 0.15993437976020486, 0.11948112286252836, 0.17174246713092967, 0.1434469389757432, 0.1273005428425872, 0.10922279692960447, 0.18756796403568599, 0.13367873207441552, 0.11172814176745938, 0.17655366445169474, 0.14934860584244772, 0.12451503605001449, 0.15672107871045637, 0.14188060569390482, 0.27367523772900576, 0.12056459082339198, 0.16092063270132598, 0.19915712561643026, 0.1684840308475611, 0.13217144029667377, 0.113670856679178, 0.1980846209290285, 0.18841735746394805, 0.18298421465719136, 0.1340633973249787, 0.1738682657175981, 0.11321959287335999, 0.15456739786488746, 0.13154827016604798, 0.12085922972423814, 0.10433981871834708, 0.11855996582742398, 0.13901590690342516, 0.16542534226388966, 0.08987231358211759, 0.13520243895095385, 0.1648660709613415, 0.11457745315135646, 0.13622371055255422, 0.083673505280099, 0.14424539188069788, 0.16646123637702434, 0.14314302059925063, 0.12866253592753393, 0.12375144477366963, 0.13635245144947095, 0.16396778398611492, 0.1649639081840271, 0.12895943767914209, 0.1320474474615618, 0.13228594983859981, 0.1278524799036681, 0.09407381262195433, 0.12769756273987126, 0.07487318404346159, 0.1865887972063191, 0.109766898656202, 0.1154734628691171, 0.11754024618369202, 0.14353234932056835, 0.17885519609778489, 0.12005765373654745, 0.1234924148077673, 0.10188426839913474, 0.06099135967550468, 0.1224749124605321, 0.09214751958840395, 0.08951393149175856, 0.08672003502923524, 0.17163332584488075, 0.08293218862576267]\n",
      "Accuracy history: [0.11, 0.176, 0.186, 0.248, 0.276, 0.3, 0.272, 0.27, 0.308, 0.318, 0.334, 0.316, 0.342, 0.336, 0.38, 0.308, 0.374, 0.356, 0.384, 0.416, 0.352, 0.38, 0.392, 0.402, 0.408, 0.42, 0.392, 0.434, 0.414, 0.422, 0.37, 0.414, 0.498, 0.496, 0.478, 0.516, 0.424, 0.458, 0.522, 0.45, 0.498, 0.478, 0.486, 0.48, 0.532, 0.524, 0.542, 0.528, 0.546, 0.572, 0.538, 0.572, 0.552, 0.562, 0.524, 0.548, 0.588, 0.572, 0.608, 0.57, 0.556, 0.604, 0.558, 0.558, 0.62, 0.596, 0.62, 0.632, 0.612, 0.618, 0.618, 0.656, 0.702, 0.656, 0.628, 0.66, 0.668, 0.674, 0.66, 0.65, 0.682, 0.708, 0.676, 0.724, 0.71, 0.698, 0.668, 0.68, 0.674, 0.714, 0.718, 0.732, 0.768, 0.776, 0.78, 0.762, 0.704, 0.768, 0.708, 0.758, 0.786, 0.736, 0.8, 0.794, 0.792, 0.824, 0.81, 0.776, 0.78, 0.858, 0.822, 0.846, 0.858, 0.86, 0.85, 0.834, 0.888, 0.854, 0.836, 0.856, 0.852, 0.892, 0.878, 0.89, 0.862, 0.87, 0.86, 0.866, 0.894, 0.896, 0.876, 0.932, 0.898, 0.912, 0.922, 0.94, 0.926, 0.884, 0.938, 0.934, 0.91, 0.93, 0.94, 0.936, 0.938, 0.94, 0.898, 0.926, 0.926, 0.93, 0.958, 0.962, 0.976, 0.984, 0.96, 0.968, 0.96, 0.968, 0.944, 0.976, 0.98, 0.978, 0.986, 0.982, 0.99, 0.994]\n"
     ]
    }
   ],
   "source": [
    "input_shape=(3,32,32)\n",
    "x_train = x_train.reshape(x_train.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "x_dev = x_dev.reshape(x_dev.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "x_val = x_val.reshape(x_val.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "stl_imgs = stl_imgs.reshape(stl_imgs.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "data = x_train, y_train, x_test, y_test, x_val, y_val, x_dev, y_dev\n",
    "print(x_train.shape)\n",
    "net = ConvNet4Accel(input_shape=(3, 32, 32))\n",
    "net.compile('adam')\n",
    "print(stl_imgs.shape)\n",
    "net.fit(x_train, y_train, x_dev, y_dev, n_epochs=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.425\n"
     ]
    }
   ],
   "source": [
    "x_test = x_test.reshape(x_test.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "print(net.accuracy(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7d) Analysis of STL-10 training quality\n",
    "\n",
    "Use your trained network that achieves 45%+ accuracy on the test set to make \"high quality\" plots showing the following \n",
    "\n",
    "- Plot the accuracy of the training and validation sets as a function of training epoch. You may have to convert iterations to epochs.\n",
    "- Plot the loss as a function of training iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(net.validation_acc_history)\n",
    "plt.plot(net.train_acc_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(net.loss_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7f) Visualize layer weights\n",
    "\n",
    "Run the following code and submit the inline image of the weight visualization of the 1st layer (convolutional layer) of the network.\n",
    "\n",
    "**Note:**\n",
    "- Setting optional parameter to `True` will let you save a .PNG file in your project folder of your weights. I'd suggest setting it to `False` unless look at your weights and they look like they are worth saving. You don't want a training run that produces undesirable weights to overwrite your good looking results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights(wts, saveFig=True, filename='convWts_adam_overfit.png'):\n",
    "    grid_sz = int(np.sqrt(len(wts)))\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for x in range(grid_sz):\n",
    "        for y in range(grid_sz):\n",
    "            lin_ind = np.ravel_multi_index((x, y), dims=(grid_sz, grid_sz))\n",
    "            plt.subplot(grid_sz, grid_sz, lin_ind+1)\n",
    "            currImg = wts[lin_ind]\n",
    "            low, high = np.min(currImg), np.max(currImg)\n",
    "            currImg = 255*(currImg - low) / (high - low)\n",
    "            currImg = currImg.astype('uint8')\n",
    "            plt.imshow(currImg)\n",
    "            plt.gca().axis('off')\n",
    "    if saveFig:\n",
    "        plt.savefig('convWts_adam_overfit.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Subsitute your trained network below\n",
    "# netT is my network's name\n",
    "# You shouldn't see RGB noise\n",
    "plot_weights(net.layers[0].wts.transpose(0, 2, 3, 1), saveFig=False, filename='convWts_adam_train_20epoch.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:** What do the learned filters look like? Does this make sense to you / is this what you expected? In which area of the brain do these filters resemble cell receptive fields?\n",
    "\n",
    "Note: you should not see RGB \"noise\". If you do, and you pass the \"overfit\" test with the Adam optimizer, you probably need to increase the number of training epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "**General advice:** When making modifications for extensions, make small changes, then check to make sure you pass test code. Also, test out the network runtime on small examples before/after the changes. If you're not careful, the simulation time can become intractable really quickly!\n",
    "\n",
    "**Remember:** One thorough extension usually is worth more than several \"shallow\" extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Pedal to the metal: achieve high accuracy on STL-10\n",
    "\n",
    "You can achieve higher (>50%) classification accuracy on the STL-10 test set. Find the hyperparameters to achieve this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Experiment with different network architectures.\n",
    "\n",
    "The design of the `Network` class is modular. As long as you're careful about shapes, adding/removing network layers (e.g. `Conv2D`, `Dense`, etc.) should be straight forward. Experiment with adding another sequence of `Conv2D` and `MaxPooling2D` layers. Add another `Dense` hidden layer before the output layer. How do the changes affect classification accuracy and loss? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Experiment with different network hyperparameters.\n",
    "\n",
    "Explore the affect one or more change below has on classification. Be careful about how the hyperparameters may affect the shape of network layers. Thorough analysis will get you more points (not try a few ad hoc values).\n",
    "\n",
    "- Experiment with different numbers of hidden units in the Dense layers.\n",
    "- Experiment different max pooling window sizes and strides.\n",
    "- Experiment with kernel sizes (not 7x7). Can you get away with smaller ones? Do they perform just as well? What is the change in runtime like? What is the impact on their visualized appearance?\n",
    "- Experiment with number of kernels in the convolutional layer. Is more/fewer better? What is the impact on their visualized appearance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. Add and test some training bells and whistles\n",
    "\n",
    "Add features like early stopping, learning rate decay (learning rate at the end of an epoch becomes some fraction of its former value), etc and assess how they affect training loss convergence and accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Additional optimizers\n",
    "\n",
    "Research other optimizers used in backpropogation and implement one or more of them within the model structure. Compare its performance to ones you have implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Optimize your algorithms\n",
    "\n",
    "Find the main performance bottlenecks in the network and improve your code to reduce runtime (e.g. reduce explicit for loops, increase vectorization, etc). Research faster algorithms to do operations like convolution and implement them. Given the complexity of the network, I suggest focusing on one area at a time and make sure everything you change passes the test code before proceeding. Quantify and discuss your performance improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Additional loss functions\n",
    "\n",
    "Implement support for sigmoid, or another activation functions and associated losses. Test it out and compare with softmax/cross entropy. Make sure any necessary changes to the layer's gradient are made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Additional datasets\n",
    "\n",
    "Do classification and analyxe the results with an image dataset of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Performance analysis\n",
    "\n",
    "Do a thorough comparative analysis of the non-accelerated network and accelerated networks with respect to runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train...\n",
      "1485 iterations. 45 iter/epoch.\n",
      "Iteration: 1/1485.\n",
      "Time taken for iteration 0: 0.4887669086456299\n",
      "Estimated time to complete: 725.8188593387604\n",
      "Iteration: 2/1485.\n",
      "Iteration: 3/1485.\n",
      "Iteration: 4/1485.\n",
      "Iteration: 5/1485.\n",
      "Iteration: 6/1485.\n",
      "Iteration: 7/1485.\n",
      "Iteration: 8/1485.\n",
      "Iteration: 9/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [2.301873147133011, 2.2500227794087517, 2.2528956186527007]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.182, Val acc: 0.1\n",
      "\n",
      "\n",
      "Iteration: 10/1485.\n",
      "Iteration: 11/1485.\n",
      "Iteration: 12/1485.\n",
      "Iteration: 13/1485.\n",
      "Iteration: 14/1485.\n",
      "Iteration: 15/1485.\n",
      "Iteration: 16/1485.\n",
      "Iteration: 17/1485.\n",
      "Iteration: 18/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [2.230990432879249, 2.1149906439690698, 2.1096766136833516]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.194, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 19/1485.\n",
      "Iteration: 20/1485.\n",
      "Iteration: 21/1485.\n",
      "Iteration: 22/1485.\n",
      "Iteration: 23/1485.\n",
      "Iteration: 24/1485.\n",
      "Iteration: 25/1485.\n",
      "Iteration: 26/1485.\n",
      "Iteration: 27/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [2.040286746403391, 2.091883500155575, 1.8921736129986013]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.23, Val acc: 0.26\n",
      "\n",
      "\n",
      "Iteration: 28/1485.\n",
      "Iteration: 29/1485.\n",
      "Iteration: 30/1485.\n",
      "Iteration: 31/1485.\n",
      "Iteration: 32/1485.\n",
      "Iteration: 33/1485.\n",
      "Iteration: 34/1485.\n",
      "Iteration: 35/1485.\n",
      "Iteration: 36/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [2.051447662682095, 1.9357271153799098, 1.9488415889766082]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.284, Val acc: 0.2\n",
      "\n",
      "\n",
      "Iteration: 37/1485.\n",
      "Iteration: 38/1485.\n",
      "Iteration: 39/1485.\n",
      "Iteration: 40/1485.\n",
      "Iteration: 41/1485.\n",
      "Iteration: 42/1485.\n",
      "Iteration: 43/1485.\n",
      "Iteration: 44/1485.\n",
      "Iteration: 45/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.8437983257686832, 1.9559151087429394, 1.9747659584310215]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.292, Val acc: 0.24\n",
      "\n",
      "\n",
      "Iteration: 46/1485.\n",
      "Iteration: 47/1485.\n",
      "Iteration: 48/1485.\n",
      "Iteration: 49/1485.\n",
      "Iteration: 50/1485.\n",
      "Iteration: 51/1485.\n",
      "Iteration: 52/1485.\n",
      "Iteration: 53/1485.\n",
      "Iteration: 54/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.7713060600649888, 1.7719742315598266, 1.9707207656255987]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.262, Val acc: 0.24\n",
      "\n",
      "\n",
      "Iteration: 55/1485.\n",
      "Iteration: 56/1485.\n",
      "Iteration: 57/1485.\n",
      "Iteration: 58/1485.\n",
      "Iteration: 59/1485.\n",
      "Iteration: 60/1485.\n",
      "Iteration: 61/1485.\n",
      "Iteration: 62/1485.\n",
      "Iteration: 63/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.8416558088355028, 1.9059486219474309, 1.8519092702673123]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.314, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 64/1485.\n",
      "Iteration: 65/1485.\n",
      "Iteration: 66/1485.\n",
      "Iteration: 67/1485.\n",
      "Iteration: 68/1485.\n",
      "Iteration: 69/1485.\n",
      "Iteration: 70/1485.\n",
      "Iteration: 71/1485.\n",
      "Iteration: 72/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.9017265208068406, 1.9300900062945936, 1.954619791558879]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.28, Val acc: 0.28\n",
      "\n",
      "\n",
      "Iteration: 73/1485.\n",
      "Iteration: 74/1485.\n",
      "Iteration: 75/1485.\n",
      "Iteration: 76/1485.\n",
      "Iteration: 77/1485.\n",
      "Iteration: 78/1485.\n",
      "Iteration: 79/1485.\n",
      "Iteration: 80/1485.\n",
      "Iteration: 81/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.9407716429381854, 1.9393610308808562, 1.7272131214182989]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.314, Val acc: 0.24\n",
      "\n",
      "\n",
      "Iteration: 82/1485.\n",
      "Iteration: 83/1485.\n",
      "Iteration: 84/1485.\n",
      "Iteration: 85/1485.\n",
      "Iteration: 86/1485.\n",
      "Iteration: 87/1485.\n",
      "Iteration: 88/1485.\n",
      "Iteration: 89/1485.\n",
      "Iteration: 90/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.8605889751813791, 1.7760573855575508, 1.635153536285366]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.32, Val acc: 0.22\n",
      "\n",
      "\n",
      "Iteration: 91/1485.\n",
      "Iteration: 92/1485.\n",
      "Iteration: 93/1485.\n",
      "Iteration: 94/1485.\n",
      "Iteration: 95/1485.\n",
      "Iteration: 96/1485.\n",
      "Iteration: 97/1485.\n",
      "Iteration: 98/1485.\n",
      "Iteration: 99/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.7852481097813846, 1.8111841786314753, 1.8345266894278303]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.356, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 100/1485.\n",
      "Iteration: 101/1485.\n",
      "Iteration: 102/1485.\n",
      "Iteration: 103/1485.\n",
      "Iteration: 104/1485.\n",
      "Iteration: 105/1485.\n",
      "Iteration: 106/1485.\n",
      "Iteration: 107/1485.\n",
      "Iteration: 108/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.7070787226445638, 1.9089027991910212, 1.8490574864228009]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.3, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 109/1485.\n",
      "Iteration: 110/1485.\n",
      "Iteration: 111/1485.\n",
      "Iteration: 112/1485.\n",
      "Iteration: 113/1485.\n",
      "Iteration: 114/1485.\n",
      "Iteration: 115/1485.\n",
      "Iteration: 116/1485.\n",
      "Iteration: 117/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.8377132045414093, 1.8240843152088355, 1.6623084599427154]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.356, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 118/1485.\n",
      "Iteration: 119/1485.\n",
      "Iteration: 120/1485.\n",
      "Iteration: 121/1485.\n",
      "Iteration: 122/1485.\n",
      "Iteration: 123/1485.\n",
      "Iteration: 124/1485.\n",
      "Iteration: 125/1485.\n",
      "Iteration: 126/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.8306131816766056, 1.630204267570904, 1.628631636415539]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.36, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 127/1485.\n",
      "Iteration: 128/1485.\n",
      "Iteration: 129/1485.\n",
      "Iteration: 130/1485.\n",
      "Iteration: 131/1485.\n",
      "Iteration: 132/1485.\n",
      "Iteration: 133/1485.\n",
      "Iteration: 134/1485.\n",
      "Iteration: 135/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.6528054045393594, 1.7916239755966046, 1.7737620309598228]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.32, Val acc: 0.28\n",
      "\n",
      "\n",
      "Iteration: 136/1485.\n",
      "Iteration: 137/1485.\n",
      "Iteration: 138/1485.\n",
      "Iteration: 139/1485.\n",
      "Iteration: 140/1485.\n",
      "Iteration: 141/1485.\n",
      "Iteration: 142/1485.\n",
      "Iteration: 143/1485.\n",
      "Iteration: 144/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.7726455080453394, 1.7724559634640467, 1.7263200837109844]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.316, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 145/1485.\n",
      "Iteration: 146/1485.\n",
      "Iteration: 147/1485.\n",
      "Iteration: 148/1485.\n",
      "Iteration: 149/1485.\n",
      "Iteration: 150/1485.\n",
      "Iteration: 151/1485.\n",
      "Iteration: 152/1485.\n",
      "Iteration: 153/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.819783170663278, 1.7552269957476978, 1.9381476954610892]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.322, Val acc: 0.28\n",
      "\n",
      "\n",
      "Iteration: 154/1485.\n",
      "Iteration: 155/1485.\n",
      "Iteration: 156/1485.\n",
      "Iteration: 157/1485.\n",
      "Iteration: 158/1485.\n",
      "Iteration: 159/1485.\n",
      "Iteration: 160/1485.\n",
      "Iteration: 161/1485.\n",
      "Iteration: 162/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.8024477671012513, 1.842571926103743, 1.710587023126368]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.326, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 163/1485.\n",
      "Iteration: 164/1485.\n",
      "Iteration: 165/1485.\n",
      "Iteration: 166/1485.\n",
      "Iteration: 167/1485.\n",
      "Iteration: 168/1485.\n",
      "Iteration: 169/1485.\n",
      "Iteration: 170/1485.\n",
      "Iteration: 171/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.6774495336083515, 1.6759392219740574, 1.7836601897564492]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.364, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 172/1485.\n",
      "Iteration: 173/1485.\n",
      "Iteration: 174/1485.\n",
      "Iteration: 175/1485.\n",
      "Iteration: 176/1485.\n",
      "Iteration: 177/1485.\n",
      "Iteration: 178/1485.\n",
      "Iteration: 179/1485.\n",
      "Iteration: 180/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.6436464138137865, 1.6196268751724514, 1.7434020830275763]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.368, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 181/1485.\n",
      "Iteration: 182/1485.\n",
      "Iteration: 183/1485.\n",
      "Iteration: 184/1485.\n",
      "Iteration: 185/1485.\n",
      "Iteration: 186/1485.\n",
      "Iteration: 187/1485.\n",
      "Iteration: 188/1485.\n",
      "Iteration: 189/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.6117549748955529, 1.680272052141252, 1.7998692957003273]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.362, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 190/1485.\n",
      "Iteration: 191/1485.\n",
      "Iteration: 192/1485.\n",
      "Iteration: 193/1485.\n",
      "Iteration: 194/1485.\n",
      "Iteration: 195/1485.\n",
      "Iteration: 196/1485.\n",
      "Iteration: 197/1485.\n",
      "Iteration: 198/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.6398577725723087, 1.663734883061584, 1.7210220349441085]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.322, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 199/1485.\n",
      "Iteration: 200/1485.\n",
      "Iteration: 201/1485.\n",
      "Iteration: 202/1485.\n",
      "Iteration: 203/1485.\n",
      "Iteration: 204/1485.\n",
      "Iteration: 205/1485.\n",
      "Iteration: 206/1485.\n",
      "Iteration: 207/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.5842668709244294, 1.730557998074906, 1.7345373379298452]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.39, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 208/1485.\n",
      "Iteration: 209/1485.\n",
      "Iteration: 210/1485.\n",
      "Iteration: 211/1485.\n",
      "Iteration: 212/1485.\n",
      "Iteration: 213/1485.\n",
      "Iteration: 214/1485.\n",
      "Iteration: 215/1485.\n",
      "Iteration: 216/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.6025201757223773, 1.5145790280285818, 1.6824116660417727]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.342, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 217/1485.\n",
      "Iteration: 218/1485.\n",
      "Iteration: 219/1485.\n",
      "Iteration: 220/1485.\n",
      "Iteration: 221/1485.\n",
      "Iteration: 222/1485.\n",
      "Iteration: 223/1485.\n",
      "Iteration: 224/1485.\n",
      "Iteration: 225/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.6429420273161262, 1.5517453936417567, 1.6501259705793327]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.416, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 226/1485.\n",
      "Iteration: 227/1485.\n",
      "Iteration: 228/1485.\n",
      "Iteration: 229/1485.\n",
      "Iteration: 230/1485.\n",
      "Iteration: 231/1485.\n",
      "Iteration: 232/1485.\n",
      "Iteration: 233/1485.\n",
      "Iteration: 234/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.7054045172410377, 1.5517633865179596, 1.6462956679560907]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.352, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 235/1485.\n",
      "Iteration: 236/1485.\n",
      "Iteration: 237/1485.\n",
      "Iteration: 238/1485.\n",
      "Iteration: 239/1485.\n",
      "Iteration: 240/1485.\n",
      "Iteration: 241/1485.\n",
      "Iteration: 242/1485.\n",
      "Iteration: 243/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.7110436020993123, 1.6715004847989232, 1.6389062885624808]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.38, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 244/1485.\n",
      "Iteration: 245/1485.\n",
      "Iteration: 246/1485.\n",
      "Iteration: 247/1485.\n",
      "Iteration: 248/1485.\n",
      "Iteration: 249/1485.\n",
      "Iteration: 250/1485.\n",
      "Iteration: 251/1485.\n",
      "Iteration: 252/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.59665360232518, 1.6164312798518714, 1.7236751788932951]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.412, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 253/1485.\n",
      "Iteration: 254/1485.\n",
      "Iteration: 255/1485.\n",
      "Iteration: 256/1485.\n",
      "Iteration: 257/1485.\n",
      "Iteration: 258/1485.\n",
      "Iteration: 259/1485.\n",
      "Iteration: 260/1485.\n",
      "Iteration: 261/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.6898399065125689, 1.6542290674917517, 1.452406349971957]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.416, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 262/1485.\n",
      "Iteration: 263/1485.\n",
      "Iteration: 264/1485.\n",
      "Iteration: 265/1485.\n",
      "Iteration: 266/1485.\n",
      "Iteration: 267/1485.\n",
      "Iteration: 268/1485.\n",
      "Iteration: 269/1485.\n",
      "Iteration: 270/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.5415944286757783, 1.5766113509709612, 1.6724267221259155]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.388, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 271/1485.\n",
      "Iteration: 272/1485.\n",
      "Iteration: 273/1485.\n",
      "Iteration: 274/1485.\n",
      "Iteration: 275/1485.\n",
      "Iteration: 276/1485.\n",
      "Iteration: 277/1485.\n",
      "Iteration: 278/1485.\n",
      "Iteration: 279/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.5184650463860525, 1.733310312603863, 1.45273642358619]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.374, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 280/1485.\n",
      "Iteration: 281/1485.\n",
      "Iteration: 282/1485.\n",
      "Iteration: 283/1485.\n",
      "Iteration: 284/1485.\n",
      "Iteration: 285/1485.\n",
      "Iteration: 286/1485.\n",
      "Iteration: 287/1485.\n",
      "Iteration: 288/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.4811937265363957, 1.5120071213520607, 1.5955602769941997]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.402, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 289/1485.\n",
      "Iteration: 290/1485.\n",
      "Iteration: 291/1485.\n",
      "Iteration: 292/1485.\n",
      "Iteration: 293/1485.\n",
      "Iteration: 294/1485.\n",
      "Iteration: 295/1485.\n",
      "Iteration: 296/1485.\n",
      "Iteration: 297/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.4565737599976951, 1.6386497816474177, 1.5852928795538075]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.442, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 298/1485.\n",
      "Iteration: 299/1485.\n",
      "Iteration: 300/1485.\n",
      "Iteration: 301/1485.\n",
      "Iteration: 302/1485.\n",
      "Iteration: 303/1485.\n",
      "Iteration: 304/1485.\n",
      "Iteration: 305/1485.\n",
      "Iteration: 306/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.5050806127491916, 1.5619986023292907, 1.5465385679162225]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.482, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 307/1485.\n",
      "Iteration: 308/1485.\n",
      "Iteration: 309/1485.\n",
      "Iteration: 310/1485.\n",
      "Iteration: 311/1485.\n",
      "Iteration: 312/1485.\n",
      "Iteration: 313/1485.\n",
      "Iteration: 314/1485.\n",
      "Iteration: 315/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.558604076701741, 1.7285232856602195, 1.5530230725944172]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.476, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 316/1485.\n",
      "Iteration: 317/1485.\n",
      "Iteration: 318/1485.\n",
      "Iteration: 319/1485.\n",
      "Iteration: 320/1485.\n",
      "Iteration: 321/1485.\n",
      "Iteration: 322/1485.\n",
      "Iteration: 323/1485.\n",
      "Iteration: 324/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.5623548278721797, 1.2879084458735508, 1.4779218207182356]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.444, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 325/1485.\n",
      "Iteration: 326/1485.\n",
      "Iteration: 327/1485.\n",
      "Iteration: 328/1485.\n",
      "Iteration: 329/1485.\n",
      "Iteration: 330/1485.\n",
      "Iteration: 331/1485.\n",
      "Iteration: 332/1485.\n",
      "Iteration: 333/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.491849018003894, 1.5007408972598908, 1.506357601471122]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.406, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 334/1485.\n",
      "Iteration: 335/1485.\n",
      "Iteration: 336/1485.\n",
      "Iteration: 337/1485.\n",
      "Iteration: 338/1485.\n",
      "Iteration: 339/1485.\n",
      "Iteration: 340/1485.\n",
      "Iteration: 341/1485.\n",
      "Iteration: 342/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.4857677806549128, 1.4389946527278192, 1.6361758659532593]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.454, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 343/1485.\n",
      "Iteration: 344/1485.\n",
      "Iteration: 345/1485.\n",
      "Iteration: 346/1485.\n",
      "Iteration: 347/1485.\n",
      "Iteration: 348/1485.\n",
      "Iteration: 349/1485.\n",
      "Iteration: 350/1485.\n",
      "Iteration: 351/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.4496513512095335, 1.453255475608344, 1.5575582382027526]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.442, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 352/1485.\n",
      "Iteration: 353/1485.\n",
      "Iteration: 354/1485.\n",
      "Iteration: 355/1485.\n",
      "Iteration: 356/1485.\n",
      "Iteration: 357/1485.\n",
      "Iteration: 358/1485.\n",
      "Iteration: 359/1485.\n",
      "Iteration: 360/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.5781035675227875, 1.410641588479197, 1.5279013954613823]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.46, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 361/1485.\n",
      "Iteration: 362/1485.\n",
      "Iteration: 363/1485.\n",
      "Iteration: 364/1485.\n",
      "Iteration: 365/1485.\n",
      "Iteration: 366/1485.\n",
      "Iteration: 367/1485.\n",
      "Iteration: 368/1485.\n",
      "Iteration: 369/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.4999388183856286, 1.5809875596776781, 1.410948735368885]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.468, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 370/1485.\n",
      "Iteration: 371/1485.\n",
      "Iteration: 372/1485.\n",
      "Iteration: 373/1485.\n",
      "Iteration: 374/1485.\n",
      "Iteration: 375/1485.\n",
      "Iteration: 376/1485.\n",
      "Iteration: 377/1485.\n",
      "Iteration: 378/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.4512285850198103, 1.4407832884959033, 1.4338142023473937]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.484, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 379/1485.\n",
      "Iteration: 380/1485.\n",
      "Iteration: 381/1485.\n",
      "Iteration: 382/1485.\n",
      "Iteration: 383/1485.\n",
      "Iteration: 384/1485.\n",
      "Iteration: 385/1485.\n",
      "Iteration: 386/1485.\n",
      "Iteration: 387/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.4340163987540848, 1.5725654314756852, 1.617204862369583]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.444, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 388/1485.\n",
      "Iteration: 389/1485.\n",
      "Iteration: 390/1485.\n",
      "Iteration: 391/1485.\n",
      "Iteration: 392/1485.\n",
      "Iteration: 393/1485.\n",
      "Iteration: 394/1485.\n",
      "Iteration: 395/1485.\n",
      "Iteration: 396/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.414874246995088, 1.508977458097116, 1.5395553848964005]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.484, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 397/1485.\n",
      "Iteration: 398/1485.\n",
      "Iteration: 399/1485.\n",
      "Iteration: 400/1485.\n",
      "Iteration: 401/1485.\n",
      "Iteration: 402/1485.\n",
      "Iteration: 403/1485.\n",
      "Iteration: 404/1485.\n",
      "Iteration: 405/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.3984743052520432, 1.4557030579985313, 1.4685430877907906]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.458, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 406/1485.\n",
      "Iteration: 407/1485.\n",
      "Iteration: 408/1485.\n",
      "Iteration: 409/1485.\n",
      "Iteration: 410/1485.\n",
      "Iteration: 411/1485.\n",
      "Iteration: 412/1485.\n",
      "Iteration: 413/1485.\n",
      "Iteration: 414/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.3033568366940036, 1.4124325625489054, 1.4310771810036262]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.482, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 415/1485.\n",
      "Iteration: 416/1485.\n",
      "Iteration: 417/1485.\n",
      "Iteration: 418/1485.\n",
      "Iteration: 419/1485.\n",
      "Iteration: 420/1485.\n",
      "Iteration: 421/1485.\n",
      "Iteration: 422/1485.\n",
      "Iteration: 423/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.2784898391360862, 1.51403852865372, 1.5014483395682618]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.506, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 424/1485.\n",
      "Iteration: 425/1485.\n",
      "Iteration: 426/1485.\n",
      "Iteration: 427/1485.\n",
      "Iteration: 428/1485.\n",
      "Iteration: 429/1485.\n",
      "Iteration: 430/1485.\n",
      "Iteration: 431/1485.\n",
      "Iteration: 432/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.3192265693340328, 1.3738786281571924, 1.3411369100825834]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.478, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 433/1485.\n",
      "Iteration: 434/1485.\n",
      "Iteration: 435/1485.\n",
      "Iteration: 436/1485.\n",
      "Iteration: 437/1485.\n",
      "Iteration: 438/1485.\n",
      "Iteration: 439/1485.\n",
      "Iteration: 440/1485.\n",
      "Iteration: 441/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.4152601246984973, 1.4642663505613505, 1.3792307534180037]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.444, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 442/1485.\n",
      "Iteration: 443/1485.\n",
      "Iteration: 444/1485.\n",
      "Iteration: 445/1485.\n",
      "Iteration: 446/1485.\n",
      "Iteration: 447/1485.\n",
      "Iteration: 448/1485.\n",
      "Iteration: 449/1485.\n",
      "Iteration: 450/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.3597240858887145, 1.4652927523714754, 1.5544307721960124]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.496, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 451/1485.\n",
      "Iteration: 452/1485.\n",
      "Iteration: 453/1485.\n",
      "Iteration: 454/1485.\n",
      "Iteration: 455/1485.\n",
      "Iteration: 456/1485.\n",
      "Iteration: 457/1485.\n",
      "Iteration: 458/1485.\n",
      "Iteration: 459/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.3501788676720319, 1.4661698097922917, 1.348096895264067]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.5, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 460/1485.\n",
      "Iteration: 461/1485.\n",
      "Iteration: 462/1485.\n",
      "Iteration: 463/1485.\n",
      "Iteration: 464/1485.\n",
      "Iteration: 465/1485.\n",
      "Iteration: 466/1485.\n",
      "Iteration: 467/1485.\n",
      "Iteration: 468/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.3010774246388925, 1.4108092629753999, 1.4913164386430975]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.47, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 469/1485.\n",
      "Iteration: 470/1485.\n",
      "Iteration: 471/1485.\n",
      "Iteration: 472/1485.\n",
      "Iteration: 473/1485.\n",
      "Iteration: 474/1485.\n",
      "Iteration: 475/1485.\n",
      "Iteration: 476/1485.\n",
      "Iteration: 477/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.5163326639369592, 1.22790077323162, 1.4863954794074752]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.428, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 478/1485.\n",
      "Iteration: 479/1485.\n",
      "Iteration: 480/1485.\n",
      "Iteration: 481/1485.\n",
      "Iteration: 482/1485.\n",
      "Iteration: 483/1485.\n",
      "Iteration: 484/1485.\n",
      "Iteration: 485/1485.\n",
      "Iteration: 486/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.485964615623005, 1.4253524197960754, 1.397202864247266]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.486, Val acc: 0.5\n",
      "\n",
      "\n",
      "Iteration: 487/1485.\n",
      "Iteration: 488/1485.\n",
      "Iteration: 489/1485.\n",
      "Iteration: 490/1485.\n",
      "Iteration: 491/1485.\n",
      "Iteration: 492/1485.\n",
      "Iteration: 493/1485.\n",
      "Iteration: 494/1485.\n",
      "Iteration: 495/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.3173035360558638, 1.439649404669488, 1.4400903081724443]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.49, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 496/1485.\n",
      "Iteration: 497/1485.\n",
      "Iteration: 498/1485.\n",
      "Iteration: 499/1485.\n",
      "Iteration: 500/1485.\n",
      "Iteration: 501/1485.\n",
      "Iteration: 502/1485.\n",
      "Iteration: 503/1485.\n",
      "Iteration: 504/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.413550007390241, 1.3071885293699117, 1.6286592619733646]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.494, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 505/1485.\n",
      "Iteration: 506/1485.\n",
      "Iteration: 507/1485.\n",
      "Iteration: 508/1485.\n",
      "Iteration: 509/1485.\n",
      "Iteration: 510/1485.\n",
      "Iteration: 511/1485.\n",
      "Iteration: 512/1485.\n",
      "Iteration: 513/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.3231102291553214, 1.4542878486712139, 1.3633881222611028]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.502, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 514/1485.\n",
      "Iteration: 515/1485.\n",
      "Iteration: 516/1485.\n",
      "Iteration: 517/1485.\n",
      "Iteration: 518/1485.\n",
      "Iteration: 519/1485.\n",
      "Iteration: 520/1485.\n",
      "Iteration: 521/1485.\n",
      "Iteration: 522/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.4539293989375819, 1.343556855175209, 1.3445161905814356]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.498, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 523/1485.\n",
      "Iteration: 524/1485.\n",
      "Iteration: 525/1485.\n",
      "Iteration: 526/1485.\n",
      "Iteration: 527/1485.\n",
      "Iteration: 528/1485.\n",
      "Iteration: 529/1485.\n",
      "Iteration: 530/1485.\n",
      "Iteration: 531/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.3176452603426307, 1.3044741689331147, 1.3133451389849757]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.534, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 532/1485.\n",
      "Iteration: 533/1485.\n",
      "Iteration: 534/1485.\n",
      "Iteration: 535/1485.\n",
      "Iteration: 536/1485.\n",
      "Iteration: 537/1485.\n",
      "Iteration: 538/1485.\n",
      "Iteration: 539/1485.\n",
      "Iteration: 540/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.269992007709889, 1.3005148992396114, 1.2406554236592409]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.532, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 541/1485.\n",
      "Iteration: 542/1485.\n",
      "Iteration: 543/1485.\n",
      "Iteration: 544/1485.\n",
      "Iteration: 545/1485.\n",
      "Iteration: 546/1485.\n",
      "Iteration: 547/1485.\n",
      "Iteration: 548/1485.\n",
      "Iteration: 549/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.4236339142765377, 1.3395100701188287, 1.4078794144346116]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.516, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 550/1485.\n",
      "Iteration: 551/1485.\n",
      "Iteration: 552/1485.\n",
      "Iteration: 553/1485.\n",
      "Iteration: 554/1485.\n",
      "Iteration: 555/1485.\n",
      "Iteration: 556/1485.\n",
      "Iteration: 557/1485.\n",
      "Iteration: 558/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.2561255224883734, 1.3292067876299836, 1.3030672708757445]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.506, Val acc: 0.48\n",
      "\n",
      "\n",
      "Iteration: 559/1485.\n",
      "Iteration: 560/1485.\n",
      "Iteration: 561/1485.\n",
      "Iteration: 562/1485.\n",
      "Iteration: 563/1485.\n",
      "Iteration: 564/1485.\n",
      "Iteration: 565/1485.\n",
      "Iteration: 566/1485.\n",
      "Iteration: 567/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.3008551570464177, 1.3089350051503499, 1.2253757576970103]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.526, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 568/1485.\n",
      "Iteration: 569/1485.\n",
      "Iteration: 570/1485.\n",
      "Iteration: 571/1485.\n",
      "Iteration: 572/1485.\n",
      "Iteration: 573/1485.\n",
      "Iteration: 574/1485.\n",
      "Iteration: 575/1485.\n",
      "Iteration: 576/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.2826464110822406, 1.3042585778096567, 1.3104385805935885]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.53, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 577/1485.\n",
      "Iteration: 578/1485.\n",
      "Iteration: 579/1485.\n",
      "Iteration: 580/1485.\n",
      "Iteration: 581/1485.\n",
      "Iteration: 582/1485.\n",
      "Iteration: 583/1485.\n",
      "Iteration: 584/1485.\n",
      "Iteration: 585/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.1288244505775813, 1.156202616925294, 1.332721394201669]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.58, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 586/1485.\n",
      "Iteration: 587/1485.\n",
      "Iteration: 588/1485.\n",
      "Iteration: 589/1485.\n",
      "Iteration: 590/1485.\n",
      "Iteration: 591/1485.\n",
      "Iteration: 592/1485.\n",
      "Iteration: 593/1485.\n",
      "Iteration: 594/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.1519215459151555, 1.007901903589812, 1.4008299085446583]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.51, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 595/1485.\n",
      "Iteration: 596/1485.\n",
      "Iteration: 597/1485.\n",
      "Iteration: 598/1485.\n",
      "Iteration: 599/1485.\n",
      "Iteration: 600/1485.\n",
      "Iteration: 601/1485.\n",
      "Iteration: 602/1485.\n",
      "Iteration: 603/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.1823119168214407, 1.2177203665488818, 1.2929611630516138]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.548, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 604/1485.\n",
      "Iteration: 605/1485.\n",
      "Iteration: 606/1485.\n",
      "Iteration: 607/1485.\n",
      "Iteration: 608/1485.\n",
      "Iteration: 609/1485.\n",
      "Iteration: 610/1485.\n",
      "Iteration: 611/1485.\n",
      "Iteration: 612/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.3403214247891548, 1.143113432709407, 1.2321746165741008]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.566, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 613/1485.\n",
      "Iteration: 614/1485.\n",
      "Iteration: 615/1485.\n",
      "Iteration: 616/1485.\n",
      "Iteration: 617/1485.\n",
      "Iteration: 618/1485.\n",
      "Iteration: 619/1485.\n",
      "Iteration: 620/1485.\n",
      "Iteration: 621/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.2416591575442226, 1.111237302597651, 1.318291017433696]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.534, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 622/1485.\n",
      "Iteration: 623/1485.\n",
      "Iteration: 624/1485.\n",
      "Iteration: 625/1485.\n",
      "Iteration: 626/1485.\n",
      "Iteration: 627/1485.\n",
      "Iteration: 628/1485.\n",
      "Iteration: 629/1485.\n",
      "Iteration: 630/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.21826925444824, 1.1864083746885494, 1.23573027886704]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.594, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 631/1485.\n",
      "Iteration: 632/1485.\n",
      "Iteration: 633/1485.\n",
      "Iteration: 634/1485.\n",
      "Iteration: 635/1485.\n",
      "Iteration: 636/1485.\n",
      "Iteration: 637/1485.\n",
      "Iteration: 638/1485.\n",
      "Iteration: 639/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.262181080019778, 1.1426459634388895, 1.084226388897256]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.544, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 640/1485.\n",
      "Iteration: 641/1485.\n",
      "Iteration: 642/1485.\n",
      "Iteration: 643/1485.\n",
      "Iteration: 644/1485.\n",
      "Iteration: 645/1485.\n",
      "Iteration: 646/1485.\n",
      "Iteration: 647/1485.\n",
      "Iteration: 648/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.149267647181754, 1.2170912215981786, 1.1899954286517869]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.602, Val acc: 0.52\n",
      "\n",
      "\n",
      "Iteration: 649/1485.\n",
      "Iteration: 650/1485.\n",
      "Iteration: 651/1485.\n",
      "Iteration: 652/1485.\n",
      "Iteration: 653/1485.\n",
      "Iteration: 654/1485.\n",
      "Iteration: 655/1485.\n",
      "Iteration: 656/1485.\n",
      "Iteration: 657/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.2292232921812687, 1.3340153408998798, 1.316506950781434]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.554, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 658/1485.\n",
      "Iteration: 659/1485.\n",
      "Iteration: 660/1485.\n",
      "Iteration: 661/1485.\n",
      "Iteration: 662/1485.\n",
      "Iteration: 663/1485.\n",
      "Iteration: 664/1485.\n",
      "Iteration: 665/1485.\n",
      "Iteration: 666/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.1376108296389291, 1.1380914335919465, 1.167066496344811]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.572, Val acc: 0.48\n",
      "\n",
      "\n",
      "Iteration: 667/1485.\n",
      "Iteration: 668/1485.\n",
      "Iteration: 669/1485.\n",
      "Iteration: 670/1485.\n",
      "Iteration: 671/1485.\n",
      "Iteration: 672/1485.\n",
      "Iteration: 673/1485.\n",
      "Iteration: 674/1485.\n",
      "Iteration: 675/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.9732794901445343, 1.2155158493544194, 1.0412060700783392]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.58, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 676/1485.\n",
      "Iteration: 677/1485.\n",
      "Iteration: 678/1485.\n",
      "Iteration: 679/1485.\n",
      "Iteration: 680/1485.\n",
      "Iteration: 681/1485.\n",
      "Iteration: 682/1485.\n",
      "Iteration: 683/1485.\n",
      "Iteration: 684/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.1179898937377633, 1.1725843336479098, 1.170602317393033]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.594, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 685/1485.\n",
      "Iteration: 686/1485.\n",
      "Iteration: 687/1485.\n",
      "Iteration: 688/1485.\n",
      "Iteration: 689/1485.\n",
      "Iteration: 690/1485.\n",
      "Iteration: 691/1485.\n",
      "Iteration: 692/1485.\n",
      "Iteration: 693/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.0778066405746527, 1.2245078988346767, 1.1453827288447078]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.588, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 694/1485.\n",
      "Iteration: 695/1485.\n",
      "Iteration: 696/1485.\n",
      "Iteration: 697/1485.\n",
      "Iteration: 698/1485.\n",
      "Iteration: 699/1485.\n",
      "Iteration: 700/1485.\n",
      "Iteration: 701/1485.\n",
      "Iteration: 702/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.2801919586678006, 0.97703494555639, 1.0875814610548387]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.592, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 703/1485.\n",
      "Iteration: 704/1485.\n",
      "Iteration: 705/1485.\n",
      "Iteration: 706/1485.\n",
      "Iteration: 707/1485.\n",
      "Iteration: 708/1485.\n",
      "Iteration: 709/1485.\n",
      "Iteration: 710/1485.\n",
      "Iteration: 711/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.0205603131007122, 1.1376361761098464, 1.1070220768561414]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.574, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 712/1485.\n",
      "Iteration: 713/1485.\n",
      "Iteration: 714/1485.\n",
      "Iteration: 715/1485.\n",
      "Iteration: 716/1485.\n",
      "Iteration: 717/1485.\n",
      "Iteration: 718/1485.\n",
      "Iteration: 719/1485.\n",
      "Iteration: 720/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.1478082270115944, 0.8837831491111774, 1.1681464638920034]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.596, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 721/1485.\n",
      "Iteration: 722/1485.\n",
      "Iteration: 723/1485.\n",
      "Iteration: 724/1485.\n",
      "Iteration: 725/1485.\n",
      "Iteration: 726/1485.\n",
      "Iteration: 727/1485.\n",
      "Iteration: 728/1485.\n",
      "Iteration: 729/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.1514198520905663, 1.282891466047233, 1.2270859749783023]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.578, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 730/1485.\n",
      "Iteration: 731/1485.\n",
      "Iteration: 732/1485.\n",
      "Iteration: 733/1485.\n",
      "Iteration: 734/1485.\n",
      "Iteration: 735/1485.\n",
      "Iteration: 736/1485.\n",
      "Iteration: 737/1485.\n",
      "Iteration: 738/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.0930238402413017, 0.9062983219563698, 1.0320299024209445]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.562, Val acc: 0.48\n",
      "\n",
      "\n",
      "Iteration: 739/1485.\n",
      "Iteration: 740/1485.\n",
      "Iteration: 741/1485.\n",
      "Iteration: 742/1485.\n",
      "Iteration: 743/1485.\n",
      "Iteration: 744/1485.\n",
      "Iteration: 745/1485.\n",
      "Iteration: 746/1485.\n",
      "Iteration: 747/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.0583500092304987, 1.1762666326568494, 1.2316055395874028]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.64, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 748/1485.\n",
      "Iteration: 749/1485.\n",
      "Iteration: 750/1485.\n",
      "Iteration: 751/1485.\n",
      "Iteration: 752/1485.\n",
      "Iteration: 753/1485.\n",
      "Iteration: 754/1485.\n",
      "Iteration: 755/1485.\n",
      "Iteration: 756/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.3264721012204108, 1.221411701337184, 0.9065824287396985]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.608, Val acc: 0.48\n",
      "\n",
      "\n",
      "Iteration: 757/1485.\n",
      "Iteration: 758/1485.\n",
      "Iteration: 759/1485.\n",
      "Iteration: 760/1485.\n",
      "Iteration: 761/1485.\n",
      "Iteration: 762/1485.\n",
      "Iteration: 763/1485.\n",
      "Iteration: 764/1485.\n",
      "Iteration: 765/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.0802620472527682, 1.1025919171874465, 1.0120444707935587]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.624, Val acc: 0.48\n",
      "\n",
      "\n",
      "Iteration: 766/1485.\n",
      "Iteration: 767/1485.\n",
      "Iteration: 768/1485.\n",
      "Iteration: 769/1485.\n",
      "Iteration: 770/1485.\n",
      "Iteration: 771/1485.\n",
      "Iteration: 772/1485.\n",
      "Iteration: 773/1485.\n",
      "Iteration: 774/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.1447184625901943, 1.0899537619231097, 0.9529898810283335]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.626, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 775/1485.\n",
      "Iteration: 776/1485.\n",
      "Iteration: 777/1485.\n",
      "Iteration: 778/1485.\n",
      "Iteration: 779/1485.\n",
      "Iteration: 780/1485.\n",
      "Iteration: 781/1485.\n",
      "Iteration: 782/1485.\n",
      "Iteration: 783/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.0990496667238314, 1.0511274319263828, 1.0916851584340321]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.606, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 784/1485.\n",
      "Iteration: 785/1485.\n",
      "Iteration: 786/1485.\n",
      "Iteration: 787/1485.\n",
      "Iteration: 788/1485.\n",
      "Iteration: 789/1485.\n",
      "Iteration: 790/1485.\n",
      "Iteration: 791/1485.\n",
      "Iteration: 792/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.0364291762384132, 1.1909746131119958, 1.0429743639887814]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.59, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 793/1485.\n",
      "Iteration: 794/1485.\n",
      "Iteration: 795/1485.\n",
      "Iteration: 796/1485.\n",
      "Iteration: 797/1485.\n",
      "Iteration: 798/1485.\n",
      "Iteration: 799/1485.\n",
      "Iteration: 800/1485.\n",
      "Iteration: 801/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.1963783449148222, 1.2563248625307917, 1.1093455191618644]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.694, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 802/1485.\n",
      "Iteration: 803/1485.\n",
      "Iteration: 804/1485.\n",
      "Iteration: 805/1485.\n",
      "Iteration: 806/1485.\n",
      "Iteration: 807/1485.\n",
      "Iteration: 808/1485.\n",
      "Iteration: 809/1485.\n",
      "Iteration: 810/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.1138202091600506, 1.1476933928544109, 1.106431851216113]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.632, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 811/1485.\n",
      "Iteration: 812/1485.\n",
      "Iteration: 813/1485.\n",
      "Iteration: 814/1485.\n",
      "Iteration: 815/1485.\n",
      "Iteration: 816/1485.\n",
      "Iteration: 817/1485.\n",
      "Iteration: 818/1485.\n",
      "Iteration: 819/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.8609561814126732, 1.0034614600777065, 1.018394639053404]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.604, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 820/1485.\n",
      "Iteration: 821/1485.\n",
      "Iteration: 822/1485.\n",
      "Iteration: 823/1485.\n",
      "Iteration: 824/1485.\n",
      "Iteration: 825/1485.\n",
      "Iteration: 826/1485.\n",
      "Iteration: 827/1485.\n",
      "Iteration: 828/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.0445380097422632, 0.8883567805140967, 0.9661278246757052]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.564, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 829/1485.\n",
      "Iteration: 830/1485.\n",
      "Iteration: 831/1485.\n",
      "Iteration: 832/1485.\n",
      "Iteration: 833/1485.\n",
      "Iteration: 834/1485.\n",
      "Iteration: 835/1485.\n",
      "Iteration: 836/1485.\n",
      "Iteration: 837/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.111135023748373, 0.9141386447688744, 0.8805842883181758]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.634, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 838/1485.\n",
      "Iteration: 839/1485.\n",
      "Iteration: 840/1485.\n",
      "Iteration: 841/1485.\n",
      "Iteration: 842/1485.\n",
      "Iteration: 843/1485.\n",
      "Iteration: 844/1485.\n",
      "Iteration: 845/1485.\n",
      "Iteration: 846/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.0002711688318313, 0.9913188854642503, 1.026928002895162]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.582, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 847/1485.\n",
      "Iteration: 848/1485.\n",
      "Iteration: 849/1485.\n",
      "Iteration: 850/1485.\n",
      "Iteration: 851/1485.\n",
      "Iteration: 852/1485.\n",
      "Iteration: 853/1485.\n",
      "Iteration: 854/1485.\n",
      "Iteration: 855/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.9955307838435138, 0.9468434672907078, 0.909510587194518]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.656, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 856/1485.\n",
      "Iteration: 857/1485.\n",
      "Iteration: 858/1485.\n",
      "Iteration: 859/1485.\n",
      "Iteration: 860/1485.\n",
      "Iteration: 861/1485.\n",
      "Iteration: 862/1485.\n",
      "Iteration: 863/1485.\n",
      "Iteration: 864/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.9811743122051134, 1.0150771635573825, 0.9703557499567347]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.658, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 865/1485.\n",
      "Iteration: 866/1485.\n",
      "Iteration: 867/1485.\n",
      "Iteration: 868/1485.\n",
      "Iteration: 869/1485.\n",
      "Iteration: 870/1485.\n",
      "Iteration: 871/1485.\n",
      "Iteration: 872/1485.\n",
      "Iteration: 873/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.9406615313276104, 0.9109401796251315, 0.8402450801497874]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.676, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 874/1485.\n",
      "Iteration: 875/1485.\n",
      "Iteration: 876/1485.\n",
      "Iteration: 877/1485.\n",
      "Iteration: 878/1485.\n",
      "Iteration: 879/1485.\n",
      "Iteration: 880/1485.\n",
      "Iteration: 881/1485.\n",
      "Iteration: 882/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.834298197414362, 0.887610646386123, 0.986976499480563]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.66, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 883/1485.\n",
      "Iteration: 884/1485.\n",
      "Iteration: 885/1485.\n",
      "Iteration: 886/1485.\n",
      "Iteration: 887/1485.\n",
      "Iteration: 888/1485.\n",
      "Iteration: 889/1485.\n",
      "Iteration: 890/1485.\n",
      "Iteration: 891/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.9276640249169738, 0.927547249218368, 0.9471106566545434]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.712, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 892/1485.\n",
      "Iteration: 893/1485.\n",
      "Iteration: 894/1485.\n",
      "Iteration: 895/1485.\n",
      "Iteration: 896/1485.\n",
      "Iteration: 897/1485.\n",
      "Iteration: 898/1485.\n",
      "Iteration: 899/1485.\n",
      "Iteration: 900/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.9067815395714018, 1.0454335731345998, 1.0607225993335299]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.658, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 901/1485.\n",
      "Iteration: 902/1485.\n",
      "Iteration: 903/1485.\n",
      "Iteration: 904/1485.\n",
      "Iteration: 905/1485.\n",
      "Iteration: 906/1485.\n",
      "Iteration: 907/1485.\n",
      "Iteration: 908/1485.\n",
      "Iteration: 909/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.8427859543136914, 0.942160493663663, 0.9567560222790195]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.642, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 910/1485.\n",
      "Iteration: 911/1485.\n",
      "Iteration: 912/1485.\n",
      "Iteration: 913/1485.\n",
      "Iteration: 914/1485.\n",
      "Iteration: 915/1485.\n",
      "Iteration: 916/1485.\n",
      "Iteration: 917/1485.\n",
      "Iteration: 918/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.9418253904827638, 0.9093623970922358, 0.8483660994376551]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.654, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 919/1485.\n",
      "Iteration: 920/1485.\n",
      "Iteration: 921/1485.\n",
      "Iteration: 922/1485.\n",
      "Iteration: 923/1485.\n",
      "Iteration: 924/1485.\n",
      "Iteration: 925/1485.\n",
      "Iteration: 926/1485.\n",
      "Iteration: 927/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.9651747623553592, 1.159818800455569, 0.9212908881239409]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.696, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 928/1485.\n",
      "Iteration: 929/1485.\n",
      "Iteration: 930/1485.\n",
      "Iteration: 931/1485.\n",
      "Iteration: 932/1485.\n",
      "Iteration: 933/1485.\n",
      "Iteration: 934/1485.\n",
      "Iteration: 935/1485.\n",
      "Iteration: 936/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.8446944992537003, 1.096209980508435, 0.8915794111385088]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.712, Val acc: 0.48\n",
      "\n",
      "\n",
      "Iteration: 937/1485.\n",
      "Iteration: 938/1485.\n",
      "Iteration: 939/1485.\n",
      "Iteration: 940/1485.\n",
      "Iteration: 941/1485.\n",
      "Iteration: 942/1485.\n",
      "Iteration: 943/1485.\n",
      "Iteration: 944/1485.\n",
      "Iteration: 945/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.7688078617990485, 0.7644698957362109, 0.8773279212360339]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.68, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 946/1485.\n",
      "Iteration: 947/1485.\n",
      "Iteration: 948/1485.\n",
      "Iteration: 949/1485.\n",
      "Iteration: 950/1485.\n",
      "Iteration: 951/1485.\n",
      "Iteration: 952/1485.\n",
      "Iteration: 953/1485.\n",
      "Iteration: 954/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.7865605967915533, 0.9845096679606521, 0.9899391147072014]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.668, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 955/1485.\n",
      "Iteration: 956/1485.\n",
      "Iteration: 957/1485.\n",
      "Iteration: 958/1485.\n",
      "Iteration: 959/1485.\n",
      "Iteration: 960/1485.\n",
      "Iteration: 961/1485.\n",
      "Iteration: 962/1485.\n",
      "Iteration: 963/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.7509827576596279, 0.7351952118999756, 0.7910339870439728]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.668, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 964/1485.\n",
      "Iteration: 965/1485.\n",
      "Iteration: 966/1485.\n",
      "Iteration: 967/1485.\n",
      "Iteration: 968/1485.\n",
      "Iteration: 969/1485.\n",
      "Iteration: 970/1485.\n",
      "Iteration: 971/1485.\n",
      "Iteration: 972/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.7365149117299004, 0.9026675413469291, 0.695172115863497]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.756, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 973/1485.\n",
      "Iteration: 974/1485.\n",
      "Iteration: 975/1485.\n",
      "Iteration: 976/1485.\n",
      "Iteration: 977/1485.\n",
      "Iteration: 978/1485.\n",
      "Iteration: 979/1485.\n",
      "Iteration: 980/1485.\n",
      "Iteration: 981/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.925344185901762, 0.8238442869202974, 0.9877696822090704]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.662, Val acc: 0.5\n",
      "\n",
      "\n",
      "Iteration: 982/1485.\n",
      "Iteration: 983/1485.\n",
      "Iteration: 984/1485.\n",
      "Iteration: 985/1485.\n",
      "Iteration: 986/1485.\n",
      "Iteration: 987/1485.\n",
      "Iteration: 988/1485.\n",
      "Iteration: 989/1485.\n",
      "Iteration: 990/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.8248673100379585, 0.7355965321264111, 0.8985336771716618]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.664, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 991/1485.\n",
      "Iteration: 992/1485.\n",
      "Iteration: 993/1485.\n",
      "Iteration: 994/1485.\n",
      "Iteration: 995/1485.\n",
      "Iteration: 996/1485.\n",
      "Iteration: 997/1485.\n",
      "Iteration: 998/1485.\n",
      "Iteration: 999/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.8892037729181188, 0.7011140008324215, 0.740212031718093]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.734, Val acc: 0.48\n",
      "\n",
      "\n",
      "Iteration: 1000/1485.\n",
      "Iteration: 1001/1485.\n",
      "Iteration: 1002/1485.\n",
      "Iteration: 1003/1485.\n",
      "Iteration: 1004/1485.\n",
      "Iteration: 1005/1485.\n",
      "Iteration: 1006/1485.\n",
      "Iteration: 1007/1485.\n",
      "Iteration: 1008/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.9184380152952702, 0.9290785442962054, 0.6458994910487427]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.734, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 1009/1485.\n",
      "Iteration: 1010/1485.\n",
      "Iteration: 1011/1485.\n",
      "Iteration: 1012/1485.\n",
      "Iteration: 1013/1485.\n",
      "Iteration: 1014/1485.\n",
      "Iteration: 1015/1485.\n",
      "Iteration: 1016/1485.\n",
      "Iteration: 1017/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.6110802963595195, 0.8353633715815714, 0.9436510668157254]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.708, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 1018/1485.\n",
      "Iteration: 1019/1485.\n",
      "Iteration: 1020/1485.\n",
      "Iteration: 1021/1485.\n",
      "Iteration: 1022/1485.\n",
      "Iteration: 1023/1485.\n",
      "Iteration: 1024/1485.\n",
      "Iteration: 1025/1485.\n",
      "Iteration: 1026/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.8409701561882207, 0.8740852922054083, 0.7641421266457356]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.688, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1027/1485.\n",
      "Iteration: 1028/1485.\n",
      "Iteration: 1029/1485.\n",
      "Iteration: 1030/1485.\n",
      "Iteration: 1031/1485.\n",
      "Iteration: 1032/1485.\n",
      "Iteration: 1033/1485.\n",
      "Iteration: 1034/1485.\n",
      "Iteration: 1035/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.810738861945341, 0.8330717876841013, 0.7041309004750573]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.708, Val acc: 0.56\n",
      "\n",
      "\n",
      "Iteration: 1036/1485.\n",
      "Iteration: 1037/1485.\n",
      "Iteration: 1038/1485.\n",
      "Iteration: 1039/1485.\n",
      "Iteration: 1040/1485.\n",
      "Iteration: 1041/1485.\n",
      "Iteration: 1042/1485.\n",
      "Iteration: 1043/1485.\n",
      "Iteration: 1044/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.8060342692596143, 0.6729492997373338, 0.8502771092327411]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.748, Val acc: 0.48\n",
      "\n",
      "\n",
      "Iteration: 1045/1485.\n",
      "Iteration: 1046/1485.\n",
      "Iteration: 1047/1485.\n",
      "Iteration: 1048/1485.\n",
      "Iteration: 1049/1485.\n",
      "Iteration: 1050/1485.\n",
      "Iteration: 1051/1485.\n",
      "Iteration: 1052/1485.\n",
      "Iteration: 1053/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.7908500295635194, 0.7012661165339732, 0.650527484159589]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.734, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 1054/1485.\n",
      "Iteration: 1055/1485.\n",
      "Iteration: 1056/1485.\n",
      "Iteration: 1057/1485.\n",
      "Iteration: 1058/1485.\n",
      "Iteration: 1059/1485.\n",
      "Iteration: 1060/1485.\n",
      "Iteration: 1061/1485.\n",
      "Iteration: 1062/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.9388494247142442, 0.9571191397076626, 0.8698137248879403]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.694, Val acc: 0.54\n",
      "\n",
      "\n",
      "Iteration: 1063/1485.\n",
      "Iteration: 1064/1485.\n",
      "Iteration: 1065/1485.\n",
      "Iteration: 1066/1485.\n",
      "Iteration: 1067/1485.\n",
      "Iteration: 1068/1485.\n",
      "Iteration: 1069/1485.\n",
      "Iteration: 1070/1485.\n",
      "Iteration: 1071/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.7380067901991326, 0.7549184449389458, 0.6938185720656685]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.748, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 1072/1485.\n",
      "Iteration: 1073/1485.\n",
      "Iteration: 1074/1485.\n",
      "Iteration: 1075/1485.\n",
      "Iteration: 1076/1485.\n",
      "Iteration: 1077/1485.\n",
      "Iteration: 1078/1485.\n",
      "Iteration: 1079/1485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1080/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.7618266713276903, 0.6544097563019945, 0.6942676964617512]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.758, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 1081/1485.\n",
      "Iteration: 1082/1485.\n",
      "Iteration: 1083/1485.\n",
      "Iteration: 1084/1485.\n",
      "Iteration: 1085/1485.\n",
      "Iteration: 1086/1485.\n",
      "Iteration: 1087/1485.\n",
      "Iteration: 1088/1485.\n",
      "Iteration: 1089/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.8399014777650902, 0.7735057554807135, 0.7409869177520839]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.72, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 1090/1485.\n",
      "Iteration: 1091/1485.\n",
      "Iteration: 1092/1485.\n",
      "Iteration: 1093/1485.\n",
      "Iteration: 1094/1485.\n",
      "Iteration: 1095/1485.\n",
      "Iteration: 1096/1485.\n",
      "Iteration: 1097/1485.\n",
      "Iteration: 1098/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.8304020598200471, 0.9913156545225121, 0.9114896348877648]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.722, Val acc: 0.52\n",
      "\n",
      "\n",
      "Iteration: 1099/1485.\n",
      "Iteration: 1100/1485.\n",
      "Iteration: 1101/1485.\n",
      "Iteration: 1102/1485.\n",
      "Iteration: 1103/1485.\n",
      "Iteration: 1104/1485.\n",
      "Iteration: 1105/1485.\n",
      "Iteration: 1106/1485.\n",
      "Iteration: 1107/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.8801226846927083, 0.8727642513708814, 0.6981917657202653]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.726, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1108/1485.\n",
      "Iteration: 1109/1485.\n",
      "Iteration: 1110/1485.\n",
      "Iteration: 1111/1485.\n",
      "Iteration: 1112/1485.\n",
      "Iteration: 1113/1485.\n",
      "Iteration: 1114/1485.\n",
      "Iteration: 1115/1485.\n",
      "Iteration: 1116/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.6316050864357038, 0.5791042187264205, 0.8872640252761416]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.722, Val acc: 0.54\n",
      "\n",
      "\n",
      "Iteration: 1117/1485.\n",
      "Iteration: 1118/1485.\n",
      "Iteration: 1119/1485.\n",
      "Iteration: 1120/1485.\n",
      "Iteration: 1121/1485.\n",
      "Iteration: 1122/1485.\n",
      "Iteration: 1123/1485.\n",
      "Iteration: 1124/1485.\n",
      "Iteration: 1125/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.63291807424533, 0.6792139173041799, 0.621702055230249]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.754, Val acc: 0.56\n",
      "\n",
      "\n",
      "Iteration: 1126/1485.\n",
      "Iteration: 1127/1485.\n",
      "Iteration: 1128/1485.\n",
      "Iteration: 1129/1485.\n",
      "Iteration: 1130/1485.\n",
      "Iteration: 1131/1485.\n",
      "Iteration: 1132/1485.\n",
      "Iteration: 1133/1485.\n",
      "Iteration: 1134/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.7197501131775544, 0.7694791531468543, 0.7438660433046468]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.756, Val acc: 0.5\n",
      "\n",
      "\n",
      "Iteration: 1135/1485.\n",
      "Iteration: 1136/1485.\n",
      "Iteration: 1137/1485.\n",
      "Iteration: 1138/1485.\n",
      "Iteration: 1139/1485.\n",
      "Iteration: 1140/1485.\n",
      "Iteration: 1141/1485.\n",
      "Iteration: 1142/1485.\n",
      "Iteration: 1143/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.6631156681014011, 0.5730173081895705, 0.6119822667371536]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.734, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 1144/1485.\n",
      "Iteration: 1145/1485.\n",
      "Iteration: 1146/1485.\n",
      "Iteration: 1147/1485.\n",
      "Iteration: 1148/1485.\n",
      "Iteration: 1149/1485.\n",
      "Iteration: 1150/1485.\n",
      "Iteration: 1151/1485.\n",
      "Iteration: 1152/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.6952082168574143, 0.6152646015233146, 0.8531127641946726]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.796, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1153/1485.\n",
      "Iteration: 1154/1485.\n",
      "Iteration: 1155/1485.\n",
      "Iteration: 1156/1485.\n",
      "Iteration: 1157/1485.\n",
      "Iteration: 1158/1485.\n",
      "Iteration: 1159/1485.\n",
      "Iteration: 1160/1485.\n",
      "Iteration: 1161/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.5209582227267812, 0.7639332054499453, 0.7665743395499984]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.73, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 1162/1485.\n",
      "Iteration: 1163/1485.\n",
      "Iteration: 1164/1485.\n",
      "Iteration: 1165/1485.\n",
      "Iteration: 1166/1485.\n",
      "Iteration: 1167/1485.\n",
      "Iteration: 1168/1485.\n",
      "Iteration: 1169/1485.\n",
      "Iteration: 1170/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.5731906816326281, 0.7685427264285799, 0.7956345723199787]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.68, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 1171/1485.\n",
      "Iteration: 1172/1485.\n",
      "Iteration: 1173/1485.\n",
      "Iteration: 1174/1485.\n",
      "Iteration: 1175/1485.\n",
      "Iteration: 1176/1485.\n",
      "Iteration: 1177/1485.\n",
      "Iteration: 1178/1485.\n",
      "Iteration: 1179/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.6268765319014213, 0.480560748121569, 0.7228226575614985]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.696, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1180/1485.\n",
      "Iteration: 1181/1485.\n",
      "Iteration: 1182/1485.\n",
      "Iteration: 1183/1485.\n",
      "Iteration: 1184/1485.\n",
      "Iteration: 1185/1485.\n",
      "Iteration: 1186/1485.\n",
      "Iteration: 1187/1485.\n",
      "Iteration: 1188/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.751318176380917, 0.663475217216961, 0.7686533706525158]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.732, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 1189/1485.\n",
      "Iteration: 1190/1485.\n",
      "Iteration: 1191/1485.\n",
      "Iteration: 1192/1485.\n",
      "Iteration: 1193/1485.\n",
      "Iteration: 1194/1485.\n",
      "Iteration: 1195/1485.\n",
      "Iteration: 1196/1485.\n",
      "Iteration: 1197/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.7042272756782721, 0.8056101972571197, 0.6453387372560261]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.792, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 1198/1485.\n",
      "Iteration: 1199/1485.\n",
      "Iteration: 1200/1485.\n",
      "Iteration: 1201/1485.\n",
      "Iteration: 1202/1485.\n",
      "Iteration: 1203/1485.\n",
      "Iteration: 1204/1485.\n",
      "Iteration: 1205/1485.\n",
      "Iteration: 1206/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.639082832930546, 0.7381953824752867, 0.6782729584242994]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.814, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 1207/1485.\n",
      "Iteration: 1208/1485.\n",
      "Iteration: 1209/1485.\n",
      "Iteration: 1210/1485.\n",
      "Iteration: 1211/1485.\n",
      "Iteration: 1212/1485.\n",
      "Iteration: 1213/1485.\n",
      "Iteration: 1214/1485.\n",
      "Iteration: 1215/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.6068370849682713, 0.5879709992917811, 0.5672012636685473]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.806, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1216/1485.\n",
      "Iteration: 1217/1485.\n",
      "Iteration: 1218/1485.\n",
      "Iteration: 1219/1485.\n",
      "Iteration: 1220/1485.\n",
      "Iteration: 1221/1485.\n",
      "Iteration: 1222/1485.\n",
      "Iteration: 1223/1485.\n",
      "Iteration: 1224/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.5780939199020525, 0.7247082640470212, 0.5653762183912656]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.83, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 1225/1485.\n",
      "Iteration: 1226/1485.\n",
      "Iteration: 1227/1485.\n",
      "Iteration: 1228/1485.\n",
      "Iteration: 1229/1485.\n",
      "Iteration: 1230/1485.\n",
      "Iteration: 1231/1485.\n",
      "Iteration: 1232/1485.\n",
      "Iteration: 1233/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.78460821479328, 0.7481795855584596, 0.5735905983712006]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.786, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1234/1485.\n",
      "Iteration: 1235/1485.\n",
      "Iteration: 1236/1485.\n",
      "Iteration: 1237/1485.\n",
      "Iteration: 1238/1485.\n",
      "Iteration: 1239/1485.\n",
      "Iteration: 1240/1485.\n",
      "Iteration: 1241/1485.\n",
      "Iteration: 1242/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.49782751485361315, 0.518695511448751, 0.4330243458312796]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.788, Val acc: 0.52\n",
      "\n",
      "\n",
      "Iteration: 1243/1485.\n",
      "Iteration: 1244/1485.\n",
      "Iteration: 1245/1485.\n",
      "Iteration: 1246/1485.\n",
      "Iteration: 1247/1485.\n",
      "Iteration: 1248/1485.\n",
      "Iteration: 1249/1485.\n",
      "Iteration: 1250/1485.\n",
      "Iteration: 1251/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.7345405892814383, 0.8045199797421269, 0.7636770092255766]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.708, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 1252/1485.\n",
      "Iteration: 1253/1485.\n",
      "Iteration: 1254/1485.\n",
      "Iteration: 1255/1485.\n",
      "Iteration: 1256/1485.\n",
      "Iteration: 1257/1485.\n",
      "Iteration: 1258/1485.\n",
      "Iteration: 1259/1485.\n",
      "Iteration: 1260/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.5175385932679349, 0.5081483352699074, 0.5447556507061871]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.808, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 1261/1485.\n",
      "Iteration: 1262/1485.\n",
      "Iteration: 1263/1485.\n",
      "Iteration: 1264/1485.\n",
      "Iteration: 1265/1485.\n",
      "Iteration: 1266/1485.\n",
      "Iteration: 1267/1485.\n",
      "Iteration: 1268/1485.\n",
      "Iteration: 1269/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.5089409524188038, 0.6254075863815982, 0.5875134817913333]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.802, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 1270/1485.\n",
      "Iteration: 1271/1485.\n",
      "Iteration: 1272/1485.\n",
      "Iteration: 1273/1485.\n",
      "Iteration: 1274/1485.\n",
      "Iteration: 1275/1485.\n",
      "Iteration: 1276/1485.\n",
      "Iteration: 1277/1485.\n",
      "Iteration: 1278/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.6160500412594098, 0.530167483128414, 0.4309412512116603]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.794, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 1279/1485.\n",
      "Iteration: 1280/1485.\n",
      "Iteration: 1281/1485.\n",
      "Iteration: 1282/1485.\n",
      "Iteration: 1283/1485.\n",
      "Iteration: 1284/1485.\n",
      "Iteration: 1285/1485.\n",
      "Iteration: 1286/1485.\n",
      "Iteration: 1287/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.579840972661745, 0.48281517248800215, 0.5425944702214803]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.842, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 1288/1485.\n",
      "Iteration: 1289/1485.\n",
      "Iteration: 1290/1485.\n",
      "Iteration: 1291/1485.\n",
      "Iteration: 1292/1485.\n",
      "Iteration: 1293/1485.\n",
      "Iteration: 1294/1485.\n",
      "Iteration: 1295/1485.\n",
      "Iteration: 1296/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.7177733470744266, 0.459175577854445, 0.47101352608564384]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.846, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 1297/1485.\n",
      "Iteration: 1298/1485.\n",
      "Iteration: 1299/1485.\n",
      "Iteration: 1300/1485.\n",
      "Iteration: 1301/1485.\n",
      "Iteration: 1302/1485.\n",
      "Iteration: 1303/1485.\n",
      "Iteration: 1304/1485.\n",
      "Iteration: 1305/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.672252163739545, 0.947235740734324, 0.6084319125240255]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.812, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 1306/1485.\n",
      "Iteration: 1307/1485.\n",
      "Iteration: 1308/1485.\n",
      "Iteration: 1309/1485.\n",
      "Iteration: 1310/1485.\n",
      "Iteration: 1311/1485.\n",
      "Iteration: 1312/1485.\n",
      "Iteration: 1313/1485.\n",
      "Iteration: 1314/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.5068521615231688, 0.463545003902302, 0.4666935005219137]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.79, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 1315/1485.\n",
      "Iteration: 1316/1485.\n",
      "Iteration: 1317/1485.\n",
      "Iteration: 1318/1485.\n",
      "Iteration: 1319/1485.\n",
      "Iteration: 1320/1485.\n",
      "Iteration: 1321/1485.\n",
      "Iteration: 1322/1485.\n",
      "Iteration: 1323/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.3813650650741876, 0.4080350613834247, 0.47137253347218006]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.85, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 1324/1485.\n",
      "Iteration: 1325/1485.\n",
      "Iteration: 1326/1485.\n",
      "Iteration: 1327/1485.\n",
      "Iteration: 1328/1485.\n",
      "Iteration: 1329/1485.\n",
      "Iteration: 1330/1485.\n",
      "Iteration: 1331/1485.\n",
      "Iteration: 1332/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.4088379942447534, 0.4672052283475314, 0.504108226367286]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.878, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1333/1485.\n",
      "Iteration: 1334/1485.\n",
      "Iteration: 1335/1485.\n",
      "Iteration: 1336/1485.\n",
      "Iteration: 1337/1485.\n",
      "Iteration: 1338/1485.\n",
      "Iteration: 1339/1485.\n",
      "Iteration: 1340/1485.\n",
      "Iteration: 1341/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.6220278938817991, 0.5962830695900962, 0.34129214069782876]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.874, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 1342/1485.\n",
      "Iteration: 1343/1485.\n",
      "Iteration: 1344/1485.\n",
      "Iteration: 1345/1485.\n",
      "Iteration: 1346/1485.\n",
      "Iteration: 1347/1485.\n",
      "Iteration: 1348/1485.\n",
      "Iteration: 1349/1485.\n",
      "Iteration: 1350/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.6044496139368165, 0.5840754718879066, 0.5044818365703043]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.854, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1351/1485.\n",
      "Iteration: 1352/1485.\n",
      "Iteration: 1353/1485.\n",
      "Iteration: 1354/1485.\n",
      "Iteration: 1355/1485.\n",
      "Iteration: 1356/1485.\n",
      "Iteration: 1357/1485.\n",
      "Iteration: 1358/1485.\n",
      "Iteration: 1359/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.32899429764581906, 0.4552106207073862, 0.7136078117397855]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.764, Val acc: 0.48\n",
      "\n",
      "\n",
      "Iteration: 1360/1485.\n",
      "Iteration: 1361/1485.\n",
      "Iteration: 1362/1485.\n",
      "Iteration: 1363/1485.\n",
      "Iteration: 1364/1485.\n",
      "Iteration: 1365/1485.\n",
      "Iteration: 1366/1485.\n",
      "Iteration: 1367/1485.\n",
      "Iteration: 1368/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.46024045344941455, 0.3861036337542681, 0.33061367599746955]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.834, Val acc: 0.52\n",
      "\n",
      "\n",
      "Iteration: 1369/1485.\n",
      "Iteration: 1370/1485.\n",
      "Iteration: 1371/1485.\n",
      "Iteration: 1372/1485.\n",
      "Iteration: 1373/1485.\n",
      "Iteration: 1374/1485.\n",
      "Iteration: 1375/1485.\n",
      "Iteration: 1376/1485.\n",
      "Iteration: 1377/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.5709725690818614, 0.7383884668077255, 0.7443166812124444]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.866, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 1378/1485.\n",
      "Iteration: 1379/1485.\n",
      "Iteration: 1380/1485.\n",
      "Iteration: 1381/1485.\n",
      "Iteration: 1382/1485.\n",
      "Iteration: 1383/1485.\n",
      "Iteration: 1384/1485.\n",
      "Iteration: 1385/1485.\n",
      "Iteration: 1386/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.6862983479637055, 0.5373638291230602, 0.36242148872632324]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.8, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 1387/1485.\n",
      "Iteration: 1388/1485.\n",
      "Iteration: 1389/1485.\n",
      "Iteration: 1390/1485.\n",
      "Iteration: 1391/1485.\n",
      "Iteration: 1392/1485.\n",
      "Iteration: 1393/1485.\n",
      "Iteration: 1394/1485.\n",
      "Iteration: 1395/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.6533441609655329, 0.43619782183473177, 0.528587017352147]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.91, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 1396/1485.\n",
      "Iteration: 1397/1485.\n",
      "Iteration: 1398/1485.\n",
      "Iteration: 1399/1485.\n",
      "Iteration: 1400/1485.\n",
      "Iteration: 1401/1485.\n",
      "Iteration: 1402/1485.\n",
      "Iteration: 1403/1485.\n",
      "Iteration: 1404/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.350461735448233, 0.41170360474014395, 0.42076174464184746]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.882, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 1405/1485.\n",
      "Iteration: 1406/1485.\n",
      "Iteration: 1407/1485.\n",
      "Iteration: 1408/1485.\n",
      "Iteration: 1409/1485.\n",
      "Iteration: 1410/1485.\n",
      "Iteration: 1411/1485.\n",
      "Iteration: 1412/1485.\n",
      "Iteration: 1413/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.3260826188729237, 0.40531616950327987, 0.3785891485339032]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.906, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 1414/1485.\n",
      "Iteration: 1415/1485.\n",
      "Iteration: 1416/1485.\n",
      "Iteration: 1417/1485.\n",
      "Iteration: 1418/1485.\n",
      "Iteration: 1419/1485.\n",
      "Iteration: 1420/1485.\n",
      "Iteration: 1421/1485.\n",
      "Iteration: 1422/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.3576551865052129, 0.36084371367343004, 0.4148165053537193]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.862, Val acc: 0.54\n",
      "\n",
      "\n",
      "Iteration: 1423/1485.\n",
      "Iteration: 1424/1485.\n",
      "Iteration: 1425/1485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1426/1485.\n",
      "Iteration: 1427/1485.\n",
      "Iteration: 1428/1485.\n",
      "Iteration: 1429/1485.\n",
      "Iteration: 1430/1485.\n",
      "Iteration: 1431/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.48276099649321697, 0.3527491098812574, 0.41736246153234535]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.808, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 1432/1485.\n",
      "Iteration: 1433/1485.\n",
      "Iteration: 1434/1485.\n",
      "Iteration: 1435/1485.\n",
      "Iteration: 1436/1485.\n",
      "Iteration: 1437/1485.\n",
      "Iteration: 1438/1485.\n",
      "Iteration: 1439/1485.\n",
      "Iteration: 1440/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.3348515160734279, 0.4061479869006761, 0.41179483405976414]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.884, Val acc: 0.5\n",
      "\n",
      "\n",
      "Iteration: 1441/1485.\n",
      "Iteration: 1442/1485.\n",
      "Iteration: 1443/1485.\n",
      "Iteration: 1444/1485.\n",
      "Iteration: 1445/1485.\n",
      "Iteration: 1446/1485.\n",
      "Iteration: 1447/1485.\n",
      "Iteration: 1448/1485.\n",
      "Iteration: 1449/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.5211939153493819, 0.4032457683985728, 0.4898039398492619]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.832, Val acc: 0.48\n",
      "\n",
      "\n",
      "Iteration: 1450/1485.\n",
      "Iteration: 1451/1485.\n",
      "Iteration: 1452/1485.\n",
      "Iteration: 1453/1485.\n",
      "Iteration: 1454/1485.\n",
      "Iteration: 1455/1485.\n",
      "Iteration: 1456/1485.\n",
      "Iteration: 1457/1485.\n",
      "Iteration: 1458/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.41313489222349736, 0.39115101084865694, 0.37118335767564764]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.85, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 1459/1485.\n",
      "Iteration: 1460/1485.\n",
      "Iteration: 1461/1485.\n",
      "Iteration: 1462/1485.\n",
      "Iteration: 1463/1485.\n",
      "Iteration: 1464/1485.\n",
      "Iteration: 1465/1485.\n",
      "Iteration: 1466/1485.\n",
      "Iteration: 1467/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.36217147059330607, 0.387726352198783, 0.4493382025518831]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.866, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1468/1485.\n",
      "Iteration: 1469/1485.\n",
      "Iteration: 1470/1485.\n",
      "Iteration: 1471/1485.\n",
      "Iteration: 1472/1485.\n",
      "Iteration: 1473/1485.\n",
      "Iteration: 1474/1485.\n",
      "Iteration: 1475/1485.\n",
      "Iteration: 1476/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.47202133505515453, 0.3926162134692519, 0.39086659087194336]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.894, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1477/1485.\n",
      "Iteration: 1478/1485.\n",
      "Iteration: 1479/1485.\n",
      "Iteration: 1480/1485.\n",
      "Iteration: 1481/1485.\n",
      "Iteration: 1482/1485.\n",
      "Iteration: 1483/1485.\n",
      "Iteration: 1484/1485.\n",
      "Iteration: 1485/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.45236176908635317, 0.4599645705081885, 0.5542665687539174]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.87, Val acc: 0.52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------FINAL OUTPUT----------------\n",
      "  Train acc: 0.896, Val acc: 0.52\n",
      "Loss history: [2.3024789886306367, 2.3022782304464005, 2.3027355094703834, 2.302908300934751, 2.301313538266866, 2.2997475820966815, 2.301873147133011, 2.2500227794087517, 2.2528956186527007, 2.2553535514337635, 2.232264837396803, 2.186655129036276, 2.2135830241707137, 2.1866392299219055, 2.2478099557307916, 2.230990432879249, 2.1149906439690698, 2.1096766136833516, 2.029248812438277, 2.157151373389407, 2.0652548287714967, 1.9917966797913114, 1.9839803965226401, 1.895538890169808, 2.040286746403391, 2.091883500155575, 1.8921736129986013, 1.9860102856943298, 1.9657236573277361, 1.8864232168059119, 1.88401019275079, 1.8304871288600362, 1.9866992411445683, 2.051447662682095, 1.9357271153799098, 1.9488415889766082, 1.9676938569918159, 1.9052640121584248, 1.845657303552177, 1.8644559332688722, 2.083433445113628, 1.9581851806104886, 1.8437983257686832, 1.9559151087429394, 1.9747659584310215, 1.880710737469236, 2.103372695915405, 2.0215818989054886, 1.8511676808493474, 1.876521960226766, 1.8443780198003026, 1.7713060600649888, 1.7719742315598266, 1.9707207656255987, 1.980819996234403, 1.8230172016229051, 1.968865610723164, 1.9784552739790644, 1.7572298015207037, 1.8087773128447373, 1.8416558088355028, 1.9059486219474309, 1.8519092702673123, 1.829075111494044, 1.9959876909941856, 1.877187275096096, 1.7312219503561233, 1.6965375050542202, 1.8918808983021638, 1.9017265208068406, 1.9300900062945936, 1.954619791558879, 1.927137852155666, 1.9765861547367756, 1.908756217701468, 1.8136563570562907, 1.8893119226550805, 1.8932207590826073, 1.9407716429381854, 1.9393610308808562, 1.7272131214182989, 1.8181180209943628, 1.7592326395886628, 1.9240010824412395, 1.967125350514675, 1.814538347069329, 1.6945362391861434, 1.8605889751813791, 1.7760573855575508, 1.635153536285366, 1.7907586587644073, 1.892673553855733, 1.8965549896223453, 1.8382375262950266, 1.8211874076590877, 1.8220204098218709, 1.7852481097813846, 1.8111841786314753, 1.8345266894278303, 1.7496350712066366, 1.7097432948594253, 1.8126927087520743, 1.757421543682245, 1.6577876265478326, 1.729156743531727, 1.7070787226445638, 1.9089027991910212, 1.8490574864228009, 1.7942166896911367, 1.7340627422075239, 1.8784903735473173, 1.6595034616928948, 1.7483694853219605, 1.7441447596092672, 1.8377132045414093, 1.8240843152088355, 1.6623084599427154, 1.7260202746994253, 1.7269292873594844, 1.5646280576944944, 1.6368987642460817, 1.7326017700004688, 1.6728696632278037, 1.8306131816766056, 1.630204267570904, 1.628631636415539, 1.7712301632455367, 1.8511590678572938, 1.867558505742272, 1.872682827814583, 1.9081986040852281, 1.6340169330637977, 1.6528054045393594, 1.7916239755966046, 1.7737620309598228, 1.7744319541206588, 1.8999798083945547, 1.717451790682322, 1.8685193291335003, 1.6492220582530914, 1.8163209239236344, 1.7726455080453394, 1.7724559634640467, 1.7263200837109844, 1.8183454854449983, 1.8541802599812036, 1.6022696890898098, 1.620751917302077, 1.6352392330350682, 1.7387731132009376, 1.819783170663278, 1.7552269957476978, 1.9381476954610892, 1.6377730940643245, 1.705546841234543, 1.8418736446110444, 1.7090063159613704, 1.5375891797800127, 1.6127493793248644, 1.8024477671012513, 1.842571926103743, 1.710587023126368, 1.7766120138977615, 1.9016180255236754, 1.7007970992249182, 1.4855760068223265, 1.6024833791076072, 1.763618188592308, 1.6774495336083515, 1.6759392219740574, 1.7836601897564492, 1.6995958908913518, 1.8560434505832775, 1.7504713950118373, 1.7522216039973717, 1.7548998568797023, 1.5267345938604562, 1.6436464138137865, 1.6196268751724514, 1.7434020830275763, 1.718427083721557, 1.7648898429472002, 1.5670047885971832, 1.5910323302543337, 1.7950836347152876, 1.7961012939421341, 1.6117549748955529, 1.680272052141252, 1.7998692957003273, 1.8249867165709237, 1.7013060241029947, 1.615702678025339, 1.71265139743791, 1.6127303155493158, 1.523589570065152, 1.6398577725723087, 1.663734883061584, 1.7210220349441085, 1.5947125536527487, 1.610221156434655, 1.600515273841487, 1.6714435518672741, 1.6484806690487666, 1.5828622027679242, 1.5842668709244294, 1.730557998074906, 1.7345373379298452, 1.5307654456351738, 1.6335461669958289, 1.581449291279709, 1.5695588951965453, 1.760807713324895, 1.707784181237844, 1.6025201757223773, 1.5145790280285818, 1.6824116660417727, 1.7753487905286136, 1.5540650360248751, 1.7074042163518066, 1.4216563851355961, 1.585371277078722, 1.6389162776088193, 1.6429420273161262, 1.5517453936417567, 1.6501259705793327, 1.587545378806359, 1.7878286035593502, 1.7971189458913663, 1.6210305205063713, 1.4468778594962424, 1.4221015731829374, 1.7054045172410377, 1.5517633865179596, 1.6462956679560907, 1.748800477252578, 1.4469512940847817, 1.5832516446281313, 1.5876350459740038, 1.5682953581034387, 1.4665194820467466, 1.7110436020993123, 1.6715004847989232, 1.6389062885624808, 1.5446669983992747, 1.633079519884246, 1.7532552501054108, 1.5449916611677765, 1.5450289655753817, 1.6785256494305676, 1.59665360232518, 1.6164312798518714, 1.7236751788932951, 1.69592592924712, 1.6847417584005735, 1.5299081107457122, 1.604286802517593, 1.5618771599368644, 1.5681737996718206, 1.6898399065125689, 1.6542290674917517, 1.452406349971957, 1.7450849030495854, 1.6208715682706822, 1.4775340085879825, 1.6892607901555103, 1.583968087095703, 1.5699228053935732, 1.5415944286757783, 1.5766113509709612, 1.6724267221259155, 1.5803311099349446, 1.485896052143281, 1.6902540487150128, 1.6499851042535199, 1.5593339078457917, 1.6178088384240414, 1.5184650463860525, 1.733310312603863, 1.45273642358619, 1.552361684273866, 1.5515982068475687, 1.4075779162429174, 1.655521952550729, 1.6776615561899544, 1.610887592713975, 1.4811937265363957, 1.5120071213520607, 1.5955602769941997, 1.6901652127289135, 1.5692269678425705, 1.5001708233239794, 1.5331759926105233, 1.5070216046848621, 1.4119992016111065, 1.4565737599976951, 1.6386497816474177, 1.5852928795538075, 1.4805024042709818, 1.6415484837563126, 1.592904055832215, 1.7552546023094688, 1.7287283288643627, 1.4868372466427346, 1.5050806127491916, 1.5619986023292907, 1.5465385679162225, 1.5779353877237856, 1.5042209026215547, 1.7258594356396666, 1.548572879132218, 1.6412592847939067, 1.4182733408383055, 1.558604076701741, 1.7285232856602195, 1.5530230725944172, 1.3333873666476956, 1.3363538706881795, 1.5992846762920312, 1.396954822933844, 1.4806303737068627, 1.4408376817704487, 1.5623548278721797, 1.2879084458735508, 1.4779218207182356, 1.4576629645676145, 1.7345819571773178, 1.5547928926869319, 1.339639022721926, 1.3945855733246888, 1.4487744921860781, 1.491849018003894, 1.5007408972598908, 1.506357601471122, 1.4789831670042677, 1.3807856323470205, 1.687594016646422, 1.55817035975392, 1.439717620642554, 1.4480737158849937, 1.4857677806549128, 1.4389946527278192, 1.6361758659532593, 1.4496612725310547, 1.538370740756813, 1.5802240252663604, 1.5604490372592423, 1.563682863495996, 1.6393157670476182, 1.4496513512095335, 1.453255475608344, 1.5575582382027526, 1.3598138397945263, 1.7913291998220247, 1.5237770214301924, 1.4964943004410836, 1.354003534292017, 1.3646295363301257, 1.5781035675227875, 1.410641588479197, 1.5279013954613823, 1.3595408638226738, 1.6004276028727937, 1.5503515272674298, 1.4661501111201143, 1.4813897423324205, 1.5704068397013993, 1.4999388183856286, 1.5809875596776781, 1.410948735368885, 1.4465552725577075, 1.5108287631232284, 1.4693140678733325, 1.446067602309479, 1.548483929276145, 1.593814564185946, 1.4512285850198103, 1.4407832884959033, 1.4338142023473937, 1.336939484243902, 1.3190471516038784, 1.4202458067773354, 1.2927698315015497, 1.5028079606891374, 1.6215762094389123, 1.4340163987540848, 1.5725654314756852, 1.617204862369583, 1.4086604673170435, 1.3946645018205746, 1.4519283265085254, 1.411910349062285, 1.4270125522529218, 1.3815929601861288, 1.414874246995088, 1.508977458097116, 1.5395553848964005, 1.3739873831269551, 1.444698567797497, 1.3444111971130606, 1.4737315182255253, 1.6138136224107476, 1.489690290957374, 1.3984743052520432, 1.4557030579985313, 1.4685430877907906, 1.5818131096169257, 1.4862774449736111, 1.4815006494327627, 1.5804979173973601, 1.387572286467221, 1.417782144262315, 1.3033568366940036, 1.4124325625489054, 1.4310771810036262, 1.4573158439789788, 1.4145750657242004, 1.3649030935158835, 1.3637069734134133, 1.4918294310452707, 1.528222741721574, 1.2784898391360862, 1.51403852865372, 1.5014483395682618, 1.4817903702860584, 1.4482629156343831, 1.4274530675436328, 1.3578294580429282, 1.3864317843408562, 1.3788114910237588, 1.3192265693340328, 1.3738786281571924, 1.3411369100825834, 1.3136771930742461, 1.333992790960584, 1.515855965112158, 1.4397117196422897, 1.3834543036671352, 1.4184899956467367, 1.4152601246984973, 1.4642663505613505, 1.3792307534180037, 1.376912467456102, 1.4502874768609204, 1.4379912050709813, 1.4454448100391961, 1.3379765265712233, 1.3760152349503674, 1.3597240858887145, 1.4652927523714754, 1.5544307721960124, 1.4145097052319406, 1.1118228895689752, 1.3189212683226552, 1.4242151205675877, 1.3119210803429926, 1.4226169648590679, 1.3501788676720319, 1.4661698097922917, 1.348096895264067, 1.3544182226887287, 1.303998168196488, 1.469978977792341, 1.3267056943117073, 1.2689559782126136, 1.2568267628155874, 1.3010774246388925, 1.4108092629753999, 1.4913164386430975, 1.3015250436875387, 1.51925823339131, 1.3699487659578062, 1.4390501894328762, 1.4142847509367429, 1.389207908118704, 1.5163326639369592, 1.22790077323162, 1.4863954794074752, 1.367012325789925, 1.3471695315192573, 1.43346470981752, 1.2584181395840621, 1.416300560913611, 1.4322457616983264, 1.485964615623005, 1.4253524197960754, 1.397202864247266, 1.2428351803368087, 1.341907964737492, 1.3109273653817306, 1.3390659522155886, 1.2365156088624683, 1.3185068190737017, 1.3173035360558638, 1.439649404669488, 1.4400903081724443, 1.336997435328038, 1.5283251235215936, 1.4252639034468908, 1.235072654845967, 1.2652268566876452, 1.325821571603751, 1.413550007390241, 1.3071885293699117, 1.6286592619733646, 1.2371467732250618, 1.181415214728442, 1.7984126239946607, 1.3608634713367758, 1.2935979473717156, 1.17634798113773, 1.3231102291553214, 1.4542878486712139, 1.3633881222611028, 1.3104735308253097, 1.2892667077105942, 1.3526902665621778, 1.1937365278714593, 1.1499042734456166, 1.304863216261378, 1.4539293989375819, 1.343556855175209, 1.3445161905814356, 1.3265814779785245, 1.3964155616260812, 1.4053399789643481, 1.3959379015960238, 1.400605092527705, 1.4120373210918549, 1.3176452603426307, 1.3044741689331147, 1.3133451389849757, 1.3427056938129645, 1.3335658937414754, 1.1860365043532652, 1.2130441775681027, 1.4370538631031347, 1.3654203185022222, 1.269992007709889, 1.3005148992396114, 1.2406554236592409, 1.212695155584001, 1.24229194234159, 1.2964991666777834, 1.3215910189698936, 1.3348044434674677, 1.4075164338449975, 1.4236339142765377, 1.3395100701188287, 1.4078794144346116, 1.261263222941026, 1.2263920898655716, 1.388611584522902, 1.2868263004328069, 1.2911967623202363, 1.2997546137110108, 1.2561255224883734, 1.3292067876299836, 1.3030672708757445, 1.3307243103558315, 1.295852841625673, 1.2854331497914302, 1.2369817962707617, 1.3341885192685277, 1.2856754874758431, 1.3008551570464177, 1.3089350051503499, 1.2253757576970103, 1.2760242463183873, 1.5048837735986649, 1.1669123228535447, 1.2403884241834089, 1.2849752377593178, 1.3023958228928947, 1.2826464110822406, 1.3042585778096567, 1.3104385805935885, 1.2494420178192192, 1.2253679028364728, 1.347902015504959, 1.2799540455522584, 1.326934774607634, 1.2215827294575727, 1.1288244505775813, 1.156202616925294, 1.332721394201669, 1.2167058817647884, 1.364783676121181, 1.2851052771136844, 1.4283121290289595, 1.0978891558696964, 1.252139050828615, 1.1519215459151555, 1.007901903589812, 1.4008299085446583, 1.2577591305787192, 1.385468021466591, 1.2141504390919802, 1.2249577691104174, 1.1620581011120654, 1.2992953297133871, 1.1823119168214407, 1.2177203665488818, 1.2929611630516138, 1.1628077455253705, 1.3401580207510093, 1.147606346936437, 1.3385415858338565, 1.2813882307141686, 1.1319856136861959, 1.3403214247891548, 1.143113432709407, 1.2321746165741008, 1.1088452199415328, 1.4931898958507068, 1.2631562617267555, 1.0881425633588193, 1.0389594332575738, 1.27093700775518, 1.2416591575442226, 1.111237302597651, 1.318291017433696, 1.2266509267178047, 1.1738788647460259, 1.2109666575413405, 1.2531881394578308, 1.1858568274746009, 1.3723177827697632, 1.21826925444824, 1.1864083746885494, 1.23573027886704, 1.1773760726812628, 1.2202298461068788, 1.3881191436135059, 1.3269736233143115, 1.4207093150688486, 1.26590872970298, 1.262181080019778, 1.1426459634388895, 1.084226388897256, 1.1810466246674796, 1.1165052302526048, 1.2940857591316919, 1.3386285380282883, 1.1466069606456046, 1.0437722182944962, 1.149267647181754, 1.2170912215981786, 1.1899954286517869, 1.0616540664313758, 1.1576310355227226, 1.205630921947949, 1.1186763825675552, 1.1200450699385436, 1.017587517573052, 1.2292232921812687, 1.3340153408998798, 1.316506950781434, 1.2973793607238993, 1.2209934592060685, 1.1121853735009342, 1.2188198725358816, 1.2188116027513154, 1.1509034092505877, 1.1376108296389291, 1.1380914335919465, 1.167066496344811, 1.268136504852911, 1.1671766492185427, 1.3910140178977122, 1.3091557734015586, 1.3209726322133193, 1.1957506225328662, 0.9732794901445343, 1.2155158493544194, 1.0412060700783392, 1.2716490753962475, 1.2837997303570552, 1.186574681500765, 1.2019298844876287, 1.1951259523690976, 1.134126704918176, 1.1179898937377633, 1.1725843336479098, 1.170602317393033, 1.0999370058667919, 1.1408668824961385, 1.3367955453029443, 1.2273283745434895, 1.0221003208353479, 1.1591620761719958, 1.0778066405746527, 1.2245078988346767, 1.1453827288447078, 1.1354270769270125, 1.210006826963321, 1.1917617825050482, 1.2534797062716805, 1.084275820530938, 1.151766943106378, 1.2801919586678006, 0.97703494555639, 1.0875814610548387, 1.0822231456776095, 1.1188771987166177, 1.0702751421071661, 1.1578156042098233, 1.1119459346725826, 1.4426087012725142, 1.0205603131007122, 1.1376361761098464, 1.1070220768561414, 1.0896316974405797, 1.0749644620632919, 1.1640536804208348, 1.3117783953495044, 1.0176695186567932, 1.0067393070193233, 1.1478082270115944, 0.8837831491111774, 1.1681464638920034, 1.1569562698403733, 1.135923097280172, 1.222503372941241, 1.1001449350453874, 1.0843709409405906, 1.0165445310428847, 1.1514198520905663, 1.282891466047233, 1.2270859749783023, 1.0964721809965947, 1.1171861233571836, 1.1945665713805667, 1.1962442710994712, 1.2985487100288062, 1.1354041997776683, 1.0930238402413017, 0.9062983219563698, 1.0320299024209445, 1.0165762615721274, 1.0386566856800417, 1.1203515511057018, 1.2370288631285422, 1.180190999946732, 0.9077561964654884, 1.0583500092304987, 1.1762666326568494, 1.2316055395874028, 1.2185294839693235, 1.0751417506877257, 0.9553408640998134, 1.1010950223035172, 1.0663694342916978, 1.0046900728575303, 1.3264721012204108, 1.221411701337184, 0.9065824287396985, 1.1476377962681599, 1.0085214048208753, 1.022228173708689, 1.1453149170627477, 0.951562982920119, 1.0991347162271665, 1.0802620472527682, 1.1025919171874465, 1.0120444707935587, 1.0446194000791755, 1.0441149880999312, 1.1094345601290674, 1.1157037247205026, 1.1079201043847722, 1.2114165327622024, 1.1447184625901943, 1.0899537619231097, 0.9529898810283335, 1.0201060883739124, 1.069529860456165, 1.0206485919022328, 1.153845562045142, 1.0081309204250102, 0.9416761861686118, 1.0990496667238314, 1.0511274319263828, 1.0916851584340321, 0.9936533623481058, 0.9794873661643294, 1.0101250582292862, 1.190281614609124, 1.1370317998755446, 1.147768474514566, 1.0364291762384132, 1.1909746131119958, 1.0429743639887814, 1.032781739664956, 0.9341637152722878, 1.1096679956314335, 1.0837256056461586, 1.0679317011897425, 1.2150545233073289, 1.1963783449148222, 1.2563248625307917, 1.1093455191618644, 0.8580839703626486, 1.070759991530997, 0.9775114522740953, 0.7750248876020822, 0.9142105690346732, 1.0473461193757012, 1.1138202091600506, 1.1476933928544109, 1.106431851216113, 0.8544938448092111, 1.004316665615424, 1.087919773213041, 0.8266902610688781, 1.0433832027658672, 1.0800386087084788, 0.8609561814126732, 1.0034614600777065, 1.018394639053404, 1.2136973557366166, 1.0657168555405552, 0.8756760060084717, 1.0089959604107273, 0.963612489909508, 1.0705778234119894, 1.0445380097422632, 0.8883567805140967, 0.9661278246757052, 1.094258074473285, 0.9609447320876681, 0.97952291173798, 0.9418581403477512, 0.9678452666961075, 1.048164006000882, 1.111135023748373, 0.9141386447688744, 0.8805842883181758, 0.9283613867702936, 0.9091176278821075, 0.8840066928811453, 1.0631907257418767, 0.8836351163060457, 0.9550400835653069, 1.0002711688318313, 0.9913188854642503, 1.026928002895162, 0.9848104173520794, 1.2299752390157042, 1.1977686000157595, 0.8984911840772801, 1.1011688125254968, 0.9983453661073594, 0.9955307838435138, 0.9468434672907078, 0.909510587194518, 1.1253139731947766, 1.0716513401653012, 0.8323091301152695, 0.8033881625458149, 0.9340950266829424, 0.9347298361404102, 0.9811743122051134, 1.0150771635573825, 0.9703557499567347, 1.002023115794512, 1.1583927272091, 1.1389225323409897, 0.8287293715994889, 0.9594362433576943, 0.9651763139046743, 0.9406615313276104, 0.9109401796251315, 0.8402450801497874, 0.8993306840381551, 0.9203805594356443, 0.9494986121256136, 1.260157651870213, 1.1364064968618839, 0.9361268577712974, 0.834298197414362, 0.887610646386123, 0.986976499480563, 0.9349269625320845, 0.9154996164390806, 0.8956195062231102, 1.1965210644380495, 1.1491452916643086, 0.9868082239001867, 0.9276640249169738, 0.927547249218368, 0.9471106566545434, 0.9439387099389673, 1.2617032832569093, 1.0918161031293876, 0.9359513259160026, 1.0005629046835183, 0.8431817170683717, 0.9067815395714018, 1.0454335731345998, 1.0607225993335299, 0.9284555103308827, 0.8889902729595506, 0.8453352300834388, 0.9044211899914317, 1.2016508742209506, 1.1392877811558633, 0.8427859543136914, 0.942160493663663, 0.9567560222790195, 0.9001036940163971, 1.0923346240406593, 1.0504026420988841, 1.0504927919060854, 1.0055270694424743, 0.8750630103879873, 0.9418253904827638, 0.9093623970922358, 0.8483660994376551, 0.9440505918873296, 0.8052700481230392, 0.8969721518253788, 0.8441416127403056, 0.9425372456206291, 1.0154920529063884, 0.9651747623553592, 1.159818800455569, 0.9212908881239409, 0.8894325539669015, 0.9490027681514778, 0.7680052289173821, 0.7788368911252508, 0.8772787456289197, 0.9950990375529589, 0.8446944992537003, 1.096209980508435, 0.8915794111385088, 0.8917172677336788, 0.8882111473502241, 0.8940487752767899, 0.9029288055892086, 0.9708241694439553, 0.9758707850778803, 0.7688078617990485, 0.7644698957362109, 0.8773279212360339, 0.9720239179682796, 1.2096057892546412, 1.1386095768240525, 1.0025232948544407, 0.7884835502250092, 0.9677387926643198, 0.7865605967915533, 0.9845096679606521, 0.9899391147072014, 0.998080905719292, 0.9835469952947185, 1.0573192080376035, 0.8186562338025266, 0.781322570119101, 0.8513975728073395, 0.7509827576596279, 0.7351952118999756, 0.7910339870439728, 0.9648914222733302, 0.7602476865992219, 0.9286800759217532, 0.7707811687961049, 0.8681681851075764, 0.8072384675851269, 0.7365149117299004, 0.9026675413469291, 0.695172115863497, 0.9643899828405463, 0.7950411983503386, 0.844852681352459, 0.9858054471866057, 0.7381531479847583, 0.641436840537466, 0.925344185901762, 0.8238442869202974, 0.9877696822090704, 0.8727044839815251, 0.8901940572131204, 0.8691533487776164, 0.9506055688340068, 0.7683277925373674, 0.7372942543530882, 0.8248673100379585, 0.7355965321264111, 0.8985336771716618, 1.0645870990363573, 0.9327845713995587, 0.8600766525070012, 0.9324541762162308, 0.7963965160447584, 0.8796784771038181, 0.8892037729181188, 0.7011140008324215, 0.740212031718093, 0.759857377549701, 0.7737587314824046, 0.6624370165889663, 0.7984174261380129, 0.7687606460111707, 0.907890823605465, 0.9184380152952702, 0.9290785442962054, 0.6458994910487427, 0.8580703032547122, 0.964061057738365, 0.7715671233138239, 0.8208932476062036, 0.8544034913152838, 0.7335781454661704, 0.6110802963595195, 0.8353633715815714, 0.9436510668157254, 0.7767704266335211, 0.6935201053274967, 0.6480774461631179, 0.8068981660244134, 0.7676563986117942, 1.0622644110317279, 0.8409701561882207, 0.8740852922054083, 0.7641421266457356, 0.9414219667917654, 0.679655442798968, 0.8204519190034044, 0.8192129148081577, 0.7484246673288304, 0.8526813579334901, 0.810738861945341, 0.8330717876841013, 0.7041309004750573, 0.6989965373449032, 0.6116929005314095, 0.8095716926020522, 0.7271604073567041, 0.7660435988018219, 0.8403449434939843, 0.8060342692596143, 0.6729492997373338, 0.8502771092327411, 0.8712451946988661, 0.8904277774096941, 0.8229122648881694, 0.6918342802457076, 0.835569997779197, 0.9514971965089359, 0.7908500295635194, 0.7012661165339732, 0.650527484159589, 0.8088783456368549, 0.9062169323385443, 0.7394342775838851, 0.833189567284852, 0.7768171761471243, 0.8085390927209507, 0.9388494247142442, 0.9571191397076626, 0.8698137248879403, 0.7989321409366248, 0.8747904065192467, 0.7745648387326276, 0.6733495948277575, 0.631280186556708, 0.7687715598976288, 0.7380067901991326, 0.7549184449389458, 0.6938185720656685, 0.6358802096428253, 0.5912716866307419, 0.7310814843745533, 0.7133350325091442, 0.8576556868665892, 0.7819184528332089, 0.7618266713276903, 0.6544097563019945, 0.6942676964617512, 0.8130878257280665, 0.7680202620643115, 0.8690925244498852, 0.7865384330052879, 0.674672358698073, 0.7487469494607318, 0.8399014777650902, 0.7735057554807135, 0.7409869177520839, 0.6181373233015278, 0.692683044283754, 0.7594938292444674, 0.7146292380892171, 0.6398397120152421, 0.758941060841323, 0.8304020598200471, 0.9913156545225121, 0.9114896348877648, 0.7664094894592777, 0.8062324988107785, 0.49931421542420795, 0.6695786626816087, 0.8480457705219964, 0.7677102087850315, 0.8801226846927083, 0.8727642513708814, 0.6981917657202653, 0.6508681376858675, 0.7264839147797473, 0.6311385495912453, 0.677771894629054, 0.6566449986220368, 0.6960422703136817, 0.6316050864357038, 0.5791042187264205, 0.8872640252761416, 0.8971670210614566, 0.5998561299765518, 0.8369132606402285, 0.6469296580484108, 0.7921488312133415, 0.6942640180805943, 0.63291807424533, 0.6792139173041799, 0.621702055230249, 0.8740055181131651, 0.6958607255711808, 0.5027110993344732, 0.6002411566818945, 0.6530603140302739, 0.6904739737897623, 0.7197501131775544, 0.7694791531468543, 0.7438660433046468, 0.8057285683229126, 0.700779647854927, 0.6267610345200137, 0.6444585561933669, 0.5535277428231518, 0.6905418697644161, 0.6631156681014011, 0.5730173081895705, 0.6119822667371536, 0.6886644749801795, 0.8052520318576777, 0.8311925935156335, 0.6151195664425734, 0.7154510666847677, 0.6858296497813186, 0.6952082168574143, 0.6152646015233146, 0.8531127641946726, 0.7137150371252708, 0.6965971169081958, 0.6113456681078981, 0.6397573487166488, 0.7159705025317628, 0.5564138378909195, 0.5209582227267812, 0.7639332054499453, 0.7665743395499984, 0.6758724495519818, 0.6340380998888409, 0.61075143120388, 0.6425222515215107, 0.47343066884023743, 0.44218034546287194, 0.5731906816326281, 0.7685427264285799, 0.7956345723199787, 0.8857449102107671, 0.8216368082967754, 0.7416670882531605, 0.6692103692058019, 0.60909688249891, 0.6465878620819169, 0.6268765319014213, 0.480560748121569, 0.7228226575614985, 0.913611719918719, 0.7891526904417043, 0.5282603481994363, 0.5958828313496889, 0.6756242025237839, 0.6284768423591891, 0.751318176380917, 0.663475217216961, 0.7686533706525158, 0.5866543965869458, 0.5661055839418677, 0.5560058793824952, 0.5146573762948892, 0.7314989591911646, 0.7737970409737069, 0.7042272756782721, 0.8056101972571197, 0.6453387372560261, 0.6852459865252974, 0.8719752558180776, 0.6168056695251137, 0.662876156060655, 0.7299489810185381, 0.6981276927442016, 0.639082832930546, 0.7381953824752867, 0.6782729584242994, 0.5332625862229178, 0.614975234620626, 0.5633144470918557, 0.561892528976615, 0.5360021545752186, 0.4992538176864789, 0.6068370849682713, 0.5879709992917811, 0.5672012636685473, 0.7737137479912238, 0.6932053896725066, 0.5317670094961892, 0.7478085719172636, 0.8422421106359231, 0.598725874349902, 0.5780939199020525, 0.7247082640470212, 0.5653762183912656, 0.6973036934636574, 0.7803042924798839, 0.6105666914671202, 0.6922256425909313, 0.7096756203992024, 0.9076812465903783, 0.78460821479328, 0.7481795855584596, 0.5735905983712006, 0.504587512203424, 0.5709531777104719, 0.4819238804723075, 0.5109414931494493, 0.45436904232375896, 0.3714092889008827, 0.49782751485361315, 0.518695511448751, 0.4330243458312796, 0.588371763527748, 0.48765374759145164, 0.5882082725651606, 0.6553169179051384, 0.5584006341954411, 0.6511834793235253, 0.7345405892814383, 0.8045199797421269, 0.7636770092255766, 0.7420715652486994, 0.696458348277435, 0.9228182562203716, 0.7037616387701828, 0.5219914829055362, 0.5274818907063041, 0.5175385932679349, 0.5081483352699074, 0.5447556507061871, 0.4626249404570795, 0.5908699750867835, 0.6268295209337161, 0.6594339419991571, 0.47491934049419016, 0.4936333796109752, 0.5089409524188038, 0.6254075863815982, 0.5875134817913333, 0.5478264287999791, 0.5436804693789193, 0.5134865984672932, 0.6186853313215033, 0.7042901450496776, 0.6744576715321183, 0.6160500412594098, 0.530167483128414, 0.4309412512116603, 0.501945099886992, 0.5785006059522668, 0.6415700262076969, 0.5648524374656854, 0.5958127286335239, 0.460705277295776, 0.579840972661745, 0.48281517248800215, 0.5425944702214803, 0.522684189281604, 0.39258038766740383, 0.5060800815070446, 0.5014562946517742, 0.7314870306368886, 0.5620860013635531, 0.7177733470744266, 0.459175577854445, 0.47101352608564384, 0.6226065048780213, 0.56243686967506, 0.771850703924666, 0.5146299938815309, 0.5605478661819577, 0.398109409297154, 0.672252163739545, 0.947235740734324, 0.6084319125240255, 0.4853055754165206, 0.5678965066462734, 0.5587375468798447, 0.6207936602719447, 0.5037000782875158, 0.6109787574972604, 0.5068521615231688, 0.463545003902302, 0.4666935005219137, 0.6499856000999255, 0.4147224035507224, 0.5423353504796931, 0.5862354995072538, 0.7128155797084136, 0.6941498013610387, 0.3813650650741876, 0.4080350613834247, 0.47137253347218006, 0.42518135778645677, 0.44891842465471427, 0.5929194620950092, 0.5112919788103891, 0.6392953262364872, 0.42478295325102994, 0.4088379942447534, 0.4672052283475314, 0.504108226367286, 0.4792897271523487, 0.36655027819217034, 0.41360715561490347, 0.5528065518215072, 0.5050608469000888, 0.6177563920545692, 0.6220278938817991, 0.5962830695900962, 0.34129214069782876, 0.5156029654591444, 0.5144220140965168, 0.4963398685125689, 0.6112014488952194, 0.5882105052957515, 0.8419702710048818, 0.6044496139368165, 0.5840754718879066, 0.5044818365703043, 0.4972241581792059, 0.4485750575939993, 0.2928552583796957, 0.49958831296049605, 0.40466845265775675, 0.4805947233985103, 0.32899429764581906, 0.4552106207073862, 0.7136078117397855, 0.9364060487429829, 0.6238050841339642, 0.45696375420726937, 0.4319713671236072, 0.35224041283364393, 0.4453652906303504, 0.46024045344941455, 0.3861036337542681, 0.33061367599746955, 0.5554809273377369, 0.48241177964566073, 0.3500311733664785, 0.5648648863556277, 0.7229887843277504, 0.9632935532978728, 0.5709725690818614, 0.7383884668077255, 0.7443166812124444, 0.490611270972076, 0.427754846840307, 0.44348025425557985, 0.3746259672687606, 0.41752977098323124, 0.5433169750769296, 0.6862983479637055, 0.5373638291230602, 0.36242148872632324, 0.4419952091386643, 0.5178328801760721, 0.5022297445915311, 0.41792322119900877, 0.4579871068906066, 0.7201954205416461, 0.6533441609655329, 0.43619782183473177, 0.528587017352147, 0.534583513292372, 0.4072222558289127, 0.5674890375060327, 0.4353328571865059, 0.4580538830202747, 0.38390426073341105, 0.350461735448233, 0.41170360474014395, 0.42076174464184746, 0.417192085694779, 0.39854638228301964, 0.5430507143876976, 0.6680603430207697, 0.5381234330781135, 0.4908744901291518, 0.3260826188729237, 0.40531616950327987, 0.3785891485339032, 0.42927212408993065, 0.7690172235431727, 0.670630652808405, 0.5621966082161002, 0.43090643237629567, 0.4555860204973364, 0.3576551865052129, 0.36084371367343004, 0.4148165053537193, 0.316684944369143, 0.46391377135550604, 0.3848783906872628, 0.48678019486508806, 0.4636760018577544, 0.48582935619551626, 0.48276099649321697, 0.3527491098812574, 0.41736246153234535, 0.44568166061011566, 0.3994081719705263, 0.4414000704811928, 0.41914553116674735, 0.32582137229321656, 0.45242284627851936, 0.3348515160734279, 0.4061479869006761, 0.41179483405976414, 0.4055740613280282, 0.4951316037877911, 0.5742155239689248, 0.6455388083706852, 0.6089756919857433, 0.42406138522414133, 0.5211939153493819, 0.4032457683985728, 0.4898039398492619, 0.47489280702192077, 0.43492562054586414, 0.37980380866723734, 0.5040954070187832, 0.4172479122427002, 0.4136604794231112, 0.41313489222349736, 0.39115101084865694, 0.37118335767564764, 0.32631316764256313, 0.40895186990826554, 0.5258715893527203, 0.4006382465868111, 0.328065931483058, 0.3974230871270131, 0.36217147059330607, 0.387726352198783, 0.4493382025518831, 0.3848259799226251, 0.3286886604835379, 0.518551739677484, 0.748200729267494, 0.718897364677747, 0.6551306999631805, 0.47202133505515453, 0.3926162134692519, 0.39086659087194336, 0.27945944286728663, 0.28623254444228385, 0.31751504150080656, 0.3292453053890853, 0.3143563334030787, 0.3202395206931041, 0.45236176908635317, 0.4599645705081885, 0.5542665687539174]\n",
      "Accuracy history: [0.182, 0.194, 0.23, 0.284, 0.292, 0.262, 0.314, 0.28, 0.314, 0.32, 0.356, 0.3, 0.356, 0.36, 0.32, 0.316, 0.322, 0.326, 0.364, 0.368, 0.362, 0.322, 0.39, 0.342, 0.416, 0.352, 0.38, 0.412, 0.416, 0.388, 0.374, 0.402, 0.442, 0.482, 0.476, 0.444, 0.406, 0.454, 0.442, 0.46, 0.468, 0.484, 0.444, 0.484, 0.458, 0.482, 0.506, 0.478, 0.444, 0.496, 0.5, 0.47, 0.428, 0.486, 0.49, 0.494, 0.502, 0.498, 0.534, 0.532, 0.516, 0.506, 0.526, 0.53, 0.58, 0.51, 0.548, 0.566, 0.534, 0.594, 0.544, 0.602, 0.554, 0.572, 0.58, 0.594, 0.588, 0.592, 0.574, 0.596, 0.578, 0.562, 0.64, 0.608, 0.624, 0.626, 0.606, 0.59, 0.694, 0.632, 0.604, 0.564, 0.634, 0.582, 0.656, 0.658, 0.676, 0.66, 0.712, 0.658, 0.642, 0.654, 0.696, 0.712, 0.68, 0.668, 0.668, 0.756, 0.662, 0.664, 0.734, 0.734, 0.708, 0.688, 0.708, 0.748, 0.734, 0.694, 0.748, 0.758, 0.72, 0.722, 0.726, 0.722, 0.754, 0.756, 0.734, 0.796, 0.73, 0.68, 0.696, 0.732, 0.792, 0.814, 0.806, 0.83, 0.786, 0.788, 0.708, 0.808, 0.802, 0.794, 0.842, 0.846, 0.812, 0.79, 0.85, 0.878, 0.874, 0.854, 0.764, 0.834, 0.866, 0.8, 0.91, 0.882, 0.906, 0.862, 0.808, 0.884, 0.832, 0.85, 0.866, 0.894, 0.87, 0.896]\n"
     ]
    }
   ],
   "source": [
    "# RMSprop\n",
    "rms = ConvNet4Accel(input_shape=input_shape, verbose=False)\n",
    "rms.compile('rmsprop')\n",
    "rms.fit(x_train,y_train, x_dev, y_dev, n_epochs=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.405"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rms.accuracy(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of RMSProp**\n",
    "To preface this analysis, it must be stated we used this article as a starting point for the implementation of RMSProp: https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a.\n",
    "\n",
    "We predicted that RMSProp is comparable to Adam in performance. RMSProp is an extension of RProp, which makes a weight by weight comparison of the weight gradient and if the signs are the same it increases learning rate, and if they are different, it decreases. This is because if the sign of the gradient is the same for two updates in a row, it is going in the right direction, so it can speed up its descent of the loss space. If the signs are different, that means that it went too far in loss space, so it should decrease the adjustment to the weight for the next update.\n",
    "\n",
    "RProp is good for training when the batch size is the entire epoch, but fails when there are mini-batches because it has to divide by different gradients each time. RMSProp extends on the idea of RProp by eliminating the problem of mini-batches. It solves this problem by introducing the root of the square of the gradients (root mean square -> RMS), which is a moving average of the gradient. The moving average solves the problem of a gradi\n",
    "\n",
    "We created a ConvNet4Accel network using RMSProp and the full dataset run as we used on Adam in part 7. Using these two runs, we can build a comparison of the two optimizers. \n",
    "\n",
    "Time: From a time-per-iteration perspective, RMSProp takes about .489 seconds and a total of 726 seconds to complete for 1485 iterations. Adam takes .522 seconds per iteration and a total of 775 seconds to run. If these were running for longer, that time difference would be even larger. Therefore, the smaller amount of computation in RMSProp makes it a more efficient algorithm to run. \n",
    "\n",
    "Accuracy: Our Adam optimizer scores a higher accuracy. We can see there is a 40.5% accuracy compared to a 42.5% accuracy when the models are both run for 33 epochs on default parameters. So, if accuracy matters less and there is only so long to train, then RMSProp is the optimizer to use. It would be curious to see if there would be an improvement if they were trained on a time basis rather than an epochs basis. If given the same amount of time, does RMSProp do better rather than Adam?\n",
    "\n",
    "Below are some graphs that plot the loss of RMSProp and Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rms.loss_history, label=\"RMSProp Loss History\")\n",
    "plt.plot(net.loss_history, label=\"Adam Loss History\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rms.train_acc_history, label=\"RMSProp Accuracy History\")\n",
    "plt.plot(net.train_acc_history, label=\"Adam Accuracy History\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the graphs above, Blue is RMSProp, Green is Adam. It is clear the Adam is more performant in the epochs. Both Loss and Accuracy from training were worse in RMSProp than Adam. If we had to choose one, we would take Adam, even though we found RMSProp to be faster.\n",
    "\n",
    "We chose to do this extension because we have seen RMSProp in other classes and documentation. We saw it in class and in Keras documentation as well as Medium articles so we decided we wanted to understand what it was. Now we know! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
