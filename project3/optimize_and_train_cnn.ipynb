{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cole Turner and Ethan Seal**\n",
    "\n",
    "Fall 2019\n",
    "\n",
    "CS343: Neural Networks\n",
    "\n",
    "Project 3: Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(['seaborn-colorblind', 'seaborn-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=7)\n",
    "\n",
    "# Automatically reload your external source code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global note: Make sure any debug printouts do not appear if `verbose=False`!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4) Implement weight optimizers for gradient descent\n",
    "\n",
    "To change the weights during training, we need an optimization algorithm to have our loss decrease over epochs as we learn the structure of the input patterns. Until now, we used **Stochastic gradient descent (SGD)**, which is the simplest algorithm. We will implement 3 popular algorithms:\n",
    "\n",
    "- `SGD` (stochastic gradient descent)\n",
    "- `SGD_Momentum` (stochastic gradient descent with momentum)\n",
    "- `Adam` (Adaptive Moment Estimation)\n",
    "\n",
    "Implement each of these according to the update equations (in `optimizer.py::update_weights` in each subclass). Let's use $w_t$ in the math below to represent the weights in a layer at time step $t$, $dw$ to represent the gradient of the weights in a layer, and $\\eta$ represent the learning rate. We use vectorized notation below (update applies to all weights element-wise). Then:\n",
    "\n",
    "**SGD**: \n",
    "\n",
    "$w_{t} = w_{t-1} - \\eta \\times dw$\n",
    "\n",
    "**SGD (momentum)**:\n",
    "\n",
    "$v_{t} = m \\times v_{t-1} - \\eta \\times dw$\n",
    "\n",
    "$w_{t} = w_{t-1} + v_t$\n",
    "\n",
    "where $v_t$ is called the `velocity` at time $t$. At the first time step (0), velocity should be set to all zeros and have the same shape as $w$. $m$ is a constant that determines how much of the gradient obtained on the previous time step should factor into the weight update for the current time step.\n",
    "\n",
    "\n",
    "**Adam**:\n",
    "\n",
    "$m_{t} = \\beta_1 \\times m_{t-1} + (1 - \\beta_1)\\times dw$\n",
    "\n",
    "$v_{t} = \\beta_2 \\times v_{t-1} + (1 - \\beta_2)\\times dw^2$\n",
    "\n",
    "$n = m_{t} / \\left (1-(\\beta_1^t) \\right )$\n",
    "\n",
    "$u = v_{t} / \\left (1-(\\beta_2^t) \\right )$\n",
    "\n",
    "$w_{t} = w_{t-1} - \\left ( \\eta \\times n \\right ) / \\left ( \\sqrt(u) + \\epsilon \\right ) $\n",
    "\n",
    "\n",
    "Like SGD (momentum), Adam records momentum terms $m$ and $v$. At time step 0, you should initialize them to zeros in an array equal in size to the weights. $n$ and $u$ are variables computed on each time step. The remaining quantities are constants. Note that $t$ keeps track of the integer time step, and needs to be incremented on each update. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimizer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Test SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD: Wts after 1 iter [-3.1764052 -2.0400157 -1.0978738 -0.2240893  0.8132442  2.0977278]\n",
      "SGD: Wts after 2 iter [-3.3528105 -2.0800314 -1.1957476 -0.4481786  0.6264884  2.1954556]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "wts = np.arange(-3, 3, dtype=np.float64)\n",
    "d_wts = np.random.randn(len(wts))\n",
    "\n",
    "optimizer = SGD()\n",
    "optimizer.prepare(wts, d_wts)\n",
    "\n",
    "new_wts_1 = optimizer.update_weights()\n",
    "new_wts_2 = optimizer.update_weights()\n",
    "\n",
    "print(f'SGD: Wts after 1 iter {new_wts_1}')\n",
    "print(f'SGD: Wts after 2 iter {new_wts_2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should be:\n",
    "\n",
    "    SGD: Wts after 1 iter [-3.1764052 -2.0400157 -1.0978738 -0.2240893  0.8132442  2.0977278]\n",
    "    SGD: Wts after 2 iter [-3.3528105 -2.0800314 -1.1957476 -0.4481786  0.6264884  2.1954556]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Test SGD_Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD M: Wts after 1 iter\n",
      "[[ 1.6879486  0.3879897  0.9343517  2.2075258]\n",
      " [ 1.7181501 -0.9567621  0.9187816 -0.0659476]\n",
      " [ 0.1520801  0.3452366  0.0576     1.52849  ]]\n",
      "SGD M: Wts after 2 iter\n",
      "[[ 1.5661825  0.3685217  0.8633335  2.1541379]\n",
      " [ 1.4790974 -0.9239367  0.8686908  0.0707077]\n",
      " [ 0.5605585  0.2406577 -0.0807098  1.6472364]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "wts = np.random.randn(3, 4)\n",
    "d_wts = np.random.randn(3, 4)\n",
    "\n",
    "optimizer = SGD_Momentum(lr=0.1, m=0.6)\n",
    "optimizer.prepare(wts, d_wts)\n",
    "\n",
    "new_wts_1 = optimizer.update_weights()\n",
    "new_wts_2 = optimizer.update_weights()\n",
    "\n",
    "print(f'SGD M: Wts after 1 iter\\n{new_wts_1}')\n",
    "print(f'SGD M: Wts after 2 iter\\n{new_wts_2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should be:\n",
    "\n",
    "    SGD M: Wts after 1 iter\n",
    "    [[ 1.6879486  0.3879897  0.9343517  2.2075258]\n",
    "     [ 1.7181501 -0.9567621  0.9187816 -0.0659476]\n",
    "     [ 0.1520801  0.3452366  0.0576     1.52849  ]]\n",
    "    SGD M: Wts after 2 iter\n",
    "    [[ 1.5661825  0.3685217  0.8633335  2.1541379]\n",
    "     [ 1.4790974 -0.9239367  0.8686908  0.0707077]\n",
    "     [ 0.5605585  0.2406577 -0.0807098  1.6472364]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Test Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam: Wts after 1 iter\n",
      "[[ 1.6640523  0.3001572  0.878738   2.1408932]\n",
      " [ 1.767558  -0.8772779  0.8500884 -0.0513572]\n",
      " [-0.0032189  0.3105985  0.0440436  1.5542735]]\n",
      "Adam: Wts after 2 iter\n",
      "[[ 1.5640523  0.2001572  0.778738   2.0408932]\n",
      " [ 1.667558  -0.7772779  0.7500884  0.0486428]\n",
      " [ 0.0967811  0.2105985 -0.0559564  1.6542735]]\n",
      "Adam: Wts after 3 iter\n",
      "[[ 1.4640523  0.1001572  0.678738   1.9408932]\n",
      " [ 1.567558  -0.6772779  0.6500884  0.1486428]\n",
      " [ 0.1967811  0.1105985 -0.1559564  1.7542735]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "wts = np.random.randn(3, 4)\n",
    "d_wts = np.random.randn(3, 4)\n",
    "\n",
    "optimizer = Adam(lr=0.1)\n",
    "optimizer.prepare(wts, d_wts)\n",
    "\n",
    "new_wts_1 = optimizer.update_weights()\n",
    "new_wts_2 = optimizer.update_weights()\n",
    "new_wts_3 = optimizer.update_weights()\n",
    "\n",
    "print(f'Adam: Wts after 1 iter\\n{new_wts_1}')\n",
    "print(f'Adam: Wts after 2 iter\\n{new_wts_2}')\n",
    "print(f'Adam: Wts after 3 iter\\n{new_wts_3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should be:\n",
    "\n",
    "    Adam: Wts after 1 iter\n",
    "    [[ 1.6640523  0.3001572  0.878738   2.1408932]\n",
    "     [ 1.767558  -0.8772779  0.8500884 -0.0513572]\n",
    "     [-0.0032189  0.3105985  0.0440436  1.5542735]]\n",
    "    Adam: Wts after 2 iter\n",
    "    [[ 1.5640523  0.2001572  0.778738   2.0408932]\n",
    "     [ 1.667558  -0.7772779  0.7500884  0.0486428]\n",
    "     [ 0.0967811  0.2105985 -0.0559564  1.6542735]]\n",
    "    Adam: Wts after 3 iter\n",
    "    [[ 1.4640523  0.1001572  0.678738   1.9408932]\n",
    "     [ 1.567558  -0.6772779  0.6500884  0.1486428]\n",
    "     [ 0.1967811  0.1105985 -0.1559564  1.7542735]]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5) Write network training methods\n",
    "\n",
    "Implement methods in `network.py` to actually train the network, using all the building blocks that you have created. The methods to implement are:\n",
    "\n",
    "- `predict`\n",
    "- `fit`. Add an optional parameter `print_every=1` that controls the frequency (in iterations) with which to wait before printing out the loss and iteration number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6) Overfitting a convolutional neural network\n",
    "\n",
    "Usually we try to prevent overfitting, but we can use it as a valuable debugging tool to test out a complex backprop-style neural network. Assuming everything is working, it is almost always the case that we should be able to overfit a tiny dataset with a huge model with tons of parameters (i.e. your CNN). You will use this strategy to verify that your network is working.\n",
    "\n",
    "Let's use a small amount of real data from STL-10. If everything is working properly, the network should overfit and you should see a significant drop in the loss from its starting value of ~2.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6a) Move your `preprocess_data.py` from the MLP project\n",
    "\n",
    "Make the one following change:\n",
    "\n",
    "- Re-arrange dimensions of `imgs` so that when it is returned, `shape=(Num imgs, RGB color chans, height, width)` (No longer flatten non-batch dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_stl10_dataset\n",
    "import preprocess_data\n",
    "from network import ConvNet4\n",
    "import optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b) Load in STL-10 at 16x16 resolution\n",
    "\n",
    "If you don't want to wait for STL-10 to download from the internet and resize, copy over your data and numpy folders from your MLP project.\n",
    "\n",
    "**Notes:**\n",
    "- You will need to download the new version of `load_stl10_dataset`.\n",
    "- The different train/test split here won't work if you hard coded the proportions in your `create_splits` implementation! *This isn't catastrophic, it just means that it will take longer to compute accuracy on the validation set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images are: (5000, 96, 96, 3)\n",
      "Labels are: (5000,)\n",
      "Resizing 5000 images to 16x16...Done!\n",
      "Saving Numpy arrays the images and labels to ./numpy...Done!\n",
      "imgs.shape (5000, 16, 16, 3)\n",
      "data.shape (5000, 768)\n",
      "Train data shape:  (4548, 768)\n",
      "Train labels shape:  (4548,)\n",
      "Test data shape:  (400, 768)\n",
      "Test labels shape:  (400,)\n",
      "Validation data shape:  (2, 768)\n",
      "Validation labels shape:  (2,)\n",
      "dev data shape:  (50, 768)\n",
      "dev labels shape:  (50,)\n"
     ]
    }
   ],
   "source": [
    "# Download the STL-10 dataset from the internet, convert it to Numpy ndarray, resize to 16x16\n",
    "# cache it locally on your computer for faster loading next time.\n",
    "load_stl10_dataset.purge_cached_dataset()\n",
    "stl_imgs, stl_labels = load_stl10_dataset.load(scale_fact=6)\n",
    "# preprocess\n",
    "stl_imgs, stl_labels = preprocess_data.preprocess_stl(stl_imgs, stl_labels)\n",
    "# create splits\n",
    "x_train, y_train, x_test, y_test, x_val, y_val, x_dev, y_dev = preprocess_data.create_splits(\n",
    "    stl_imgs, stl_labels, n_train_samps=4548, n_test_samps=400, n_valid_samps=2, n_dev_samps=50)\n",
    "\n",
    "print ('Train data shape: ', x_train.shape)\n",
    "print ('Train labels shape: ', y_train.shape)\n",
    "print ('Test data shape: ', x_test.shape)\n",
    "print ('Test labels shape: ', y_test.shape)\n",
    "print ('Validation data shape: ', x_val.shape)\n",
    "print ('Validation labels shape: ', y_val.shape)\n",
    "print ('dev data shape: ', x_dev.shape)\n",
    "print ('dev labels shape: ', y_dev.shape)\n",
    "\n",
    "classes = np.loadtxt(os.path.join('data', 'stl10_binary', 'class_names.txt'), dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6c) Train and overfit the network on a small STL-10 sample with each optimizer\n",
    "\n",
    "**Goal:** If your network works, you should see a drop in loss over epochs to 0.\n",
    "\n",
    "In 3 seperate cells below\n",
    "\n",
    "- Create 3 different `ConvNet4` networks.\n",
    "- Compile each with a different optimizer (each net uses a different optimizer).\n",
    "- Train each on the **dev** set and validate on the tiny validation set (we dont care about out-of-training-set performance here).\n",
    "\n",
    "You will be making plots demonstrating the overfitting for each optimizer below. **You should train the nets with the same number of epochs such that at least 2/3 of them clearly show loss convergence to a small value; one optimizer may not converge yet, and that's ok**. Cut off the simulations based on the 2/3 that do converge.\n",
    "\n",
    "Guidelines:\n",
    "\n",
    "- Weight scales and learning rates of `1e-2` should work well.\n",
    "- Start by testing the Adam optimizer.\n",
    "- Remember that the input shape is (3, 16, 16). You need to specify this to the network constructor.\n",
    "- The hyperparameters are up to you, though I wouldn't recommend a batch size that is too small (close to 1), otherwise it may be tricky to see whether the loss is actually decreasing on average.\n",
    "- Decreasing `acc_freq` will make the `fit` function evaluate the training and validation accuracy more often. This is a computationally intensive process, so small values come with an increase in training time. On the other hand, checking the accuracy too infrequently means you won't know whether the network is trending toward overfitting the training data, which is what you're checking for.\n",
    "- Each training session takes ~30 mins on my laptop.\n",
    "\n",
    "**Caveat emptor:** Training convolutional networks is notoriously computationally intensive. If you experiment with hyperparameters, each training session may take several hours. Use the loss/accuracy print outs to quickly gauge whether your hyperparameter choices are getting your network to decrease in loss. Monitor print outs and interrupt the Jupyter kernel if things are not trending in the right direction. Consider using the Davis 102 iMacs if this is running too slow on your laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_scale = 1e-2\n",
    "lr = 1e-2\n",
    "input_shape = (3, 16, 16)\n",
    "mini_batch_sz = 10\n",
    "n_epochs = 150\n",
    "\n",
    "#preprocessing flattened it when we actually wanted it not flattened.\n",
    "x_dev = x_dev.reshape(x_dev.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "x_val = x_val.reshape(x_val.shape[0], input_shape[0], input_shape[1], input_shape[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train...\n",
      "750 iterations. 5 iter/epoch.\n",
      "Iteration: 1/750.\n",
      "Time taken for iteration 0: 1.6660547256469727\n",
      "Estimated time to complete: 1249.5410442352295\n",
      "Iteration: 2/750.\n",
      "Iteration: 3/750.\n",
      "Iteration: 4/750.\n",
      "Iteration: 5/750.\n",
      "Iteration: 6/750.\n",
      "Iteration: 7/750.\n",
      "Iteration: 8/750.\n",
      "Iteration: 9/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [2.2845796024885723, 2.2924845991086413, 2.2430353643670484]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.16, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 10/750.\n",
      "Iteration: 11/750.\n",
      "Iteration: 12/750.\n",
      "Iteration: 13/750.\n",
      "Iteration: 14/750.\n",
      "Iteration: 15/750.\n",
      "Iteration: 16/750.\n",
      "Iteration: 17/750.\n",
      "Iteration: 18/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [2.196307338513974, 2.1725589737331377, 1.8555820840431605]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.24, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 19/750.\n",
      "Iteration: 20/750.\n",
      "Iteration: 21/750.\n",
      "Iteration: 22/750.\n",
      "Iteration: 23/750.\n",
      "Iteration: 24/750.\n",
      "Iteration: 25/750.\n",
      "Iteration: 26/750.\n",
      "Iteration: 27/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [2.312345181935326, 2.2428537670038167, 1.8153889792853417]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 28/750.\n",
      "Iteration: 29/750.\n",
      "Iteration: 30/750.\n",
      "Iteration: 31/750.\n",
      "Iteration: 32/750.\n",
      "Iteration: 33/750.\n",
      "Iteration: 34/750.\n",
      "Iteration: 35/750.\n",
      "Iteration: 36/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.7621807182453688, 1.833807183850169, 2.0549246832422936]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.24, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 37/750.\n",
      "Iteration: 38/750.\n",
      "Iteration: 39/750.\n",
      "Iteration: 40/750.\n",
      "Iteration: 41/750.\n",
      "Iteration: 42/750.\n",
      "Iteration: 43/750.\n",
      "Iteration: 44/750.\n",
      "Iteration: 45/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [2.1230847051097714, 2.00511916232943, 1.9732593373540852]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.26, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 46/750.\n",
      "Iteration: 47/750.\n",
      "Iteration: 48/750.\n",
      "Iteration: 49/750.\n",
      "Iteration: 50/750.\n",
      "Iteration: 51/750.\n",
      "Iteration: 52/750.\n",
      "Iteration: 53/750.\n",
      "Iteration: 54/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.8132958306345657, 2.0563964202738827, 1.6435401383740862]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.42, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 55/750.\n",
      "Iteration: 56/750.\n",
      "Iteration: 57/750.\n",
      "Iteration: 58/750.\n",
      "Iteration: 59/750.\n",
      "Iteration: 60/750.\n",
      "Iteration: 61/750.\n",
      "Iteration: 62/750.\n",
      "Iteration: 63/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [2.1429283671619177, 1.3842210419573906, 1.8503751364566963]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.42, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 64/750.\n",
      "Iteration: 65/750.\n",
      "Iteration: 66/750.\n",
      "Iteration: 67/750.\n",
      "Iteration: 68/750.\n",
      "Iteration: 69/750.\n",
      "Iteration: 70/750.\n",
      "Iteration: 71/750.\n",
      "Iteration: 72/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.4492693059040154, 1.8337719840693965, 1.788756308516624]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.42, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 73/750.\n",
      "Iteration: 74/750.\n",
      "Iteration: 75/750.\n",
      "Iteration: 76/750.\n",
      "Iteration: 77/750.\n",
      "Iteration: 78/750.\n",
      "Iteration: 79/750.\n",
      "Iteration: 80/750.\n",
      "Iteration: 81/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.4360661578335743, 1.5099931642285551, 1.4284829401566097]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.46, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 82/750.\n",
      "Iteration: 83/750.\n",
      "Iteration: 84/750.\n",
      "Iteration: 85/750.\n",
      "Iteration: 86/750.\n",
      "Iteration: 87/750.\n",
      "Iteration: 88/750.\n",
      "Iteration: 89/750.\n",
      "Iteration: 90/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.3285889474860497, 1.476922730116448, 1.0004611806216006]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.46, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 91/750.\n",
      "Iteration: 92/750.\n",
      "Iteration: 93/750.\n",
      "Iteration: 94/750.\n",
      "Iteration: 95/750.\n",
      "Iteration: 96/750.\n",
      "Iteration: 97/750.\n",
      "Iteration: 98/750.\n",
      "Iteration: 99/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.4980733151569927, 1.2347177336803679, 1.2437637714007388]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.56, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 100/750.\n",
      "Iteration: 101/750.\n",
      "Iteration: 102/750.\n",
      "Iteration: 103/750.\n",
      "Iteration: 104/750.\n",
      "Iteration: 105/750.\n",
      "Iteration: 106/750.\n",
      "Iteration: 107/750.\n",
      "Iteration: 108/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.1590602893676074, 1.31185675119782, 0.7630824680897652]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.54, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 109/750.\n",
      "Iteration: 110/750.\n",
      "Iteration: 111/750.\n",
      "Iteration: 112/750.\n",
      "Iteration: 113/750.\n",
      "Iteration: 114/750.\n",
      "Iteration: 115/750.\n",
      "Iteration: 116/750.\n",
      "Iteration: 117/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.1463413255311268, 0.8585563678333387, 1.010251243340975]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.62, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 118/750.\n",
      "Iteration: 119/750.\n",
      "Iteration: 120/750.\n",
      "Iteration: 121/750.\n",
      "Iteration: 122/750.\n",
      "Iteration: 123/750.\n",
      "Iteration: 124/750.\n",
      "Iteration: 125/750.\n",
      "Iteration: 126/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.9948293101744844, 0.6375138809069608, 0.9847764571024111]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.68, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 127/750.\n",
      "Iteration: 128/750.\n",
      "Iteration: 129/750.\n",
      "Iteration: 130/750.\n",
      "Iteration: 131/750.\n",
      "Iteration: 132/750.\n",
      "Iteration: 133/750.\n",
      "Iteration: 134/750.\n",
      "Iteration: 135/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.0188620864883335, 1.5110819357261827, 0.5282107176868567]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.72, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 136/750.\n",
      "Iteration: 137/750.\n",
      "Iteration: 138/750.\n",
      "Iteration: 139/750.\n",
      "Iteration: 140/750.\n",
      "Iteration: 141/750.\n",
      "Iteration: 142/750.\n",
      "Iteration: 143/750.\n",
      "Iteration: 144/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.0612828686712, 0.7893206818157843, 0.7050813317119973]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.88, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 145/750.\n",
      "Iteration: 146/750.\n",
      "Iteration: 147/750.\n",
      "Iteration: 148/750.\n",
      "Iteration: 149/750.\n",
      "Iteration: 150/750.\n",
      "Iteration: 151/750.\n",
      "Iteration: 152/750.\n",
      "Iteration: 153/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.7095368860314037, 0.7967863091393386, 0.34622555323144777]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.88, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 154/750.\n",
      "Iteration: 155/750.\n",
      "Iteration: 156/750.\n",
      "Iteration: 157/750.\n",
      "Iteration: 158/750.\n",
      "Iteration: 159/750.\n",
      "Iteration: 160/750.\n",
      "Iteration: 161/750.\n",
      "Iteration: 162/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.4147177801834292, 0.1976633903218503, 0.6149993686508091]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.88, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 163/750.\n",
      "Iteration: 164/750.\n",
      "Iteration: 165/750.\n",
      "Iteration: 166/750.\n",
      "Iteration: 167/750.\n",
      "Iteration: 168/750.\n",
      "Iteration: 169/750.\n",
      "Iteration: 170/750.\n",
      "Iteration: 171/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.5417135652175434, 0.5127860907579702, 0.4423694055698868]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.9, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 172/750.\n",
      "Iteration: 173/750.\n",
      "Iteration: 174/750.\n",
      "Iteration: 175/750.\n",
      "Iteration: 176/750.\n",
      "Iteration: 177/750.\n",
      "Iteration: 178/750.\n",
      "Iteration: 179/750.\n",
      "Iteration: 180/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.3257512884451925, 0.23361025284867254, 0.4208937622866595]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.9, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 181/750.\n",
      "Iteration: 182/750.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 183/750.\n",
      "Iteration: 184/750.\n",
      "Iteration: 185/750.\n",
      "Iteration: 186/750.\n",
      "Iteration: 187/750.\n",
      "Iteration: 188/750.\n",
      "Iteration: 189/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.2526039181014987, 0.2661376588799025, 0.29333143778269355]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.96, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 190/750.\n",
      "Iteration: 191/750.\n",
      "Iteration: 192/750.\n",
      "Iteration: 193/750.\n",
      "Iteration: 194/750.\n",
      "Iteration: 195/750.\n",
      "Iteration: 196/750.\n",
      "Iteration: 197/750.\n",
      "Iteration: 198/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.1112629225244871, 0.07248974144177063, 0.31392346579773295]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 199/750.\n",
      "Iteration: 200/750.\n",
      "Iteration: 201/750.\n",
      "Iteration: 202/750.\n",
      "Iteration: 203/750.\n",
      "Iteration: 204/750.\n",
      "Iteration: 205/750.\n",
      "Iteration: 206/750.\n",
      "Iteration: 207/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.10167808297320831, 0.07616059586943538, 0.17105394612346236]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 208/750.\n",
      "Iteration: 209/750.\n",
      "Iteration: 210/750.\n",
      "Iteration: 211/750.\n",
      "Iteration: 212/750.\n",
      "Iteration: 213/750.\n",
      "Iteration: 214/750.\n",
      "Iteration: 215/750.\n",
      "Iteration: 216/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.04604406308275605, 0.2700110150812488, 0.1730756502564659]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.98, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 217/750.\n",
      "Iteration: 218/750.\n",
      "Iteration: 219/750.\n",
      "Iteration: 220/750.\n",
      "Iteration: 221/750.\n",
      "Iteration: 222/750.\n",
      "Iteration: 223/750.\n",
      "Iteration: 224/750.\n",
      "Iteration: 225/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.198977290108442, 0.14312686463978938, 0.03261416902984667]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 226/750.\n",
      "Iteration: 227/750.\n",
      "Iteration: 228/750.\n",
      "Iteration: 229/750.\n",
      "Iteration: 230/750.\n",
      "Iteration: 231/750.\n",
      "Iteration: 232/750.\n",
      "Iteration: 233/750.\n",
      "Iteration: 234/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.12942711737335458, 0.07879379011385884, 0.062299998236475564]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 235/750.\n",
      "Iteration: 236/750.\n",
      "Iteration: 237/750.\n",
      "Iteration: 238/750.\n",
      "Iteration: 239/750.\n",
      "Iteration: 240/750.\n",
      "Iteration: 241/750.\n",
      "Iteration: 242/750.\n",
      "Iteration: 243/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0771401766704466, 0.061047090270442084, 0.07818698688785436]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 244/750.\n",
      "Iteration: 245/750.\n",
      "Iteration: 246/750.\n",
      "Iteration: 247/750.\n",
      "Iteration: 248/750.\n",
      "Iteration: 249/750.\n",
      "Iteration: 250/750.\n",
      "Iteration: 251/750.\n",
      "Iteration: 252/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.02739438570280992, 0.049484310869164085, 0.03895022424867601]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 253/750.\n",
      "Iteration: 254/750.\n",
      "Iteration: 255/750.\n",
      "Iteration: 256/750.\n",
      "Iteration: 257/750.\n",
      "Iteration: 258/750.\n",
      "Iteration: 259/750.\n",
      "Iteration: 260/750.\n",
      "Iteration: 261/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.05045440734712168, 0.032420542822017895, 0.06781557374825271]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 262/750.\n",
      "Iteration: 263/750.\n",
      "Iteration: 264/750.\n",
      "Iteration: 265/750.\n",
      "Iteration: 266/750.\n",
      "Iteration: 267/750.\n",
      "Iteration: 268/750.\n",
      "Iteration: 269/750.\n",
      "Iteration: 270/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.03012681363097762, 0.04319413731979136, 0.02004262906877935]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 271/750.\n",
      "Iteration: 272/750.\n",
      "Iteration: 273/750.\n",
      "Iteration: 274/750.\n",
      "Iteration: 275/750.\n",
      "Iteration: 276/750.\n",
      "Iteration: 277/750.\n",
      "Iteration: 278/750.\n",
      "Iteration: 279/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.03676389344140057, 0.06299340377472308, 0.03454798975959152]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 280/750.\n",
      "Iteration: 281/750.\n",
      "Iteration: 282/750.\n",
      "Iteration: 283/750.\n",
      "Iteration: 284/750.\n",
      "Iteration: 285/750.\n",
      "Iteration: 286/750.\n",
      "Iteration: 287/750.\n",
      "Iteration: 288/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.021970949590423193, 0.008230711804036533, 0.04246734441958538]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 289/750.\n",
      "Iteration: 290/750.\n",
      "Iteration: 291/750.\n",
      "Iteration: 292/750.\n",
      "Iteration: 293/750.\n",
      "Iteration: 294/750.\n",
      "Iteration: 295/750.\n",
      "Iteration: 296/750.\n",
      "Iteration: 297/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.024177658392888598, 0.017464338244271845, 0.022840108982812742]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 298/750.\n",
      "Iteration: 299/750.\n",
      "Iteration: 300/750.\n",
      "Iteration: 301/750.\n",
      "Iteration: 302/750.\n",
      "Iteration: 303/750.\n",
      "Iteration: 304/750.\n",
      "Iteration: 305/750.\n",
      "Iteration: 306/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.025275973834323114, 0.030583171943476435, 0.00984498326368509]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 307/750.\n",
      "Iteration: 308/750.\n",
      "Iteration: 309/750.\n",
      "Iteration: 310/750.\n",
      "Iteration: 311/750.\n",
      "Iteration: 312/750.\n",
      "Iteration: 313/750.\n",
      "Iteration: 314/750.\n",
      "Iteration: 315/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0203630879049658, 0.01757106751657544, 0.010262528385192541]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 316/750.\n",
      "Iteration: 317/750.\n",
      "Iteration: 318/750.\n",
      "Iteration: 319/750.\n",
      "Iteration: 320/750.\n",
      "Iteration: 321/750.\n",
      "Iteration: 322/750.\n",
      "Iteration: 323/750.\n",
      "Iteration: 324/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.02262841694298828, 0.01758433643867625, 0.012217754002525399]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 325/750.\n",
      "Iteration: 326/750.\n",
      "Iteration: 327/750.\n",
      "Iteration: 328/750.\n",
      "Iteration: 329/750.\n",
      "Iteration: 330/750.\n",
      "Iteration: 331/750.\n",
      "Iteration: 332/750.\n",
      "Iteration: 333/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.015909905810328133, 0.01982560712916763, 0.013691125194401766]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 334/750.\n",
      "Iteration: 335/750.\n",
      "Iteration: 336/750.\n",
      "Iteration: 337/750.\n",
      "Iteration: 338/750.\n",
      "Iteration: 339/750.\n",
      "Iteration: 340/750.\n",
      "Iteration: 341/750.\n",
      "Iteration: 342/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.015110213725853711, 0.009527124064834098, 0.009334541094691867]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 343/750.\n",
      "Iteration: 344/750.\n",
      "Iteration: 345/750.\n",
      "Iteration: 346/750.\n",
      "Iteration: 347/750.\n",
      "Iteration: 348/750.\n",
      "Iteration: 349/750.\n",
      "Iteration: 350/750.\n",
      "Iteration: 351/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.01383921247847794, 0.017888971550242515, 0.012172627766161605]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 352/750.\n",
      "Iteration: 353/750.\n",
      "Iteration: 354/750.\n",
      "Iteration: 355/750.\n",
      "Iteration: 356/750.\n",
      "Iteration: 357/750.\n",
      "Iteration: 358/750.\n",
      "Iteration: 359/750.\n",
      "Iteration: 360/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.007692151459628377, 0.00860071117621646, 0.01337183452254036]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 361/750.\n",
      "Iteration: 362/750.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 363/750.\n",
      "Iteration: 364/750.\n",
      "Iteration: 365/750.\n",
      "Iteration: 366/750.\n",
      "Iteration: 367/750.\n",
      "Iteration: 368/750.\n",
      "Iteration: 369/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.016828954981043625, 0.008532854220705712, 0.014868278238738564]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 370/750.\n",
      "Iteration: 371/750.\n",
      "Iteration: 372/750.\n",
      "Iteration: 373/750.\n",
      "Iteration: 374/750.\n",
      "Iteration: 375/750.\n",
      "Iteration: 376/750.\n",
      "Iteration: 377/750.\n",
      "Iteration: 378/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.009769343830962701, 0.014373982427621002, 0.00897708285199268]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 379/750.\n",
      "Iteration: 380/750.\n",
      "Iteration: 381/750.\n",
      "Iteration: 382/750.\n",
      "Iteration: 383/750.\n",
      "Iteration: 384/750.\n",
      "Iteration: 385/750.\n",
      "Iteration: 386/750.\n",
      "Iteration: 387/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0061601486759959275, 0.011447400788466693, 0.010592205644013353]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 388/750.\n",
      "Iteration: 389/750.\n",
      "Iteration: 390/750.\n",
      "Iteration: 391/750.\n",
      "Iteration: 392/750.\n",
      "Iteration: 393/750.\n",
      "Iteration: 394/750.\n",
      "Iteration: 395/750.\n",
      "Iteration: 396/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.01006021025842885, 0.007103090284885151, 0.008748900552573213]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 397/750.\n",
      "Iteration: 398/750.\n",
      "Iteration: 399/750.\n",
      "Iteration: 400/750.\n",
      "Iteration: 401/750.\n",
      "Iteration: 402/750.\n",
      "Iteration: 403/750.\n",
      "Iteration: 404/750.\n",
      "Iteration: 405/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.006573226155617962, 0.008800037481439274, 0.007748006161016941]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 406/750.\n",
      "Iteration: 407/750.\n",
      "Iteration: 408/750.\n",
      "Iteration: 409/750.\n",
      "Iteration: 410/750.\n",
      "Iteration: 411/750.\n",
      "Iteration: 412/750.\n",
      "Iteration: 413/750.\n",
      "Iteration: 414/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.00853616274950926, 0.010528373331639172, 0.007736132999175462]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 415/750.\n",
      "Iteration: 416/750.\n",
      "Iteration: 417/750.\n",
      "Iteration: 418/750.\n",
      "Iteration: 419/750.\n",
      "Iteration: 420/750.\n",
      "Iteration: 421/750.\n",
      "Iteration: 422/750.\n",
      "Iteration: 423/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.007007899223572968, 0.00644505755233859, 0.006488757529163384]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 424/750.\n",
      "Iteration: 425/750.\n",
      "Iteration: 426/750.\n",
      "Iteration: 427/750.\n",
      "Iteration: 428/750.\n",
      "Iteration: 429/750.\n",
      "Iteration: 430/750.\n",
      "Iteration: 431/750.\n",
      "Iteration: 432/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.006053104493819026, 0.006276737155629471, 0.004831008743959072]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 433/750.\n",
      "Iteration: 434/750.\n",
      "Iteration: 435/750.\n",
      "Iteration: 436/750.\n",
      "Iteration: 437/750.\n",
      "Iteration: 438/750.\n",
      "Iteration: 439/750.\n",
      "Iteration: 440/750.\n",
      "Iteration: 441/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0052529349195933744, 0.00732885029553593, 0.003807727914067893]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 442/750.\n",
      "Iteration: 443/750.\n",
      "Iteration: 444/750.\n",
      "Iteration: 445/750.\n",
      "Iteration: 446/750.\n",
      "Iteration: 447/750.\n",
      "Iteration: 448/750.\n",
      "Iteration: 449/750.\n",
      "Iteration: 450/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.002921542476523221, 0.003654111207226949, 0.0032846278323096566]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 451/750.\n",
      "Iteration: 452/750.\n",
      "Iteration: 453/750.\n",
      "Iteration: 454/750.\n",
      "Iteration: 455/750.\n",
      "Iteration: 456/750.\n",
      "Iteration: 457/750.\n",
      "Iteration: 458/750.\n",
      "Iteration: 459/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0037279718019347834, 0.010223271739306093, 0.005651504630691279]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 460/750.\n",
      "Iteration: 461/750.\n",
      "Iteration: 462/750.\n",
      "Iteration: 463/750.\n",
      "Iteration: 464/750.\n",
      "Iteration: 465/750.\n",
      "Iteration: 466/750.\n",
      "Iteration: 467/750.\n",
      "Iteration: 468/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.006245607454844204, 0.005093410584849283, 0.0015902946597379669]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 469/750.\n",
      "Iteration: 470/750.\n",
      "Iteration: 471/750.\n",
      "Iteration: 472/750.\n",
      "Iteration: 473/750.\n",
      "Iteration: 474/750.\n",
      "Iteration: 475/750.\n",
      "Iteration: 476/750.\n",
      "Iteration: 477/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.00439410620243234, 0.006613928781317931, 0.003303326800842834]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 478/750.\n",
      "Iteration: 479/750.\n",
      "Iteration: 480/750.\n",
      "Iteration: 481/750.\n",
      "Iteration: 482/750.\n",
      "Iteration: 483/750.\n",
      "Iteration: 484/750.\n",
      "Iteration: 485/750.\n",
      "Iteration: 486/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0031054786424553732, 0.004483647626745672, 0.005548467720664309]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 487/750.\n",
      "Iteration: 488/750.\n",
      "Iteration: 489/750.\n",
      "Iteration: 490/750.\n",
      "Iteration: 491/750.\n",
      "Iteration: 492/750.\n",
      "Iteration: 493/750.\n",
      "Iteration: 494/750.\n",
      "Iteration: 495/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.00414603240714461, 0.009087762031101948, 0.0044056132546797895]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 496/750.\n",
      "Iteration: 497/750.\n",
      "Iteration: 498/750.\n",
      "Iteration: 499/750.\n",
      "Iteration: 500/750.\n",
      "Iteration: 501/750.\n",
      "Iteration: 502/750.\n",
      "Iteration: 503/750.\n",
      "Iteration: 504/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.002901846718032038, 0.004217244810202035, 0.005485786030073221]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 505/750.\n",
      "Iteration: 506/750.\n",
      "Iteration: 507/750.\n",
      "Iteration: 508/750.\n",
      "Iteration: 509/750.\n",
      "Iteration: 510/750.\n",
      "Iteration: 511/750.\n",
      "Iteration: 512/750.\n",
      "Iteration: 513/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0026945877464181088, 0.003658771101894766, 0.006712447580500221]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 514/750.\n",
      "Iteration: 515/750.\n",
      "Iteration: 516/750.\n",
      "Iteration: 517/750.\n",
      "Iteration: 518/750.\n",
      "Iteration: 519/750.\n",
      "Iteration: 520/750.\n",
      "Iteration: 521/750.\n",
      "Iteration: 522/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.004699095964409763, 0.003251637175180632, 0.003271414183918524]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 523/750.\n",
      "Iteration: 524/750.\n",
      "Iteration: 525/750.\n",
      "Iteration: 526/750.\n",
      "Iteration: 527/750.\n",
      "Iteration: 528/750.\n",
      "Iteration: 529/750.\n",
      "Iteration: 530/750.\n",
      "Iteration: 531/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0042051060893134274, 0.0038884924191254335, 0.002962506320237182]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 532/750.\n",
      "Iteration: 533/750.\n",
      "Iteration: 534/750.\n",
      "Iteration: 535/750.\n",
      "Iteration: 536/750.\n",
      "Iteration: 537/750.\n",
      "Iteration: 538/750.\n",
      "Iteration: 539/750.\n",
      "Iteration: 540/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.00298678405030627, 0.0027571970161227623, 0.003610517848250861]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 541/750.\n",
      "Iteration: 542/750.\n",
      "Iteration: 543/750.\n",
      "Iteration: 544/750.\n",
      "Iteration: 545/750.\n",
      "Iteration: 546/750.\n",
      "Iteration: 547/750.\n",
      "Iteration: 548/750.\n",
      "Iteration: 549/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.002019017040326986, 0.003892469532275133, 0.0035536293901333015]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 550/750.\n",
      "Iteration: 551/750.\n",
      "Iteration: 552/750.\n",
      "Iteration: 553/750.\n",
      "Iteration: 554/750.\n",
      "Iteration: 555/750.\n",
      "Iteration: 556/750.\n",
      "Iteration: 557/750.\n",
      "Iteration: 558/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0033645742777648154, 0.0025017081009596756, 0.003921704589888853]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 559/750.\n",
      "Iteration: 560/750.\n",
      "Iteration: 561/750.\n",
      "Iteration: 562/750.\n",
      "Iteration: 563/750.\n",
      "Iteration: 564/750.\n",
      "Iteration: 565/750.\n",
      "Iteration: 566/750.\n",
      "Iteration: 567/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.002981434723113973, 0.0013819217487384989, 0.0032949836318237656]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 568/750.\n",
      "Iteration: 569/750.\n",
      "Iteration: 570/750.\n",
      "Iteration: 571/750.\n",
      "Iteration: 572/750.\n",
      "Iteration: 573/750.\n",
      "Iteration: 574/750.\n",
      "Iteration: 575/750.\n",
      "Iteration: 576/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.002842557068409717, 0.0024130196333980534, 0.003387120788490034]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 577/750.\n",
      "Iteration: 578/750.\n",
      "Iteration: 579/750.\n",
      "Iteration: 580/750.\n",
      "Iteration: 581/750.\n",
      "Iteration: 582/750.\n",
      "Iteration: 583/750.\n",
      "Iteration: 584/750.\n",
      "Iteration: 585/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.002065377079902587, 0.002318131956749458, 0.0026479241884973977]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 586/750.\n",
      "Iteration: 587/750.\n",
      "Iteration: 588/750.\n",
      "Iteration: 589/750.\n",
      "Iteration: 590/750.\n",
      "Iteration: 591/750.\n",
      "Iteration: 592/750.\n",
      "Iteration: 593/750.\n",
      "Iteration: 594/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0028781183348701484, 0.005051741188932144, 0.0037435946121089254]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 595/750.\n",
      "Iteration: 596/750.\n",
      "Iteration: 597/750.\n",
      "Iteration: 598/750.\n",
      "Iteration: 599/750.\n",
      "Iteration: 600/750.\n",
      "Iteration: 601/750.\n",
      "Iteration: 602/750.\n",
      "Iteration: 603/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.00299654626254093, 0.003778291377038163, 0.0020178715544124266]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 604/750.\n",
      "Iteration: 605/750.\n",
      "Iteration: 606/750.\n",
      "Iteration: 607/750.\n",
      "Iteration: 608/750.\n",
      "Iteration: 609/750.\n",
      "Iteration: 610/750.\n",
      "Iteration: 611/750.\n",
      "Iteration: 612/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0035613454363410397, 0.0020711168632821953, 0.0017361541606328416]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 613/750.\n",
      "Iteration: 614/750.\n",
      "Iteration: 615/750.\n",
      "Iteration: 616/750.\n",
      "Iteration: 617/750.\n",
      "Iteration: 618/750.\n",
      "Iteration: 619/750.\n",
      "Iteration: 620/750.\n",
      "Iteration: 621/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0023794135897436224, 0.002434979975880696, 0.002614170525187537]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 622/750.\n",
      "Iteration: 623/750.\n",
      "Iteration: 624/750.\n",
      "Iteration: 625/750.\n",
      "Iteration: 626/750.\n",
      "Iteration: 627/750.\n",
      "Iteration: 628/750.\n",
      "Iteration: 629/750.\n",
      "Iteration: 630/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.002221729193596045, 0.0031591962917994717, 0.0026405344122758093]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 631/750.\n",
      "Iteration: 632/750.\n",
      "Iteration: 633/750.\n",
      "Iteration: 634/750.\n",
      "Iteration: 635/750.\n",
      "Iteration: 636/750.\n",
      "Iteration: 637/750.\n",
      "Iteration: 638/750.\n",
      "Iteration: 639/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0017053509258779108, 0.002381196224160104, 0.0024068952505818864]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 640/750.\n",
      "Iteration: 641/750.\n",
      "Iteration: 642/750.\n",
      "Iteration: 643/750.\n",
      "Iteration: 644/750.\n",
      "Iteration: 645/750.\n",
      "Iteration: 646/750.\n",
      "Iteration: 647/750.\n",
      "Iteration: 648/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0020553921174259224, 0.0025360110659460853, 0.0015094830603940579]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 649/750.\n",
      "Iteration: 650/750.\n",
      "Iteration: 651/750.\n",
      "Iteration: 652/750.\n",
      "Iteration: 653/750.\n",
      "Iteration: 654/750.\n",
      "Iteration: 655/750.\n",
      "Iteration: 656/750.\n",
      "Iteration: 657/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0015051251377963782, 0.0017464836483383286, 0.0011076337311476472]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 658/750.\n",
      "Iteration: 659/750.\n",
      "Iteration: 660/750.\n",
      "Iteration: 661/750.\n",
      "Iteration: 662/750.\n",
      "Iteration: 663/750.\n",
      "Iteration: 664/750.\n",
      "Iteration: 665/750.\n",
      "Iteration: 666/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0017189487269323558, 0.0017419271948256071, 0.001517056077212622]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 667/750.\n",
      "Iteration: 668/750.\n",
      "Iteration: 669/750.\n",
      "Iteration: 670/750.\n",
      "Iteration: 671/750.\n",
      "Iteration: 672/750.\n",
      "Iteration: 673/750.\n",
      "Iteration: 674/750.\n",
      "Iteration: 675/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.002583472775782791, 0.003131088204767879, 0.0017153648706021698]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 676/750.\n",
      "Iteration: 677/750.\n",
      "Iteration: 678/750.\n",
      "Iteration: 679/750.\n",
      "Iteration: 680/750.\n",
      "Iteration: 681/750.\n",
      "Iteration: 682/750.\n",
      "Iteration: 683/750.\n",
      "Iteration: 684/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0015867622306953801, 0.0011906861619485998, 0.00190017199205851]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 685/750.\n",
      "Iteration: 686/750.\n",
      "Iteration: 687/750.\n",
      "Iteration: 688/750.\n",
      "Iteration: 689/750.\n",
      "Iteration: 690/750.\n",
      "Iteration: 691/750.\n",
      "Iteration: 692/750.\n",
      "Iteration: 693/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0019919869100415086, 0.0022330823709953945, 0.0027735989679547437]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 694/750.\n",
      "Iteration: 695/750.\n",
      "Iteration: 696/750.\n",
      "Iteration: 697/750.\n",
      "Iteration: 698/750.\n",
      "Iteration: 699/750.\n",
      "Iteration: 700/750.\n",
      "Iteration: 701/750.\n",
      "Iteration: 702/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0019377714647102334, 0.0021468643512156747, 0.001613401638587184]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 703/750.\n",
      "Iteration: 704/750.\n",
      "Iteration: 705/750.\n",
      "Iteration: 706/750.\n",
      "Iteration: 707/750.\n",
      "Iteration: 708/750.\n",
      "Iteration: 709/750.\n",
      "Iteration: 710/750.\n",
      "Iteration: 711/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.00086116675978332, 0.001959338149785799, 0.0013139268230490256]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 712/750.\n",
      "Iteration: 713/750.\n",
      "Iteration: 714/750.\n",
      "Iteration: 715/750.\n",
      "Iteration: 716/750.\n",
      "Iteration: 717/750.\n",
      "Iteration: 718/750.\n",
      "Iteration: 719/750.\n",
      "Iteration: 720/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.000922927035018237, 0.0017368995073886746, 0.0009850095962978867]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 721/750.\n",
      "Iteration: 722/750.\n",
      "Iteration: 723/750.\n",
      "Iteration: 724/750.\n",
      "Iteration: 725/750.\n",
      "Iteration: 726/750.\n",
      "Iteration: 727/750.\n",
      "Iteration: 728/750.\n",
      "Iteration: 729/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.00160438701564368, 0.0015889317417497155, 0.0020780690656655564]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 730/750.\n",
      "Iteration: 731/750.\n",
      "Iteration: 732/750.\n",
      "Iteration: 733/750.\n",
      "Iteration: 734/750.\n",
      "Iteration: 735/750.\n",
      "Iteration: 736/750.\n",
      "Iteration: 737/750.\n",
      "Iteration: 738/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0010388349009595177, 0.0013960343425225312, 0.0021584754233142122]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 739/750.\n",
      "Iteration: 740/750.\n",
      "Iteration: 741/750.\n",
      "Iteration: 742/750.\n",
      "Iteration: 743/750.\n",
      "Iteration: 744/750.\n",
      "Iteration: 745/750.\n",
      "Iteration: 746/750.\n",
      "Iteration: 747/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.001567330547213729, 0.0015021132826861594, 0.0016439893587461441]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 748/750.\n",
      "Iteration: 749/750.\n",
      "Iteration: 750/750.\n",
      "\n",
      "\n",
      "----------------FINAL OUTPUT----------------\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "Loss history: [2.3055575589923514, 2.300406899071696, 2.3029133221585023, 2.294747648840793, 2.288576523611883, 2.290180646017032, 2.2845796024885723, 2.2924845991086413, 2.2430353643670484, 2.206360550015477, 2.1353708524936774, 2.3061608483970777, 2.2316064461131218, 2.271748997353242, 2.13538884465873, 2.196307338513974, 2.1725589737331377, 1.8555820840431605, 2.2766782177253577, 1.934898385421089, 1.871502819358501, 1.9138400595387086, 1.7624269599259677, 2.4884701186691163, 2.312345181935326, 2.2428537670038167, 1.8153889792853417, 2.042472231906978, 2.0274354973933386, 2.064275502731321, 1.9516374136257122, 1.897432557211994, 1.6579292576131857, 1.7621807182453688, 1.833807183850169, 2.0549246832422936, 1.8723331847903255, 1.6442609969283204, 1.9184862164623917, 1.9046591136566846, 1.8139214361928027, 2.005325645143333, 2.1230847051097714, 2.00511916232943, 1.9732593373540852, 2.0184364213823445, 2.6499883370507757, 1.5295603987287203, 2.14307588096561, 1.6253938407019612, 1.8354510726197937, 1.8132958306345657, 2.0563964202738827, 1.6435401383740862, 1.8748368238786222, 1.828618938846007, 1.7541986087968249, 1.806783549190694, 2.1103791353341728, 1.8129516954502638, 2.1429283671619177, 1.3842210419573906, 1.8503751364566963, 1.6541310471876487, 1.7109102547436128, 1.5414166184470133, 2.0695831223441785, 1.8010124632203686, 1.7868103714244592, 1.4492693059040154, 1.8337719840693965, 1.788756308516624, 1.5214341048054703, 1.3312439993016554, 1.5178424544515476, 1.382287926451269, 1.6661837300331095, 1.511687800988327, 1.4360661578335743, 1.5099931642285551, 1.4284829401566097, 1.875279531052607, 1.3028650293485664, 2.0328953693539904, 1.1246929247966073, 1.0400213378264602, 1.0522843412446876, 1.3285889474860497, 1.476922730116448, 1.0004611806216006, 1.140158729501649, 1.219787349219302, 1.4518095314654245, 1.6059248107977853, 1.1818203407602288, 1.315681706105867, 1.4980733151569927, 1.2347177336803679, 1.2437637714007388, 0.9512570234349385, 1.2619418598021444, 1.1709905748167786, 1.2123945793386361, 0.9025242810217403, 1.2810415819546384, 1.1590602893676074, 1.31185675119782, 0.7630824680897652, 1.2410263152957888, 1.1463289555778127, 1.3242519901328729, 0.8602249197955484, 1.2629893396069092, 1.0838840137850903, 1.1463413255311268, 0.8585563678333387, 1.010251243340975, 0.9108188527342874, 1.1657749006634384, 0.8501433736954743, 1.158100675622405, 1.2207810975212778, 0.6518704700850245, 0.9948293101744844, 0.6375138809069608, 0.9847764571024111, 0.6513278452428268, 1.0415511171954228, 1.032705407100247, 0.6644553302503602, 0.9375853799071927, 1.0291258904932936, 1.0188620864883335, 1.5110819357261827, 0.5282107176868567, 0.7851462760323777, 0.6624157430401119, 0.6488930768763339, 0.4787559503043197, 0.5610481598343083, 0.8771298906666285, 1.0612828686712, 0.7893206818157843, 0.7050813317119973, 0.8077446501275436, 0.7209304969752148, 0.48393176482096134, 0.5696522016964861, 0.3705807523842548, 0.44725253533085835, 0.7095368860314037, 0.7967863091393386, 0.34622555323144777, 0.7874294421665962, 0.28001171706999456, 0.6417807190258413, 0.503012086297786, 0.40403112139353486, 0.7426523787714432, 0.4147177801834292, 0.1976633903218503, 0.6149993686508091, 0.33555014104080705, 0.4207246377423761, 0.7233705795063519, 0.5423455286862345, 0.2921541497299826, 0.395845769945578, 0.5417135652175434, 0.5127860907579702, 0.4423694055698868, 0.3236602867531084, 0.14447520925983015, 0.4420298535896451, 0.4239328888498195, 0.21793086374474135, 0.49037664023562694, 0.3257512884451925, 0.23361025284867254, 0.4208937622866595, 0.2746972798672402, 0.4300013296525805, 0.36844921168523465, 0.23540087771781135, 0.41544197284494255, 0.2823062465263835, 0.2526039181014987, 0.2661376588799025, 0.29333143778269355, 0.3487392255354627, 0.20258932667943813, 0.43918308371232995, 0.30036378963684185, 0.22148967050890503, 0.11776599062723939, 0.1112629225244871, 0.07248974144177063, 0.31392346579773295, 0.11642156820684864, 0.23277828595846, 0.27315102825750665, 0.10417861245265847, 0.19039342095246847, 0.1679942273827797, 0.10167808297320831, 0.07616059586943538, 0.17105394612346236, 0.12500151358681744, 0.23919722520178413, 0.07845111215034989, 0.1245876885694504, 0.08742772534386235, 0.09124494133054097, 0.04604406308275605, 0.2700110150812488, 0.1730756502564659, 0.1568864321992886, 0.301464783995552, 0.08661943850772261, 0.17291598426721763, 0.07844323760715582, 0.1762749192416052, 0.198977290108442, 0.14312686463978938, 0.03261416902984667, 0.03054111706547881, 0.037543319868447673, 0.08079612114761033, 0.08678542311860352, 0.060218295902046116, 0.11474317879204149, 0.12942711737335458, 0.07879379011385884, 0.062299998236475564, 0.03503384923598232, 0.05560824559122778, 0.1030194030756865, 0.0990760726210339, 0.13145477710517545, 0.01610179754280706, 0.0771401766704466, 0.061047090270442084, 0.07818698688785436, 0.0918949199397228, 0.09319227375868416, 0.08811378867269333, 0.0734461336023442, 0.03174724699505884, 0.06212261310142785, 0.02739438570280992, 0.049484310869164085, 0.03895022424867601, 0.04981987358064388, 0.05074515396988789, 0.03983638177250487, 0.08492574436301814, 0.05200150962079785, 0.030332530426612006, 0.05045440734712168, 0.032420542822017895, 0.06781557374825271, 0.07558561610917186, 0.027929277068080117, 0.05190464068235282, 0.012895986292345257, 0.03725704383043042, 0.02313330775136576, 0.03012681363097762, 0.04319413731979136, 0.02004262906877935, 0.02229894280837623, 0.02751672654121899, 0.035268494043214145, 0.039428610645982264, 0.03204125921046961, 0.02661427731158411, 0.03676389344140057, 0.06299340377472308, 0.03454798975959152, 0.030682535393103484, 0.04194412309936003, 0.015105433473034708, 0.027204625626327257, 0.040608940778716955, 0.023246911952024316, 0.021970949590423193, 0.008230711804036533, 0.04246734441958538, 0.030043344573878424, 0.01763246479153635, 0.0284837154134865, 0.03091560022319131, 0.016077278588038684, 0.026727421826212808, 0.024177658392888598, 0.017464338244271845, 0.022840108982812742, 0.017288535993820235, 0.02148621491454814, 0.028915874613482787, 0.011973472052999645, 0.021289459056199462, 0.012232264444989205, 0.025275973834323114, 0.030583171943476435, 0.00984498326368509, 0.028018216705707557, 0.023579674426418645, 0.02469447606856087, 0.014756764283264868, 0.029010128608519467, 0.0195356637239778, 0.0203630879049658, 0.01757106751657544, 0.010262528385192541, 0.013262062860265728, 0.021245177339018713, 0.021721729017228963, 0.03689103708585828, 0.01351651153150445, 0.009935061205575439, 0.02262841694298828, 0.01758433643867625, 0.012217754002525399, 0.01932559752139203, 0.014958402214521075, 0.027567374480082448, 0.018025596817403688, 0.010517139914541018, 0.017699444398169662, 0.015909905810328133, 0.01982560712916763, 0.013691125194401766, 0.016550432755662575, 0.009078229764578996, 0.007944549330278787, 0.01326805701525457, 0.025850371833740872, 0.013628392763211048, 0.015110213725853711, 0.009527124064834098, 0.009334541094691867, 0.006085630296447723, 0.017934603923836508, 0.009869102393588697, 0.01877156124176056, 0.010938960103062646, 0.009361092362202691, 0.01383921247847794, 0.017888971550242515, 0.012172627766161605, 0.012892033167378431, 0.008206894884965657, 0.00905059199327114, 0.017796027177664306, 0.009163779352044385, 0.009489457224419108, 0.007692151459628377, 0.00860071117621646, 0.01337183452254036, 0.01293385673719119, 0.008399997383587529, 0.01264211171765953, 0.013858170752131613, 0.004903633772470935, 0.009291095305653377, 0.016828954981043625, 0.008532854220705712, 0.014868278238738564, 0.009907119201207511, 0.015346446124348127, 0.011719231907154463, 0.004210578679004389, 0.014362194165312063, 0.013576512265913249, 0.009769343830962701, 0.014373982427621002, 0.00897708285199268, 0.007879795891827508, 0.009862970582605868, 0.007378127775919613, 0.007813068998589977, 0.006245027504013833, 0.008922686984725212, 0.0061601486759959275, 0.011447400788466693, 0.010592205644013353, 0.008779988807601034, 0.007384445939282525, 0.005231718442068179, 0.005374112996673978, 0.007218698803703233, 0.009120530892147403, 0.01006021025842885, 0.007103090284885151, 0.008748900552573213, 0.0035180300370962697, 0.007995185752619328, 0.006997362700971801, 0.009063252542229545, 0.00764816543458346, 0.010021398502396002, 0.006573226155617962, 0.008800037481439274, 0.007748006161016941, 0.010937734736951477, 0.006072712121079891, 0.00538165310937602, 0.007465023465125563, 0.004909194654504705, 0.0058572163199658075, 0.00853616274950926, 0.010528373331639172, 0.007736132999175462, 0.00718902707290607, 0.0029909235888185715, 0.010247816293518714, 0.006803988991259701, 0.0063724690003217335, 0.002905587914533317, 0.007007899223572968, 0.00644505755233859, 0.006488757529163384, 0.002572594387541828, 0.0044282391344211405, 0.006817876694734597, 0.004539010813511677, 0.004361713562243303, 0.0064384843265000965, 0.006053104493819026, 0.006276737155629471, 0.004831008743959072, 0.010958469366237057, 0.0060074596681144, 0.0029252938844696874, 0.0030119681989681257, 0.006741203732546441, 0.005249409183349996, 0.0052529349195933744, 0.00732885029553593, 0.003807727914067893, 0.0040358186911486175, 0.004833614462360483, 0.0047207717788715015, 0.003263436409534696, 0.00383955053305814, 0.009459019838161612, 0.002921542476523221, 0.003654111207226949, 0.0032846278323096566, 0.004885562824869446, 0.00508569049506098, 0.006373271890193562, 0.0050280810405093435, 0.006200431003390502, 0.0035311075063430976, 0.0037279718019347834, 0.010223271739306093, 0.005651504630691279, 0.005008483937049452, 0.006027997653171636, 0.006361605154960152, 0.0034867165884126454, 0.0029331952938917664, 0.004020571156771747, 0.006245607454844204, 0.005093410584849283, 0.0015902946597379669, 0.004266418964347722, 0.005104216601482122, 0.0033153953966085464, 0.002263779852399839, 0.007160398650149055, 0.006600048804114131, 0.00439410620243234, 0.006613928781317931, 0.003303326800842834, 0.004055481427072928, 0.006521513929952537, 0.004458408414755986, 0.005556058564968627, 0.0032512623862984338, 0.0029908740123645145, 0.0031054786424553732, 0.004483647626745672, 0.005548467720664309, 0.006621132659978442, 0.004664450628537602, 0.005383152499169567, 0.0044664641921742234, 0.006147528404749515, 0.004589144306922832, 0.00414603240714461, 0.009087762031101948, 0.0044056132546797895, 0.003909099621062447, 0.002374078828294372, 0.004199905211404633, 0.006057050601898557, 0.0032105797489554015, 0.0031396686273902057, 0.002901846718032038, 0.004217244810202035, 0.005485786030073221, 0.00456064989139799, 0.004663277704692785, 0.003895897854008306, 0.0035728899014997257, 0.0027536640593061232, 0.004219869830662639, 0.0026945877464181088, 0.003658771101894766, 0.006712447580500221, 0.0034696071602221797, 0.003184528785590471, 0.006798037894825511, 0.0049662754478161, 0.0027370586846505564, 0.004805179326626523, 0.004699095964409763, 0.003251637175180632, 0.003271414183918524, 0.005589838323135825, 0.0032441389877758903, 0.0067858601622701, 0.006118630688254335, 0.004131691877312017, 0.003082671607921422, 0.0042051060893134274, 0.0038884924191254335, 0.002962506320237182, 0.002076157311928311, 0.0020558838118918516, 0.003356738651684689, 0.0028620378255369678, 0.0029686248846644923, 0.00349765788185177, 0.00298678405030627, 0.0027571970161227623, 0.003610517848250861, 0.004466379683559644, 0.003580558601558644, 0.0022491600988507276, 0.0038862574565933887, 0.0020585755986470416, 0.003350119130455706, 0.002019017040326986, 0.003892469532275133, 0.0035536293901333015, 0.0032306450342034334, 0.002800186661059094, 0.003231323578908408, 0.002385128178917209, 0.0031244951981927393, 0.004250466780830907, 0.0033645742777648154, 0.0025017081009596756, 0.003921704589888853, 0.004116295634103122, 0.003620619447717498, 0.0017603315439187269, 0.002469395053677466, 0.0029986673784294093, 0.0032258431313620115, 0.002981434723113973, 0.0013819217487384989, 0.0032949836318237656, 0.004161248977183076, 0.0025140866251171495, 0.002230685022040397, 0.003939352093486844, 0.00295376376826937, 0.0030149312382889482, 0.002842557068409717, 0.0024130196333980534, 0.003387120788490034, 0.002919653903554604, 0.004610478841244241, 0.004830742754672676, 0.004849249306296692, 0.004092267674917679, 0.00323413799897149, 0.002065377079902587, 0.002318131956749458, 0.0026479241884973977, 0.002350889617063655, 0.002304473890369692, 0.004179157886665444, 0.00311171460734571, 0.002343209887573981, 0.0036532783388440442, 0.0028781183348701484, 0.005051741188932144, 0.0037435946121089254, 0.004010236480847776, 0.0017777984537796284, 0.0018778375051706848, 0.0035253957550480178, 0.004959555307564778, 0.001887589324937939, 0.00299654626254093, 0.003778291377038163, 0.0020178715544124266, 0.0030543333680973016, 0.002629996715882432, 0.00205269095445219, 0.0020654795626069322, 0.002139058456069485, 0.002084374852789543, 0.0035613454363410397, 0.0020711168632821953, 0.0017361541606328416, 0.002791846028621271, 0.0015494622708039484, 0.0026310909319199483, 0.0017459589478171595, 0.003877744793507751, 0.0026055125449147, 0.0023794135897436224, 0.002434979975880696, 0.002614170525187537, 0.002936776859233544, 0.001795591592682955, 0.002159181356940997, 0.0024298340813780532, 0.0022545333375030376, 0.0020952799545892044, 0.002221729193596045, 0.0031591962917994717, 0.0026405344122758093, 0.002253969437781386, 0.0020810698389622123, 0.002098951589179424, 0.0023879481220214368, 0.0024659611953409643, 0.0012442477664569135, 0.0017053509258779108, 0.002381196224160104, 0.0024068952505818864, 0.001942113174759474, 0.002249302904875942, 0.001991766694607963, 0.0021797384551068, 0.0024118015747329564, 0.0027592926374591935, 0.0020553921174259224, 0.0025360110659460853, 0.0015094830603940579, 0.0018396449015827107, 0.0019136425148368588, 0.0035232864151260052, 0.0037251412669029194, 0.002507815100855471, 0.0023615706936043353, 0.0015051251377963782, 0.0017464836483383286, 0.0011076337311476472, 0.0025276373927947977, 0.0019052993614125083, 0.0019418524240472032, 0.0017731584356773104, 0.0021402632026583635, 0.0022181837621589057, 0.0017189487269323558, 0.0017419271948256071, 0.001517056077212622, 0.0036580975340142043, 0.0021318460724613457, 0.0022641845365276372, 0.001735248554264855, 0.0023377994092444844, 0.0013196547302400724, 0.002583472775782791, 0.003131088204767879, 0.0017153648706021698, 0.0014351853360627023, 0.001400209395550599, 0.0024686255004100384, 0.0025826632567593215, 0.0022213754367474653, 0.0033098318179195334, 0.0015867622306953801, 0.0011906861619485998, 0.00190017199205851, 0.002267275949970542, 0.0017973148529912772, 0.0017866884705282319, 0.0015852220841670378, 0.002458232138383762, 0.0014244905431833356, 0.0019919869100415086, 0.0022330823709953945, 0.0027735989679547437, 0.0026790056440027204, 0.0016648337697080765, 0.0020633290652193647, 0.0018820694134797256, 0.0023719812136731443, 0.0019624419874155175, 0.0019377714647102334, 0.0021468643512156747, 0.001613401638587184, 0.0027210783105111533, 0.0013589544920751294, 0.000832850823144387, 0.0012942071864269781, 0.002052153088747626, 0.0018600081981792272, 0.00086116675978332, 0.001959338149785799, 0.0013139268230490256, 0.0014306314232718851, 0.0016959028832292259, 0.0017518964170656918, 0.0017331592446227701, 0.0015918302126712758, 0.0011052993354616284, 0.000922927035018237, 0.0017368995073886746, 0.0009850095962978867, 0.0011681268652901126, 0.0006304484831412962, 0.001487507723879396, 0.0026474813828503124, 0.001394117871138313, 0.0008204897749794848, 0.00160438701564368, 0.0015889317417497155, 0.0020780690656655564, 0.0010636636904031375, 0.0016175576179632877, 0.0008621424073317424, 0.0017633454725117536, 0.0019517233887101733, 0.0016230313448372285, 0.0010388349009595177, 0.0013960343425225312, 0.0021584754233142122, 0.0015085948267311207, 0.0015261316094420914, 0.0014562299200879358, 0.0021428721253259246, 0.0015409984000469296, 0.001926323120287074, 0.001567330547213729, 0.0015021132826861594, 0.0016439893587461441, 0.0016136400756901178, 0.0007363023758899155, 0.0013633331881789958]\n",
      "Accuracy history: [0.16, 0.24, 0.22, 0.24, 0.26, 0.42, 0.42, 0.42, 0.46, 0.46, 0.56, 0.54, 0.62, 0.68, 0.72, 0.88, 0.88, 0.88, 0.9, 0.9, 0.96, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Adam\n",
    "\n",
    "adam = ConvNet4(input_shape=input_shape, wt_scale=wt_scale, verbose=False)\n",
    "adam.compile('adam')\n",
    "adam.fit(x_dev, y_dev, x_val, y_val, mini_batch_sz=mini_batch_sz, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train...\n",
      "750 iterations. 5 iter/epoch.\n",
      "Iteration: 1/750.\n",
      "Time taken for iteration 0: 1.6611967086791992\n",
      "Estimated time to complete: 1245.8975315093994\n",
      "Iteration: 2/750.\n",
      "Iteration: 3/750.\n",
      "Iteration: 4/750.\n",
      "Iteration: 5/750.\n",
      "Iteration: 6/750.\n",
      "Iteration: 7/750.\n",
      "Iteration: 8/750.\n",
      "Iteration: 9/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3083244218204695, 2.305725695413475, 2.297431716772699]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.16, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 10/750.\n",
      "Iteration: 11/750.\n",
      "Iteration: 12/750.\n",
      "Iteration: 13/750.\n",
      "Iteration: 14/750.\n",
      "Iteration: 15/750.\n",
      "Iteration: 16/750.\n",
      "Iteration: 17/750.\n",
      "Iteration: 18/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3018450824131884, 2.3036695764710173, 2.3017066047698584]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.16, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 19/750.\n",
      "Iteration: 20/750.\n",
      "Iteration: 21/750.\n",
      "Iteration: 22/750.\n",
      "Iteration: 23/750.\n",
      "Iteration: 24/750.\n",
      "Iteration: 25/750.\n",
      "Iteration: 26/750.\n",
      "Iteration: 27/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.296826694035708, 2.2988054741544746, 2.298301305372676]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.16, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 28/750.\n",
      "Iteration: 29/750.\n",
      "Iteration: 30/750.\n",
      "Iteration: 31/750.\n",
      "Iteration: 32/750.\n",
      "Iteration: 33/750.\n",
      "Iteration: 34/750.\n",
      "Iteration: 35/750.\n",
      "Iteration: 36/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2966168260260056, 2.299502510069381, 2.297408434557621]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.16, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 37/750.\n",
      "Iteration: 38/750.\n",
      "Iteration: 39/750.\n",
      "Iteration: 40/750.\n",
      "Iteration: 41/750.\n",
      "Iteration: 42/750.\n",
      "Iteration: 43/750.\n",
      "Iteration: 44/750.\n",
      "Iteration: 45/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.298535173218746, 2.2969774199484214, 2.287743382443448]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.16, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 46/750.\n",
      "Iteration: 47/750.\n",
      "Iteration: 48/750.\n",
      "Iteration: 49/750.\n",
      "Iteration: 50/750.\n",
      "Iteration: 51/750.\n",
      "Iteration: 52/750.\n",
      "Iteration: 53/750.\n",
      "Iteration: 54/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2839536623842185, 2.2938137555028617, 2.302438358369893]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 55/750.\n",
      "Iteration: 56/750.\n",
      "Iteration: 57/750.\n",
      "Iteration: 58/750.\n",
      "Iteration: 59/750.\n",
      "Iteration: 60/750.\n",
      "Iteration: 61/750.\n",
      "Iteration: 62/750.\n",
      "Iteration: 63/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3025548231332373, 2.2845544381527216, 2.281798459737148]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 64/750.\n",
      "Iteration: 65/750.\n",
      "Iteration: 66/750.\n",
      "Iteration: 67/750.\n",
      "Iteration: 68/750.\n",
      "Iteration: 69/750.\n",
      "Iteration: 70/750.\n",
      "Iteration: 71/750.\n",
      "Iteration: 72/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2835935731873747, 2.2789792517783667, 2.2761709594171795]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 73/750.\n",
      "Iteration: 74/750.\n",
      "Iteration: 75/750.\n",
      "Iteration: 76/750.\n",
      "Iteration: 77/750.\n",
      "Iteration: 78/750.\n",
      "Iteration: 79/750.\n",
      "Iteration: 80/750.\n",
      "Iteration: 81/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.274863026115056, 2.294948740664123, 2.2861885214757716]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 82/750.\n",
      "Iteration: 83/750.\n",
      "Iteration: 84/750.\n",
      "Iteration: 85/750.\n",
      "Iteration: 86/750.\n",
      "Iteration: 87/750.\n",
      "Iteration: 88/750.\n",
      "Iteration: 89/750.\n",
      "Iteration: 90/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2580556523144146, 2.2770504059269747, 2.2811090608307416]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 91/750.\n",
      "Iteration: 92/750.\n",
      "Iteration: 93/750.\n",
      "Iteration: 94/750.\n",
      "Iteration: 95/750.\n",
      "Iteration: 96/750.\n",
      "Iteration: 97/750.\n",
      "Iteration: 98/750.\n",
      "Iteration: 99/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2767012191425837, 2.261673609481268, 2.272762194269741]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 100/750.\n",
      "Iteration: 101/750.\n",
      "Iteration: 102/750.\n",
      "Iteration: 103/750.\n",
      "Iteration: 104/750.\n",
      "Iteration: 105/750.\n",
      "Iteration: 106/750.\n",
      "Iteration: 107/750.\n",
      "Iteration: 108/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2664487825242845, 2.286639542318909, 2.247916495356978]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 109/750.\n",
      "Iteration: 110/750.\n",
      "Iteration: 111/750.\n",
      "Iteration: 112/750.\n",
      "Iteration: 113/750.\n",
      "Iteration: 114/750.\n",
      "Iteration: 115/750.\n",
      "Iteration: 116/750.\n",
      "Iteration: 117/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2729363999356873, 2.273953395587147, 2.2786906412783487]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 118/750.\n",
      "Iteration: 119/750.\n",
      "Iteration: 120/750.\n",
      "Iteration: 121/750.\n",
      "Iteration: 122/750.\n",
      "Iteration: 123/750.\n",
      "Iteration: 124/750.\n",
      "Iteration: 125/750.\n",
      "Iteration: 126/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2537882165991663, 2.278439164214905, 2.2486419643059565]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 127/750.\n",
      "Iteration: 128/750.\n",
      "Iteration: 129/750.\n",
      "Iteration: 130/750.\n",
      "Iteration: 131/750.\n",
      "Iteration: 132/750.\n",
      "Iteration: 133/750.\n",
      "Iteration: 134/750.\n",
      "Iteration: 135/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.32051304490723, 2.3152721759448656, 2.2505928687197856]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 136/750.\n",
      "Iteration: 137/750.\n",
      "Iteration: 138/750.\n",
      "Iteration: 139/750.\n",
      "Iteration: 140/750.\n",
      "Iteration: 141/750.\n",
      "Iteration: 142/750.\n",
      "Iteration: 143/750.\n",
      "Iteration: 144/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2918283532741106, 2.230395258419411, 2.2907653948807876]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 145/750.\n",
      "Iteration: 146/750.\n",
      "Iteration: 147/750.\n",
      "Iteration: 148/750.\n",
      "Iteration: 149/750.\n",
      "Iteration: 150/750.\n",
      "Iteration: 151/750.\n",
      "Iteration: 152/750.\n",
      "Iteration: 153/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.250193863582836, 2.2973584825330042, 2.268586391634377]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 154/750.\n",
      "Iteration: 155/750.\n",
      "Iteration: 156/750.\n",
      "Iteration: 157/750.\n",
      "Iteration: 158/750.\n",
      "Iteration: 159/750.\n",
      "Iteration: 160/750.\n",
      "Iteration: 161/750.\n",
      "Iteration: 162/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2824466859916486, 2.244650987574829, 2.2543351594952203]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 163/750.\n",
      "Iteration: 164/750.\n",
      "Iteration: 165/750.\n",
      "Iteration: 166/750.\n",
      "Iteration: 167/750.\n",
      "Iteration: 168/750.\n",
      "Iteration: 169/750.\n",
      "Iteration: 170/750.\n",
      "Iteration: 171/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.232717186500342, 2.2397897020179474, 2.2738842139177944]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 172/750.\n",
      "Iteration: 173/750.\n",
      "Iteration: 174/750.\n",
      "Iteration: 175/750.\n",
      "Iteration: 176/750.\n",
      "Iteration: 177/750.\n",
      "Iteration: 178/750.\n",
      "Iteration: 179/750.\n",
      "Iteration: 180/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2679793280689706, 2.28689228759865, 2.276174764319612]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 181/750.\n",
      "Iteration: 182/750.\n",
      "Iteration: 183/750.\n",
      "Iteration: 184/750.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 185/750.\n",
      "Iteration: 186/750.\n",
      "Iteration: 187/750.\n",
      "Iteration: 188/750.\n",
      "Iteration: 189/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2809092996558507, 2.2789688282647362, 2.28805322376837]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 190/750.\n",
      "Iteration: 191/750.\n",
      "Iteration: 192/750.\n",
      "Iteration: 193/750.\n",
      "Iteration: 194/750.\n",
      "Iteration: 195/750.\n",
      "Iteration: 196/750.\n",
      "Iteration: 197/750.\n",
      "Iteration: 198/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2551803876097742, 2.268825191182065, 2.2279083463692317]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 199/750.\n",
      "Iteration: 200/750.\n",
      "Iteration: 201/750.\n",
      "Iteration: 202/750.\n",
      "Iteration: 203/750.\n",
      "Iteration: 204/750.\n",
      "Iteration: 205/750.\n",
      "Iteration: 206/750.\n",
      "Iteration: 207/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.308294898786341, 2.2447185877161426, 2.2561030174177947]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 208/750.\n",
      "Iteration: 209/750.\n",
      "Iteration: 210/750.\n",
      "Iteration: 211/750.\n",
      "Iteration: 212/750.\n",
      "Iteration: 213/750.\n",
      "Iteration: 214/750.\n",
      "Iteration: 215/750.\n",
      "Iteration: 216/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.274031923422989, 2.292507221173354, 2.2214233061154625]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 217/750.\n",
      "Iteration: 218/750.\n",
      "Iteration: 219/750.\n",
      "Iteration: 220/750.\n",
      "Iteration: 221/750.\n",
      "Iteration: 222/750.\n",
      "Iteration: 223/750.\n",
      "Iteration: 224/750.\n",
      "Iteration: 225/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3173336547347247, 2.267883190300659, 2.216650036147166]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 226/750.\n",
      "Iteration: 227/750.\n",
      "Iteration: 228/750.\n",
      "Iteration: 229/750.\n",
      "Iteration: 230/750.\n",
      "Iteration: 231/750.\n",
      "Iteration: 232/750.\n",
      "Iteration: 233/750.\n",
      "Iteration: 234/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3015755984933626, 2.238681765961866, 2.2016660365529783]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 235/750.\n",
      "Iteration: 236/750.\n",
      "Iteration: 237/750.\n",
      "Iteration: 238/750.\n",
      "Iteration: 239/750.\n",
      "Iteration: 240/750.\n",
      "Iteration: 241/750.\n",
      "Iteration: 242/750.\n",
      "Iteration: 243/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3058068408717873, 2.2569811975613874, 2.269927435171348]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 244/750.\n",
      "Iteration: 245/750.\n",
      "Iteration: 246/750.\n",
      "Iteration: 247/750.\n",
      "Iteration: 248/750.\n",
      "Iteration: 249/750.\n",
      "Iteration: 250/750.\n",
      "Iteration: 251/750.\n",
      "Iteration: 252/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2473843431840854, 2.1699650093380507, 2.330035628675186]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 253/750.\n",
      "Iteration: 254/750.\n",
      "Iteration: 255/750.\n",
      "Iteration: 256/750.\n",
      "Iteration: 257/750.\n",
      "Iteration: 258/750.\n",
      "Iteration: 259/750.\n",
      "Iteration: 260/750.\n",
      "Iteration: 261/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.271419112296858, 2.201561180448635, 2.2923443997167285]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 262/750.\n",
      "Iteration: 263/750.\n",
      "Iteration: 264/750.\n",
      "Iteration: 265/750.\n",
      "Iteration: 266/750.\n",
      "Iteration: 267/750.\n",
      "Iteration: 268/750.\n",
      "Iteration: 269/750.\n",
      "Iteration: 270/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3423970038143236, 2.2769262981458995, 2.3062019756584777]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 271/750.\n",
      "Iteration: 272/750.\n",
      "Iteration: 273/750.\n",
      "Iteration: 274/750.\n",
      "Iteration: 275/750.\n",
      "Iteration: 276/750.\n",
      "Iteration: 277/750.\n",
      "Iteration: 278/750.\n",
      "Iteration: 279/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2407741533860976, 2.172899416933032, 2.1984333670970226]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 280/750.\n",
      "Iteration: 281/750.\n",
      "Iteration: 282/750.\n",
      "Iteration: 283/750.\n",
      "Iteration: 284/750.\n",
      "Iteration: 285/750.\n",
      "Iteration: 286/750.\n",
      "Iteration: 287/750.\n",
      "Iteration: 288/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2981574744501674, 2.2600462418147753, 2.2166681453444483]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 289/750.\n",
      "Iteration: 290/750.\n",
      "Iteration: 291/750.\n",
      "Iteration: 292/750.\n",
      "Iteration: 293/750.\n",
      "Iteration: 294/750.\n",
      "Iteration: 295/750.\n",
      "Iteration: 296/750.\n",
      "Iteration: 297/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2155338896642607, 2.197460097265643, 2.318388907515723]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 298/750.\n",
      "Iteration: 299/750.\n",
      "Iteration: 300/750.\n",
      "Iteration: 301/750.\n",
      "Iteration: 302/750.\n",
      "Iteration: 303/750.\n",
      "Iteration: 304/750.\n",
      "Iteration: 305/750.\n",
      "Iteration: 306/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.183530894514179, 2.1687370910024586, 2.280636857520292]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 307/750.\n",
      "Iteration: 308/750.\n",
      "Iteration: 309/750.\n",
      "Iteration: 310/750.\n",
      "Iteration: 311/750.\n",
      "Iteration: 312/750.\n",
      "Iteration: 313/750.\n",
      "Iteration: 314/750.\n",
      "Iteration: 315/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3083214739652798, 2.2727792010817764, 2.2652212535838214]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 316/750.\n",
      "Iteration: 317/750.\n",
      "Iteration: 318/750.\n",
      "Iteration: 319/750.\n",
      "Iteration: 320/750.\n",
      "Iteration: 321/750.\n",
      "Iteration: 322/750.\n",
      "Iteration: 323/750.\n",
      "Iteration: 324/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2743787708138794, 2.205430793999642, 2.216025726528513]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 325/750.\n",
      "Iteration: 326/750.\n",
      "Iteration: 327/750.\n",
      "Iteration: 328/750.\n",
      "Iteration: 329/750.\n",
      "Iteration: 330/750.\n",
      "Iteration: 331/750.\n",
      "Iteration: 332/750.\n",
      "Iteration: 333/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2523932586141244, 2.2818711031090086, 2.2729374546977894]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 334/750.\n",
      "Iteration: 335/750.\n",
      "Iteration: 336/750.\n",
      "Iteration: 337/750.\n",
      "Iteration: 338/750.\n",
      "Iteration: 339/750.\n",
      "Iteration: 340/750.\n",
      "Iteration: 341/750.\n",
      "Iteration: 342/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3870551622869107, 2.227611855833517, 2.2965227388282172]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 343/750.\n",
      "Iteration: 344/750.\n",
      "Iteration: 345/750.\n",
      "Iteration: 346/750.\n",
      "Iteration: 347/750.\n",
      "Iteration: 348/750.\n",
      "Iteration: 349/750.\n",
      "Iteration: 350/750.\n",
      "Iteration: 351/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1972767659635544, 2.1985326184695064, 2.2349970739214924]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 352/750.\n",
      "Iteration: 353/750.\n",
      "Iteration: 354/750.\n",
      "Iteration: 355/750.\n",
      "Iteration: 356/750.\n",
      "Iteration: 357/750.\n",
      "Iteration: 358/750.\n",
      "Iteration: 359/750.\n",
      "Iteration: 360/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1742901250718467, 2.0799756410980557, 2.2220841861419194]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 361/750.\n",
      "Iteration: 362/750.\n",
      "Iteration: 363/750.\n",
      "Iteration: 364/750.\n",
      "Iteration: 365/750.\n",
      "Iteration: 366/750.\n",
      "Iteration: 367/750.\n",
      "Iteration: 368/750.\n",
      "Iteration: 369/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1819677069929813, 2.2273059778822533, 2.1947998105505557]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 370/750.\n",
      "Iteration: 371/750.\n",
      "Iteration: 372/750.\n",
      "Iteration: 373/750.\n",
      "Iteration: 374/750.\n",
      "Iteration: 375/750.\n",
      "Iteration: 376/750.\n",
      "Iteration: 377/750.\n",
      "Iteration: 378/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.24355648076423, 2.176654193386759, 2.343633754517829]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 379/750.\n",
      "Iteration: 380/750.\n",
      "Iteration: 381/750.\n",
      "Iteration: 382/750.\n",
      "Iteration: 383/750.\n",
      "Iteration: 384/750.\n",
      "Iteration: 385/750.\n",
      "Iteration: 386/750.\n",
      "Iteration: 387/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.258344127286521, 2.287439493420776, 2.2160262565649242]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 388/750.\n",
      "Iteration: 389/750.\n",
      "Iteration: 390/750.\n",
      "Iteration: 391/750.\n",
      "Iteration: 392/750.\n",
      "Iteration: 393/750.\n",
      "Iteration: 394/750.\n",
      "Iteration: 395/750.\n",
      "Iteration: 396/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2432384498691653, 2.247009227711138, 2.3254272749122347]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 397/750.\n",
      "Iteration: 398/750.\n",
      "Iteration: 399/750.\n",
      "Iteration: 400/750.\n",
      "Iteration: 401/750.\n",
      "Iteration: 402/750.\n",
      "Iteration: 403/750.\n",
      "Iteration: 404/750.\n",
      "Iteration: 405/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.241971823851108, 2.1875700843869, 2.158207424004244]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 406/750.\n",
      "Iteration: 407/750.\n",
      "Iteration: 408/750.\n",
      "Iteration: 409/750.\n",
      "Iteration: 410/750.\n",
      "Iteration: 411/750.\n",
      "Iteration: 412/750.\n",
      "Iteration: 413/750.\n",
      "Iteration: 414/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2484347859408387, 2.3060932894119572, 2.2624999998559]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 415/750.\n",
      "Iteration: 416/750.\n",
      "Iteration: 417/750.\n",
      "Iteration: 418/750.\n",
      "Iteration: 419/750.\n",
      "Iteration: 420/750.\n",
      "Iteration: 421/750.\n",
      "Iteration: 422/750.\n",
      "Iteration: 423/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1886142866691873, 2.128478050810751, 2.311385551409236]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 424/750.\n",
      "Iteration: 425/750.\n",
      "Iteration: 426/750.\n",
      "Iteration: 427/750.\n",
      "Iteration: 428/750.\n",
      "Iteration: 429/750.\n",
      "Iteration: 430/750.\n",
      "Iteration: 431/750.\n",
      "Iteration: 432/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.194539654885159, 2.2900531022302424, 2.2121713336740445]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 433/750.\n",
      "Iteration: 434/750.\n",
      "Iteration: 435/750.\n",
      "Iteration: 436/750.\n",
      "Iteration: 437/750.\n",
      "Iteration: 438/750.\n",
      "Iteration: 439/750.\n",
      "Iteration: 440/750.\n",
      "Iteration: 441/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.134445198940538, 2.2699183887282217, 2.1507394401438193]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 442/750.\n",
      "Iteration: 443/750.\n",
      "Iteration: 444/750.\n",
      "Iteration: 445/750.\n",
      "Iteration: 446/750.\n",
      "Iteration: 447/750.\n",
      "Iteration: 448/750.\n",
      "Iteration: 449/750.\n",
      "Iteration: 450/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1917973532475976, 2.2950102348914974, 2.100521391053965]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 451/750.\n",
      "Iteration: 452/750.\n",
      "Iteration: 453/750.\n",
      "Iteration: 454/750.\n",
      "Iteration: 455/750.\n",
      "Iteration: 456/750.\n",
      "Iteration: 457/750.\n",
      "Iteration: 458/750.\n",
      "Iteration: 459/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.208949986017907, 2.302902253085499, 2.2244442228505825]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 460/750.\n",
      "Iteration: 461/750.\n",
      "Iteration: 462/750.\n",
      "Iteration: 463/750.\n",
      "Iteration: 464/750.\n",
      "Iteration: 465/750.\n",
      "Iteration: 466/750.\n",
      "Iteration: 467/750.\n",
      "Iteration: 468/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.311667349891113, 2.2451032878593913, 2.114113198046391]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 469/750.\n",
      "Iteration: 470/750.\n",
      "Iteration: 471/750.\n",
      "Iteration: 472/750.\n",
      "Iteration: 473/750.\n",
      "Iteration: 474/750.\n",
      "Iteration: 475/750.\n",
      "Iteration: 476/750.\n",
      "Iteration: 477/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.362584979184528, 2.1842563367266337, 2.2175075899941072]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 478/750.\n",
      "Iteration: 479/750.\n",
      "Iteration: 480/750.\n",
      "Iteration: 481/750.\n",
      "Iteration: 482/750.\n",
      "Iteration: 483/750.\n",
      "Iteration: 484/750.\n",
      "Iteration: 485/750.\n",
      "Iteration: 486/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.274318562637799, 2.065706641955925, 2.1479803585598147]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 487/750.\n",
      "Iteration: 488/750.\n",
      "Iteration: 489/750.\n",
      "Iteration: 490/750.\n",
      "Iteration: 491/750.\n",
      "Iteration: 492/750.\n",
      "Iteration: 493/750.\n",
      "Iteration: 494/750.\n",
      "Iteration: 495/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2761224118898045, 2.1852569399565933, 2.208590116865381]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 496/750.\n",
      "Iteration: 497/750.\n",
      "Iteration: 498/750.\n",
      "Iteration: 499/750.\n",
      "Iteration: 500/750.\n",
      "Iteration: 501/750.\n",
      "Iteration: 502/750.\n",
      "Iteration: 503/750.\n",
      "Iteration: 504/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.116283809246591, 2.2028335737626565, 2.2326783799474863]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 505/750.\n",
      "Iteration: 506/750.\n",
      "Iteration: 507/750.\n",
      "Iteration: 508/750.\n",
      "Iteration: 509/750.\n",
      "Iteration: 510/750.\n",
      "Iteration: 511/750.\n",
      "Iteration: 512/750.\n",
      "Iteration: 513/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1968076409861594, 2.274975264968471, 1.9855868075389775]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 514/750.\n",
      "Iteration: 515/750.\n",
      "Iteration: 516/750.\n",
      "Iteration: 517/750.\n",
      "Iteration: 518/750.\n",
      "Iteration: 519/750.\n",
      "Iteration: 520/750.\n",
      "Iteration: 521/750.\n",
      "Iteration: 522/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1953933392272265, 2.215828703238852, 2.3100137441845474]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 523/750.\n",
      "Iteration: 524/750.\n",
      "Iteration: 525/750.\n",
      "Iteration: 526/750.\n",
      "Iteration: 527/750.\n",
      "Iteration: 528/750.\n",
      "Iteration: 529/750.\n",
      "Iteration: 530/750.\n",
      "Iteration: 531/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.146009311380914, 2.2716558721394833, 2.2301757314982242]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 532/750.\n",
      "Iteration: 533/750.\n",
      "Iteration: 534/750.\n",
      "Iteration: 535/750.\n",
      "Iteration: 536/750.\n",
      "Iteration: 537/750.\n",
      "Iteration: 538/750.\n",
      "Iteration: 539/750.\n",
      "Iteration: 540/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.332063398600958, 2.28230492085212, 1.9736515415982288]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 541/750.\n",
      "Iteration: 542/750.\n",
      "Iteration: 543/750.\n",
      "Iteration: 544/750.\n",
      "Iteration: 545/750.\n",
      "Iteration: 546/750.\n",
      "Iteration: 547/750.\n",
      "Iteration: 548/750.\n",
      "Iteration: 549/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1829420813707525, 2.2437436638194823, 2.286485280603498]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 550/750.\n",
      "Iteration: 551/750.\n",
      "Iteration: 552/750.\n",
      "Iteration: 553/750.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 554/750.\n",
      "Iteration: 555/750.\n",
      "Iteration: 556/750.\n",
      "Iteration: 557/750.\n",
      "Iteration: 558/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1614771451312933, 2.3313536288551155, 2.166155356616962]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 559/750.\n",
      "Iteration: 560/750.\n",
      "Iteration: 561/750.\n",
      "Iteration: 562/750.\n",
      "Iteration: 563/750.\n",
      "Iteration: 564/750.\n",
      "Iteration: 565/750.\n",
      "Iteration: 566/750.\n",
      "Iteration: 567/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1527029769325354, 2.256560466136464, 2.298755900566209]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 568/750.\n",
      "Iteration: 569/750.\n",
      "Iteration: 570/750.\n",
      "Iteration: 571/750.\n",
      "Iteration: 572/750.\n",
      "Iteration: 573/750.\n",
      "Iteration: 574/750.\n",
      "Iteration: 575/750.\n",
      "Iteration: 576/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3348348069425224, 2.2370508707565415, 2.1151456857666564]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 577/750.\n",
      "Iteration: 578/750.\n",
      "Iteration: 579/750.\n",
      "Iteration: 580/750.\n",
      "Iteration: 581/750.\n",
      "Iteration: 582/750.\n",
      "Iteration: 583/750.\n",
      "Iteration: 584/750.\n",
      "Iteration: 585/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.250810597110293, 2.293818748703291, 2.020107784339772]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 586/750.\n",
      "Iteration: 587/750.\n",
      "Iteration: 588/750.\n",
      "Iteration: 589/750.\n",
      "Iteration: 590/750.\n",
      "Iteration: 591/750.\n",
      "Iteration: 592/750.\n",
      "Iteration: 593/750.\n",
      "Iteration: 594/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.206940417505602, 2.2418594893978243, 2.435203323547574]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 595/750.\n",
      "Iteration: 596/750.\n",
      "Iteration: 597/750.\n",
      "Iteration: 598/750.\n",
      "Iteration: 599/750.\n",
      "Iteration: 600/750.\n",
      "Iteration: 601/750.\n",
      "Iteration: 602/750.\n",
      "Iteration: 603/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2084709801104907, 2.303666300630048, 2.2445135191978163]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 604/750.\n",
      "Iteration: 605/750.\n",
      "Iteration: 606/750.\n",
      "Iteration: 607/750.\n",
      "Iteration: 608/750.\n",
      "Iteration: 609/750.\n",
      "Iteration: 610/750.\n",
      "Iteration: 611/750.\n",
      "Iteration: 612/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1552491626113706, 2.1625910894636866, 2.2808288405473975]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 613/750.\n",
      "Iteration: 614/750.\n",
      "Iteration: 615/750.\n",
      "Iteration: 616/750.\n",
      "Iteration: 617/750.\n",
      "Iteration: 618/750.\n",
      "Iteration: 619/750.\n",
      "Iteration: 620/750.\n",
      "Iteration: 621/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.221663604770886, 2.1665319540090957, 2.1961497029878596]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 622/750.\n",
      "Iteration: 623/750.\n",
      "Iteration: 624/750.\n",
      "Iteration: 625/750.\n",
      "Iteration: 626/750.\n",
      "Iteration: 627/750.\n",
      "Iteration: 628/750.\n",
      "Iteration: 629/750.\n",
      "Iteration: 630/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2191093408749887, 2.396593898677007, 2.171257568036445]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 631/750.\n",
      "Iteration: 632/750.\n",
      "Iteration: 633/750.\n",
      "Iteration: 634/750.\n",
      "Iteration: 635/750.\n",
      "Iteration: 636/750.\n",
      "Iteration: 637/750.\n",
      "Iteration: 638/750.\n",
      "Iteration: 639/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2136993033344794, 2.1609558943243043, 2.252445795017344]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 640/750.\n",
      "Iteration: 641/750.\n",
      "Iteration: 642/750.\n",
      "Iteration: 643/750.\n",
      "Iteration: 644/750.\n",
      "Iteration: 645/750.\n",
      "Iteration: 646/750.\n",
      "Iteration: 647/750.\n",
      "Iteration: 648/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.270517546564173, 2.077304121289031, 2.317812946959583]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 649/750.\n",
      "Iteration: 650/750.\n",
      "Iteration: 651/750.\n",
      "Iteration: 652/750.\n",
      "Iteration: 653/750.\n",
      "Iteration: 654/750.\n",
      "Iteration: 655/750.\n",
      "Iteration: 656/750.\n",
      "Iteration: 657/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.156857904148581, 2.1572597766780865, 2.3406381410100545]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 658/750.\n",
      "Iteration: 659/750.\n",
      "Iteration: 660/750.\n",
      "Iteration: 661/750.\n",
      "Iteration: 662/750.\n",
      "Iteration: 663/750.\n",
      "Iteration: 664/750.\n",
      "Iteration: 665/750.\n",
      "Iteration: 666/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3015316106955814, 2.286232444163045, 2.162189343749455]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 667/750.\n",
      "Iteration: 668/750.\n",
      "Iteration: 669/750.\n",
      "Iteration: 670/750.\n",
      "Iteration: 671/750.\n",
      "Iteration: 672/750.\n",
      "Iteration: 673/750.\n",
      "Iteration: 674/750.\n",
      "Iteration: 675/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2639214328186092, 2.2495302165354953, 2.3265115638659126]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 676/750.\n",
      "Iteration: 677/750.\n",
      "Iteration: 678/750.\n",
      "Iteration: 679/750.\n",
      "Iteration: 680/750.\n",
      "Iteration: 681/750.\n",
      "Iteration: 682/750.\n",
      "Iteration: 683/750.\n",
      "Iteration: 684/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.05322340943015, 2.2531749255380404, 2.166140649838978]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 685/750.\n",
      "Iteration: 686/750.\n",
      "Iteration: 687/750.\n",
      "Iteration: 688/750.\n",
      "Iteration: 689/750.\n",
      "Iteration: 690/750.\n",
      "Iteration: 691/750.\n",
      "Iteration: 692/750.\n",
      "Iteration: 693/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1525457102214616, 2.2363613751447224, 2.1949898306646323]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 694/750.\n",
      "Iteration: 695/750.\n",
      "Iteration: 696/750.\n",
      "Iteration: 697/750.\n",
      "Iteration: 698/750.\n",
      "Iteration: 699/750.\n",
      "Iteration: 700/750.\n",
      "Iteration: 701/750.\n",
      "Iteration: 702/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2611339353388433, 2.3471530584952562, 2.148841912322166]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 703/750.\n",
      "Iteration: 704/750.\n",
      "Iteration: 705/750.\n",
      "Iteration: 706/750.\n",
      "Iteration: 707/750.\n",
      "Iteration: 708/750.\n",
      "Iteration: 709/750.\n",
      "Iteration: 710/750.\n",
      "Iteration: 711/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.279490786046512, 2.3235368702024597, 2.1051195390993214]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 712/750.\n",
      "Iteration: 713/750.\n",
      "Iteration: 714/750.\n",
      "Iteration: 715/750.\n",
      "Iteration: 716/750.\n",
      "Iteration: 717/750.\n",
      "Iteration: 718/750.\n",
      "Iteration: 719/750.\n",
      "Iteration: 720/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2942693326887547, 2.2811890204616265, 2.1792211232871224]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 721/750.\n",
      "Iteration: 722/750.\n",
      "Iteration: 723/750.\n",
      "Iteration: 724/750.\n",
      "Iteration: 725/750.\n",
      "Iteration: 726/750.\n",
      "Iteration: 727/750.\n",
      "Iteration: 728/750.\n",
      "Iteration: 729/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.004881074775065, 2.165659208624099, 2.3316752411036696]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 730/750.\n",
      "Iteration: 731/750.\n",
      "Iteration: 732/750.\n",
      "Iteration: 733/750.\n",
      "Iteration: 734/750.\n",
      "Iteration: 735/750.\n",
      "Iteration: 736/750.\n",
      "Iteration: 737/750.\n",
      "Iteration: 738/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.254104787989307, 2.237855966679909, 2.157645520877249]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 739/750.\n",
      "Iteration: 740/750.\n",
      "Iteration: 741/750.\n",
      "Iteration: 742/750.\n",
      "Iteration: 743/750.\n",
      "Iteration: 744/750.\n",
      "Iteration: 745/750.\n",
      "Iteration: 746/750.\n",
      "Iteration: 747/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3353460865512017, 2.3626829171958854, 2.1615144716245003]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 748/750.\n",
      "Iteration: 749/750.\n",
      "Iteration: 750/750.\n",
      "\n",
      "\n",
      "----------------FINAL OUTPUT----------------\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "Loss history: [2.309333752799659, 2.3100475210286935, 2.302734681556495, 2.303403251768486, 2.3008919111426778, 2.3030688070130423, 2.3083244218204695, 2.305725695413475, 2.297431716772699, 2.2971185583164577, 2.295047037977388, 2.3075030230866163, 2.2950493262693032, 2.302557765275875, 2.302703579304868, 2.3018450824131884, 2.3036695764710173, 2.3017066047698584, 2.3026332858630787, 2.3012372917903128, 2.3057936665043886, 2.300111243151785, 2.304657471487298, 2.294923033034066, 2.296826694035708, 2.2988054741544746, 2.298301305372676, 2.308902546197689, 2.293720393659111, 2.3003195881098004, 2.289276288094972, 2.3027816559633787, 2.2924948880619533, 2.2966168260260056, 2.299502510069381, 2.297408434557621, 2.2952866492919974, 2.2881488785226263, 2.303345064379366, 2.2877893250629198, 2.3013919178983135, 2.2975664705523684, 2.298535173218746, 2.2969774199484214, 2.287743382443448, 2.280444956983327, 2.2919499877728744, 2.298898649446118, 2.3153907594305907, 2.3021109911026945, 2.3012461177238124, 2.2839536623842185, 2.2938137555028617, 2.302438358369893, 2.2965300061329255, 2.294813767132136, 2.280370548936137, 2.2904263157753526, 2.286187019459692, 2.2942040775522634, 2.3025548231332373, 2.2845544381527216, 2.281798459737148, 2.297068291329227, 2.2617424650288287, 2.2809299499204694, 2.2538198629755417, 2.271909689382298, 2.2776021913092346, 2.2835935731873747, 2.2789792517783667, 2.2761709594171795, 2.3143959334197306, 2.3177621604066103, 2.2884189660660303, 2.311392910466611, 2.275393185958308, 2.286765087652806, 2.274863026115056, 2.294948740664123, 2.2861885214757716, 2.2871390892288956, 2.2743610652958326, 2.263448886273185, 2.289666590899042, 2.2975086874459687, 2.2790189825112743, 2.2580556523144146, 2.2770504059269747, 2.2811090608307416, 2.2757719949998716, 2.2933836279835105, 2.290496385895961, 2.2614085040767975, 2.28383061215381, 2.277935653130013, 2.2767012191425837, 2.261673609481268, 2.272762194269741, 2.2714647010266664, 2.290487473501098, 2.2658902911532883, 2.2773112089049175, 2.2770148055491357, 2.2772368045511544, 2.2664487825242845, 2.286639542318909, 2.247916495356978, 2.305542273864441, 2.2512646507169696, 2.2485440855344465, 2.272688105860993, 2.260213811437478, 2.2630233761375496, 2.2729363999356873, 2.273953395587147, 2.2786906412783487, 2.3192567603996523, 2.281512004316321, 2.262797660909111, 2.2803547558436756, 2.3100277732334074, 2.2617148656766113, 2.2537882165991663, 2.278439164214905, 2.2486419643059565, 2.289976849054879, 2.2591118473935636, 2.246622518184403, 2.2485539756996356, 2.2714022353054437, 2.270946773985217, 2.32051304490723, 2.3152721759448656, 2.2505928687197856, 2.271279483717185, 2.2719340757255693, 2.2735338698687357, 2.2513331669232333, 2.258728522823861, 2.31674771521857, 2.2918283532741106, 2.230395258419411, 2.2907653948807876, 2.3037061892965354, 2.28888440081966, 2.277688608511672, 2.288715704807792, 2.2466543833451955, 2.2653392763452334, 2.250193863582836, 2.2973584825330042, 2.268586391634377, 2.222452286381358, 2.3213260910671365, 2.2621099588300413, 2.2481418825953106, 2.261055779220247, 2.269432519652733, 2.2824466859916486, 2.244650987574829, 2.2543351594952203, 2.292507476194037, 2.2723139494422697, 2.2869218409553285, 2.279710771930238, 2.266694714661016, 2.2587020252489243, 2.232717186500342, 2.2397897020179474, 2.2738842139177944, 2.3130731927395285, 2.2755739239371615, 2.277945426675915, 2.243454052034584, 2.2645562990858603, 2.2133727421426292, 2.2679793280689706, 2.28689228759865, 2.276174764319612, 2.2236739566899186, 2.298363742985818, 2.2729309430404605, 2.2563693135874368, 2.240465637311098, 2.2955739515634828, 2.2809092996558507, 2.2789688282647362, 2.28805322376837, 2.3166855884055906, 2.2527713544474053, 2.2731325887768667, 2.2651127106971516, 2.2173477328947846, 2.2098539807486257, 2.2551803876097742, 2.268825191182065, 2.2279083463692317, 2.301795306412694, 2.276189958551281, 2.3712431846473616, 2.2868328038979064, 2.225494902506178, 2.2219202034810803, 2.308294898786341, 2.2447185877161426, 2.2561030174177947, 2.2817544299604884, 2.3094376965185885, 2.3225717067902347, 2.2474790805137927, 2.226260243316582, 2.2742641512258435, 2.274031923422989, 2.292507221173354, 2.2214233061154625, 2.3038294789458624, 2.2006161740230277, 2.3114010439995507, 2.2498249385676092, 2.2499307261201498, 2.26822350700755, 2.3173336547347247, 2.267883190300659, 2.216650036147166, 2.242851204365984, 2.308774772880274, 2.208584330916785, 2.251177796565325, 2.2929429254504283, 2.2742531109980155, 2.3015755984933626, 2.238681765961866, 2.2016660365529783, 2.2414976699522264, 2.2339527682828093, 2.2419668631047154, 2.22561029152744, 2.2696078286368095, 2.289551143034933, 2.3058068408717873, 2.2569811975613874, 2.269927435171348, 2.26324520377211, 2.1625316569152084, 2.2589574526575413, 2.2686326588972605, 2.277235929152427, 2.316094084563435, 2.2473843431840854, 2.1699650093380507, 2.330035628675186, 2.236761831921097, 2.2167136729817942, 2.1725469404099202, 2.3115513746023284, 2.2496455469082437, 2.2959816020938013, 2.271419112296858, 2.201561180448635, 2.2923443997167285, 2.2735742312378213, 2.25518484706304, 2.2075910110023926, 2.245772882471312, 2.2168554586432294, 2.1799703079039072, 2.3423970038143236, 2.2769262981458995, 2.3062019756584777, 2.179473352098399, 2.3465312122995425, 2.2788156964679245, 2.2823152490006624, 2.291660751892977, 2.273853471694125, 2.2407741533860976, 2.172899416933032, 2.1984333670970226, 2.178768841035081, 2.260190263650482, 2.194242320129877, 2.301754317479586, 2.1379310461165866, 2.1671659270788104, 2.2981574744501674, 2.2600462418147753, 2.2166681453444483, 2.3116682133374518, 2.27308235347993, 2.315415519813724, 2.2454148717035594, 2.2420022123738086, 2.2460575891304244, 2.2155338896642607, 2.197460097265643, 2.318388907515723, 2.3021859078807085, 2.286161200886601, 2.339669902771893, 2.197040245797655, 2.282705282487249, 2.1695848944874228, 2.183530894514179, 2.1687370910024586, 2.280636857520292, 2.176898355230328, 2.1658383758779904, 2.2287497642505523, 2.184245077130056, 2.254430533214847, 2.296480705564703, 2.3083214739652798, 2.2727792010817764, 2.2652212535838214, 2.3004397483350063, 2.2647615090690967, 2.1270535971968676, 2.234923844557901, 2.3556544580008727, 2.320327028296227, 2.2743787708138794, 2.205430793999642, 2.216025726528513, 2.2339827155221434, 2.1774274007415815, 2.2233050871556332, 2.2044080679554936, 2.2527007936336836, 2.2318030781141664, 2.2523932586141244, 2.2818711031090086, 2.2729374546977894, 2.2574072904957827, 2.3021979157163655, 2.2759473621417485, 2.204111729271655, 2.189252558882519, 2.237886234675441, 2.3870551622869107, 2.227611855833517, 2.2965227388282172, 2.174574405759914, 2.2223644710891475, 2.39034188002874, 2.1858808523300532, 2.296386752140095, 2.2378239480935656, 2.1972767659635544, 2.1985326184695064, 2.2349970739214924, 2.1900170750618506, 2.1709146497469116, 2.1477515443108333, 2.179970896350563, 2.24916708009886, 2.2343122848263763, 2.1742901250718467, 2.0799756410980557, 2.2220841861419194, 2.247956174372676, 2.1803141868710143, 2.1836113036816145, 2.1633272251408706, 2.225359362065844, 2.2165446854092736, 2.1819677069929813, 2.2273059778822533, 2.1947998105505557, 2.2708157561509386, 2.157979293178159, 2.174449023672463, 2.139562600685668, 2.235704364427927, 2.156121229015864, 2.24355648076423, 2.176654193386759, 2.343633754517829, 2.292389334541857, 2.276347535490325, 2.211671708181607, 2.2094181486726217, 2.185462330048158, 2.205960619547817, 2.258344127286521, 2.287439493420776, 2.2160262565649242, 2.203038506014433, 2.171591728644529, 2.251956833505868, 2.2446382834380154, 2.3595602557073363, 2.2099139511800328, 2.2432384498691653, 2.247009227711138, 2.3254272749122347, 2.301744621986177, 2.2252482918131613, 2.2291867398624254, 2.113158742224389, 2.189558115279206, 2.2351086865998013, 2.241971823851108, 2.1875700843869, 2.158207424004244, 2.236749938926907, 2.226051608823242, 2.2771237789620407, 2.27284293806041, 2.2980481552943437, 2.176719451395235, 2.2484347859408387, 2.3060932894119572, 2.2624999998559, 2.1618495127116626, 2.3237064027617147, 2.240239675948818, 2.2147104791774455, 2.2982272673545237, 2.3358278030850026, 2.1886142866691873, 2.128478050810751, 2.311385551409236, 2.2407913353908095, 2.2632245758216527, 2.2672079876550835, 2.252928202675109, 2.220774177698773, 2.2748848137560786, 2.194539654885159, 2.2900531022302424, 2.2121713336740445, 2.0913593308298855, 2.2703914639860456, 2.1523936948973397, 2.2416866824376345, 2.21617490475356, 2.246419970940201, 2.134445198940538, 2.2699183887282217, 2.1507394401438193, 2.261806802927795, 2.2308286221768205, 2.281423296231043, 2.31822669922193, 2.144160823199504, 2.1513400802171367, 2.1917973532475976, 2.2950102348914974, 2.100521391053965, 2.2267112330326637, 2.1420273375967542, 2.239265776719974, 2.240059615437333, 2.1599768407343682, 2.2681748784584044, 2.208949986017907, 2.302902253085499, 2.2244442228505825, 2.263264839591967, 2.1429665401663955, 2.1882899043981214, 2.246000540413417, 2.334004475609229, 2.2200255863545557, 2.311667349891113, 2.2451032878593913, 2.114113198046391, 2.1379800774131756, 2.220794836257581, 2.254915990121502, 2.1857252690471274, 2.1547434933653054, 2.3005650631533237, 2.362584979184528, 2.1842563367266337, 2.2175075899941072, 2.3064659369084306, 2.2739830313540934, 2.2070774735280523, 2.263569936190832, 2.257398307509753, 2.2027435879602897, 2.274318562637799, 2.065706641955925, 2.1479803585598147, 2.116617104891958, 2.190731908778447, 2.1471997685634467, 2.072579382669971, 2.3231734245159856, 2.23225608264937, 2.2761224118898045, 2.1852569399565933, 2.208590116865381, 2.2565222860804144, 2.0983197982911714, 2.3377358893142426, 2.182478165500148, 2.1482805444065542, 2.31464717537705, 2.116283809246591, 2.2028335737626565, 2.2326783799474863, 2.265263127103594, 2.2437697487925976, 2.167442523067767, 2.2645192117642092, 2.216527928137511, 2.187732284931041, 2.1968076409861594, 2.274975264968471, 1.9855868075389775, 2.2597717458141573, 2.2821159178218937, 2.198963079030357, 2.1891945104577792, 2.190512050266871, 2.244300651790686, 2.1953933392272265, 2.215828703238852, 2.3100137441845474, 2.3159477531534285, 2.266643430772907, 2.3032593686739, 2.394652653544321, 2.254032433202731, 2.1311157260927445, 2.146009311380914, 2.2716558721394833, 2.2301757314982242, 2.2678814937265264, 2.213838422499859, 2.1760596121838343, 2.273506105418976, 2.251507362888061, 2.34512189411497, 2.332063398600958, 2.28230492085212, 1.9736515415982288, 2.1189227626523808, 2.114206787754031, 2.151698611244652, 2.2705814786836984, 2.177990638800233, 2.149726026830303, 2.1829420813707525, 2.2437436638194823, 2.286485280603498, 2.24980885955483, 2.1938046741035966, 2.166350240360741, 2.2258846532623258, 2.34285458114567, 2.1929663692256596, 2.1614771451312933, 2.3313536288551155, 2.166155356616962, 2.307011222268517, 2.205694221413372, 2.2193724314442504, 2.2778575348161674, 2.179588781404833, 2.0986308429095795, 2.1527029769325354, 2.256560466136464, 2.298755900566209, 2.1344338590413137, 2.2271608822114453, 2.284828067580046, 2.2062007773753085, 2.2073675067406073, 2.2657247281164135, 2.3348348069425224, 2.2370508707565415, 2.1151456857666564, 2.2414551117609363, 2.242918937566582, 2.0946004292917, 2.011953260774737, 2.3388613365973963, 2.2385748500313736, 2.250810597110293, 2.293818748703291, 2.020107784339772, 2.28195275115603, 2.255223923374266, 2.14869507768359, 2.2096173142820246, 2.12365362130284, 2.165925894978741, 2.206940417505602, 2.2418594893978243, 2.435203323547574, 2.20321458119135, 2.070248470128996, 2.2187936744119288, 2.161224616410725, 2.10497447919098, 2.3018433928622675, 2.2084709801104907, 2.303666300630048, 2.2445135191978163, 2.14649496794392, 2.0165073561594746, 2.2256396388575146, 2.169012236912601, 2.2624673734805483, 2.2824335920666723, 2.1552491626113706, 2.1625910894636866, 2.2808288405473975, 2.2439545751133325, 2.2382588261264305, 2.0898953426194247, 2.281819890727656, 2.1287797113418496, 2.052971964456318, 2.221663604770886, 2.1665319540090957, 2.1961497029878596, 2.243790478326919, 2.293736071871257, 2.0943813936038187, 2.1252794685292478, 2.196465547035758, 2.134140944633871, 2.2191093408749887, 2.396593898677007, 2.171257568036445, 2.155883916476155, 2.1104135783744593, 2.0524650095148735, 2.2488262598015436, 2.0561069788442414, 2.1472249905974543, 2.2136993033344794, 2.1609558943243043, 2.252445795017344, 2.2339208167457225, 2.2423719595775986, 2.1862903605124084, 2.2134832077203015, 2.174377667422002, 2.1115971157864943, 2.270517546564173, 2.077304121289031, 2.317812946959583, 2.268672642916388, 2.3044974800034406, 2.239222797193361, 2.0344313210390585, 2.2635853161527493, 2.328787323928576, 2.156857904148581, 2.1572597766780865, 2.3406381410100545, 2.2613898090021998, 2.220838742755532, 2.127641741219276, 2.2885573407925426, 2.3954004390646886, 2.1131574528161963, 2.3015316106955814, 2.286232444163045, 2.162189343749455, 2.2110244404878454, 2.282304150695663, 2.2319069197630688, 2.0955823281878074, 2.2538220790481094, 2.309866398808077, 2.2639214328186092, 2.2495302165354953, 2.3265115638659126, 2.2928396386392604, 2.1773725694758985, 2.2069998234792862, 2.230035014498944, 2.2812789349642024, 2.2869453871015626, 2.05322340943015, 2.2531749255380404, 2.166140649838978, 2.160660281549361, 2.129177684204914, 2.1192023458260607, 2.2709323561132626, 2.1739322421048892, 2.3058492710722223, 2.1525457102214616, 2.2363613751447224, 2.1949898306646323, 2.2208228374884667, 2.1805919721518943, 2.2467683982408158, 2.01945280802785, 2.324196217749735, 2.219503405308274, 2.2611339353388433, 2.3471530584952562, 2.148841912322166, 2.2105145700352185, 2.10299896001295, 2.2452306411883876, 2.2659922834657737, 2.0824698025658726, 2.296669271511598, 2.279490786046512, 2.3235368702024597, 2.1051195390993214, 2.1919482176571354, 2.3903194487425594, 2.274765857341968, 2.319324972765244, 2.1576878470952674, 2.2717888821075, 2.2942693326887547, 2.2811890204616265, 2.1792211232871224, 2.345779364711508, 2.0234519079395117, 2.1855523912559702, 2.241816207815938, 2.352432464466387, 2.2271404793424554, 2.004881074775065, 2.165659208624099, 2.3316752411036696, 2.1963508768128817, 2.0938752581510442, 2.37773310826539, 2.224772979726787, 2.2509349442974904, 2.10176376249588, 2.254104787989307, 2.237855966679909, 2.157645520877249, 2.090897439914133, 2.0999701055851396, 2.1815854494495666, 2.0745644609802456, 2.21998773174819, 2.0882375961335344, 2.3353460865512017, 2.3626829171958854, 2.1615144716245003, 2.0949775769564627, 2.1786421785573125, 2.2810184967981635]\n",
      "Accuracy history: [0.16, 0.16, 0.16, 0.16, 0.16, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22]\n"
     ]
    }
   ],
   "source": [
    "# SGD-M\n",
    "sgd_m = ConvNet4(input_shape=input_shape, wt_scale=wt_scale, verbose=False)\n",
    "sgd_m.compile('sgd_momentum')\n",
    "sgd_m.fit(x_dev, y_dev, x_val, y_val, mini_batch_sz=mini_batch_sz, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train...\n",
      "750 iterations. 5 iter/epoch.\n",
      "Iteration: 1/750.\n",
      "Time taken for iteration 0: 1.6547000408172607\n",
      "Estimated time to complete: 1241.0250306129456\n",
      "Iteration: 2/750.\n",
      "Iteration: 3/750.\n",
      "Iteration: 4/750.\n",
      "Iteration: 5/750.\n",
      "Iteration: 6/750.\n",
      "Iteration: 7/750.\n",
      "Iteration: 8/750.\n",
      "Iteration: 9/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.2667797754626706, 2.275256568294465, 2.2556253335188514]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 10/750.\n",
      "Iteration: 11/750.\n",
      "Iteration: 12/750.\n",
      "Iteration: 13/750.\n",
      "Iteration: 14/750.\n",
      "Iteration: 15/750.\n",
      "Iteration: 16/750.\n",
      "Iteration: 17/750.\n",
      "Iteration: 18/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.2727310367417854, 2.204103899788421, 2.258343837284645]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 19/750.\n",
      "Iteration: 20/750.\n",
      "Iteration: 21/750.\n",
      "Iteration: 22/750.\n",
      "Iteration: 23/750.\n",
      "Iteration: 24/750.\n",
      "Iteration: 25/750.\n",
      "Iteration: 26/750.\n",
      "Iteration: 27/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.1944675031511447, 2.2390716805564996, 2.281966984895512]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 28/750.\n",
      "Iteration: 29/750.\n",
      "Iteration: 30/750.\n",
      "Iteration: 31/750.\n",
      "Iteration: 32/750.\n",
      "Iteration: 33/750.\n",
      "Iteration: 34/750.\n",
      "Iteration: 35/750.\n",
      "Iteration: 36/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.2104394749579153, 2.186158200387568, 2.314671231617274]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 37/750.\n",
      "Iteration: 38/750.\n",
      "Iteration: 39/750.\n",
      "Iteration: 40/750.\n",
      "Iteration: 41/750.\n",
      "Iteration: 42/750.\n",
      "Iteration: 43/750.\n",
      "Iteration: 44/750.\n",
      "Iteration: 45/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.262181205808585, 2.1475236738810057, 2.1927888638495125]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 46/750.\n",
      "Iteration: 47/750.\n",
      "Iteration: 48/750.\n",
      "Iteration: 49/750.\n",
      "Iteration: 50/750.\n",
      "Iteration: 51/750.\n",
      "Iteration: 52/750.\n",
      "Iteration: 53/750.\n",
      "Iteration: 54/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.1454005350969774, 2.3007085399330136, 2.254623388762617]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 55/750.\n",
      "Iteration: 56/750.\n",
      "Iteration: 57/750.\n",
      "Iteration: 58/750.\n",
      "Iteration: 59/750.\n",
      "Iteration: 60/750.\n",
      "Iteration: 61/750.\n",
      "Iteration: 62/750.\n",
      "Iteration: 63/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.2224804656011212, 2.132319431106546, 2.160046661484646]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 64/750.\n",
      "Iteration: 65/750.\n",
      "Iteration: 66/750.\n",
      "Iteration: 67/750.\n",
      "Iteration: 68/750.\n",
      "Iteration: 69/750.\n",
      "Iteration: 70/750.\n",
      "Iteration: 71/750.\n",
      "Iteration: 72/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.062876644831836, 2.1102064822282625, 2.342895680648545]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 73/750.\n",
      "Iteration: 74/750.\n",
      "Iteration: 75/750.\n",
      "Iteration: 76/750.\n",
      "Iteration: 77/750.\n",
      "Iteration: 78/750.\n",
      "Iteration: 79/750.\n",
      "Iteration: 80/750.\n",
      "Iteration: 81/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.245438425118237, 2.1422264296376468, 2.328747695698904]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 82/750.\n",
      "Iteration: 83/750.\n",
      "Iteration: 84/750.\n",
      "Iteration: 85/750.\n",
      "Iteration: 86/750.\n",
      "Iteration: 87/750.\n",
      "Iteration: 88/750.\n",
      "Iteration: 89/750.\n",
      "Iteration: 90/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.2147544396521504, 2.182436801416794, 2.3272995062125292]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 91/750.\n",
      "Iteration: 92/750.\n",
      "Iteration: 93/750.\n",
      "Iteration: 94/750.\n",
      "Iteration: 95/750.\n",
      "Iteration: 96/750.\n",
      "Iteration: 97/750.\n",
      "Iteration: 98/750.\n",
      "Iteration: 99/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.141941328348959, 2.112567311006279, 2.2184563897284577]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 100/750.\n",
      "Iteration: 101/750.\n",
      "Iteration: 102/750.\n",
      "Iteration: 103/750.\n",
      "Iteration: 104/750.\n",
      "Iteration: 105/750.\n",
      "Iteration: 106/750.\n",
      "Iteration: 107/750.\n",
      "Iteration: 108/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.280583389083929, 2.0969955815327004, 1.8145079639294601]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 109/750.\n",
      "Iteration: 110/750.\n",
      "Iteration: 111/750.\n",
      "Iteration: 112/750.\n",
      "Iteration: 113/750.\n",
      "Iteration: 114/750.\n",
      "Iteration: 115/750.\n",
      "Iteration: 116/750.\n",
      "Iteration: 117/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.9084026833757428, 2.1167140453380213, 2.3740182895421293]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 118/750.\n",
      "Iteration: 119/750.\n",
      "Iteration: 120/750.\n",
      "Iteration: 121/750.\n",
      "Iteration: 122/750.\n",
      "Iteration: 123/750.\n",
      "Iteration: 124/750.\n",
      "Iteration: 125/750.\n",
      "Iteration: 126/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.346263581167927, 1.8869907552364524, 2.461087123313006]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.24, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 127/750.\n",
      "Iteration: 128/750.\n",
      "Iteration: 129/750.\n",
      "Iteration: 130/750.\n",
      "Iteration: 131/750.\n",
      "Iteration: 132/750.\n",
      "Iteration: 133/750.\n",
      "Iteration: 134/750.\n",
      "Iteration: 135/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.810131401373492, 2.1380569646358207, 2.21698438382967]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.24, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 136/750.\n",
      "Iteration: 137/750.\n",
      "Iteration: 138/750.\n",
      "Iteration: 139/750.\n",
      "Iteration: 140/750.\n",
      "Iteration: 141/750.\n",
      "Iteration: 142/750.\n",
      "Iteration: 143/750.\n",
      "Iteration: 144/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.8719002668156919, 2.0312890920937923, 2.0585284689321237]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.28, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 145/750.\n",
      "Iteration: 146/750.\n",
      "Iteration: 147/750.\n",
      "Iteration: 148/750.\n",
      "Iteration: 149/750.\n",
      "Iteration: 150/750.\n",
      "Iteration: 151/750.\n",
      "Iteration: 152/750.\n",
      "Iteration: 153/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.7564821912496176, 1.9865067476393694, 1.943536227476239]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.32, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 154/750.\n",
      "Iteration: 155/750.\n",
      "Iteration: 156/750.\n",
      "Iteration: 157/750.\n",
      "Iteration: 158/750.\n",
      "Iteration: 159/750.\n",
      "Iteration: 160/750.\n",
      "Iteration: 161/750.\n",
      "Iteration: 162/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.8245981558917728, 1.9881851896046179, 1.9763830868347005]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.36, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 163/750.\n",
      "Iteration: 164/750.\n",
      "Iteration: 165/750.\n",
      "Iteration: 166/750.\n",
      "Iteration: 167/750.\n",
      "Iteration: 168/750.\n",
      "Iteration: 169/750.\n",
      "Iteration: 170/750.\n",
      "Iteration: 171/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.080556145782496, 1.624516182196767, 1.592757630272275]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.32, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 172/750.\n",
      "Iteration: 173/750.\n",
      "Iteration: 174/750.\n",
      "Iteration: 175/750.\n",
      "Iteration: 176/750.\n",
      "Iteration: 177/750.\n",
      "Iteration: 178/750.\n",
      "Iteration: 179/750.\n",
      "Iteration: 180/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.69090942977038, 1.9665592430540089, 1.8076948352084192]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.38, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 181/750.\n",
      "Iteration: 182/750.\n",
      "Iteration: 183/750.\n",
      "Iteration: 184/750.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 185/750.\n",
      "Iteration: 186/750.\n",
      "Iteration: 187/750.\n",
      "Iteration: 188/750.\n",
      "Iteration: 189/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.7687937906780133, 1.5499046858392924, 1.7933401927001027]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.44, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 190/750.\n",
      "Iteration: 191/750.\n",
      "Iteration: 192/750.\n",
      "Iteration: 193/750.\n",
      "Iteration: 194/750.\n",
      "Iteration: 195/750.\n",
      "Iteration: 196/750.\n",
      "Iteration: 197/750.\n",
      "Iteration: 198/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.8762247744837355, 1.8434306913313045, 1.646268527799408]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.4, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 199/750.\n",
      "Iteration: 200/750.\n",
      "Iteration: 201/750.\n",
      "Iteration: 202/750.\n",
      "Iteration: 203/750.\n",
      "Iteration: 204/750.\n",
      "Iteration: 205/750.\n",
      "Iteration: 206/750.\n",
      "Iteration: 207/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.5854351235733253, 1.6089370888972951, 2.058576125229221]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.36, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 208/750.\n",
      "Iteration: 209/750.\n",
      "Iteration: 210/750.\n",
      "Iteration: 211/750.\n",
      "Iteration: 212/750.\n",
      "Iteration: 213/750.\n",
      "Iteration: 214/750.\n",
      "Iteration: 215/750.\n",
      "Iteration: 216/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.784510600880739, 1.7362789713859994, 1.446772620741509]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.46, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 217/750.\n",
      "Iteration: 218/750.\n",
      "Iteration: 219/750.\n",
      "Iteration: 220/750.\n",
      "Iteration: 221/750.\n",
      "Iteration: 222/750.\n",
      "Iteration: 223/750.\n",
      "Iteration: 224/750.\n",
      "Iteration: 225/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.367081370829135, 1.7940175672363345, 1.0815554925170499]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.32, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 226/750.\n",
      "Iteration: 227/750.\n",
      "Iteration: 228/750.\n",
      "Iteration: 229/750.\n",
      "Iteration: 230/750.\n",
      "Iteration: 231/750.\n",
      "Iteration: 232/750.\n",
      "Iteration: 233/750.\n",
      "Iteration: 234/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.5370650534071886, 1.4994189511311506, 1.2382007588936237]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.34, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 235/750.\n",
      "Iteration: 236/750.\n",
      "Iteration: 237/750.\n",
      "Iteration: 238/750.\n",
      "Iteration: 239/750.\n",
      "Iteration: 240/750.\n",
      "Iteration: 241/750.\n",
      "Iteration: 242/750.\n",
      "Iteration: 243/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.2365533918378877, 1.4200319681257227, 1.8429169176312614]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.52, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 244/750.\n",
      "Iteration: 245/750.\n",
      "Iteration: 246/750.\n",
      "Iteration: 247/750.\n",
      "Iteration: 248/750.\n",
      "Iteration: 249/750.\n",
      "Iteration: 250/750.\n",
      "Iteration: 251/750.\n",
      "Iteration: 252/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.565128906395955, 0.9596664751092816, 1.229862366192645]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.58, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 253/750.\n",
      "Iteration: 254/750.\n",
      "Iteration: 255/750.\n",
      "Iteration: 256/750.\n",
      "Iteration: 257/750.\n",
      "Iteration: 258/750.\n",
      "Iteration: 259/750.\n",
      "Iteration: 260/750.\n",
      "Iteration: 261/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.9498037380592687, 1.0349733756213138, 1.866582896709912]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.62, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 262/750.\n",
      "Iteration: 263/750.\n",
      "Iteration: 264/750.\n",
      "Iteration: 265/750.\n",
      "Iteration: 266/750.\n",
      "Iteration: 267/750.\n",
      "Iteration: 268/750.\n",
      "Iteration: 269/750.\n",
      "Iteration: 270/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.1245383727738474, 1.0784979611253103, 1.2490428991528333]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.56, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 271/750.\n",
      "Iteration: 272/750.\n",
      "Iteration: 273/750.\n",
      "Iteration: 274/750.\n",
      "Iteration: 275/750.\n",
      "Iteration: 276/750.\n",
      "Iteration: 277/750.\n",
      "Iteration: 278/750.\n",
      "Iteration: 279/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.6562065181910253, 0.6959926880204463, 0.3588537720135193]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.62, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 280/750.\n",
      "Iteration: 281/750.\n",
      "Iteration: 282/750.\n",
      "Iteration: 283/750.\n",
      "Iteration: 284/750.\n",
      "Iteration: 285/750.\n",
      "Iteration: 286/750.\n",
      "Iteration: 287/750.\n",
      "Iteration: 288/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.7398576115325761, 0.9311622601760052, 1.2553016971047823]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.7, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 289/750.\n",
      "Iteration: 290/750.\n",
      "Iteration: 291/750.\n",
      "Iteration: 292/750.\n",
      "Iteration: 293/750.\n",
      "Iteration: 294/750.\n",
      "Iteration: 295/750.\n",
      "Iteration: 296/750.\n",
      "Iteration: 297/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.6652714303068812, 0.8960706763006234, 0.9986283064194643]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.7, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 298/750.\n",
      "Iteration: 299/750.\n",
      "Iteration: 300/750.\n",
      "Iteration: 301/750.\n",
      "Iteration: 302/750.\n",
      "Iteration: 303/750.\n",
      "Iteration: 304/750.\n",
      "Iteration: 305/750.\n",
      "Iteration: 306/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.7121118251675107, 0.7926670437404754, 0.42816989121324867]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.8, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 307/750.\n",
      "Iteration: 308/750.\n",
      "Iteration: 309/750.\n",
      "Iteration: 310/750.\n",
      "Iteration: 311/750.\n",
      "Iteration: 312/750.\n",
      "Iteration: 313/750.\n",
      "Iteration: 314/750.\n",
      "Iteration: 315/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.5999820458657846, 0.5932785082964959, 0.726994351994561]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.7, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 316/750.\n",
      "Iteration: 317/750.\n",
      "Iteration: 318/750.\n",
      "Iteration: 319/750.\n",
      "Iteration: 320/750.\n",
      "Iteration: 321/750.\n",
      "Iteration: 322/750.\n",
      "Iteration: 323/750.\n",
      "Iteration: 324/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.5185312490896163, 0.6175317543712656, 0.8038231033579081]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.84, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 325/750.\n",
      "Iteration: 326/750.\n",
      "Iteration: 327/750.\n",
      "Iteration: 328/750.\n",
      "Iteration: 329/750.\n",
      "Iteration: 330/750.\n",
      "Iteration: 331/750.\n",
      "Iteration: 332/750.\n",
      "Iteration: 333/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.3721801906432122, 0.48718576388629914, 0.7962355987553126]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.76, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 334/750.\n",
      "Iteration: 335/750.\n",
      "Iteration: 336/750.\n",
      "Iteration: 337/750.\n",
      "Iteration: 338/750.\n",
      "Iteration: 339/750.\n",
      "Iteration: 340/750.\n",
      "Iteration: 341/750.\n",
      "Iteration: 342/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.45703582118824787, 0.4455654487389473, 0.6131551995768093]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.86, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 343/750.\n",
      "Iteration: 344/750.\n",
      "Iteration: 345/750.\n",
      "Iteration: 346/750.\n",
      "Iteration: 347/750.\n",
      "Iteration: 348/750.\n",
      "Iteration: 349/750.\n",
      "Iteration: 350/750.\n",
      "Iteration: 351/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.3600096173785579, 0.24084408281141323, 0.44647209122066905]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.92, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 352/750.\n",
      "Iteration: 353/750.\n",
      "Iteration: 354/750.\n",
      "Iteration: 355/750.\n",
      "Iteration: 356/750.\n",
      "Iteration: 357/750.\n",
      "Iteration: 358/750.\n",
      "Iteration: 359/750.\n",
      "Iteration: 360/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.21452174186284392, 0.7726704370448986, 0.45634574429319485]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.88, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 361/750.\n",
      "Iteration: 362/750.\n",
      "Iteration: 363/750.\n",
      "Iteration: 364/750.\n",
      "Iteration: 365/750.\n",
      "Iteration: 366/750.\n",
      "Iteration: 367/750.\n",
      "Iteration: 368/750.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 369/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.3295559392454376, 0.5024100581564189, 0.4312812816763762]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.92, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 370/750.\n",
      "Iteration: 371/750.\n",
      "Iteration: 372/750.\n",
      "Iteration: 373/750.\n",
      "Iteration: 374/750.\n",
      "Iteration: 375/750.\n",
      "Iteration: 376/750.\n",
      "Iteration: 377/750.\n",
      "Iteration: 378/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.17926499542018026, 0.08146063120636833, 0.14776578401052776]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.96, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 379/750.\n",
      "Iteration: 380/750.\n",
      "Iteration: 381/750.\n",
      "Iteration: 382/750.\n",
      "Iteration: 383/750.\n",
      "Iteration: 384/750.\n",
      "Iteration: 385/750.\n",
      "Iteration: 386/750.\n",
      "Iteration: 387/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.12690574396394314, 0.14626360697760457, 0.27095333847671027]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.96, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 388/750.\n",
      "Iteration: 389/750.\n",
      "Iteration: 390/750.\n",
      "Iteration: 391/750.\n",
      "Iteration: 392/750.\n",
      "Iteration: 393/750.\n",
      "Iteration: 394/750.\n",
      "Iteration: 395/750.\n",
      "Iteration: 396/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.05127357695541305, 0.03077601974795683, 0.09978465409029391]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.94, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 397/750.\n",
      "Iteration: 398/750.\n",
      "Iteration: 399/750.\n",
      "Iteration: 400/750.\n",
      "Iteration: 401/750.\n",
      "Iteration: 402/750.\n",
      "Iteration: 403/750.\n",
      "Iteration: 404/750.\n",
      "Iteration: 405/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.7968578466671137, 0.12050352961368434, 0.13951782523844286]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.86, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 406/750.\n",
      "Iteration: 407/750.\n",
      "Iteration: 408/750.\n",
      "Iteration: 409/750.\n",
      "Iteration: 410/750.\n",
      "Iteration: 411/750.\n",
      "Iteration: 412/750.\n",
      "Iteration: 413/750.\n",
      "Iteration: 414/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.0725913969922685, 0.05298564169811315, 0.11045412407002203]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.94, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 415/750.\n",
      "Iteration: 416/750.\n",
      "Iteration: 417/750.\n",
      "Iteration: 418/750.\n",
      "Iteration: 419/750.\n",
      "Iteration: 420/750.\n",
      "Iteration: 421/750.\n",
      "Iteration: 422/750.\n",
      "Iteration: 423/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.4215869937130474, 0.10524540448488609, 0.06936716339445756]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.98, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 424/750.\n",
      "Iteration: 425/750.\n",
      "Iteration: 426/750.\n",
      "Iteration: 427/750.\n",
      "Iteration: 428/750.\n",
      "Iteration: 429/750.\n",
      "Iteration: 430/750.\n",
      "Iteration: 431/750.\n",
      "Iteration: 432/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.12616707832464547, 0.045277642345445346, 0.057425751483152626]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 433/750.\n",
      "Iteration: 434/750.\n",
      "Iteration: 435/750.\n",
      "Iteration: 436/750.\n",
      "Iteration: 437/750.\n",
      "Iteration: 438/750.\n",
      "Iteration: 439/750.\n",
      "Iteration: 440/750.\n",
      "Iteration: 441/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.06250334432634279, 0.06565315499960024, 0.09353956286963783]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 442/750.\n",
      "Iteration: 443/750.\n",
      "Iteration: 444/750.\n",
      "Iteration: 445/750.\n",
      "Iteration: 446/750.\n",
      "Iteration: 447/750.\n",
      "Iteration: 448/750.\n",
      "Iteration: 449/750.\n",
      "Iteration: 450/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.04103252661290321, 0.023090117515521657, 0.02139307379917506]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 451/750.\n",
      "Iteration: 452/750.\n",
      "Iteration: 453/750.\n",
      "Iteration: 454/750.\n",
      "Iteration: 455/750.\n",
      "Iteration: 456/750.\n",
      "Iteration: 457/750.\n",
      "Iteration: 458/750.\n",
      "Iteration: 459/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.061574340908845206, 0.03061612113091562, 0.0361981308188108]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 460/750.\n",
      "Iteration: 461/750.\n",
      "Iteration: 462/750.\n",
      "Iteration: 463/750.\n",
      "Iteration: 464/750.\n",
      "Iteration: 465/750.\n",
      "Iteration: 466/750.\n",
      "Iteration: 467/750.\n",
      "Iteration: 468/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.04146044511708326, 0.04634376549811827, 0.02264088468215882]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 469/750.\n",
      "Iteration: 470/750.\n",
      "Iteration: 471/750.\n",
      "Iteration: 472/750.\n",
      "Iteration: 473/750.\n",
      "Iteration: 474/750.\n",
      "Iteration: 475/750.\n",
      "Iteration: 476/750.\n",
      "Iteration: 477/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.0529215937297416, 0.015122620732716927, 0.016567539291492402]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 478/750.\n",
      "Iteration: 479/750.\n",
      "Iteration: 480/750.\n",
      "Iteration: 481/750.\n",
      "Iteration: 482/750.\n",
      "Iteration: 483/750.\n",
      "Iteration: 484/750.\n",
      "Iteration: 485/750.\n",
      "Iteration: 486/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.024942579891443245, 0.014512102097624636, 0.017145682969261618]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 487/750.\n",
      "Iteration: 488/750.\n",
      "Iteration: 489/750.\n",
      "Iteration: 490/750.\n",
      "Iteration: 491/750.\n",
      "Iteration: 492/750.\n",
      "Iteration: 493/750.\n",
      "Iteration: 494/750.\n",
      "Iteration: 495/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.01651922213586791, 0.0232326344336274, 0.012857308652123273]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 496/750.\n",
      "Iteration: 497/750.\n",
      "Iteration: 498/750.\n",
      "Iteration: 499/750.\n",
      "Iteration: 500/750.\n",
      "Iteration: 501/750.\n",
      "Iteration: 502/750.\n",
      "Iteration: 503/750.\n",
      "Iteration: 504/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.01592147211275496, 0.009630283114641458, 0.019034746972734173]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 505/750.\n",
      "Iteration: 506/750.\n",
      "Iteration: 507/750.\n",
      "Iteration: 508/750.\n",
      "Iteration: 509/750.\n",
      "Iteration: 510/750.\n",
      "Iteration: 511/750.\n",
      "Iteration: 512/750.\n",
      "Iteration: 513/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.01378188759272413, 0.015363995120761626, 0.015868681066423168]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 514/750.\n",
      "Iteration: 515/750.\n",
      "Iteration: 516/750.\n",
      "Iteration: 517/750.\n",
      "Iteration: 518/750.\n",
      "Iteration: 519/750.\n",
      "Iteration: 520/750.\n",
      "Iteration: 521/750.\n",
      "Iteration: 522/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.009422600680863272, 0.012448477097153542, 0.011722024897707557]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 523/750.\n",
      "Iteration: 524/750.\n",
      "Iteration: 525/750.\n",
      "Iteration: 526/750.\n",
      "Iteration: 527/750.\n",
      "Iteration: 528/750.\n",
      "Iteration: 529/750.\n",
      "Iteration: 530/750.\n",
      "Iteration: 531/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.014234979332353945, 0.01259274166389674, 0.010110019908980701]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 532/750.\n",
      "Iteration: 533/750.\n",
      "Iteration: 534/750.\n",
      "Iteration: 535/750.\n",
      "Iteration: 536/750.\n",
      "Iteration: 537/750.\n",
      "Iteration: 538/750.\n",
      "Iteration: 539/750.\n",
      "Iteration: 540/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.011304716332335658, 0.01568272955105841, 0.009243550549294243]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 541/750.\n",
      "Iteration: 542/750.\n",
      "Iteration: 543/750.\n",
      "Iteration: 544/750.\n",
      "Iteration: 545/750.\n",
      "Iteration: 546/750.\n",
      "Iteration: 547/750.\n",
      "Iteration: 548/750.\n",
      "Iteration: 549/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.009461286540340778, 0.015398113683632223, 0.007788799428446233]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 550/750.\n",
      "Iteration: 551/750.\n",
      "Iteration: 552/750.\n",
      "Iteration: 553/750.\n",
      "Iteration: 554/750.\n",
      "Iteration: 555/750.\n",
      "Iteration: 556/750.\n",
      "Iteration: 557/750.\n",
      "Iteration: 558/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.023462035444168056, 0.0072876130475124055, 0.007217936771467708]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 559/750.\n",
      "Iteration: 560/750.\n",
      "Iteration: 561/750.\n",
      "Iteration: 562/750.\n",
      "Iteration: 563/750.\n",
      "Iteration: 564/750.\n",
      "Iteration: 565/750.\n",
      "Iteration: 566/750.\n",
      "Iteration: 567/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.011486809869905357, 0.014080469611488039, 0.006042325705134973]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 568/750.\n",
      "Iteration: 569/750.\n",
      "Iteration: 570/750.\n",
      "Iteration: 571/750.\n",
      "Iteration: 572/750.\n",
      "Iteration: 573/750.\n",
      "Iteration: 574/750.\n",
      "Iteration: 575/750.\n",
      "Iteration: 576/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.00531625625296299, 0.008090467558170616, 0.007376739239527481]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 577/750.\n",
      "Iteration: 578/750.\n",
      "Iteration: 579/750.\n",
      "Iteration: 580/750.\n",
      "Iteration: 581/750.\n",
      "Iteration: 582/750.\n",
      "Iteration: 583/750.\n",
      "Iteration: 584/750.\n",
      "Iteration: 585/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.011827042260566472, 0.004132437648233549, 0.00841132813573615]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 586/750.\n",
      "Iteration: 587/750.\n",
      "Iteration: 588/750.\n",
      "Iteration: 589/750.\n",
      "Iteration: 590/750.\n",
      "Iteration: 591/750.\n",
      "Iteration: 592/750.\n",
      "Iteration: 593/750.\n",
      "Iteration: 594/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.012874743552425738, 0.006268778404724124, 0.008973090645707517]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 595/750.\n",
      "Iteration: 596/750.\n",
      "Iteration: 597/750.\n",
      "Iteration: 598/750.\n",
      "Iteration: 599/750.\n",
      "Iteration: 600/750.\n",
      "Iteration: 601/750.\n",
      "Iteration: 602/750.\n",
      "Iteration: 603/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.01861776886689462, 0.00892701187826333, 0.0037573490446317624]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 604/750.\n",
      "Iteration: 605/750.\n",
      "Iteration: 606/750.\n",
      "Iteration: 607/750.\n",
      "Iteration: 608/750.\n",
      "Iteration: 609/750.\n",
      "Iteration: 610/750.\n",
      "Iteration: 611/750.\n",
      "Iteration: 612/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.01449660654662139, 0.01239944670797838, 0.019480864669444573]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 613/750.\n",
      "Iteration: 614/750.\n",
      "Iteration: 615/750.\n",
      "Iteration: 616/750.\n",
      "Iteration: 617/750.\n",
      "Iteration: 618/750.\n",
      "Iteration: 619/750.\n",
      "Iteration: 620/750.\n",
      "Iteration: 621/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.007348764169813485, 0.005429214354160296, 0.009889597384216563]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 622/750.\n",
      "Iteration: 623/750.\n",
      "Iteration: 624/750.\n",
      "Iteration: 625/750.\n",
      "Iteration: 626/750.\n",
      "Iteration: 627/750.\n",
      "Iteration: 628/750.\n",
      "Iteration: 629/750.\n",
      "Iteration: 630/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.007102762083355663, 0.006198899235130445, 0.006963071672403594]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 631/750.\n",
      "Iteration: 632/750.\n",
      "Iteration: 633/750.\n",
      "Iteration: 634/750.\n",
      "Iteration: 635/750.\n",
      "Iteration: 636/750.\n",
      "Iteration: 637/750.\n",
      "Iteration: 638/750.\n",
      "Iteration: 639/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.010189974498328942, 0.006265418445138659, 0.009006211923601624]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 640/750.\n",
      "Iteration: 641/750.\n",
      "Iteration: 642/750.\n",
      "Iteration: 643/750.\n",
      "Iteration: 644/750.\n",
      "Iteration: 645/750.\n",
      "Iteration: 646/750.\n",
      "Iteration: 647/750.\n",
      "Iteration: 648/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.009768225769362357, 0.004369052790291455, 0.009330586000170749]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 649/750.\n",
      "Iteration: 650/750.\n",
      "Iteration: 651/750.\n",
      "Iteration: 652/750.\n",
      "Iteration: 653/750.\n",
      "Iteration: 654/750.\n",
      "Iteration: 655/750.\n",
      "Iteration: 656/750.\n",
      "Iteration: 657/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.006903764936367092, 0.008378244942623849, 0.006026925378267827]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 658/750.\n",
      "Iteration: 659/750.\n",
      "Iteration: 660/750.\n",
      "Iteration: 661/750.\n",
      "Iteration: 662/750.\n",
      "Iteration: 663/750.\n",
      "Iteration: 664/750.\n",
      "Iteration: 665/750.\n",
      "Iteration: 666/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.005647861855906821, 0.006373796991816481, 0.0036909966583682525]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 667/750.\n",
      "Iteration: 668/750.\n",
      "Iteration: 669/750.\n",
      "Iteration: 670/750.\n",
      "Iteration: 671/750.\n",
      "Iteration: 672/750.\n",
      "Iteration: 673/750.\n",
      "Iteration: 674/750.\n",
      "Iteration: 675/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.0035338558313725097, 0.004852368511403443, 0.006987997809365887]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 676/750.\n",
      "Iteration: 677/750.\n",
      "Iteration: 678/750.\n",
      "Iteration: 679/750.\n",
      "Iteration: 680/750.\n",
      "Iteration: 681/750.\n",
      "Iteration: 682/750.\n",
      "Iteration: 683/750.\n",
      "Iteration: 684/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.004503677118881455, 0.004347344683964638, 0.005801303881045078]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 685/750.\n",
      "Iteration: 686/750.\n",
      "Iteration: 687/750.\n",
      "Iteration: 688/750.\n",
      "Iteration: 689/750.\n",
      "Iteration: 690/750.\n",
      "Iteration: 691/750.\n",
      "Iteration: 692/750.\n",
      "Iteration: 693/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.008569560094122864, 0.004063908132902382, 0.00772795803548363]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 694/750.\n",
      "Iteration: 695/750.\n",
      "Iteration: 696/750.\n",
      "Iteration: 697/750.\n",
      "Iteration: 698/750.\n",
      "Iteration: 699/750.\n",
      "Iteration: 700/750.\n",
      "Iteration: 701/750.\n",
      "Iteration: 702/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.004720701108118153, 0.004657498544578502, 0.006138915366123302]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 703/750.\n",
      "Iteration: 704/750.\n",
      "Iteration: 705/750.\n",
      "Iteration: 706/750.\n",
      "Iteration: 707/750.\n",
      "Iteration: 708/750.\n",
      "Iteration: 709/750.\n",
      "Iteration: 710/750.\n",
      "Iteration: 711/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.003572075049107347, 0.0035199080476802816, 0.004912580903612035]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 712/750.\n",
      "Iteration: 713/750.\n",
      "Iteration: 714/750.\n",
      "Iteration: 715/750.\n",
      "Iteration: 716/750.\n",
      "Iteration: 717/750.\n",
      "Iteration: 718/750.\n",
      "Iteration: 719/750.\n",
      "Iteration: 720/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.010649138571592715, 0.0077419436390885115, 0.007040307072026977]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 721/750.\n",
      "Iteration: 722/750.\n",
      "Iteration: 723/750.\n",
      "Iteration: 724/750.\n",
      "Iteration: 725/750.\n",
      "Iteration: 726/750.\n",
      "Iteration: 727/750.\n",
      "Iteration: 728/750.\n",
      "Iteration: 729/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.005056200145948542, 0.0028941767692891984, 0.0044539151251743285]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 730/750.\n",
      "Iteration: 731/750.\n",
      "Iteration: 732/750.\n",
      "Iteration: 733/750.\n",
      "Iteration: 734/750.\n",
      "Iteration: 735/750.\n",
      "Iteration: 736/750.\n",
      "Iteration: 737/750.\n",
      "Iteration: 738/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.005925159649311415, 0.005571045170012228, 0.002954864165883129]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 739/750.\n",
      "Iteration: 740/750.\n",
      "Iteration: 741/750.\n",
      "Iteration: 742/750.\n",
      "Iteration: 743/750.\n",
      "Iteration: 744/750.\n",
      "Iteration: 745/750.\n",
      "Iteration: 746/750.\n",
      "Iteration: 747/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.00718462533908763, 0.004472624064811412, 0.005578008915382949]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 748/750.\n",
      "Iteration: 749/750.\n",
      "Iteration: 750/750.\n",
      "\n",
      "\n",
      "----------------FINAL OUTPUT----------------\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "Loss history: [2.304373733952622, 2.3001941262256884, 2.303508335898844, 2.2942834211171674, 2.302257539684362, 2.2846940314338897, 2.2667797754626706, 2.275256568294465, 2.2556253335188514, 2.255724245474301, 2.343866761998466, 2.273996881707418, 2.277199410984449, 2.2972349987580034, 2.237717600372464, 2.2727310367417854, 2.204103899788421, 2.258343837284645, 2.2590217321572386, 2.2410365036727793, 2.21648210922612, 2.257682756751057, 2.1638925675409975, 2.3051962858235155, 2.1944675031511447, 2.2390716805564996, 2.281966984895512, 2.2260888371900878, 2.3167035067377824, 2.1894941797773226, 2.2825784282113477, 2.3185600416673253, 2.2054036260114587, 2.2104394749579153, 2.186158200387568, 2.314671231617274, 2.1841281528840213, 2.2162186641313677, 2.2318659828023284, 2.215329853793867, 2.2706347464264036, 2.238357849701217, 2.262181205808585, 2.1475236738810057, 2.1927888638495125, 2.1374337344344143, 2.334798103876355, 2.314091974017603, 2.100504717556598, 2.310267870364869, 2.08967420692944, 2.1454005350969774, 2.3007085399330136, 2.254623388762617, 2.2115336061775284, 2.237354164274268, 2.2526442310975536, 2.1206935324215714, 2.091221586124789, 2.2756473590375514, 2.2224804656011212, 2.132319431106546, 2.160046661484646, 2.1065756443487866, 2.294840035022363, 2.304341258435041, 2.260400324063611, 2.1889218481103163, 2.1682860883507287, 2.062876644831836, 2.1102064822282625, 2.342895680648545, 2.272264401800523, 2.2724597301609637, 2.2953864638718358, 2.2955029132038853, 2.027709232618005, 2.335128325909235, 2.245438425118237, 2.1422264296376468, 2.328747695698904, 2.1241701530745263, 2.211319922999345, 2.404273097629641, 2.1776793791750038, 2.311223172066713, 2.253595040493041, 2.2147544396521504, 2.182436801416794, 2.3272995062125292, 2.14755198939707, 2.224311403857144, 2.193804558545494, 2.27463863108589, 1.983868980764864, 2.0552797263934353, 2.141941328348959, 2.112567311006279, 2.2184563897284577, 2.2253812604806753, 2.155493279442576, 2.04488320323522, 2.208044770196635, 2.2270848300181565, 2.0493456358370006, 2.280583389083929, 2.0969955815327004, 1.8145079639294601, 1.733432267376918, 2.361115924278763, 2.011099680766377, 2.1170719359633607, 2.021971444499357, 2.1079372889774723, 1.9084026833757428, 2.1167140453380213, 2.3740182895421293, 1.8632060517607802, 2.076402464514721, 2.215907711831237, 2.4446407943538606, 2.1757815782797603, 2.1450355930985054, 2.346263581167927, 1.8869907552364524, 2.461087123313006, 2.0273978330067357, 2.1504613281145346, 2.088402859836269, 2.198812654945001, 2.2531395007423036, 2.059688649164976, 1.810131401373492, 2.1380569646358207, 2.21698438382967, 2.4823956220819454, 2.1195679167362993, 1.971004143341482, 2.0001928766222687, 2.0518658433650567, 1.8909377547150426, 1.8719002668156919, 2.0312890920937923, 2.0585284689321237, 1.9562310562304412, 2.2622172964686587, 2.3135086353729846, 1.7294282976062227, 2.1329809755411775, 1.6555943980913044, 1.7564821912496176, 1.9865067476393694, 1.943536227476239, 1.978874337334997, 2.2721219696167485, 1.3500510895461262, 1.945111821369137, 2.128804782670874, 2.0435319259775824, 1.8245981558917728, 1.9881851896046179, 1.9763830868347005, 1.410153978585237, 1.9877138408190764, 1.5592077802792934, 2.1382530196473235, 1.6794143710999903, 1.7629319662927774, 2.080556145782496, 1.624516182196767, 1.592757630272275, 1.6688359778036816, 2.1488719483329546, 1.749666852250499, 1.8956478752597332, 1.9571766940492932, 1.8259836749279672, 1.69090942977038, 1.9665592430540089, 1.8076948352084192, 1.7684841448798214, 1.9426014940154408, 1.7010945896422616, 1.7521388139652931, 1.6375481266407115, 1.6952798468192982, 1.7687937906780133, 1.5499046858392924, 1.7933401927001027, 1.6386323965508027, 1.6667082364842258, 1.5691874815949882, 1.6214483501900927, 1.6512104077564604, 1.3213007010113835, 1.8762247744837355, 1.8434306913313045, 1.646268527799408, 1.9997997873481772, 1.6309667060129955, 1.1701267890877338, 1.4891833294811843, 1.70317424237858, 1.5404069149122528, 1.5854351235733253, 1.6089370888972951, 2.058576125229221, 1.4337536442625005, 1.6372614896313955, 1.8904958727682826, 1.607664259219223, 1.7799635780008747, 1.7339694812403939, 1.784510600880739, 1.7362789713859994, 1.446772620741509, 1.7854874993092509, 1.7322336368741515, 1.4470217991003047, 1.287346335620474, 1.589470404148197, 1.4931802740816207, 1.367081370829135, 1.7940175672363345, 1.0815554925170499, 1.7961263543101298, 1.6367526511272754, 1.5904461673912726, 1.2824299904658127, 1.4529797833257037, 1.8568912108204836, 1.5370650534071886, 1.4994189511311506, 1.2382007588936237, 1.8971348537323285, 1.6130488323359, 1.1399157682988508, 1.6915364333186673, 1.1268907625369757, 1.7847238692913756, 1.2365533918378877, 1.4200319681257227, 1.8429169176312614, 1.3573146631657107, 1.3613831185322685, 1.4558154294226406, 1.2780111208324212, 0.7886692002199776, 1.2513134697770254, 1.565128906395955, 0.9596664751092816, 1.229862366192645, 1.081924466891789, 1.111507314381988, 0.9787962615669147, 0.6116717571822116, 0.9510060655520807, 1.1903071320327847, 0.9498037380592687, 1.0349733756213138, 1.866582896709912, 1.0865466015454803, 0.6267400342107526, 0.9140208774985443, 0.9798408473607394, 1.1648825935370135, 1.2159846140092239, 1.1245383727738474, 1.0784979611253103, 1.2490428991528333, 0.6407967725801771, 1.068668324933873, 0.6870142273109042, 0.927031672449401, 0.4454871539636873, 0.5097538075567264, 0.6562065181910253, 0.6959926880204463, 0.3588537720135193, 0.6224377220658721, 1.7307132444984323, 2.3000145835268153, 1.327206831417534, 0.9257733388814313, 1.2153523303143385, 0.7398576115325761, 0.9311622601760052, 1.2553016971047823, 1.133802533321011, 0.5773086413340248, 0.8756150802856881, 1.0928458184275962, 1.0155751885345552, 0.8983073758017677, 0.6652714303068812, 0.8960706763006234, 0.9986283064194643, 0.44583114642604027, 1.0500690080684956, 0.8119711476721613, 1.0797259989022492, 1.1591392809881758, 0.5589498619539535, 0.7121118251675107, 0.7926670437404754, 0.42816989121324867, 1.3000331944944883, 0.7871258827220289, 0.6142199270606485, 0.7738938012672615, 0.7036560755282376, 0.7959690693617248, 0.5999820458657846, 0.5932785082964959, 0.726994351994561, 0.3667169119883454, 0.40103360430521057, 1.1649978002926777, 0.6092126977255794, 0.35891871619390314, 0.4949443402085074, 0.5185312490896163, 0.6175317543712656, 0.8038231033579081, 0.2553670226039733, 0.3339115350747408, 0.5362680466660616, 0.39771775162361667, 0.5402554749542119, 0.3149396882057357, 0.3721801906432122, 0.48718576388629914, 0.7962355987553126, 0.4918489195598988, 0.25226505624543255, 0.15026628045480792, 0.7376997211704017, 0.7590797499739401, 0.38793019759373837, 0.45703582118824787, 0.4455654487389473, 0.6131551995768093, 0.7531816428423523, 0.41108880104674034, 0.29370583260422584, 0.5912291931437593, 0.21482833435145687, 0.3093758810872063, 0.3600096173785579, 0.24084408281141323, 0.44647209122066905, 0.19964547106471958, 0.29215732521324317, 0.3130263662296953, 0.24575520068006115, 0.13825717262552809, 0.36514773550099505, 0.21452174186284392, 0.7726704370448986, 0.45634574429319485, 0.3122368050781844, 0.05776697602024016, 0.1854773946131284, 0.5503131811487881, 0.28743680936224414, 0.3771210398268423, 0.3295559392454376, 0.5024100581564189, 0.4312812816763762, 0.16620777183144497, 0.3508810148044679, 0.16407267229376166, 0.24960096171718807, 0.1099984918803749, 0.25971806004965553, 0.17926499542018026, 0.08146063120636833, 0.14776578401052776, 0.21537143605947007, 0.2795480336964628, 0.09655567892633884, 0.12237956574301555, 0.16769773791632528, 0.2053869851110839, 0.12690574396394314, 0.14626360697760457, 0.27095333847671027, 0.08534698567233853, 0.11189195969800379, 0.04948403915152692, 0.04686323923690525, 0.025367597061111982, 0.09769466920014397, 0.05127357695541305, 0.03077601974795683, 0.09978465409029391, 0.08285593039943791, 0.08336358672151922, 0.296778078865661, 0.17747115931833968, 0.1652346467031926, 0.9328940050886166, 0.7968578466671137, 0.12050352961368434, 0.13951782523844286, 1.5305899356410402, 0.5357295783816367, 0.25324114847254797, 0.20073981006510697, 0.2233964466265701, 0.24295233857099802, 0.0725913969922685, 0.05298564169811315, 0.11045412407002203, 0.4968842232084023, 0.4371184149349949, 1.4942854079244765, 0.14652136624419634, 0.15948199664138601, 0.31005253162862445, 0.4215869937130474, 0.10524540448488609, 0.06936716339445756, 0.028366359470039727, 0.06622668476741339, 0.05638199354172865, 0.09400144409042956, 0.07466870217471178, 0.06626108132385396, 0.12616707832464547, 0.045277642345445346, 0.057425751483152626, 0.04994955847014086, 0.0357417098933736, 0.05729720134239927, 0.0380212057132703, 0.022473503359104828, 0.10765829459673788, 0.06250334432634279, 0.06565315499960024, 0.09353956286963783, 0.06560212981643271, 0.08354932861172695, 0.04462981999921014, 0.035550513887743794, 0.04394705129940501, 0.02369816594578828, 0.04103252661290321, 0.023090117515521657, 0.02139307379917506, 0.027697651054030034, 0.047769814246892944, 0.030182028669400757, 0.0102742852100776, 0.059926868062292596, 0.04037808543071978, 0.061574340908845206, 0.03061612113091562, 0.0361981308188108, 0.11871876214480453, 0.02711514343001913, 0.030976357804806915, 0.02748553665434398, 0.021510635515774104, 0.019245058776419233, 0.04146044511708326, 0.04634376549811827, 0.02264088468215882, 0.0392049201174265, 0.006836150385772681, 0.020894789802302166, 0.007139786195501827, 0.02259755622753597, 0.027050669788486522, 0.0529215937297416, 0.015122620732716927, 0.016567539291492402, 0.02348200079721816, 0.047905095082202344, 0.039325259914641225, 0.024334853338468125, 0.021740722869688995, 0.023134379727173526, 0.024942579891443245, 0.014512102097624636, 0.017145682969261618, 0.014833737849948887, 0.030325040465723686, 0.025440286233534438, 0.013319141922429036, 0.034172089968108166, 0.017870379722329913, 0.01651922213586791, 0.0232326344336274, 0.012857308652123273, 0.015452327329549577, 0.011569529579973335, 0.01606499371361354, 0.0254439479702484, 0.005483964579962343, 0.03755204699226703, 0.01592147211275496, 0.009630283114641458, 0.019034746972734173, 0.026398567195622516, 0.03537987162931496, 0.027986562536791812, 0.02010668388338037, 0.016697632867202675, 0.009339512873053887, 0.01378188759272413, 0.015363995120761626, 0.015868681066423168, 0.01925600705433446, 0.0162695367607405, 0.011625233886545794, 0.01253275354471131, 0.011914555606381896, 0.01619025255379878, 0.009422600680863272, 0.012448477097153542, 0.011722024897707557, 0.010980889587049568, 0.019698072511830034, 0.008106453248236514, 0.024261969313031276, 0.01664952295875847, 0.02209273164248876, 0.014234979332353945, 0.01259274166389674, 0.010110019908980701, 0.01836947418802377, 0.004577332326274213, 0.015563620312384625, 0.010464749807813395, 0.031001429885089412, 0.013934688045500736, 0.011304716332335658, 0.01568272955105841, 0.009243550549294243, 0.015496693887549928, 0.009004373868964697, 0.012918905489206253, 0.014236148726334837, 0.007616032784086871, 0.011923181484998463, 0.009461286540340778, 0.015398113683632223, 0.007788799428446233, 0.015113599583182646, 0.021553783004299915, 0.01639392292162116, 0.008440645614175579, 0.012806029385153225, 0.00874927098586356, 0.023462035444168056, 0.0072876130475124055, 0.007217936771467708, 0.01782194198233033, 0.01207155237968858, 0.011752139959465968, 0.0062419636601818256, 0.013935065942190003, 0.004752039193712535, 0.011486809869905357, 0.014080469611488039, 0.006042325705134973, 0.011840332992193676, 0.007672160077540881, 0.014259665348858825, 0.015814291732097942, 0.00996384980651853, 0.009753477038422397, 0.00531625625296299, 0.008090467558170616, 0.007376739239527481, 0.007561630080063654, 0.0048067482718969325, 0.01455321646510775, 0.009534030105478314, 0.006823118986963241, 0.012258925685638085, 0.011827042260566472, 0.004132437648233549, 0.00841132813573615, 0.01163982120993098, 0.008515131479418538, 0.01037824653241514, 0.011728255494906156, 0.008709818568441973, 0.009080710544627063, 0.012874743552425738, 0.006268778404724124, 0.008973090645707517, 0.00789153802921903, 0.010576438959029565, 0.01370747626669494, 0.010576139975443058, 0.008087506534542226, 0.008060868983640241, 0.01861776886689462, 0.00892701187826333, 0.0037573490446317624, 0.016570516146758602, 0.01005173608680383, 0.0073664631730625445, 0.011563938003768235, 0.005498498875273184, 0.007866180621534696, 0.01449660654662139, 0.01239944670797838, 0.019480864669444573, 0.008557679950982921, 0.008240917611021543, 0.007697114025691263, 0.004959484933526304, 0.0076770077535636624, 0.005585870863360915, 0.007348764169813485, 0.005429214354160296, 0.009889597384216563, 0.005898542378264898, 0.006844108812315968, 0.008954046744797191, 0.006692399897707854, 0.0068495914899186355, 0.0038659909177255446, 0.007102762083355663, 0.006198899235130445, 0.006963071672403594, 0.009938492067863744, 0.005734729090470414, 0.010965489489377032, 0.00696197330801953, 0.009014750912451984, 0.005234961588205492, 0.010189974498328942, 0.006265418445138659, 0.009006211923601624, 0.0069112952326092275, 0.006952217059568428, 0.0060525169198034105, 0.004624616378595277, 0.006300349436876483, 0.006327778553716645, 0.009768225769362357, 0.004369052790291455, 0.009330586000170749, 0.011916091975691092, 0.009587720872694959, 0.009000985560248923, 0.009036224419451375, 0.008854067363251455, 0.0062079453437354085, 0.006903764936367092, 0.008378244942623849, 0.006026925378267827, 0.0070854818870063315, 0.003817874057583822, 0.006060431005049817, 0.006294697868466508, 0.004406882469618212, 0.00513493240171714, 0.005647861855906821, 0.006373796991816481, 0.0036909966583682525, 0.003103439946741614, 0.007847740087956221, 0.0050946230776867584, 0.00548313360983361, 0.0066137318919213045, 0.008360881951082057, 0.0035338558313725097, 0.004852368511403443, 0.006987997809365887, 0.006896849101987068, 0.002169801300467622, 0.003442951170971006, 0.009987380424426477, 0.005124334851919167, 0.009748315281390522, 0.004503677118881455, 0.004347344683964638, 0.005801303881045078, 0.0054503464532295424, 0.00651982963991027, 0.006015332376606099, 0.005741210110396739, 0.004278929573893216, 0.0029834098257500016, 0.008569560094122864, 0.004063908132902382, 0.00772795803548363, 0.0061756214695268055, 0.00395160928194057, 0.011982435101879504, 0.010200359248933318, 0.006216088906124246, 0.004598530518563445, 0.004720701108118153, 0.004657498544578502, 0.006138915366123302, 0.0013014428460153586, 0.004593745286225871, 0.007867314830581736, 0.00706103713156039, 0.0047291814880302055, 0.004435165978522119, 0.003572075049107347, 0.0035199080476802816, 0.004912580903612035, 0.010671017101633795, 0.005025775275609579, 0.006725925348585075, 0.004400996638160431, 0.0057950247019024625, 0.003918046208421628, 0.010649138571592715, 0.0077419436390885115, 0.007040307072026977, 0.004485089963919566, 0.0025554286710430685, 0.004692542015776534, 0.004412933727764303, 0.0028848586911630805, 0.0043098954352949245, 0.005056200145948542, 0.0028941767692891984, 0.0044539151251743285, 0.0033704169158858266, 0.004387385607255327, 0.005898412790377338, 0.0033261656118733825, 0.005787752550088934, 0.003084066630380062, 0.005925159649311415, 0.005571045170012228, 0.002954864165883129, 0.0047262716933875164, 0.005992546982146556, 0.006148414847011039, 0.002562720094109994, 0.007561353489565804, 0.004224458399807254, 0.00718462533908763, 0.004472624064811412, 0.005578008915382949, 0.0051878711302764564, 0.007220312662910829, 0.004471873677363599]\n",
      "Accuracy history: [0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.24, 0.24, 0.28, 0.32, 0.36, 0.32, 0.38, 0.44, 0.4, 0.36, 0.46, 0.32, 0.34, 0.52, 0.58, 0.62, 0.56, 0.62, 0.7, 0.7, 0.8, 0.7, 0.84, 0.76, 0.86, 0.92, 0.88, 0.92, 0.96, 0.96, 0.94, 0.86, 0.94, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# SGD\n",
    "sgd = ConvNet4(input_shape=input_shape, wt_scale=wt_scale, verbose=False)\n",
    "sgd.compile('sgd')\n",
    "sgd.fit(x_dev, y_dev, x_val, y_val, mini_batch_sz=mini_batch_sz, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**: Why does decreasing the mini-batch size make the loss print-outs more erratic?\n",
    "\n",
    "Answer: Decreasing the mini-batch size makes the loss print-outs more erratic because there are fewer samples that have been able to propagate back through the network, so each sample has much more of an impact on what the loss value will be. There is an averaging over all samples that takes place, so if all of the samples in a very small mini-batch are predicted successfully, loss will be low, but it will be high if it performs poorly on the mini-batch. The smaller the mini-batch, the greater the chance that there is a skewed distribution of correct and incorrect predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6d) Evaluate the different optimizers\n",
    "\n",
    "Make 2 \"high quality\" plots showing the following\n",
    "\n",
    "- Plot the accuracy (y axis) for the three optimizers as a function of training epoch (x axis).\n",
    "- Plot the loss (y axis) for the three optimizers as a function of training iteration (x axis).\n",
    "\n",
    "A high quality plot consists of:\n",
    "- A useful title\n",
    "- X and Y axis labels\n",
    "- A legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get accuracy values\n",
    "adam_acc = adam.train_acc_history\n",
    "sgd_m_acc = sgd_m.train_acc_history\n",
    "sgd_acc = sgd.train_acc_history\n",
    "\n",
    "#plot\n",
    "plt.plot(adam_acc, label=\"adam\")\n",
    "plt.plot(sgd_m_acc, label=\"sgd_m\")\n",
    "plt.plot(sgd_acc, label=\"sgd\")\n",
    "\n",
    "#make words show up\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy Over Iterations\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Training Iterations\")\n",
    "plt.show()\n",
    "\n",
    "#get loss values\n",
    "adam_loss = adam.loss_history\n",
    "sgd_m_loss = sgd_m.loss_history\n",
    "sgd_loss = sgd.loss_history\n",
    "\n",
    "#plot\n",
    "plt.plot(adam_loss, label=\"adam\")\n",
    "plt.plot(sgd_m_loss, label=\"sgd_m\")\n",
    "plt.plot(sgd_loss, label=\"sgd\")\n",
    "\n",
    "#make words show up\n",
    "plt.legend()\n",
    "plt.title(\"Loss Over Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Training Iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adam' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-868b4bd1052d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'adam' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**: Which optimizer works best and why do think it is best?\n",
    "\n",
    "The adam optimizer works the best. It does so because it dynamically updates learning rate.\n",
    "\n",
    "**Question 5**: What is happening with the training set accuracy and why?\n",
    "\n",
    "The training set accuracy is overfitting. It is basically memorizing the features about each image, or memorizing the dataset. Therefore, it has perfect accuracy while looking at its own training set, but once it has a non-training image, it has horrible accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Training convolutional neural network on STL-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7a) Load in STL-10 at 32x32 resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images are: (5000, 96, 96, 3)\n",
      "Labels are: (5000,)\n",
      "Resizing 5000 images to 32x32...Done!\n",
      "Saving Numpy arrays the images and labels to ./numpy...Done!\n",
      "imgs.shape (5000, 32, 32, 3)\n",
      "data.shape (5000, 3072)\n",
      "Train data shape:  (4548, 3072)\n",
      "Train labels shape:  (4548,)\n",
      "Test data shape:  (400, 3072)\n",
      "Test labels shape:  (400,)\n",
      "Validation data shape:  (2, 3072)\n",
      "Validation labels shape:  (2,)\n",
      "dev data shape:  (50, 3072)\n",
      "dev labels shape:  (50,)\n"
     ]
    }
   ],
   "source": [
    "# Download the STL-10 dataset from the internet, convert it to Numpy ndarray, resize to 32x32\n",
    "# cache it locally on your computer for faster loading next time.\n",
    "load_stl10_dataset.purge_cached_dataset()\n",
    "stl_imgs, stl_labels = load_stl10_dataset.load(scale_fact=3)\n",
    "# preprocess\n",
    "stl_imgs, stl_labels = preprocess_data.preprocess_stl(stl_imgs, stl_labels)\n",
    "# create splits\n",
    "x_train, y_train, x_test, y_test, x_val, y_val, x_dev, y_dev = preprocess_data.create_splits(\n",
    "    stl_imgs, stl_labels, n_train_samps=4548, n_test_samps=400, n_valid_samps=2, n_dev_samps=50)\n",
    "\n",
    "print ('Train data shape: ', x_train.shape)\n",
    "print ('Train labels shape: ', y_train.shape)\n",
    "print ('Test data shape: ', x_test.shape)\n",
    "print ('Test labels shape: ', y_test.shape)\n",
    "print ('Validation data shape: ', x_val.shape)\n",
    "print ('Validation labels shape: ', y_val.shape)\n",
    "print ('dev data shape: ', x_dev.shape)\n",
    "print ('dev labels shape: ', y_dev.shape)\n",
    "\n",
    "classes = np.loadtxt(os.path.join('data', 'stl10_binary', 'class_names.txt'), dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7b) Set up accelerated convolution and max pooling layers\n",
    "\n",
    "As you may have noticed, we had to downsize STL-10 to 16x16 resolution to train the network on the dev set (N=50) in a reasonable amount of time. The training set is N=4000, how will we ever manage to process that amount of data!?\n",
    "\n",
    "On one hand, this is an unfortunate inevitable reality of working with large (\"big\") datasets: you can easily find a dataset that is too time consuming to process for any computer, despite how fast/many CPU/GPUs it has.\n",
    "\n",
    "On the other hand, we can do better for this project and STL-10 :) If you were to time (profile) different parts of the training process, you'd notice that largest bottleneck is convolution and max pooling operations (both forward/backward). You implemented those operations intuitively, which does not always yield the best performance. **By swapping out forward/backward convolution and maxpooling for implementations that use different algorithms (im2col, reshaping) that are compiled to C code, we will speed up training up by several orders of magnitude**.\n",
    "\n",
    "Follow these steps to subsitute in the \"accelerated\" convolution and max pooling layers.\n",
    "\n",
    "- Install the `cython` python package: `pip3 install cython` (or `pip3 install cython --user` if working in Davis 102)\n",
    "- Dowload files `im2col_cython.pyx`, `accelerated_layer.py`, `setup.py` from the project website. Put them in your base project folder.\n",
    "- Open terminal, `cd` to Project directory.\n",
    "- Compile the im2col functions: `python3 setup.py build_ext --inplace`. A `.c` and `.so` file should have appeared in your project folder.\n",
    "- Restart Jupyter Notebook kernel\n",
    "- Create a class called `Conv4NetAccel` in `network.py` by copy-pasting the contents of `Conv4Net`. Import `accelerated_layer` at the top and replace the `Conv2D` and `MaxPool2D` layers with `Conv2DAccel` and `MaxPool2DAccel`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7c) Training convolutional neural network on STL-10\n",
    "\n",
    "You are now ready to train on the entire training set.\n",
    "\n",
    "- Create a `Conv4NetAccel` object with hyperparameters of your choice.\n",
    "- Your goal is to achieve 45% accuracy on the test and/or validation set.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- I suggest using your intuition about hyperparameters and over/underfitting to guide your choice, rather than a grid search. This should not be overly challenging.\n",
    "- Use the best / most efficient optimizer based on your prior analysis.\n",
    "- It should take on the order of 1 sec per training iteration. If that's way off, seek help as something could be wrong with running the acclerated code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network import ConvNet4Accel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4548, 3, 32, 32)\n",
      "(5000, 3, 32, 32)\n",
      "Starting to train...\n",
      "1485 iterations. 45 iter/epoch.\n",
      "Iteration: 1/1485.\n",
      "Time taken for iteration 0: 1.3155288696289062\n",
      "Estimated time to complete: 1953.5603713989258\n",
      "Iteration: 2/1485.\n",
      "Iteration: 3/1485.\n",
      "Iteration: 4/1485.\n",
      "Iteration: 5/1485.\n",
      "Iteration: 6/1485.\n",
      "Iteration: 7/1485.\n",
      "Iteration: 8/1485.\n",
      "Iteration: 9/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [2.2945056409184144, 2.2680652925025, 2.2649603772306177]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.2, Val acc: 0.14\n",
      "\n",
      "\n",
      "Iteration: 10/1485.\n",
      "Iteration: 11/1485.\n",
      "Iteration: 12/1485.\n",
      "Iteration: 13/1485.\n",
      "Iteration: 14/1485.\n",
      "Iteration: 15/1485.\n",
      "Iteration: 16/1485.\n",
      "Iteration: 17/1485.\n",
      "Iteration: 18/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [2.1343966543341, 2.1909408337106813, 1.9705364730134076]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.2, Val acc: 0.16\n",
      "\n",
      "\n",
      "Iteration: 19/1485.\n",
      "Iteration: 20/1485.\n",
      "Iteration: 21/1485.\n",
      "Iteration: 22/1485.\n",
      "Iteration: 23/1485.\n",
      "Iteration: 24/1485.\n",
      "Iteration: 25/1485.\n",
      "Iteration: 26/1485.\n",
      "Iteration: 27/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.9642737735772402, 1.975972960841398, 2.0308342032558646]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.28, Val acc: 0.18\n",
      "\n",
      "\n",
      "Iteration: 28/1485.\n",
      "Iteration: 29/1485.\n",
      "Iteration: 30/1485.\n",
      "Iteration: 31/1485.\n",
      "Iteration: 32/1485.\n",
      "Iteration: 33/1485.\n",
      "Iteration: 34/1485.\n",
      "Iteration: 35/1485.\n",
      "Iteration: 36/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.8773609437805474, 1.9295325266797005, 1.9401001812964922]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.234, Val acc: 0.2\n",
      "\n",
      "\n",
      "Iteration: 37/1485.\n",
      "Iteration: 38/1485.\n",
      "Iteration: 39/1485.\n",
      "Iteration: 40/1485.\n",
      "Iteration: 41/1485.\n",
      "Iteration: 42/1485.\n",
      "Iteration: 43/1485.\n",
      "Iteration: 44/1485.\n",
      "Iteration: 45/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.857145202719596, 1.8853686418528655, 1.8762154621412657]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.254, Val acc: 0.22\n",
      "\n",
      "\n",
      "Iteration: 46/1485.\n",
      "Iteration: 47/1485.\n",
      "Iteration: 48/1485.\n",
      "Iteration: 49/1485.\n",
      "Iteration: 50/1485.\n",
      "Iteration: 51/1485.\n",
      "Iteration: 52/1485.\n",
      "Iteration: 53/1485.\n",
      "Iteration: 54/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.9640203463059742, 1.9527985156618313, 1.7816434502116634]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.262, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 55/1485.\n",
      "Iteration: 56/1485.\n",
      "Iteration: 57/1485.\n",
      "Iteration: 58/1485.\n",
      "Iteration: 59/1485.\n",
      "Iteration: 60/1485.\n",
      "Iteration: 61/1485.\n",
      "Iteration: 62/1485.\n",
      "Iteration: 63/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.9012065903903945, 1.9320045293699044, 1.9330591220538336]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.29, Val acc: 0.26\n",
      "\n",
      "\n",
      "Iteration: 64/1485.\n",
      "Iteration: 65/1485.\n",
      "Iteration: 66/1485.\n",
      "Iteration: 67/1485.\n",
      "Iteration: 68/1485.\n",
      "Iteration: 69/1485.\n",
      "Iteration: 70/1485.\n",
      "Iteration: 71/1485.\n",
      "Iteration: 72/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.8713138259283328, 1.7955069263000594, 1.6889476411364643]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.272, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 73/1485.\n",
      "Iteration: 74/1485.\n",
      "Iteration: 75/1485.\n",
      "Iteration: 76/1485.\n",
      "Iteration: 77/1485.\n",
      "Iteration: 78/1485.\n",
      "Iteration: 79/1485.\n",
      "Iteration: 80/1485.\n",
      "Iteration: 81/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.7567686840432943, 1.6965920489419093, 1.7458815815852016]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.318, Val acc: 0.28\n",
      "\n",
      "\n",
      "Iteration: 82/1485.\n",
      "Iteration: 83/1485.\n",
      "Iteration: 84/1485.\n",
      "Iteration: 85/1485.\n",
      "Iteration: 86/1485.\n",
      "Iteration: 87/1485.\n",
      "Iteration: 88/1485.\n",
      "Iteration: 89/1485.\n",
      "Iteration: 90/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.7389414484073817, 1.7011669120591877, 1.8156279863183808]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.29, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 91/1485.\n",
      "Iteration: 92/1485.\n",
      "Iteration: 93/1485.\n",
      "Iteration: 94/1485.\n",
      "Iteration: 95/1485.\n",
      "Iteration: 96/1485.\n",
      "Iteration: 97/1485.\n",
      "Iteration: 98/1485.\n",
      "Iteration: 99/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.9274069749583553, 1.8873800604144972, 1.7088833850254161]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.334, Val acc: 0.24\n",
      "\n",
      "\n",
      "Iteration: 100/1485.\n",
      "Iteration: 101/1485.\n",
      "Iteration: 102/1485.\n",
      "Iteration: 103/1485.\n",
      "Iteration: 104/1485.\n",
      "Iteration: 105/1485.\n",
      "Iteration: 106/1485.\n",
      "Iteration: 107/1485.\n",
      "Iteration: 108/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.863986511838922, 1.8081012999828676, 1.6676946945846067]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.376, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 109/1485.\n",
      "Iteration: 110/1485.\n",
      "Iteration: 111/1485.\n",
      "Iteration: 112/1485.\n",
      "Iteration: 113/1485.\n",
      "Iteration: 114/1485.\n",
      "Iteration: 115/1485.\n",
      "Iteration: 116/1485.\n",
      "Iteration: 117/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.6219733721567067, 1.7128330874214317, 1.5341632519949886]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.352, Val acc: 0.28\n",
      "\n",
      "\n",
      "Iteration: 118/1485.\n",
      "Iteration: 119/1485.\n",
      "Iteration: 120/1485.\n",
      "Iteration: 121/1485.\n",
      "Iteration: 122/1485.\n",
      "Iteration: 123/1485.\n",
      "Iteration: 124/1485.\n",
      "Iteration: 125/1485.\n",
      "Iteration: 126/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.6069696622002971, 1.6439096581119195, 1.7111398015535093]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.396, Val acc: 0.24\n",
      "\n",
      "\n",
      "Iteration: 127/1485.\n",
      "Iteration: 128/1485.\n",
      "Iteration: 129/1485.\n",
      "Iteration: 130/1485.\n",
      "Iteration: 131/1485.\n",
      "Iteration: 132/1485.\n",
      "Iteration: 133/1485.\n",
      "Iteration: 134/1485.\n",
      "Iteration: 135/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.5806242607092535, 1.7096647472248643, 1.5342330944063138]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.402, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 136/1485.\n",
      "Iteration: 137/1485.\n",
      "Iteration: 138/1485.\n",
      "Iteration: 139/1485.\n",
      "Iteration: 140/1485.\n",
      "Iteration: 141/1485.\n",
      "Iteration: 142/1485.\n",
      "Iteration: 143/1485.\n",
      "Iteration: 144/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.6296330095848868, 1.6959605690987423, 1.8052635378946895]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.386, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 145/1485.\n",
      "Iteration: 146/1485.\n",
      "Iteration: 147/1485.\n",
      "Iteration: 148/1485.\n",
      "Iteration: 149/1485.\n",
      "Iteration: 150/1485.\n",
      "Iteration: 151/1485.\n",
      "Iteration: 152/1485.\n",
      "Iteration: 153/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.6294225771043833, 1.5482020995919052, 1.548528289052887]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.338, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 154/1485.\n",
      "Iteration: 155/1485.\n",
      "Iteration: 156/1485.\n",
      "Iteration: 157/1485.\n",
      "Iteration: 158/1485.\n",
      "Iteration: 159/1485.\n",
      "Iteration: 160/1485.\n",
      "Iteration: 161/1485.\n",
      "Iteration: 162/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.8316989218245507, 1.6444515150084973, 1.725503339953794]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.414, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 163/1485.\n",
      "Iteration: 164/1485.\n",
      "Iteration: 165/1485.\n",
      "Iteration: 166/1485.\n",
      "Iteration: 167/1485.\n",
      "Iteration: 168/1485.\n",
      "Iteration: 169/1485.\n",
      "Iteration: 170/1485.\n",
      "Iteration: 171/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.6340402272045598, 1.6038675933597402, 1.5697182636970235]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.404, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 172/1485.\n",
      "Iteration: 173/1485.\n",
      "Iteration: 174/1485.\n",
      "Iteration: 175/1485.\n",
      "Iteration: 176/1485.\n",
      "Iteration: 177/1485.\n",
      "Iteration: 178/1485.\n",
      "Iteration: 179/1485.\n",
      "Iteration: 180/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.5803337834338151, 1.555983173998955, 1.6059939735324416]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.406, Val acc: 0.28\n",
      "\n",
      "\n",
      "Iteration: 181/1485.\n",
      "Iteration: 182/1485.\n",
      "Iteration: 183/1485.\n",
      "Iteration: 184/1485.\n",
      "Iteration: 185/1485.\n",
      "Iteration: 186/1485.\n",
      "Iteration: 187/1485.\n",
      "Iteration: 188/1485.\n",
      "Iteration: 189/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.649501410829594, 1.5946770515586468, 1.6107250659666486]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.39, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 190/1485.\n",
      "Iteration: 191/1485.\n",
      "Iteration: 192/1485.\n",
      "Iteration: 193/1485.\n",
      "Iteration: 194/1485.\n",
      "Iteration: 195/1485.\n",
      "Iteration: 196/1485.\n",
      "Iteration: 197/1485.\n",
      "Iteration: 198/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.525963262379493, 1.493799444996079, 1.6490017191531163]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.418, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 199/1485.\n",
      "Iteration: 200/1485.\n",
      "Iteration: 201/1485.\n",
      "Iteration: 202/1485.\n",
      "Iteration: 203/1485.\n",
      "Iteration: 204/1485.\n",
      "Iteration: 205/1485.\n",
      "Iteration: 206/1485.\n",
      "Iteration: 207/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.5799511584422192, 1.450073748424584, 1.6379851323402128]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.432, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 208/1485.\n",
      "Iteration: 209/1485.\n",
      "Iteration: 210/1485.\n",
      "Iteration: 211/1485.\n",
      "Iteration: 212/1485.\n",
      "Iteration: 213/1485.\n",
      "Iteration: 214/1485.\n",
      "Iteration: 215/1485.\n",
      "Iteration: 216/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.5654845839737845, 1.6549128983792885, 1.64110642342924]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.414, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 217/1485.\n",
      "Iteration: 218/1485.\n",
      "Iteration: 219/1485.\n",
      "Iteration: 220/1485.\n",
      "Iteration: 221/1485.\n",
      "Iteration: 222/1485.\n",
      "Iteration: 223/1485.\n",
      "Iteration: 224/1485.\n",
      "Iteration: 225/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.5316561249903067, 1.5619478102278257, 1.468030418519422]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.468, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 226/1485.\n",
      "Iteration: 227/1485.\n",
      "Iteration: 228/1485.\n",
      "Iteration: 229/1485.\n",
      "Iteration: 230/1485.\n",
      "Iteration: 231/1485.\n",
      "Iteration: 232/1485.\n",
      "Iteration: 233/1485.\n",
      "Iteration: 234/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.7189008837405713, 1.5255966648834076, 1.5677429116465944]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.41, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 235/1485.\n",
      "Iteration: 236/1485.\n",
      "Iteration: 237/1485.\n",
      "Iteration: 238/1485.\n",
      "Iteration: 239/1485.\n",
      "Iteration: 240/1485.\n",
      "Iteration: 241/1485.\n",
      "Iteration: 242/1485.\n",
      "Iteration: 243/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.4973781735923053, 1.6261575105448294, 1.5757418731712807]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.482, Val acc: 0.28\n",
      "\n",
      "\n",
      "Iteration: 244/1485.\n",
      "Iteration: 245/1485.\n",
      "Iteration: 246/1485.\n",
      "Iteration: 247/1485.\n",
      "Iteration: 248/1485.\n",
      "Iteration: 249/1485.\n",
      "Iteration: 250/1485.\n",
      "Iteration: 251/1485.\n",
      "Iteration: 252/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.3706816194073659, 1.4708159870627358, 1.4817607212226338]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.436, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 253/1485.\n",
      "Iteration: 254/1485.\n",
      "Iteration: 255/1485.\n",
      "Iteration: 256/1485.\n",
      "Iteration: 257/1485.\n",
      "Iteration: 258/1485.\n",
      "Iteration: 259/1485.\n",
      "Iteration: 260/1485.\n",
      "Iteration: 261/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.5954789601827049, 1.5178185592342153, 1.4624802002200037]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.46, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 262/1485.\n",
      "Iteration: 263/1485.\n",
      "Iteration: 264/1485.\n",
      "Iteration: 265/1485.\n",
      "Iteration: 266/1485.\n",
      "Iteration: 267/1485.\n",
      "Iteration: 268/1485.\n",
      "Iteration: 269/1485.\n",
      "Iteration: 270/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.635116792894246, 1.4590214960990155, 1.4739889820749184]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.442, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 271/1485.\n",
      "Iteration: 272/1485.\n",
      "Iteration: 273/1485.\n",
      "Iteration: 274/1485.\n",
      "Iteration: 275/1485.\n",
      "Iteration: 276/1485.\n",
      "Iteration: 277/1485.\n",
      "Iteration: 278/1485.\n",
      "Iteration: 279/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.4428143711704333, 1.5143006287254912, 1.3628120032045115]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.474, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 280/1485.\n",
      "Iteration: 281/1485.\n",
      "Iteration: 282/1485.\n",
      "Iteration: 283/1485.\n",
      "Iteration: 284/1485.\n",
      "Iteration: 285/1485.\n",
      "Iteration: 286/1485.\n",
      "Iteration: 287/1485.\n",
      "Iteration: 288/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.52189342432878, 1.6304171742351117, 1.5862403317205696]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.444, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 289/1485.\n",
      "Iteration: 290/1485.\n",
      "Iteration: 291/1485.\n",
      "Iteration: 292/1485.\n",
      "Iteration: 293/1485.\n",
      "Iteration: 294/1485.\n",
      "Iteration: 295/1485.\n",
      "Iteration: 296/1485.\n",
      "Iteration: 297/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.4691281411904882, 1.4516466051001269, 1.6705112772884254]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.496, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 298/1485.\n",
      "Iteration: 299/1485.\n",
      "Iteration: 300/1485.\n",
      "Iteration: 301/1485.\n",
      "Iteration: 302/1485.\n",
      "Iteration: 303/1485.\n",
      "Iteration: 304/1485.\n",
      "Iteration: 305/1485.\n",
      "Iteration: 306/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.46923663605832, 1.3131847264536052, 1.4877058311490277]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.45, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 307/1485.\n",
      "Iteration: 308/1485.\n",
      "Iteration: 309/1485.\n",
      "Iteration: 310/1485.\n",
      "Iteration: 311/1485.\n",
      "Iteration: 312/1485.\n",
      "Iteration: 313/1485.\n",
      "Iteration: 314/1485.\n",
      "Iteration: 315/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.6188713462282516, 1.3494891086460343, 1.4374194012140133]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.452, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 316/1485.\n",
      "Iteration: 317/1485.\n",
      "Iteration: 318/1485.\n",
      "Iteration: 319/1485.\n",
      "Iteration: 320/1485.\n",
      "Iteration: 321/1485.\n",
      "Iteration: 322/1485.\n",
      "Iteration: 323/1485.\n",
      "Iteration: 324/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.4564177924287256, 1.3697666014458942, 1.433271857960979]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.528, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 325/1485.\n",
      "Iteration: 326/1485.\n",
      "Iteration: 327/1485.\n",
      "Iteration: 328/1485.\n",
      "Iteration: 329/1485.\n",
      "Iteration: 330/1485.\n",
      "Iteration: 331/1485.\n",
      "Iteration: 332/1485.\n",
      "Iteration: 333/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.3894159696525887, 1.3856795905885946, 1.2695774845562968]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.51, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 334/1485.\n",
      "Iteration: 335/1485.\n",
      "Iteration: 336/1485.\n",
      "Iteration: 337/1485.\n",
      "Iteration: 338/1485.\n",
      "Iteration: 339/1485.\n",
      "Iteration: 340/1485.\n",
      "Iteration: 341/1485.\n",
      "Iteration: 342/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.4195701213386298, 1.4313440721009159, 1.4730557075091628]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.52, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 343/1485.\n",
      "Iteration: 344/1485.\n",
      "Iteration: 345/1485.\n",
      "Iteration: 346/1485.\n",
      "Iteration: 347/1485.\n",
      "Iteration: 348/1485.\n",
      "Iteration: 349/1485.\n",
      "Iteration: 350/1485.\n",
      "Iteration: 351/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.3293508037281214, 1.3360627604651623, 1.2936916091746036]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.476, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 352/1485.\n",
      "Iteration: 353/1485.\n",
      "Iteration: 354/1485.\n",
      "Iteration: 355/1485.\n",
      "Iteration: 356/1485.\n",
      "Iteration: 357/1485.\n",
      "Iteration: 358/1485.\n",
      "Iteration: 359/1485.\n",
      "Iteration: 360/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.387369025174261, 1.211897283349107, 1.3435504339418671]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.518, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 361/1485.\n",
      "Iteration: 362/1485.\n",
      "Iteration: 363/1485.\n",
      "Iteration: 364/1485.\n",
      "Iteration: 365/1485.\n",
      "Iteration: 366/1485.\n",
      "Iteration: 367/1485.\n",
      "Iteration: 368/1485.\n",
      "Iteration: 369/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.3987736185409692, 1.2481585824599675, 1.2490419477133616]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.52, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 370/1485.\n",
      "Iteration: 371/1485.\n",
      "Iteration: 372/1485.\n",
      "Iteration: 373/1485.\n",
      "Iteration: 374/1485.\n",
      "Iteration: 375/1485.\n",
      "Iteration: 376/1485.\n",
      "Iteration: 377/1485.\n",
      "Iteration: 378/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.0887365888499854, 1.362445746010146, 1.4214905793810884]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.478, Val acc: 0.28\n",
      "\n",
      "\n",
      "Iteration: 379/1485.\n",
      "Iteration: 380/1485.\n",
      "Iteration: 381/1485.\n",
      "Iteration: 382/1485.\n",
      "Iteration: 383/1485.\n",
      "Iteration: 384/1485.\n",
      "Iteration: 385/1485.\n",
      "Iteration: 386/1485.\n",
      "Iteration: 387/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.2984031950702957, 1.1837436148702105, 1.090841180182948]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.572, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 388/1485.\n",
      "Iteration: 389/1485.\n",
      "Iteration: 390/1485.\n",
      "Iteration: 391/1485.\n",
      "Iteration: 392/1485.\n",
      "Iteration: 393/1485.\n",
      "Iteration: 394/1485.\n",
      "Iteration: 395/1485.\n",
      "Iteration: 396/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.2282265597661228, 1.129471901511195, 1.2002753118855447]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.516, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 397/1485.\n",
      "Iteration: 398/1485.\n",
      "Iteration: 399/1485.\n",
      "Iteration: 400/1485.\n",
      "Iteration: 401/1485.\n",
      "Iteration: 402/1485.\n",
      "Iteration: 403/1485.\n",
      "Iteration: 404/1485.\n",
      "Iteration: 405/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.4991626510471932, 1.19875933832956, 1.3651603446266987]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.566, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 406/1485.\n",
      "Iteration: 407/1485.\n",
      "Iteration: 408/1485.\n",
      "Iteration: 409/1485.\n",
      "Iteration: 410/1485.\n",
      "Iteration: 411/1485.\n",
      "Iteration: 412/1485.\n",
      "Iteration: 413/1485.\n",
      "Iteration: 414/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.2541477307202744, 1.1118544141468518, 1.2157576156743655]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.534, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 415/1485.\n",
      "Iteration: 416/1485.\n",
      "Iteration: 417/1485.\n",
      "Iteration: 418/1485.\n",
      "Iteration: 419/1485.\n",
      "Iteration: 420/1485.\n",
      "Iteration: 421/1485.\n",
      "Iteration: 422/1485.\n",
      "Iteration: 423/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.4145304911281549, 1.0754154341466238, 1.1523146479504656]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.522, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 424/1485.\n",
      "Iteration: 425/1485.\n",
      "Iteration: 426/1485.\n",
      "Iteration: 427/1485.\n",
      "Iteration: 428/1485.\n",
      "Iteration: 429/1485.\n",
      "Iteration: 430/1485.\n",
      "Iteration: 431/1485.\n",
      "Iteration: 432/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.3826517497053246, 1.0595883015947722, 1.227335357859271]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.546, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 433/1485.\n",
      "Iteration: 434/1485.\n",
      "Iteration: 435/1485.\n",
      "Iteration: 436/1485.\n",
      "Iteration: 437/1485.\n",
      "Iteration: 438/1485.\n",
      "Iteration: 439/1485.\n",
      "Iteration: 440/1485.\n",
      "Iteration: 441/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.2315474060427216, 1.1536445616826436, 1.0900985252758857]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.55, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 442/1485.\n",
      "Iteration: 443/1485.\n",
      "Iteration: 444/1485.\n",
      "Iteration: 445/1485.\n",
      "Iteration: 446/1485.\n",
      "Iteration: 447/1485.\n",
      "Iteration: 448/1485.\n",
      "Iteration: 449/1485.\n",
      "Iteration: 450/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.4535937426654102, 1.060671148329979, 1.193528659754004]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.604, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 451/1485.\n",
      "Iteration: 452/1485.\n",
      "Iteration: 453/1485.\n",
      "Iteration: 454/1485.\n",
      "Iteration: 455/1485.\n",
      "Iteration: 456/1485.\n",
      "Iteration: 457/1485.\n",
      "Iteration: 458/1485.\n",
      "Iteration: 459/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.3936511758813577, 1.187466209209959, 1.218738082400651]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.588, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 460/1485.\n",
      "Iteration: 461/1485.\n",
      "Iteration: 462/1485.\n",
      "Iteration: 463/1485.\n",
      "Iteration: 464/1485.\n",
      "Iteration: 465/1485.\n",
      "Iteration: 466/1485.\n",
      "Iteration: 467/1485.\n",
      "Iteration: 468/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.062716205729205, 1.0130634868522739, 1.366040306780987]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.582, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 469/1485.\n",
      "Iteration: 470/1485.\n",
      "Iteration: 471/1485.\n",
      "Iteration: 472/1485.\n",
      "Iteration: 473/1485.\n",
      "Iteration: 474/1485.\n",
      "Iteration: 475/1485.\n",
      "Iteration: 476/1485.\n",
      "Iteration: 477/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.07431376395428, 0.9671699950252551, 1.3049261417604723]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.562, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 478/1485.\n",
      "Iteration: 479/1485.\n",
      "Iteration: 480/1485.\n",
      "Iteration: 481/1485.\n",
      "Iteration: 482/1485.\n",
      "Iteration: 483/1485.\n",
      "Iteration: 484/1485.\n",
      "Iteration: 485/1485.\n",
      "Iteration: 486/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.1753615237512816, 1.2561370062926074, 1.1771143675055868]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.594, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 487/1485.\n",
      "Iteration: 488/1485.\n",
      "Iteration: 489/1485.\n",
      "Iteration: 490/1485.\n",
      "Iteration: 491/1485.\n",
      "Iteration: 492/1485.\n",
      "Iteration: 493/1485.\n",
      "Iteration: 494/1485.\n",
      "Iteration: 495/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.1758644736680364, 1.0648070020692335, 1.2548724418804487]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.544, Val acc: 0.28\n",
      "\n",
      "\n",
      "Iteration: 496/1485.\n",
      "Iteration: 497/1485.\n",
      "Iteration: 498/1485.\n",
      "Iteration: 499/1485.\n",
      "Iteration: 500/1485.\n",
      "Iteration: 501/1485.\n",
      "Iteration: 502/1485.\n",
      "Iteration: 503/1485.\n",
      "Iteration: 504/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.4320293679605451, 1.2869255653164777, 1.0207086034883153]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.606, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 505/1485.\n",
      "Iteration: 506/1485.\n",
      "Iteration: 507/1485.\n",
      "Iteration: 508/1485.\n",
      "Iteration: 509/1485.\n",
      "Iteration: 510/1485.\n",
      "Iteration: 511/1485.\n",
      "Iteration: 512/1485.\n",
      "Iteration: 513/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.223443363496981, 0.9049800909074748, 1.0459767088796108]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.56, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 514/1485.\n",
      "Iteration: 515/1485.\n",
      "Iteration: 516/1485.\n",
      "Iteration: 517/1485.\n",
      "Iteration: 518/1485.\n",
      "Iteration: 519/1485.\n",
      "Iteration: 520/1485.\n",
      "Iteration: 521/1485.\n",
      "Iteration: 522/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.1222914830903241, 1.126026410804883, 1.0349751923188444]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.624, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 523/1485.\n",
      "Iteration: 524/1485.\n",
      "Iteration: 525/1485.\n",
      "Iteration: 526/1485.\n",
      "Iteration: 527/1485.\n",
      "Iteration: 528/1485.\n",
      "Iteration: 529/1485.\n",
      "Iteration: 530/1485.\n",
      "Iteration: 531/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.0582901406033838, 1.0838559914163592, 1.209941343038112]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.626, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 532/1485.\n",
      "Iteration: 533/1485.\n",
      "Iteration: 534/1485.\n",
      "Iteration: 535/1485.\n",
      "Iteration: 536/1485.\n",
      "Iteration: 537/1485.\n",
      "Iteration: 538/1485.\n",
      "Iteration: 539/1485.\n",
      "Iteration: 540/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.1370380243251417, 1.1903374250203476, 0.8663253843170753]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.59, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 541/1485.\n",
      "Iteration: 542/1485.\n",
      "Iteration: 543/1485.\n",
      "Iteration: 544/1485.\n",
      "Iteration: 545/1485.\n",
      "Iteration: 546/1485.\n",
      "Iteration: 547/1485.\n",
      "Iteration: 548/1485.\n",
      "Iteration: 549/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.0633333056429681, 0.9723868328474599, 1.119915923502987]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.652, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 550/1485.\n",
      "Iteration: 551/1485.\n",
      "Iteration: 552/1485.\n",
      "Iteration: 553/1485.\n",
      "Iteration: 554/1485.\n",
      "Iteration: 555/1485.\n",
      "Iteration: 556/1485.\n",
      "Iteration: 557/1485.\n",
      "Iteration: 558/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.1416218091304755, 0.9331087220000985, 0.9706742526129272]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.672, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 559/1485.\n",
      "Iteration: 560/1485.\n",
      "Iteration: 561/1485.\n",
      "Iteration: 562/1485.\n",
      "Iteration: 563/1485.\n",
      "Iteration: 564/1485.\n",
      "Iteration: 565/1485.\n",
      "Iteration: 566/1485.\n",
      "Iteration: 567/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.0301961091439424, 1.1591819978475801, 1.0026488734325962]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.622, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 568/1485.\n",
      "Iteration: 569/1485.\n",
      "Iteration: 570/1485.\n",
      "Iteration: 571/1485.\n",
      "Iteration: 572/1485.\n",
      "Iteration: 573/1485.\n",
      "Iteration: 574/1485.\n",
      "Iteration: 575/1485.\n",
      "Iteration: 576/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.0379184973678475, 0.8279725732651524, 1.007161561202339]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.65, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 577/1485.\n",
      "Iteration: 578/1485.\n",
      "Iteration: 579/1485.\n",
      "Iteration: 580/1485.\n",
      "Iteration: 581/1485.\n",
      "Iteration: 582/1485.\n",
      "Iteration: 583/1485.\n",
      "Iteration: 584/1485.\n",
      "Iteration: 585/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.0292422617933639, 1.040769688359334, 0.9276795381347112]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.606, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 586/1485.\n",
      "Iteration: 587/1485.\n",
      "Iteration: 588/1485.\n",
      "Iteration: 589/1485.\n",
      "Iteration: 590/1485.\n",
      "Iteration: 591/1485.\n",
      "Iteration: 592/1485.\n",
      "Iteration: 593/1485.\n",
      "Iteration: 594/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.1790487478086282, 1.0012386807355638, 0.9356150034205255]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.67, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 595/1485.\n",
      "Iteration: 596/1485.\n",
      "Iteration: 597/1485.\n",
      "Iteration: 598/1485.\n",
      "Iteration: 599/1485.\n",
      "Iteration: 600/1485.\n",
      "Iteration: 601/1485.\n",
      "Iteration: 602/1485.\n",
      "Iteration: 603/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.0163871660237214, 0.9135645946502653, 0.9162050088966802]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.686, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 604/1485.\n",
      "Iteration: 605/1485.\n",
      "Iteration: 606/1485.\n",
      "Iteration: 607/1485.\n",
      "Iteration: 608/1485.\n",
      "Iteration: 609/1485.\n",
      "Iteration: 610/1485.\n",
      "Iteration: 611/1485.\n",
      "Iteration: 612/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.8735775373628054, 1.0825669479085038, 0.8408966123134064]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.632, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 613/1485.\n",
      "Iteration: 614/1485.\n",
      "Iteration: 615/1485.\n",
      "Iteration: 616/1485.\n",
      "Iteration: 617/1485.\n",
      "Iteration: 618/1485.\n",
      "Iteration: 619/1485.\n",
      "Iteration: 620/1485.\n",
      "Iteration: 621/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.8192405677821349, 1.0565702508607162, 0.9266421280887407]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.634, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 622/1485.\n",
      "Iteration: 623/1485.\n",
      "Iteration: 624/1485.\n",
      "Iteration: 625/1485.\n",
      "Iteration: 626/1485.\n",
      "Iteration: 627/1485.\n",
      "Iteration: 628/1485.\n",
      "Iteration: 629/1485.\n",
      "Iteration: 630/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [1.025727398159365, 1.0448913391498087, 0.8721447828504879]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.72, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 631/1485.\n",
      "Iteration: 632/1485.\n",
      "Iteration: 633/1485.\n",
      "Iteration: 634/1485.\n",
      "Iteration: 635/1485.\n",
      "Iteration: 636/1485.\n",
      "Iteration: 637/1485.\n",
      "Iteration: 638/1485.\n",
      "Iteration: 639/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.793852855124385, 0.8274864409708039, 0.8890060237324129]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.684, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 640/1485.\n",
      "Iteration: 641/1485.\n",
      "Iteration: 642/1485.\n",
      "Iteration: 643/1485.\n",
      "Iteration: 644/1485.\n",
      "Iteration: 645/1485.\n",
      "Iteration: 646/1485.\n",
      "Iteration: 647/1485.\n",
      "Iteration: 648/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.9295606777920393, 0.9479273521514229, 1.0242152517259728]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.698, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 649/1485.\n",
      "Iteration: 650/1485.\n",
      "Iteration: 651/1485.\n",
      "Iteration: 652/1485.\n",
      "Iteration: 653/1485.\n",
      "Iteration: 654/1485.\n",
      "Iteration: 655/1485.\n",
      "Iteration: 656/1485.\n",
      "Iteration: 657/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.8590537098336725, 0.8645991430356483, 0.8581687741996389]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.71, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 658/1485.\n",
      "Iteration: 659/1485.\n",
      "Iteration: 660/1485.\n",
      "Iteration: 661/1485.\n",
      "Iteration: 662/1485.\n",
      "Iteration: 663/1485.\n",
      "Iteration: 664/1485.\n",
      "Iteration: 665/1485.\n",
      "Iteration: 666/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.869601757707213, 0.7851657845726169, 0.8156039282452053]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.674, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 667/1485.\n",
      "Iteration: 668/1485.\n",
      "Iteration: 669/1485.\n",
      "Iteration: 670/1485.\n",
      "Iteration: 671/1485.\n",
      "Iteration: 672/1485.\n",
      "Iteration: 673/1485.\n",
      "Iteration: 674/1485.\n",
      "Iteration: 675/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.8308562779783368, 0.8544938979129149, 0.8545536879653439]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.7, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 676/1485.\n",
      "Iteration: 677/1485.\n",
      "Iteration: 678/1485.\n",
      "Iteration: 679/1485.\n",
      "Iteration: 680/1485.\n",
      "Iteration: 681/1485.\n",
      "Iteration: 682/1485.\n",
      "Iteration: 683/1485.\n",
      "Iteration: 684/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.8333101717227327, 0.8202027955073301, 0.9285965223897445]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.698, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 685/1485.\n",
      "Iteration: 686/1485.\n",
      "Iteration: 687/1485.\n",
      "Iteration: 688/1485.\n",
      "Iteration: 689/1485.\n",
      "Iteration: 690/1485.\n",
      "Iteration: 691/1485.\n",
      "Iteration: 692/1485.\n",
      "Iteration: 693/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.9980562513146078, 0.8723755361375795, 0.8313104985533412]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.726, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 694/1485.\n",
      "Iteration: 695/1485.\n",
      "Iteration: 696/1485.\n",
      "Iteration: 697/1485.\n",
      "Iteration: 698/1485.\n",
      "Iteration: 699/1485.\n",
      "Iteration: 700/1485.\n",
      "Iteration: 701/1485.\n",
      "Iteration: 702/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.8862756521577758, 0.9240733930213564, 0.8010665717557567]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.694, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 703/1485.\n",
      "Iteration: 704/1485.\n",
      "Iteration: 705/1485.\n",
      "Iteration: 706/1485.\n",
      "Iteration: 707/1485.\n",
      "Iteration: 708/1485.\n",
      "Iteration: 709/1485.\n",
      "Iteration: 710/1485.\n",
      "Iteration: 711/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.7935839824693405, 0.9314521955199005, 0.7329141085806978]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.732, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 712/1485.\n",
      "Iteration: 713/1485.\n",
      "Iteration: 714/1485.\n",
      "Iteration: 715/1485.\n",
      "Iteration: 716/1485.\n",
      "Iteration: 717/1485.\n",
      "Iteration: 718/1485.\n",
      "Iteration: 719/1485.\n",
      "Iteration: 720/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.7381318070488017, 0.626787576810444, 0.7131899973434586]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.774, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 721/1485.\n",
      "Iteration: 722/1485.\n",
      "Iteration: 723/1485.\n",
      "Iteration: 724/1485.\n",
      "Iteration: 725/1485.\n",
      "Iteration: 726/1485.\n",
      "Iteration: 727/1485.\n",
      "Iteration: 728/1485.\n",
      "Iteration: 729/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.6299960738063441, 0.6793626857112711, 0.6694559221958398]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.724, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 730/1485.\n",
      "Iteration: 731/1485.\n",
      "Iteration: 732/1485.\n",
      "Iteration: 733/1485.\n",
      "Iteration: 734/1485.\n",
      "Iteration: 735/1485.\n",
      "Iteration: 736/1485.\n",
      "Iteration: 737/1485.\n",
      "Iteration: 738/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.7193609439896008, 0.6132192164670375, 0.6313487980075201]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.788, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 739/1485.\n",
      "Iteration: 740/1485.\n",
      "Iteration: 741/1485.\n",
      "Iteration: 742/1485.\n",
      "Iteration: 743/1485.\n",
      "Iteration: 744/1485.\n",
      "Iteration: 745/1485.\n",
      "Iteration: 746/1485.\n",
      "Iteration: 747/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.7177371294282969, 0.7353813178775118, 0.550942070650297]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.786, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 748/1485.\n",
      "Iteration: 749/1485.\n",
      "Iteration: 750/1485.\n",
      "Iteration: 751/1485.\n",
      "Iteration: 752/1485.\n",
      "Iteration: 753/1485.\n",
      "Iteration: 754/1485.\n",
      "Iteration: 755/1485.\n",
      "Iteration: 756/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.5845712133165347, 0.7679720510753739, 0.6548307683112755]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.766, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 757/1485.\n",
      "Iteration: 758/1485.\n",
      "Iteration: 759/1485.\n",
      "Iteration: 760/1485.\n",
      "Iteration: 761/1485.\n",
      "Iteration: 762/1485.\n",
      "Iteration: 763/1485.\n",
      "Iteration: 764/1485.\n",
      "Iteration: 765/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.8623679337022744, 0.7531582807093351, 0.7476478970441067]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.768, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 766/1485.\n",
      "Iteration: 767/1485.\n",
      "Iteration: 768/1485.\n",
      "Iteration: 769/1485.\n",
      "Iteration: 770/1485.\n",
      "Iteration: 771/1485.\n",
      "Iteration: 772/1485.\n",
      "Iteration: 773/1485.\n",
      "Iteration: 774/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.7526260215888264, 0.6878193269119095, 0.7918336645704683]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.738, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 775/1485.\n",
      "Iteration: 776/1485.\n",
      "Iteration: 777/1485.\n",
      "Iteration: 778/1485.\n",
      "Iteration: 779/1485.\n",
      "Iteration: 780/1485.\n",
      "Iteration: 781/1485.\n",
      "Iteration: 782/1485.\n",
      "Iteration: 783/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.8577635205047867, 0.6721186454930205, 0.7980122588529233]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.71, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 784/1485.\n",
      "Iteration: 785/1485.\n",
      "Iteration: 786/1485.\n",
      "Iteration: 787/1485.\n",
      "Iteration: 788/1485.\n",
      "Iteration: 789/1485.\n",
      "Iteration: 790/1485.\n",
      "Iteration: 791/1485.\n",
      "Iteration: 792/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.6965247711749123, 0.7146381804914721, 0.7313460216931903]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.778, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 793/1485.\n",
      "Iteration: 794/1485.\n",
      "Iteration: 795/1485.\n",
      "Iteration: 796/1485.\n",
      "Iteration: 797/1485.\n",
      "Iteration: 798/1485.\n",
      "Iteration: 799/1485.\n",
      "Iteration: 800/1485.\n",
      "Iteration: 801/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.639104088996295, 0.6826142676426318, 0.5828113372349282]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.772, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 802/1485.\n",
      "Iteration: 803/1485.\n",
      "Iteration: 804/1485.\n",
      "Iteration: 805/1485.\n",
      "Iteration: 806/1485.\n",
      "Iteration: 807/1485.\n",
      "Iteration: 808/1485.\n",
      "Iteration: 809/1485.\n",
      "Iteration: 810/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.6368453196068328, 0.7231150540486618, 0.7104476307968893]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.756, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 811/1485.\n",
      "Iteration: 812/1485.\n",
      "Iteration: 813/1485.\n",
      "Iteration: 814/1485.\n",
      "Iteration: 815/1485.\n",
      "Iteration: 816/1485.\n",
      "Iteration: 817/1485.\n",
      "Iteration: 818/1485.\n",
      "Iteration: 819/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.5989734542430026, 0.6152798316839427, 0.5688693584918804]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.814, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 820/1485.\n",
      "Iteration: 821/1485.\n",
      "Iteration: 822/1485.\n",
      "Iteration: 823/1485.\n",
      "Iteration: 824/1485.\n",
      "Iteration: 825/1485.\n",
      "Iteration: 826/1485.\n",
      "Iteration: 827/1485.\n",
      "Iteration: 828/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.4980242108256734, 0.5783364495487253, 0.4981188975770872]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.806, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 829/1485.\n",
      "Iteration: 830/1485.\n",
      "Iteration: 831/1485.\n",
      "Iteration: 832/1485.\n",
      "Iteration: 833/1485.\n",
      "Iteration: 834/1485.\n",
      "Iteration: 835/1485.\n",
      "Iteration: 836/1485.\n",
      "Iteration: 837/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.4435408364788585, 0.6383947157265071, 0.4793543644921768]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.796, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 838/1485.\n",
      "Iteration: 839/1485.\n",
      "Iteration: 840/1485.\n",
      "Iteration: 841/1485.\n",
      "Iteration: 842/1485.\n",
      "Iteration: 843/1485.\n",
      "Iteration: 844/1485.\n",
      "Iteration: 845/1485.\n",
      "Iteration: 846/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.47646104295045144, 0.5684652589339815, 0.5768828558804612]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.822, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 847/1485.\n",
      "Iteration: 848/1485.\n",
      "Iteration: 849/1485.\n",
      "Iteration: 850/1485.\n",
      "Iteration: 851/1485.\n",
      "Iteration: 852/1485.\n",
      "Iteration: 853/1485.\n",
      "Iteration: 854/1485.\n",
      "Iteration: 855/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.531985959688382, 0.5474020155731396, 0.5573966772564117]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.834, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 856/1485.\n",
      "Iteration: 857/1485.\n",
      "Iteration: 858/1485.\n",
      "Iteration: 859/1485.\n",
      "Iteration: 860/1485.\n",
      "Iteration: 861/1485.\n",
      "Iteration: 862/1485.\n",
      "Iteration: 863/1485.\n",
      "Iteration: 864/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.5557405582360023, 0.5386028820064088, 0.39747268386800877]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.84, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 865/1485.\n",
      "Iteration: 866/1485.\n",
      "Iteration: 867/1485.\n",
      "Iteration: 868/1485.\n",
      "Iteration: 869/1485.\n",
      "Iteration: 870/1485.\n",
      "Iteration: 871/1485.\n",
      "Iteration: 872/1485.\n",
      "Iteration: 873/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.42472563673277636, 0.4610863083012513, 0.5980529660647144]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.79, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 874/1485.\n",
      "Iteration: 875/1485.\n",
      "Iteration: 876/1485.\n",
      "Iteration: 877/1485.\n",
      "Iteration: 878/1485.\n",
      "Iteration: 879/1485.\n",
      "Iteration: 880/1485.\n",
      "Iteration: 881/1485.\n",
      "Iteration: 882/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.4287465498045255, 0.4241252067516426, 0.4376360517972979]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.778, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 883/1485.\n",
      "Iteration: 884/1485.\n",
      "Iteration: 885/1485.\n",
      "Iteration: 886/1485.\n",
      "Iteration: 887/1485.\n",
      "Iteration: 888/1485.\n",
      "Iteration: 889/1485.\n",
      "Iteration: 890/1485.\n",
      "Iteration: 891/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.40777876469910984, 0.6828815456983636, 0.6274070248514514]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.806, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 892/1485.\n",
      "Iteration: 893/1485.\n",
      "Iteration: 894/1485.\n",
      "Iteration: 895/1485.\n",
      "Iteration: 896/1485.\n",
      "Iteration: 897/1485.\n",
      "Iteration: 898/1485.\n",
      "Iteration: 899/1485.\n",
      "Iteration: 900/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.44385785168123776, 0.4442954147836993, 0.626604113835686]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.81, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 901/1485.\n",
      "Iteration: 902/1485.\n",
      "Iteration: 903/1485.\n",
      "Iteration: 904/1485.\n",
      "Iteration: 905/1485.\n",
      "Iteration: 906/1485.\n",
      "Iteration: 907/1485.\n",
      "Iteration: 908/1485.\n",
      "Iteration: 909/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.7406809602505121, 0.4762650008054573, 0.4792868584014812]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.804, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 910/1485.\n",
      "Iteration: 911/1485.\n",
      "Iteration: 912/1485.\n",
      "Iteration: 913/1485.\n",
      "Iteration: 914/1485.\n",
      "Iteration: 915/1485.\n",
      "Iteration: 916/1485.\n",
      "Iteration: 917/1485.\n",
      "Iteration: 918/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.5075829051456716, 0.5306021821095397, 0.5096204988090773]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.812, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 919/1485.\n",
      "Iteration: 920/1485.\n",
      "Iteration: 921/1485.\n",
      "Iteration: 922/1485.\n",
      "Iteration: 923/1485.\n",
      "Iteration: 924/1485.\n",
      "Iteration: 925/1485.\n",
      "Iteration: 926/1485.\n",
      "Iteration: 927/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.6167490868899256, 0.44809564185301415, 0.4155336768544136]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.868, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 928/1485.\n",
      "Iteration: 929/1485.\n",
      "Iteration: 930/1485.\n",
      "Iteration: 931/1485.\n",
      "Iteration: 932/1485.\n",
      "Iteration: 933/1485.\n",
      "Iteration: 934/1485.\n",
      "Iteration: 935/1485.\n",
      "Iteration: 936/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.4138330921432418, 0.4004807084282942, 0.3964206891086724]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.85, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 937/1485.\n",
      "Iteration: 938/1485.\n",
      "Iteration: 939/1485.\n",
      "Iteration: 940/1485.\n",
      "Iteration: 941/1485.\n",
      "Iteration: 942/1485.\n",
      "Iteration: 943/1485.\n",
      "Iteration: 944/1485.\n",
      "Iteration: 945/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.3990630216370083, 0.4413034779850672, 0.5189559029172207]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.886, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 946/1485.\n",
      "Iteration: 947/1485.\n",
      "Iteration: 948/1485.\n",
      "Iteration: 949/1485.\n",
      "Iteration: 950/1485.\n",
      "Iteration: 951/1485.\n",
      "Iteration: 952/1485.\n",
      "Iteration: 953/1485.\n",
      "Iteration: 954/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.5154494752880876, 0.5931571084639399, 0.3839912272728465]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.854, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 955/1485.\n",
      "Iteration: 956/1485.\n",
      "Iteration: 957/1485.\n",
      "Iteration: 958/1485.\n",
      "Iteration: 959/1485.\n",
      "Iteration: 960/1485.\n",
      "Iteration: 961/1485.\n",
      "Iteration: 962/1485.\n",
      "Iteration: 963/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.332314732454535, 0.46277499726996374, 0.4484982816279891]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.842, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 964/1485.\n",
      "Iteration: 965/1485.\n",
      "Iteration: 966/1485.\n",
      "Iteration: 967/1485.\n",
      "Iteration: 968/1485.\n",
      "Iteration: 969/1485.\n",
      "Iteration: 970/1485.\n",
      "Iteration: 971/1485.\n",
      "Iteration: 972/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.33735028004138434, 0.4418998904623001, 0.36189722062509105]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.87, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 973/1485.\n",
      "Iteration: 974/1485.\n",
      "Iteration: 975/1485.\n",
      "Iteration: 976/1485.\n",
      "Iteration: 977/1485.\n",
      "Iteration: 978/1485.\n",
      "Iteration: 979/1485.\n",
      "Iteration: 980/1485.\n",
      "Iteration: 981/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.34932078417773205, 0.4715409611894055, 0.31687617129566253]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.876, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 982/1485.\n",
      "Iteration: 983/1485.\n",
      "Iteration: 984/1485.\n",
      "Iteration: 985/1485.\n",
      "Iteration: 986/1485.\n",
      "Iteration: 987/1485.\n",
      "Iteration: 988/1485.\n",
      "Iteration: 989/1485.\n",
      "Iteration: 990/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.46629172925683504, 0.43500938267860223, 0.32719748896498474]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.896, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 991/1485.\n",
      "Iteration: 992/1485.\n",
      "Iteration: 993/1485.\n",
      "Iteration: 994/1485.\n",
      "Iteration: 995/1485.\n",
      "Iteration: 996/1485.\n",
      "Iteration: 997/1485.\n",
      "Iteration: 998/1485.\n",
      "Iteration: 999/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.4421148684370541, 0.3607330391216507, 0.4486438599793833]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.89, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 1000/1485.\n",
      "Iteration: 1001/1485.\n",
      "Iteration: 1002/1485.\n",
      "Iteration: 1003/1485.\n",
      "Iteration: 1004/1485.\n",
      "Iteration: 1005/1485.\n",
      "Iteration: 1006/1485.\n",
      "Iteration: 1007/1485.\n",
      "Iteration: 1008/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.3413528471978022, 0.39361860238420593, 0.39597130982420076]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.878, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1009/1485.\n",
      "Iteration: 1010/1485.\n",
      "Iteration: 1011/1485.\n",
      "Iteration: 1012/1485.\n",
      "Iteration: 1013/1485.\n",
      "Iteration: 1014/1485.\n",
      "Iteration: 1015/1485.\n",
      "Iteration: 1016/1485.\n",
      "Iteration: 1017/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.3065744429891899, 0.32602204363426135, 0.382704088582032]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.892, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1018/1485.\n",
      "Iteration: 1019/1485.\n",
      "Iteration: 1020/1485.\n",
      "Iteration: 1021/1485.\n",
      "Iteration: 1022/1485.\n",
      "Iteration: 1023/1485.\n",
      "Iteration: 1024/1485.\n",
      "Iteration: 1025/1485.\n",
      "Iteration: 1026/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.3840850637426398, 0.30565370673018627, 0.49172776704844334]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.854, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 1027/1485.\n",
      "Iteration: 1028/1485.\n",
      "Iteration: 1029/1485.\n",
      "Iteration: 1030/1485.\n",
      "Iteration: 1031/1485.\n",
      "Iteration: 1032/1485.\n",
      "Iteration: 1033/1485.\n",
      "Iteration: 1034/1485.\n",
      "Iteration: 1035/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.34073264520418, 0.34624506712926595, 0.5219015542564746]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.884, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 1036/1485.\n",
      "Iteration: 1037/1485.\n",
      "Iteration: 1038/1485.\n",
      "Iteration: 1039/1485.\n",
      "Iteration: 1040/1485.\n",
      "Iteration: 1041/1485.\n",
      "Iteration: 1042/1485.\n",
      "Iteration: 1043/1485.\n",
      "Iteration: 1044/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.354640770072584, 0.34502209559585517, 0.40187122292271854]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.836, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 1045/1485.\n",
      "Iteration: 1046/1485.\n",
      "Iteration: 1047/1485.\n",
      "Iteration: 1048/1485.\n",
      "Iteration: 1049/1485.\n",
      "Iteration: 1050/1485.\n",
      "Iteration: 1051/1485.\n",
      "Iteration: 1052/1485.\n",
      "Iteration: 1053/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.4998907853967885, 0.31561157523742484, 0.2838023896736323]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.89, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 1054/1485.\n",
      "Iteration: 1055/1485.\n",
      "Iteration: 1056/1485.\n",
      "Iteration: 1057/1485.\n",
      "Iteration: 1058/1485.\n",
      "Iteration: 1059/1485.\n",
      "Iteration: 1060/1485.\n",
      "Iteration: 1061/1485.\n",
      "Iteration: 1062/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.36714659729040633, 0.2279914903072519, 0.3797078430551488]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.886, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 1063/1485.\n",
      "Iteration: 1064/1485.\n",
      "Iteration: 1065/1485.\n",
      "Iteration: 1066/1485.\n",
      "Iteration: 1067/1485.\n",
      "Iteration: 1068/1485.\n",
      "Iteration: 1069/1485.\n",
      "Iteration: 1070/1485.\n",
      "Iteration: 1071/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.344020575567682, 0.2918875538625437, 0.23148370526792839]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.926, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 1072/1485.\n",
      "Iteration: 1073/1485.\n",
      "Iteration: 1074/1485.\n",
      "Iteration: 1075/1485.\n",
      "Iteration: 1076/1485.\n",
      "Iteration: 1077/1485.\n",
      "Iteration: 1078/1485.\n",
      "Iteration: 1079/1485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1080/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.22669147492620803, 0.37441839611592825, 0.26350387268695524]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.914, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1081/1485.\n",
      "Iteration: 1082/1485.\n",
      "Iteration: 1083/1485.\n",
      "Iteration: 1084/1485.\n",
      "Iteration: 1085/1485.\n",
      "Iteration: 1086/1485.\n",
      "Iteration: 1087/1485.\n",
      "Iteration: 1088/1485.\n",
      "Iteration: 1089/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.2698089483308378, 0.2631008289572684, 0.24702698256726016]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.922, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 1090/1485.\n",
      "Iteration: 1091/1485.\n",
      "Iteration: 1092/1485.\n",
      "Iteration: 1093/1485.\n",
      "Iteration: 1094/1485.\n",
      "Iteration: 1095/1485.\n",
      "Iteration: 1096/1485.\n",
      "Iteration: 1097/1485.\n",
      "Iteration: 1098/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.2274657433622706, 0.2368520739575898, 0.2748440722174541]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.924, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 1099/1485.\n",
      "Iteration: 1100/1485.\n",
      "Iteration: 1101/1485.\n",
      "Iteration: 1102/1485.\n",
      "Iteration: 1103/1485.\n",
      "Iteration: 1104/1485.\n",
      "Iteration: 1105/1485.\n",
      "Iteration: 1106/1485.\n",
      "Iteration: 1107/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.3237206702439238, 0.21084776157652524, 0.2625739114672594]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.914, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 1108/1485.\n",
      "Iteration: 1109/1485.\n",
      "Iteration: 1110/1485.\n",
      "Iteration: 1111/1485.\n",
      "Iteration: 1112/1485.\n",
      "Iteration: 1113/1485.\n",
      "Iteration: 1114/1485.\n",
      "Iteration: 1115/1485.\n",
      "Iteration: 1116/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.24188381194408562, 0.2505216464671312, 0.25162910929892546]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.95, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1117/1485.\n",
      "Iteration: 1118/1485.\n",
      "Iteration: 1119/1485.\n",
      "Iteration: 1120/1485.\n",
      "Iteration: 1121/1485.\n",
      "Iteration: 1122/1485.\n",
      "Iteration: 1123/1485.\n",
      "Iteration: 1124/1485.\n",
      "Iteration: 1125/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.305897924750645, 0.23400503609310133, 0.2329059768970425]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.954, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1126/1485.\n",
      "Iteration: 1127/1485.\n",
      "Iteration: 1128/1485.\n",
      "Iteration: 1129/1485.\n",
      "Iteration: 1130/1485.\n",
      "Iteration: 1131/1485.\n",
      "Iteration: 1132/1485.\n",
      "Iteration: 1133/1485.\n",
      "Iteration: 1134/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.27609226574918777, 0.21837335549259493, 0.29578250763373853]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.958, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 1135/1485.\n",
      "Iteration: 1136/1485.\n",
      "Iteration: 1137/1485.\n",
      "Iteration: 1138/1485.\n",
      "Iteration: 1139/1485.\n",
      "Iteration: 1140/1485.\n",
      "Iteration: 1141/1485.\n",
      "Iteration: 1142/1485.\n",
      "Iteration: 1143/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.13208862495861184, 0.22730449901527777, 0.17124626767534157]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.938, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 1144/1485.\n",
      "Iteration: 1145/1485.\n",
      "Iteration: 1146/1485.\n",
      "Iteration: 1147/1485.\n",
      "Iteration: 1148/1485.\n",
      "Iteration: 1149/1485.\n",
      "Iteration: 1150/1485.\n",
      "Iteration: 1151/1485.\n",
      "Iteration: 1152/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.21182912390110964, 0.1679783999665914, 0.14881153869123206]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.958, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 1153/1485.\n",
      "Iteration: 1154/1485.\n",
      "Iteration: 1155/1485.\n",
      "Iteration: 1156/1485.\n",
      "Iteration: 1157/1485.\n",
      "Iteration: 1158/1485.\n",
      "Iteration: 1159/1485.\n",
      "Iteration: 1160/1485.\n",
      "Iteration: 1161/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.1834132220570344, 0.2678797330054663, 0.15574729826710382]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.966, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1162/1485.\n",
      "Iteration: 1163/1485.\n",
      "Iteration: 1164/1485.\n",
      "Iteration: 1165/1485.\n",
      "Iteration: 1166/1485.\n",
      "Iteration: 1167/1485.\n",
      "Iteration: 1168/1485.\n",
      "Iteration: 1169/1485.\n",
      "Iteration: 1170/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.19369516549976115, 0.21824072793573265, 0.20184950949432115]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.948, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 1171/1485.\n",
      "Iteration: 1172/1485.\n",
      "Iteration: 1173/1485.\n",
      "Iteration: 1174/1485.\n",
      "Iteration: 1175/1485.\n",
      "Iteration: 1176/1485.\n",
      "Iteration: 1177/1485.\n",
      "Iteration: 1178/1485.\n",
      "Iteration: 1179/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.1398266021124382, 0.1673959796345756, 0.21982760787202069]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.97, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 1180/1485.\n",
      "Iteration: 1181/1485.\n",
      "Iteration: 1182/1485.\n",
      "Iteration: 1183/1485.\n",
      "Iteration: 1184/1485.\n",
      "Iteration: 1185/1485.\n",
      "Iteration: 1186/1485.\n",
      "Iteration: 1187/1485.\n",
      "Iteration: 1188/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.23172947910222633, 0.14747359971322105, 0.1444148497199663]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.952, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1189/1485.\n",
      "Iteration: 1190/1485.\n",
      "Iteration: 1191/1485.\n",
      "Iteration: 1192/1485.\n",
      "Iteration: 1193/1485.\n",
      "Iteration: 1194/1485.\n",
      "Iteration: 1195/1485.\n",
      "Iteration: 1196/1485.\n",
      "Iteration: 1197/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.15259871941109976, 0.1953592624004789, 0.1763007115623659]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.954, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 1198/1485.\n",
      "Iteration: 1199/1485.\n",
      "Iteration: 1200/1485.\n",
      "Iteration: 1201/1485.\n",
      "Iteration: 1202/1485.\n",
      "Iteration: 1203/1485.\n",
      "Iteration: 1204/1485.\n",
      "Iteration: 1205/1485.\n",
      "Iteration: 1206/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.19074010226690113, 0.19219382189793766, 0.14808998087104913]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.962, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1207/1485.\n",
      "Iteration: 1208/1485.\n",
      "Iteration: 1209/1485.\n",
      "Iteration: 1210/1485.\n",
      "Iteration: 1211/1485.\n",
      "Iteration: 1212/1485.\n",
      "Iteration: 1213/1485.\n",
      "Iteration: 1214/1485.\n",
      "Iteration: 1215/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.16550034218016416, 0.15501429039023878, 0.09179006925259954]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.966, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 1216/1485.\n",
      "Iteration: 1217/1485.\n",
      "Iteration: 1218/1485.\n",
      "Iteration: 1219/1485.\n",
      "Iteration: 1220/1485.\n",
      "Iteration: 1221/1485.\n",
      "Iteration: 1222/1485.\n",
      "Iteration: 1223/1485.\n",
      "Iteration: 1224/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.2716425465179675, 0.12723796521416364, 0.16223278006086855]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.978, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 1225/1485.\n",
      "Iteration: 1226/1485.\n",
      "Iteration: 1227/1485.\n",
      "Iteration: 1228/1485.\n",
      "Iteration: 1229/1485.\n",
      "Iteration: 1230/1485.\n",
      "Iteration: 1231/1485.\n",
      "Iteration: 1232/1485.\n",
      "Iteration: 1233/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.13527488214787062, 0.16474619537667273, 0.18170896312930412]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.98, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 1234/1485.\n",
      "Iteration: 1235/1485.\n",
      "Iteration: 1236/1485.\n",
      "Iteration: 1237/1485.\n",
      "Iteration: 1238/1485.\n",
      "Iteration: 1239/1485.\n",
      "Iteration: 1240/1485.\n",
      "Iteration: 1241/1485.\n",
      "Iteration: 1242/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.14171215296656325, 0.12839036426031722, 0.12671210148467618]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.948, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 1243/1485.\n",
      "Iteration: 1244/1485.\n",
      "Iteration: 1245/1485.\n",
      "Iteration: 1246/1485.\n",
      "Iteration: 1247/1485.\n",
      "Iteration: 1248/1485.\n",
      "Iteration: 1249/1485.\n",
      "Iteration: 1250/1485.\n",
      "Iteration: 1251/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.15573518354921315, 0.13170302538363704, 0.0867693307385161]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.97, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 1252/1485.\n",
      "Iteration: 1253/1485.\n",
      "Iteration: 1254/1485.\n",
      "Iteration: 1255/1485.\n",
      "Iteration: 1256/1485.\n",
      "Iteration: 1257/1485.\n",
      "Iteration: 1258/1485.\n",
      "Iteration: 1259/1485.\n",
      "Iteration: 1260/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.10715813366926508, 0.15018809852271836, 0.09190345060529266]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.98, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 1261/1485.\n",
      "Iteration: 1262/1485.\n",
      "Iteration: 1263/1485.\n",
      "Iteration: 1264/1485.\n",
      "Iteration: 1265/1485.\n",
      "Iteration: 1266/1485.\n",
      "Iteration: 1267/1485.\n",
      "Iteration: 1268/1485.\n",
      "Iteration: 1269/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.146266363208115, 0.10221968893682462, 0.12023978795768736]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.986, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 1270/1485.\n",
      "Iteration: 1271/1485.\n",
      "Iteration: 1272/1485.\n",
      "Iteration: 1273/1485.\n",
      "Iteration: 1274/1485.\n",
      "Iteration: 1275/1485.\n",
      "Iteration: 1276/1485.\n",
      "Iteration: 1277/1485.\n",
      "Iteration: 1278/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.1106348709420879, 0.10627910846700202, 0.11181378774672862]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.978, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 1279/1485.\n",
      "Iteration: 1280/1485.\n",
      "Iteration: 1281/1485.\n",
      "Iteration: 1282/1485.\n",
      "Iteration: 1283/1485.\n",
      "Iteration: 1284/1485.\n",
      "Iteration: 1285/1485.\n",
      "Iteration: 1286/1485.\n",
      "Iteration: 1287/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.12588263084012574, 0.12783358766919403, 0.0695897730754448]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.974, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 1288/1485.\n",
      "Iteration: 1289/1485.\n",
      "Iteration: 1290/1485.\n",
      "Iteration: 1291/1485.\n",
      "Iteration: 1292/1485.\n",
      "Iteration: 1293/1485.\n",
      "Iteration: 1294/1485.\n",
      "Iteration: 1295/1485.\n",
      "Iteration: 1296/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.08948427825933551, 0.11067846573455999, 0.076788274484245]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.988, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 1297/1485.\n",
      "Iteration: 1298/1485.\n",
      "Iteration: 1299/1485.\n",
      "Iteration: 1300/1485.\n",
      "Iteration: 1301/1485.\n",
      "Iteration: 1302/1485.\n",
      "Iteration: 1303/1485.\n",
      "Iteration: 1304/1485.\n",
      "Iteration: 1305/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.0996916561204923, 0.10296659631401024, 0.1700409433063617]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.99, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 1306/1485.\n",
      "Iteration: 1307/1485.\n",
      "Iteration: 1308/1485.\n",
      "Iteration: 1309/1485.\n",
      "Iteration: 1310/1485.\n",
      "Iteration: 1311/1485.\n",
      "Iteration: 1312/1485.\n",
      "Iteration: 1313/1485.\n",
      "Iteration: 1314/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.06956225505852348, 0.1000938269871849, 0.13764577732620262]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.988, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 1315/1485.\n",
      "Iteration: 1316/1485.\n",
      "Iteration: 1317/1485.\n",
      "Iteration: 1318/1485.\n",
      "Iteration: 1319/1485.\n",
      "Iteration: 1320/1485.\n",
      "Iteration: 1321/1485.\n",
      "Iteration: 1322/1485.\n",
      "Iteration: 1323/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.07522632932290979, 0.10123521102084153, 0.08640712862992823]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.986, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 1324/1485.\n",
      "Iteration: 1325/1485.\n",
      "Iteration: 1326/1485.\n",
      "Iteration: 1327/1485.\n",
      "Iteration: 1328/1485.\n",
      "Iteration: 1329/1485.\n",
      "Iteration: 1330/1485.\n",
      "Iteration: 1331/1485.\n",
      "Iteration: 1332/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.10066995568884476, 0.09164443457801488, 0.08423857917094389]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.982, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1333/1485.\n",
      "Iteration: 1334/1485.\n",
      "Iteration: 1335/1485.\n",
      "Iteration: 1336/1485.\n",
      "Iteration: 1337/1485.\n",
      "Iteration: 1338/1485.\n",
      "Iteration: 1339/1485.\n",
      "Iteration: 1340/1485.\n",
      "Iteration: 1341/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.10006168010097953, 0.061757568833762995, 0.0787411035290103]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.988, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 1342/1485.\n",
      "Iteration: 1343/1485.\n",
      "Iteration: 1344/1485.\n",
      "Iteration: 1345/1485.\n",
      "Iteration: 1346/1485.\n",
      "Iteration: 1347/1485.\n",
      "Iteration: 1348/1485.\n",
      "Iteration: 1349/1485.\n",
      "Iteration: 1350/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.05262950893199685, 0.06711138474255074, 0.08107674505811951]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.99, Val acc: 0.48\n",
      "\n",
      "\n",
      "Iteration: 1351/1485.\n",
      "Iteration: 1352/1485.\n",
      "Iteration: 1353/1485.\n",
      "Iteration: 1354/1485.\n",
      "Iteration: 1355/1485.\n",
      "Iteration: 1356/1485.\n",
      "Iteration: 1357/1485.\n",
      "Iteration: 1358/1485.\n",
      "Iteration: 1359/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.0767246626566413, 0.06707672710257724, 0.10976570282256269]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.99, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 1360/1485.\n",
      "Iteration: 1361/1485.\n",
      "Iteration: 1362/1485.\n",
      "Iteration: 1363/1485.\n",
      "Iteration: 1364/1485.\n",
      "Iteration: 1365/1485.\n",
      "Iteration: 1366/1485.\n",
      "Iteration: 1367/1485.\n",
      "Iteration: 1368/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.10628405177280292, 0.08017128541115164, 0.12205852500738354]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.984, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 1369/1485.\n",
      "Iteration: 1370/1485.\n",
      "Iteration: 1371/1485.\n",
      "Iteration: 1372/1485.\n",
      "Iteration: 1373/1485.\n",
      "Iteration: 1374/1485.\n",
      "Iteration: 1375/1485.\n",
      "Iteration: 1376/1485.\n",
      "Iteration: 1377/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.06371347921414498, 0.06908378964616772, 0.06497943984696279]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.992, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 1378/1485.\n",
      "Iteration: 1379/1485.\n",
      "Iteration: 1380/1485.\n",
      "Iteration: 1381/1485.\n",
      "Iteration: 1382/1485.\n",
      "Iteration: 1383/1485.\n",
      "Iteration: 1384/1485.\n",
      "Iteration: 1385/1485.\n",
      "Iteration: 1386/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.06991724688023085, 0.08433317281430026, 0.08869766235070146]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.99, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1387/1485.\n",
      "Iteration: 1388/1485.\n",
      "Iteration: 1389/1485.\n",
      "Iteration: 1390/1485.\n",
      "Iteration: 1391/1485.\n",
      "Iteration: 1392/1485.\n",
      "Iteration: 1393/1485.\n",
      "Iteration: 1394/1485.\n",
      "Iteration: 1395/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.10397861677263215, 0.0746064559233347, 0.06174216810437257]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.992, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1396/1485.\n",
      "Iteration: 1397/1485.\n",
      "Iteration: 1398/1485.\n",
      "Iteration: 1399/1485.\n",
      "Iteration: 1400/1485.\n",
      "Iteration: 1401/1485.\n",
      "Iteration: 1402/1485.\n",
      "Iteration: 1403/1485.\n",
      "Iteration: 1404/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.06855227795881898, 0.06054093282463536, 0.08126881545561214]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.994, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 1405/1485.\n",
      "Iteration: 1406/1485.\n",
      "Iteration: 1407/1485.\n",
      "Iteration: 1408/1485.\n",
      "Iteration: 1409/1485.\n",
      "Iteration: 1410/1485.\n",
      "Iteration: 1411/1485.\n",
      "Iteration: 1412/1485.\n",
      "Iteration: 1413/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.07721051084276884, 0.10495487799168349, 0.08358058144485286]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.996, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 1414/1485.\n",
      "Iteration: 1415/1485.\n",
      "Iteration: 1416/1485.\n",
      "Iteration: 1417/1485.\n",
      "Iteration: 1418/1485.\n",
      "Iteration: 1419/1485.\n",
      "Iteration: 1420/1485.\n",
      "Iteration: 1421/1485.\n",
      "Iteration: 1422/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.09516812390066434, 0.07490773518091348, 0.09063670737075784]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.98, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 1423/1485.\n",
      "Iteration: 1424/1485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1425/1485.\n",
      "Iteration: 1426/1485.\n",
      "Iteration: 1427/1485.\n",
      "Iteration: 1428/1485.\n",
      "Iteration: 1429/1485.\n",
      "Iteration: 1430/1485.\n",
      "Iteration: 1431/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.062116371814991744, 0.04868525976400405, 0.07446232235961453]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.988, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 1432/1485.\n",
      "Iteration: 1433/1485.\n",
      "Iteration: 1434/1485.\n",
      "Iteration: 1435/1485.\n",
      "Iteration: 1436/1485.\n",
      "Iteration: 1437/1485.\n",
      "Iteration: 1438/1485.\n",
      "Iteration: 1439/1485.\n",
      "Iteration: 1440/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.054312241515758794, 0.07228211668751351, 0.05205652858243056]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.994, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 1441/1485.\n",
      "Iteration: 1442/1485.\n",
      "Iteration: 1443/1485.\n",
      "Iteration: 1444/1485.\n",
      "Iteration: 1445/1485.\n",
      "Iteration: 1446/1485.\n",
      "Iteration: 1447/1485.\n",
      "Iteration: 1448/1485.\n",
      "Iteration: 1449/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.0478884668298673, 0.07048963987256561, 0.0411176429904021]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.998, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 1450/1485.\n",
      "Iteration: 1451/1485.\n",
      "Iteration: 1452/1485.\n",
      "Iteration: 1453/1485.\n",
      "Iteration: 1454/1485.\n",
      "Iteration: 1455/1485.\n",
      "Iteration: 1456/1485.\n",
      "Iteration: 1457/1485.\n",
      "Iteration: 1458/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.0405640101352456, 0.09233841099464105, 0.06452497935125553]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.996, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 1459/1485.\n",
      "Iteration: 1460/1485.\n",
      "Iteration: 1461/1485.\n",
      "Iteration: 1462/1485.\n",
      "Iteration: 1463/1485.\n",
      "Iteration: 1464/1485.\n",
      "Iteration: 1465/1485.\n",
      "Iteration: 1466/1485.\n",
      "Iteration: 1467/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.0594207513449882, 0.03206580601533047, 0.053785856203513074]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 1468/1485.\n",
      "Iteration: 1469/1485.\n",
      "Iteration: 1470/1485.\n",
      "Iteration: 1471/1485.\n",
      "Iteration: 1472/1485.\n",
      "Iteration: 1473/1485.\n",
      "Iteration: 1474/1485.\n",
      "Iteration: 1475/1485.\n",
      "Iteration: 1476/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.05258875702971024, 0.0387398878987986, 0.04175071721985543]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.998, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 1477/1485.\n",
      "Iteration: 1478/1485.\n",
      "Iteration: 1479/1485.\n",
      "Iteration: 1480/1485.\n",
      "Iteration: 1481/1485.\n",
      "Iteration: 1482/1485.\n",
      "Iteration: 1483/1485.\n",
      "Iteration: 1484/1485.\n",
      "Iteration: 1485/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302529275528523\n",
      "Loss latest three: [0.06342533447327632, 0.054748102165421704, 0.06574237534651106]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.42\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------FINAL OUTPUT----------------\n",
      "  Train acc: 0.998, Val acc: 0.42\n",
      "Loss history: [2.302529275528523, 2.3024994340554237, 2.3020461459511252, 2.3002543180940664, 2.299214588837936, 2.2932951311517527, 2.2945056409184144, 2.2680652925025, 2.2649603772306177, 2.23912680928465, 2.2435801986409682, 2.207873666135628, 2.2058077043226194, 2.1685502105292738, 2.1075098004330637, 2.1343966543341, 2.1909408337106813, 1.9705364730134076, 2.035827513280629, 2.079663390939598, 2.063494388120864, 1.9829538329883087, 2.1644559870573024, 1.8916823240363416, 1.9642737735772402, 1.975972960841398, 2.0308342032558646, 2.047144475678939, 2.0569830212711757, 2.0187764669390584, 1.989193876705423, 1.8889826248645372, 1.9808448456592544, 1.8773609437805474, 1.9295325266797005, 1.9401001812964922, 1.9220231871928428, 1.901268321178519, 1.980968919849717, 1.978533087264974, 1.9717581845161136, 1.8615880587302744, 1.857145202719596, 1.8853686418528655, 1.8762154621412657, 1.890315047518197, 1.9482410782082733, 1.9161018801360252, 1.9514642968401736, 1.942353485036434, 2.059378974846107, 1.9640203463059742, 1.9527985156618313, 1.7816434502116634, 1.8394114311092888, 1.93442665048514, 1.9620863064937117, 1.8486435624863429, 1.8330912150039655, 1.810450760668165, 1.9012065903903945, 1.9320045293699044, 1.9330591220538336, 1.7853054678851346, 1.8497816598676609, 1.8416389778536513, 1.7767884507403688, 1.7943182163322908, 1.8682878245255503, 1.8713138259283328, 1.7955069263000594, 1.6889476411364643, 1.8992654108083067, 1.862181366629245, 1.831640455002765, 1.77566500533947, 2.040186555620523, 1.7887686190857155, 1.7567686840432943, 1.6965920489419093, 1.7458815815852016, 1.8391512089842783, 1.9303183479335575, 1.8872744975993585, 1.7241647739534935, 1.88519945296032, 1.8628641285684924, 1.7389414484073817, 1.7011669120591877, 1.8156279863183808, 1.606256440304038, 1.7214342592722447, 1.7289626112345104, 1.8179220706174952, 1.5368583255199324, 1.8511404867605483, 1.9274069749583553, 1.8873800604144972, 1.7088833850254161, 1.6798616270589477, 1.7557687417161387, 1.75283886835511, 1.7814216665265514, 1.6473970669315285, 1.8316509722271488, 1.863986511838922, 1.8081012999828676, 1.6676946945846067, 1.6914708144026553, 1.7758702795148318, 1.6179489331085861, 1.7635856134737018, 1.6816572695080194, 1.7086302763683532, 1.6219733721567067, 1.7128330874214317, 1.5341632519949886, 1.6882335926368484, 1.7312077624808435, 1.7083813464323583, 1.7026263026201829, 1.8089611906141658, 1.7371133490877657, 1.6069696622002971, 1.6439096581119195, 1.7111398015535093, 1.6226391843742078, 1.6867273851237006, 1.6542777503173662, 1.626120961893601, 1.6308100676766275, 1.6129127160735763, 1.5806242607092535, 1.7096647472248643, 1.5342330944063138, 1.5508386751388221, 1.5885167093731516, 1.6482422786417956, 1.6848745439150907, 1.630847271929149, 1.7798459863876794, 1.6296330095848868, 1.6959605690987423, 1.8052635378946895, 1.4731406326764298, 1.649025681395787, 1.718704068246871, 1.5855822127783996, 1.6104984569897454, 1.69667460999829, 1.6294225771043833, 1.5482020995919052, 1.548528289052887, 1.5433741117959467, 1.7673366839281734, 1.6156922503900908, 1.6862815958192212, 1.5151887133894861, 1.6285552603562181, 1.8316989218245507, 1.6444515150084973, 1.725503339953794, 1.6087820287872296, 1.5724189666486266, 1.7263929815524754, 1.6063530132002677, 1.6826882148167681, 1.602799407169844, 1.6340402272045598, 1.6038675933597402, 1.5697182636970235, 1.6252096866166996, 1.7127237694109392, 1.5667337964927395, 1.7402694173086775, 1.6343067586217686, 1.5652586826487216, 1.5803337834338151, 1.555983173998955, 1.6059939735324416, 1.604080857114685, 1.761515056905725, 1.5587708822295878, 1.6766899647120965, 1.549971350305654, 1.7060792407342087, 1.649501410829594, 1.5946770515586468, 1.6107250659666486, 1.646184944261917, 1.7765614871414244, 1.638080673783519, 1.6201839943585876, 1.6207625944150155, 1.6254131163877272, 1.525963262379493, 1.493799444996079, 1.6490017191531163, 1.5490851103612209, 1.5307818077102209, 1.7338739660876354, 1.6631676318554849, 1.6527637962062705, 1.5814035000320146, 1.5799511584422192, 1.450073748424584, 1.6379851323402128, 1.6014593874470726, 1.4773633250862792, 1.5789580987708334, 1.4510126472411546, 1.7291296848557576, 1.7567106734452518, 1.5654845839737845, 1.6549128983792885, 1.64110642342924, 1.5099621628969708, 1.5919542898475119, 1.478140986426894, 1.4386552479750168, 1.7086713726532552, 1.5676622055856795, 1.5316561249903067, 1.5619478102278257, 1.468030418519422, 1.61784264694724, 1.5460976830471722, 1.4269736420461956, 1.6847915495093446, 1.6082258331769637, 1.528519748835276, 1.7189008837405713, 1.5255966648834076, 1.5677429116465944, 1.4854671494167868, 1.4466515762268128, 1.693507239677407, 1.4028027748133014, 1.4402465866549299, 1.5012662574295714, 1.4973781735923053, 1.6261575105448294, 1.5757418731712807, 1.4794056012262493, 1.4284923401756626, 1.5069032189995404, 1.5522054173759006, 1.5901998016845562, 1.5203543488199578, 1.3706816194073659, 1.4708159870627358, 1.4817607212226338, 1.4533265823765078, 1.5461741965901659, 1.3906856866485169, 1.5857715602754883, 1.4513505348906122, 1.50250650830503, 1.5954789601827049, 1.5178185592342153, 1.4624802002200037, 1.384794733657011, 1.5203227520495222, 1.4931560519038425, 1.4305131788510523, 1.4134157329520505, 1.4738386152968905, 1.635116792894246, 1.4590214960990155, 1.4739889820749184, 1.7359151830905717, 1.5258270385118724, 1.4564876775382483, 1.4585744927435116, 1.5269675574510018, 1.3943180929929193, 1.4428143711704333, 1.5143006287254912, 1.3628120032045115, 1.4187118088332207, 1.4281836728741046, 1.4705857766969712, 1.3952658164568374, 1.2531447238165725, 1.4333540021359457, 1.52189342432878, 1.6304171742351117, 1.5862403317205696, 1.5478531837375409, 1.3515402191153225, 1.466984543556194, 1.5832702214371932, 1.3462018092909664, 1.4870423443361518, 1.4691281411904882, 1.4516466051001269, 1.6705112772884254, 1.4420286182327968, 1.522080525642416, 1.5276387355383045, 1.4894756354092986, 1.529730405023329, 1.260442751298348, 1.46923663605832, 1.3131847264536052, 1.4877058311490277, 1.3999288435622699, 1.3728502994340328, 1.481226108519008, 1.3389587010254076, 1.4088129835617127, 1.4126274742615261, 1.6188713462282516, 1.3494891086460343, 1.4374194012140133, 1.432849894939758, 1.3490394061532613, 1.4220633471373563, 1.4432153332875126, 1.4974124266425595, 1.4018431352576508, 1.4564177924287256, 1.3697666014458942, 1.433271857960979, 1.5448361588526225, 1.469101944938167, 1.3771843443272962, 1.4965689012589667, 1.4074050413096826, 1.3797523610499187, 1.3894159696525887, 1.3856795905885946, 1.2695774845562968, 1.2834158750014357, 1.4186152813860795, 1.3314727231475951, 1.374391893853443, 1.3879090285452347, 1.3307478161913182, 1.4195701213386298, 1.4313440721009159, 1.4730557075091628, 1.4008787208339721, 1.35447770853835, 1.272019829546233, 1.4538119844144364, 1.2247326812428236, 1.5028580148507837, 1.3293508037281214, 1.3360627604651623, 1.2936916091746036, 1.4222197902367315, 1.412370842305508, 1.252149662791645, 1.3154191458771691, 1.3356326015398003, 1.3188988462999955, 1.387369025174261, 1.211897283349107, 1.3435504339418671, 1.3275404039578356, 1.1916918698437096, 1.3778867505114665, 1.4386817944006183, 1.3357679071745476, 1.2916888310549948, 1.3987736185409692, 1.2481585824599675, 1.2490419477133616, 1.1954568021876737, 1.3582463357371415, 1.2788152184229151, 1.3354182233526575, 1.3314099257865106, 1.2448780852552122, 1.0887365888499854, 1.362445746010146, 1.4214905793810884, 1.251094637653379, 1.308804607444233, 1.2997460976521904, 1.3479327147230626, 1.3942445539746542, 1.3401460730254096, 1.2984031950702957, 1.1837436148702105, 1.090841180182948, 1.3797726318995107, 1.3234615501395226, 1.283919745408124, 1.229079846824162, 1.1583673926140097, 1.2933020194323586, 1.2282265597661228, 1.129471901511195, 1.2002753118855447, 1.1842837978977745, 1.319325325447288, 1.212984575038972, 1.3061025124416588, 1.3019010304243506, 1.2525270529850756, 1.4991626510471932, 1.19875933832956, 1.3651603446266987, 1.2569820195797548, 1.3357277870298176, 1.3221755247699554, 1.220640264989808, 1.0873775808146662, 1.0508860358956031, 1.2541477307202744, 1.1118544141468518, 1.2157576156743655, 1.3653743612828657, 1.3231506841789054, 1.124625491497554, 1.3706715696440401, 1.3383798114132035, 1.2531293452438517, 1.4145304911281549, 1.0754154341466238, 1.1523146479504656, 1.2951530145168801, 1.1700378039363286, 1.302547643086061, 1.3290762061216077, 1.3541008122792573, 1.355778105402917, 1.3826517497053246, 1.0595883015947722, 1.227335357859271, 1.2866479449139012, 1.0937298851509434, 1.2423827271100532, 1.1849220474595321, 1.2518336865092399, 1.2892351299736153, 1.2315474060427216, 1.1536445616826436, 1.0900985252758857, 1.1348661085765783, 1.3662701948578575, 1.1039725393698618, 1.0002002571152449, 1.3002907661606335, 1.1558747142271524, 1.4535937426654102, 1.060671148329979, 1.193528659754004, 1.181008169102156, 1.2158621201432163, 1.1403937085921718, 1.252832504981188, 1.1565527001001212, 1.0874774003143393, 1.3936511758813577, 1.187466209209959, 1.218738082400651, 1.113710371995166, 1.1976273035681781, 1.0875644297097504, 1.1512351486155312, 1.0831948181439466, 1.0897989605977927, 1.062716205729205, 1.0130634868522739, 1.366040306780987, 1.1080099353232704, 0.9403510493927236, 1.2720235633440236, 1.0678016692187433, 1.2661610495220788, 1.3337250383413335, 1.07431376395428, 0.9671699950252551, 1.3049261417604723, 1.1632470398279535, 1.05463846511709, 1.306422527859918, 1.2389510315455952, 1.1310942963326072, 1.330862584722677, 1.1753615237512816, 1.2561370062926074, 1.1771143675055868, 1.2894986539439928, 1.1933440982177648, 1.231353274414666, 1.228137280268691, 1.0763738750474714, 1.2345482469126825, 1.1758644736680364, 1.0648070020692335, 1.2548724418804487, 1.3160210725194776, 1.3369492454603944, 1.1045252829559946, 1.2224646783031814, 1.0020438282069473, 1.2531274574017177, 1.4320293679605451, 1.2869255653164777, 1.0207086034883153, 1.0209269100713387, 1.1677429090615485, 1.118210612457035, 1.2590294325261921, 1.2898580054426672, 1.2998876361153042, 1.223443363496981, 0.9049800909074748, 1.0459767088796108, 1.20507759426569, 1.133755300729846, 1.056231595938127, 1.1441988535434109, 1.1407394205991768, 1.0861277771024813, 1.1222914830903241, 1.126026410804883, 1.0349751923188444, 1.217424385217134, 1.099092423290415, 1.0174195205848362, 0.9530009865670392, 0.9261618130591152, 1.0262784526059499, 1.0582901406033838, 1.0838559914163592, 1.209941343038112, 1.1704155274101418, 0.9850542509232526, 1.0833424823184614, 1.0609527300369566, 1.0108432541849504, 1.0673093395344222, 1.1370380243251417, 1.1903374250203476, 0.8663253843170753, 1.1072085278038497, 1.1698746532619013, 1.011501175535043, 0.9308407774097761, 1.0804533948847685, 1.038923300771502, 1.0633333056429681, 0.9723868328474599, 1.119915923502987, 1.1378540435877076, 1.1217668296332364, 1.095891490345729, 1.0663947280619903, 1.0163369095873194, 1.0631365027552482, 1.1416218091304755, 0.9331087220000985, 0.9706742526129272, 1.1973191258615257, 0.925973816797266, 1.043716524915893, 1.2261914477573428, 1.058157070293958, 1.027268481935511, 1.0301961091439424, 1.1591819978475801, 1.0026488734325962, 1.066308214121586, 0.9917412609287348, 0.9124047922345241, 0.811027646403782, 0.9473635358717307, 0.8637945775959395, 1.0379184973678475, 0.8279725732651524, 1.007161561202339, 0.9916669725006014, 0.932629572615846, 1.077437290177878, 0.9988633260796179, 1.0477180206798502, 1.0247987613815925, 1.0292422617933639, 1.040769688359334, 0.9276795381347112, 1.0848353129271031, 1.0528264543678847, 1.1200779260693154, 0.906588594228323, 0.918221576679761, 0.8580740892929205, 1.1790487478086282, 1.0012386807355638, 0.9356150034205255, 0.96513327656284, 1.0569007756752773, 1.155922796819653, 0.9307342154641903, 0.9490410228515258, 0.7939452443029442, 1.0163871660237214, 0.9135645946502653, 0.9162050088966802, 0.9098456362908705, 0.9114645690156296, 0.9432212714591001, 0.7757379029987861, 1.1858215267466572, 1.0887859328075, 0.8735775373628054, 1.0825669479085038, 0.8408966123134064, 1.195193764558497, 0.9712541804706117, 1.0082214742923405, 0.8887439322627124, 0.8157830978330973, 1.036991545681309, 0.8192405677821349, 1.0565702508607162, 0.9266421280887407, 1.0183529709724832, 1.0519789115785219, 0.947835302344877, 1.0289184485632326, 0.8713080614689053, 0.9138820025335841, 1.025727398159365, 1.0448913391498087, 0.8721447828504879, 0.9067072183111418, 0.8861834549833177, 0.9804240947641549, 0.887587093165386, 0.8256825585734945, 0.7659886762245567, 0.793852855124385, 0.8274864409708039, 0.8890060237324129, 0.852448601642587, 0.8313387808746091, 0.9589480501992974, 0.9178958716691361, 0.925681147053048, 0.8145161814755918, 0.9295606777920393, 0.9479273521514229, 1.0242152517259728, 0.8039646443170543, 0.8291677226440638, 0.9522582644145624, 0.8710696229105663, 0.6896086334031339, 0.9287016543660651, 0.8590537098336725, 0.8645991430356483, 0.8581687741996389, 0.8590032071463375, 0.8535299712380321, 0.8581864962143998, 0.8076062477539675, 0.9502083587491794, 0.8480311273987328, 0.869601757707213, 0.7851657845726169, 0.8156039282452053, 1.009364680134279, 0.8306400295533146, 0.930696422421567, 0.9541261764440832, 0.8713888603106534, 0.8844572347494593, 0.8308562779783368, 0.8544938979129149, 0.8545536879653439, 0.8327157305923403, 0.9824450417884011, 1.012056200847601, 0.9301570057923152, 0.9490120628043929, 0.8921077943970015, 0.8333101717227327, 0.8202027955073301, 0.9285965223897445, 0.7833799227417313, 0.8660292705260106, 0.9170692673988833, 0.7894909691076357, 0.985666223104396, 0.7203495516826852, 0.9980562513146078, 0.8723755361375795, 0.8313104985533412, 0.8337248256811074, 0.8386574879242167, 0.8856770033734325, 0.7330833022805981, 0.621699513135353, 0.7747024383880657, 0.8862756521577758, 0.9240733930213564, 0.8010665717557567, 0.8636201000239732, 1.0019149049106781, 0.8656559431649421, 0.7885150002543121, 0.7628803430538483, 0.7993296102296331, 0.7935839824693405, 0.9314521955199005, 0.7329141085806978, 0.7198813199749334, 0.8077085086256091, 0.8779399825607398, 0.7664435329875933, 0.8352541899565299, 0.7847298302907618, 0.7381318070488017, 0.626787576810444, 0.7131899973434586, 0.6616267906860241, 0.8583124546189264, 0.7118373089923565, 0.6665525987569859, 0.9733987727727555, 0.7234008680260093, 0.6299960738063441, 0.6793626857112711, 0.6694559221958398, 0.7788963596472553, 0.6766031378012265, 0.7749259551577151, 0.7398808177695902, 0.6715494687160132, 0.7921704244048187, 0.7193609439896008, 0.6132192164670375, 0.6313487980075201, 0.620887142813121, 0.7535294669657735, 0.5298255948705995, 0.6934680493632718, 0.8582115782503528, 0.651286808264045, 0.7177371294282969, 0.7353813178775118, 0.550942070650297, 0.9232692876945443, 0.7669612055816053, 0.8095104453099989, 0.716712780838171, 0.7118764904571337, 0.7718206100583221, 0.5845712133165347, 0.7679720510753739, 0.6548307683112755, 0.8217224577549042, 0.6972696959537301, 0.5647125358969429, 0.7883241489420396, 0.5770671169810018, 0.7206679067347215, 0.8623679337022744, 0.7531582807093351, 0.7476478970441067, 0.885529249741597, 0.7714351987323427, 0.9292904353979222, 0.7311304889363975, 0.6698558274616869, 0.6978787104104178, 0.7526260215888264, 0.6878193269119095, 0.7918336645704683, 0.7110273002544314, 0.7931039554057223, 0.7123468097824204, 0.7672529300887461, 0.6633047363471465, 0.8023514551676858, 0.8577635205047867, 0.6721186454930205, 0.7980122588529233, 0.7746164103882357, 0.657304905587758, 0.6162374059889193, 0.6979985611641515, 0.6612723867859949, 0.7741236632909002, 0.6965247711749123, 0.7146381804914721, 0.7313460216931903, 0.5935173709346239, 0.6784496846786428, 0.7707725506607662, 0.665113236778601, 0.6161994188476578, 0.7079741649516578, 0.639104088996295, 0.6826142676426318, 0.5828113372349282, 0.5765284833441482, 0.6850979805963169, 0.6332461734037783, 0.6603208607826084, 0.793937352330993, 0.702526140316046, 0.6368453196068328, 0.7231150540486618, 0.7104476307968893, 0.6236180261560762, 0.6615674218603238, 0.5587901932109444, 0.6633326991659441, 0.574324765649791, 0.6863534351926096, 0.5989734542430026, 0.6152798316839427, 0.5688693584918804, 0.5166095609713903, 0.48353718456753586, 0.6934947982454167, 0.5643063664041865, 0.5553879207504863, 0.6473492643936791, 0.4980242108256734, 0.5783364495487253, 0.4981188975770872, 0.5087107971107586, 0.5812051361620109, 0.478047615860641, 0.5649813561700393, 0.6533809656119454, 0.5376198158561526, 0.4435408364788585, 0.6383947157265071, 0.4793543644921768, 0.7387076996449042, 0.5360507519724195, 0.6055463103763168, 0.4698666829857186, 0.45727495709119886, 0.4859562726372172, 0.47646104295045144, 0.5684652589339815, 0.5768828558804612, 0.46235219881501816, 0.5986402414777435, 0.5833156471907947, 0.5968076294677437, 0.5409140961942185, 0.48297836680779577, 0.531985959688382, 0.5474020155731396, 0.5573966772564117, 0.47002565678524905, 0.5566583174130011, 0.5387796571369566, 0.41700940756314175, 0.510029962093554, 0.5166350442169021, 0.5557405582360023, 0.5386028820064088, 0.39747268386800877, 0.5034682477556937, 0.4933879340494551, 0.510992202120253, 0.4460238528700232, 0.5726995302399004, 0.40690640544309803, 0.42472563673277636, 0.4610863083012513, 0.5980529660647144, 0.5804454676035661, 0.5364055653392915, 0.475401774124591, 0.5604702720325245, 0.6124136967568246, 0.49841035918399834, 0.4287465498045255, 0.4241252067516426, 0.4376360517972979, 0.5639303771483789, 0.5562089765468391, 0.5926618355758241, 0.43105337390533727, 0.5394268761736979, 0.5251153453594937, 0.40777876469910984, 0.6828815456983636, 0.6274070248514514, 0.6143101686503323, 0.5140622849190726, 0.4602738971174641, 0.5426790179198713, 0.5416984758891821, 0.5982775648941828, 0.44385785168123776, 0.4442954147836993, 0.626604113835686, 0.5646267150321126, 0.40334974170633586, 0.44245557857532336, 0.45651821470671194, 0.516174051933371, 0.5364353288781455, 0.7406809602505121, 0.4762650008054573, 0.4792868584014812, 0.5248509166167316, 0.7182955453953441, 0.47408550868298954, 0.5641944441139313, 0.6117141515489867, 0.3596507340335319, 0.5075829051456716, 0.5306021821095397, 0.5096204988090773, 0.4459362486461951, 0.4583953357017107, 0.5271276033812309, 0.44647545412205963, 0.6192809587924196, 0.4324584493096258, 0.6167490868899256, 0.44809564185301415, 0.4155336768544136, 0.3614792555327422, 0.5543177384926009, 0.45997754408811964, 0.4466028008744562, 0.4830396021975461, 0.5548925010173403, 0.4138330921432418, 0.4004807084282942, 0.3964206891086724, 0.489603153904939, 0.4124419180297714, 0.4902393085968356, 0.6224358781347271, 0.5255124009679067, 0.4831858322604501, 0.3990630216370083, 0.4413034779850672, 0.5189559029172207, 0.47004232411517166, 0.4336867488653231, 0.4035652670044991, 0.35278275975927925, 0.40440212269609227, 0.36229023034019747, 0.5154494752880876, 0.5931571084639399, 0.3839912272728465, 0.566950379234383, 0.4511955341114272, 0.4120681337380516, 0.6414534894672892, 0.4372433696218053, 0.45065124849144433, 0.332314732454535, 0.46277499726996374, 0.4484982816279891, 0.40151020598147924, 0.36843512637721587, 0.4349481763539964, 0.44717251498088895, 0.41205337445091234, 0.5358961237287858, 0.33735028004138434, 0.4418998904623001, 0.36189722062509105, 0.28518582895824046, 0.45573707707847205, 0.4388404206907574, 0.33238201667939976, 0.33211678364604935, 0.4529433041077837, 0.34932078417773205, 0.4715409611894055, 0.31687617129566253, 0.4037660998178463, 0.3912244658985891, 0.47517649113028254, 0.3710261852502344, 0.3312355375607478, 0.42999385609267393, 0.46629172925683504, 0.43500938267860223, 0.32719748896498474, 0.37098683081383493, 0.3692929973269006, 0.40166152307976183, 0.4341486676719121, 0.3482758167456159, 0.37652430998131486, 0.4421148684370541, 0.3607330391216507, 0.4486438599793833, 0.40003458596856745, 0.29368639634619004, 0.2839738485873131, 0.41313593881190047, 0.2878260875552023, 0.3659220990704071, 0.3413528471978022, 0.39361860238420593, 0.39597130982420076, 0.2868383663821221, 0.42208909315166, 0.3364550916170615, 0.46098637864022557, 0.4053446158815585, 0.43469542099511754, 0.3065744429891899, 0.32602204363426135, 0.382704088582032, 0.36014035585905535, 0.37034776164160194, 0.33327084849081945, 0.3908620142492979, 0.39246270380750203, 0.29677042640995427, 0.3840850637426398, 0.30565370673018627, 0.49172776704844334, 0.4679187443915603, 0.3680153291861744, 0.383890865593932, 0.4491595187108815, 0.42021401291525556, 0.4137482706613854, 0.34073264520418, 0.34624506712926595, 0.5219015542564746, 0.2908927736285578, 0.30157022417879353, 0.4712092580030567, 0.4102501880669713, 0.43529593963826385, 0.28388221844927514, 0.354640770072584, 0.34502209559585517, 0.40187122292271854, 0.5168183821918249, 0.34008045330492137, 0.2869862399574742, 0.3366109814747389, 0.3065898506616741, 0.3943667431236074, 0.4998907853967885, 0.31561157523742484, 0.2838023896736323, 0.2688825220968915, 0.26274173662037603, 0.28908318116547177, 0.4076678112526038, 0.482098045658008, 0.33479556178158226, 0.36714659729040633, 0.2279914903072519, 0.3797078430551488, 0.33059234867946047, 0.3923104391643788, 0.4118634825474653, 0.31588891724293966, 0.2833820473878079, 0.3537849891094861, 0.344020575567682, 0.2918875538625437, 0.23148370526792839, 0.33646578893256696, 0.339340522306453, 0.37650132205822756, 0.2385563705506186, 0.2952715749703899, 0.35470993628554726, 0.22669147492620803, 0.37441839611592825, 0.26350387268695524, 0.2583742990803239, 0.3085841946248492, 0.23619799861109442, 0.2673595460882522, 0.2676691193529402, 0.2567168026962009, 0.2698089483308378, 0.2631008289572684, 0.24702698256726016, 0.28106382635483546, 0.22147217707149225, 0.3125371796837272, 0.2712010018067153, 0.23880980792538328, 0.3106010683048708, 0.2274657433622706, 0.2368520739575898, 0.2748440722174541, 0.22032238106301483, 0.35143055807982465, 0.33908260408119495, 0.2300876095954242, 0.24028065635726475, 0.2784993171953818, 0.3237206702439238, 0.21084776157652524, 0.2625739114672594, 0.16739554718089777, 0.15428004147527893, 0.3384427803159217, 0.3793937658248651, 0.18858334145175357, 0.36594860453574185, 0.24188381194408562, 0.2505216464671312, 0.25162910929892546, 0.26204237919925544, 0.2333917084139075, 0.2657895296039514, 0.262194779137982, 0.17412784069434292, 0.20383124366294986, 0.305897924750645, 0.23400503609310133, 0.2329059768970425, 0.19432045009191587, 0.21742440016273162, 0.26500695032035104, 0.2381170041543083, 0.31731046608352725, 0.29909169234038374, 0.27609226574918777, 0.21837335549259493, 0.29578250763373853, 0.2685670274210628, 0.23939656363258824, 0.20041949491714944, 0.28064122369120104, 0.17662016993443705, 0.20483689789545964, 0.13208862495861184, 0.22730449901527777, 0.17124626767534157, 0.29881456383955807, 0.20287230329918898, 0.18518480627416828, 0.21624822229403456, 0.15852158242079833, 0.22018469938138463, 0.21182912390110964, 0.1679783999665914, 0.14881153869123206, 0.2449523838175713, 0.17439470025245057, 0.25136877247033046, 0.23446336388999084, 0.20224615020746187, 0.22627274775498837, 0.1834132220570344, 0.2678797330054663, 0.15574729826710382, 0.2420984208365459, 0.1366948434227656, 0.14237200806320993, 0.19066664139302483, 0.1747624132869114, 0.20997562262018077, 0.19369516549976115, 0.21824072793573265, 0.20184950949432115, 0.22734020600739105, 0.11345042808202833, 0.2167285736460969, 0.13467493971814595, 0.15528405846568974, 0.12863873584494642, 0.1398266021124382, 0.1673959796345756, 0.21982760787202069, 0.23084412004863566, 0.20430167932502774, 0.191296410078233, 0.16442899580729453, 0.15850256397551635, 0.1691728419862542, 0.23172947910222633, 0.14747359971322105, 0.1444148497199663, 0.2040530522497459, 0.1627542261292796, 0.13856798148316238, 0.13795328844247404, 0.21721897689194034, 0.2108129004154473, 0.15259871941109976, 0.1953592624004789, 0.1763007115623659, 0.1796534625463454, 0.20587206313089795, 0.14571925244817996, 0.20578683741567017, 0.19228465574419543, 0.19104965522555428, 0.19074010226690113, 0.19219382189793766, 0.14808998087104913, 0.10360793352812263, 0.15650707584218415, 0.11647409980193799, 0.2541525311934325, 0.17071646071349697, 0.11733433221305423, 0.16550034218016416, 0.15501429039023878, 0.09179006925259954, 0.17680585574448837, 0.16565004940541575, 0.1658972239569769, 0.10344565289033794, 0.14327623205438206, 0.15256682309262506, 0.2716425465179675, 0.12723796521416364, 0.16223278006086855, 0.1557513267014277, 0.11787043880496459, 0.15983584121415212, 0.18238033888745414, 0.17805077920396284, 0.10647972852718038, 0.13527488214787062, 0.16474619537667273, 0.18170896312930412, 0.09437056433341669, 0.10738902030178082, 0.12129490874064765, 0.1660758246423885, 0.18345325580380908, 0.16113623305541325, 0.14171215296656325, 0.12839036426031722, 0.12671210148467618, 0.1936698084306716, 0.23606886831768298, 0.11812287208723733, 0.17866523237356433, 0.13908998877283676, 0.24236427792904805, 0.15573518354921315, 0.13170302538363704, 0.0867693307385161, 0.17748846545337252, 0.13106375723651215, 0.16983619587540827, 0.12918041849593398, 0.13140703513080537, 0.1685423347635671, 0.10715813366926508, 0.15018809852271836, 0.09190345060529266, 0.12880724237416236, 0.1366489304380906, 0.15235803680402044, 0.10968964801716496, 0.11165569337028322, 0.08710434986581073, 0.146266363208115, 0.10221968893682462, 0.12023978795768736, 0.17341342848946717, 0.11870683906231193, 0.13379961814872018, 0.12429118190249008, 0.15007767248456844, 0.11821122085539823, 0.1106348709420879, 0.10627910846700202, 0.11181378774672862, 0.07978466897131581, 0.08500046135646246, 0.10403378061016934, 0.11336457526928011, 0.12366528727049074, 0.10263334528427916, 0.12588263084012574, 0.12783358766919403, 0.0695897730754448, 0.18448331656100464, 0.11095271116951289, 0.08763408490341375, 0.11021733893632098, 0.09872333368439716, 0.15975599163727972, 0.08948427825933551, 0.11067846573455999, 0.076788274484245, 0.11125024036503671, 0.11705559479285203, 0.09392090007899682, 0.11272948548575779, 0.12826360444488277, 0.14984008669207838, 0.0996916561204923, 0.10296659631401024, 0.1700409433063617, 0.09911987505745241, 0.09686151840841144, 0.09138118979384757, 0.11509403440740769, 0.07207342571570495, 0.09656262562650307, 0.06956225505852348, 0.1000938269871849, 0.13764577732620262, 0.10592615740667266, 0.141414656407786, 0.08976103235166072, 0.14703555703809973, 0.10434151284329163, 0.09654071629611326, 0.07522632932290979, 0.10123521102084153, 0.08640712862992823, 0.10986610679099622, 0.10908857150069791, 0.08804816354787866, 0.07769386968271687, 0.0785521037349887, 0.08992746782857385, 0.10066995568884476, 0.09164443457801488, 0.08423857917094389, 0.08089669341913844, 0.10317273170590172, 0.08674406691403405, 0.10179860189175637, 0.07701045900440721, 0.06964802327667928, 0.10006168010097953, 0.061757568833762995, 0.0787411035290103, 0.10209566377653845, 0.09255499216623463, 0.045211304036131245, 0.09304190758521778, 0.09851537431693531, 0.08958552705136917, 0.05262950893199685, 0.06711138474255074, 0.08107674505811951, 0.06209026723076285, 0.12451674545439861, 0.08571956927244073, 0.10743760891463155, 0.06618807352526926, 0.08101755853423505, 0.0767246626566413, 0.06707672710257724, 0.10976570282256269, 0.07678639186482057, 0.056148274457105914, 0.1165554100769367, 0.1354389749175217, 0.11097840032087004, 0.11677563920688872, 0.10628405177280292, 0.08017128541115164, 0.12205852500738354, 0.10238943654239091, 0.10269152081642371, 0.064632116480832, 0.06662803381012279, 0.10019990799596515, 0.09356285336153713, 0.06371347921414498, 0.06908378964616772, 0.06497943984696279, 0.06612341887632411, 0.04870286323560838, 0.09978034616859476, 0.06878286045345397, 0.09983635662967091, 0.07475770176571381, 0.06991724688023085, 0.08433317281430026, 0.08869766235070146, 0.11041991906394298, 0.06649246097784826, 0.07596233190522614, 0.09288002674931896, 0.06005828696817151, 0.0836269116682751, 0.10397861677263215, 0.0746064559233347, 0.06174216810437257, 0.06353066823467517, 0.058257527036646606, 0.07603359886997535, 0.06905974499174108, 0.04735355667508677, 0.0700572164686285, 0.06855227795881898, 0.06054093282463536, 0.08126881545561214, 0.07825065727130254, 0.0830645167601555, 0.07255320290637811, 0.04349483639206396, 0.060631748534466824, 0.0651066542406545, 0.07721051084276884, 0.10495487799168349, 0.08358058144485286, 0.0599862161309537, 0.0655064399642642, 0.041376967206917296, 0.09085785764260244, 0.0842323944925183, 0.1020430765301104, 0.09516812390066434, 0.07490773518091348, 0.09063670737075784, 0.10585060002843726, 0.08565399464960173, 0.04401685560389872, 0.06238965234208437, 0.08279511959563009, 0.08204539305315312, 0.062116371814991744, 0.04868525976400405, 0.07446232235961453, 0.04636865731656846, 0.08939741819963525, 0.046418985572789354, 0.058074895969498205, 0.05724462476007347, 0.06027123389254802, 0.054312241515758794, 0.07228211668751351, 0.05205652858243056, 0.06093478353072795, 0.07508355664240332, 0.08169149254447394, 0.05884260328688141, 0.04322864996586504, 0.07329144613182231, 0.0478884668298673, 0.07048963987256561, 0.0411176429904021, 0.06226301097788668, 0.06809256346961144, 0.053043211771449265, 0.05687857272595638, 0.05541734542430052, 0.02579466819272644, 0.0405640101352456, 0.09233841099464105, 0.06452497935125553, 0.04688152689775438, 0.03646991331905154, 0.07424070375359346, 0.0770804190364147, 0.05397934967008313, 0.030742235004603772, 0.0594207513449882, 0.03206580601533047, 0.053785856203513074, 0.03490288147593508, 0.046553945876752455, 0.06613378723295366, 0.08906929604636528, 0.0415515814660225, 0.06445482044795141, 0.05258875702971024, 0.0387398878987986, 0.04175071721985543, 0.05463191412845778, 0.05832164978929888, 0.0546565257675402, 0.06495859365571917, 0.05087233355669466, 0.07079740831257005, 0.06342533447327632, 0.054748102165421704, 0.06574237534651106]\n",
      "Accuracy history: [0.2, 0.2, 0.28, 0.234, 0.254, 0.262, 0.29, 0.272, 0.318, 0.29, 0.334, 0.376, 0.352, 0.396, 0.402, 0.386, 0.338, 0.414, 0.404, 0.406, 0.39, 0.418, 0.432, 0.414, 0.468, 0.41, 0.482, 0.436, 0.46, 0.442, 0.474, 0.444, 0.496, 0.45, 0.452, 0.528, 0.51, 0.52, 0.476, 0.518, 0.52, 0.478, 0.572, 0.516, 0.566, 0.534, 0.522, 0.546, 0.55, 0.604, 0.588, 0.582, 0.562, 0.594, 0.544, 0.606, 0.56, 0.624, 0.626, 0.59, 0.652, 0.672, 0.622, 0.65, 0.606, 0.67, 0.686, 0.632, 0.634, 0.72, 0.684, 0.698, 0.71, 0.674, 0.7, 0.698, 0.726, 0.694, 0.732, 0.774, 0.724, 0.788, 0.786, 0.766, 0.768, 0.738, 0.71, 0.778, 0.772, 0.756, 0.814, 0.806, 0.796, 0.822, 0.834, 0.84, 0.79, 0.778, 0.806, 0.81, 0.804, 0.812, 0.868, 0.85, 0.886, 0.854, 0.842, 0.87, 0.876, 0.896, 0.89, 0.878, 0.892, 0.854, 0.884, 0.836, 0.89, 0.886, 0.926, 0.914, 0.922, 0.924, 0.914, 0.95, 0.954, 0.958, 0.938, 0.958, 0.966, 0.948, 0.97, 0.952, 0.954, 0.962, 0.966, 0.978, 0.98, 0.948, 0.97, 0.98, 0.986, 0.978, 0.974, 0.988, 0.99, 0.988, 0.986, 0.982, 0.988, 0.99, 0.99, 0.984, 0.992, 0.99, 0.992, 0.994, 0.996, 0.98, 0.988, 0.994, 0.998, 0.996, 1.0, 0.998, 1.0, 0.998]\n"
     ]
    }
   ],
   "source": [
    "input_shape=(3,32,32)\n",
    "x_train = x_train.reshape(x_train.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "x_dev = x_dev.reshape(x_dev.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "x_val = x_val.reshape(x_val.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "stl_imgs = stl_imgs.reshape(stl_imgs.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "data = x_train, y_train, x_test, y_test, x_val, y_val, x_dev, y_dev\n",
    "print(x_train.shape)\n",
    "net = ConvNet4Accel(input_shape=(3, 32, 32))\n",
    "net.compile('adam')\n",
    "print(stl_imgs.shape)\n",
    "net.fit(stl_imgs[:x_train.shape[0], :, :, :], stl_labels[:y_train.shape[0]], x_dev, y_dev, n_epochs=33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7d) Analysis of STL-10 training quality\n",
    "\n",
    "Use your trained network that achieves 45%+ accuracy on the test set to make \"high quality\" plots showing the following \n",
    "\n",
    "- Plot the accuracy of the training and validation sets as a function of training epoch. You may have to convert iterations to epochs.\n",
    "- Plot the loss as a function of training iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(net.validation_acc_history)\n",
    "plt.plot(net.train_acc_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(net.loss_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7f) Visualize layer weights\n",
    "\n",
    "Run the following code and submit the inline image of the weight visualization of the 1st layer (convolutional layer) of the network.\n",
    "\n",
    "**Note:**\n",
    "- Setting optional parameter to `True` will let you save a .PNG file in your project folder of your weights. I'd suggest setting it to `False` unless look at your weights and they look like they are worth saving. You don't want a training run that produces undesirable weights to overwrite your good looking results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights(wts, saveFig=True, filename='convWts_adam_overfit.png'):\n",
    "    grid_sz = int(np.sqrt(len(wts)))\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for x in range(grid_sz):\n",
    "        for y in range(grid_sz):\n",
    "            lin_ind = np.ravel_multi_index((x, y), dims=(grid_sz, grid_sz))\n",
    "            plt.subplot(grid_sz, grid_sz, lin_ind+1)\n",
    "            currImg = wts[lin_ind]\n",
    "            low, high = np.min(currImg), np.max(currImg)\n",
    "            currImg = 255*(currImg - low) / (high - low)\n",
    "            currImg = currImg.astype('uint8')\n",
    "            plt.imshow(currImg)\n",
    "            plt.gca().axis('off')\n",
    "    if saveFig:\n",
    "        plt.savefig('convWts_adam_overfit.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Subsitute your trained network below\n",
    "# netT is my network's name\n",
    "# You shouldn't see RGB noise\n",
    "plot_weights(net.layers[0].wts.transpose(0, 2, 3, 1), saveFig=False, filename='convWts_adam_train_20epoch.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:** What do the learned filters look like? Does this make sense to you / is this what you expected? In which area of the brain do these filters resemble cell receptive fields?\n",
    "\n",
    "Note: you should not see RGB \"noise\". If you do, and you pass the \"overfit\" test with the Adam optimizer, you probably need to increase the number of training epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "**General advice:** When making modifications for extensions, make small changes, then check to make sure you pass test code. Also, test out the network runtime on small examples before/after the changes. If you're not careful, the simulation time can become intractable really quickly!\n",
    "\n",
    "**Remember:** One thorough extension usually is worth more than several \"shallow\" extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Pedal to the metal: achieve high accuracy on STL-10\n",
    "\n",
    "You can achieve higher (>50%) classification accuracy on the STL-10 test set. Find the hyperparameters to achieve this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Experiment with different network architectures.\n",
    "\n",
    "The design of the `Network` class is modular. As long as you're careful about shapes, adding/removing network layers (e.g. `Conv2D`, `Dense`, etc.) should be straight forward. Experiment with adding another sequence of `Conv2D` and `MaxPooling2D` layers. Add another `Dense` hidden layer before the output layer. How do the changes affect classification accuracy and loss? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Experiment with different network hyperparameters.\n",
    "\n",
    "Explore the affect one or more change below has on classification. Be careful about how the hyperparameters may affect the shape of network layers. Thorough analysis will get you more points (not try a few ad hoc values).\n",
    "\n",
    "- Experiment with different numbers of hidden units in the Dense layers.\n",
    "- Experiment different max pooling window sizes and strides.\n",
    "- Experiment with kernel sizes (not 7x7). Can you get away with smaller ones? Do they perform just as well? What is the change in runtime like? What is the impact on their visualized appearance?\n",
    "- Experiment with number of kernels in the convolutional layer. Is more/fewer better? What is the impact on their visualized appearance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. Add and test some training bells and whistles\n",
    "\n",
    "Add features like early stopping, learning rate decay (learning rate at the end of an epoch becomes some fraction of its former value), etc and assess how they affect training loss convergence and accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Additional optimizers\n",
    "\n",
    "Research other optimizers used in backpropogation and implement one or more of them within the model structure. Compare its performance to ones you have implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Optimize your algorithms\n",
    "\n",
    "Find the main performance bottlenecks in the network and improve your code to reduce runtime (e.g. reduce explicit for loops, increase vectorization, etc). Research faster algorithms to do operations like convolution and implement them. Given the complexity of the network, I suggest focusing on one area at a time and make sure everything you change passes the test code before proceeding. Quantify and discuss your performance improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Additional loss functions\n",
    "\n",
    "Implement support for sigmoid, or another activation functions and associated losses. Test it out and compare with softmax/cross entropy. Make sure any necessary changes to the layer's gradient are made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Additional datasets\n",
    "\n",
    "Do classification and analyxe the results with an image dataset of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Performance analysis\n",
    "\n",
    "Do a thorough comparative analysis of the non-accelerated network and accelerated networks with respect to runtime."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
