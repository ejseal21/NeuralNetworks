{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cole Turner and Ethan Seal**\n",
    "\n",
    "Fall 2019\n",
    "\n",
    "CS343: Neural Networks\n",
    "\n",
    "Project 3: Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(['seaborn-colorblind', 'seaborn-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=7)\n",
    "\n",
    "# Automatically reload your external source code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global note: Make sure any debug printouts do not appear if `verbose=False`!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4) Implement weight optimizers for gradient descent\n",
    "\n",
    "To change the weights during training, we need an optimization algorithm to have our loss decrease over epochs as we learn the structure of the input patterns. Until now, we used **Stochastic gradient descent (SGD)**, which is the simplest algorithm. We will implement 3 popular algorithms:\n",
    "\n",
    "- `SGD` (stochastic gradient descent)\n",
    "- `SGD_Momentum` (stochastic gradient descent with momentum)\n",
    "- `Adam` (Adaptive Moment Estimation)\n",
    "\n",
    "Implement each of these according to the update equations (in `optimizer.py::update_weights` in each subclass). Let's use $w_t$ in the math below to represent the weights in a layer at time step $t$, $dw$ to represent the gradient of the weights in a layer, and $\\eta$ represent the learning rate. We use vectorized notation below (update applies to all weights element-wise). Then:\n",
    "\n",
    "**SGD**: \n",
    "\n",
    "$w_{t} = w_{t-1} - \\eta \\times dw$\n",
    "\n",
    "**SGD (momentum)**:\n",
    "\n",
    "$v_{t} = m \\times v_{t-1} - \\eta \\times dw$\n",
    "\n",
    "$w_{t} = w_{t-1} + v_t$\n",
    "\n",
    "where $v_t$ is called the `velocity` at time $t$. At the first time step (0), velocity should be set to all zeros and have the same shape as $w$. $m$ is a constant that determines how much of the gradient obtained on the previous time step should factor into the weight update for the current time step.\n",
    "\n",
    "\n",
    "**Adam**:\n",
    "\n",
    "$m_{t} = \\beta_1 \\times m_{t-1} + (1 - \\beta_1)\\times dw$\n",
    "\n",
    "$v_{t} = \\beta_2 \\times v_{t-1} + (1 - \\beta_2)\\times dw^2$\n",
    "\n",
    "$n = m_{t} / \\left (1-(\\beta_1^t) \\right )$\n",
    "\n",
    "$u = v_{t} / \\left (1-(\\beta_2^t) \\right )$\n",
    "\n",
    "$w_{t} = w_{t-1} - \\left ( \\eta \\times n \\right ) / \\left ( \\sqrt(u) + \\epsilon \\right ) $\n",
    "\n",
    "\n",
    "Like SGD (momentum), Adam records momentum terms $m$ and $v$. At time step 0, you should initialize them to zeros in an array equal in size to the weights. $n$ and $u$ are variables computed on each time step. The remaining quantities are constants. Note that $t$ keeps track of the integer time step, and needs to be incremented on each update. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimizer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Test SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD: Wts after 1 iter [-3.1764052 -2.0400157 -1.0978738 -0.2240893  0.8132442  2.0977278]\n",
      "SGD: Wts after 2 iter [-3.3528105 -2.0800314 -1.1957476 -0.4481786  0.6264884  2.1954556]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "wts = np.arange(-3, 3, dtype=np.float64)\n",
    "d_wts = np.random.randn(len(wts))\n",
    "\n",
    "optimizer = SGD()\n",
    "optimizer.prepare(wts, d_wts)\n",
    "\n",
    "new_wts_1 = optimizer.update_weights()\n",
    "new_wts_2 = optimizer.update_weights()\n",
    "\n",
    "print(f'SGD: Wts after 1 iter {new_wts_1}')\n",
    "print(f'SGD: Wts after 2 iter {new_wts_2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should be:\n",
    "\n",
    "    SGD: Wts after 1 iter [-3.1764052 -2.0400157 -1.0978738 -0.2240893  0.8132442  2.0977278]\n",
    "    SGD: Wts after 2 iter [-3.3528105 -2.0800314 -1.1957476 -0.4481786  0.6264884  2.1954556]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Test SGD_Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD M: Wts after 1 iter\n",
      "[[ 1.6879486  0.3879897  0.9343517  2.2075258]\n",
      " [ 1.7181501 -0.9567621  0.9187816 -0.0659476]\n",
      " [ 0.1520801  0.3452366  0.0576     1.52849  ]]\n",
      "SGD M: Wts after 2 iter\n",
      "[[ 1.5661825  0.3685217  0.8633335  2.1541379]\n",
      " [ 1.4790974 -0.9239367  0.8686908  0.0707077]\n",
      " [ 0.5605585  0.2406577 -0.0807098  1.6472364]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "wts = np.random.randn(3, 4)\n",
    "d_wts = np.random.randn(3, 4)\n",
    "\n",
    "optimizer = SGD_Momentum(lr=0.1, m=0.6)\n",
    "optimizer.prepare(wts, d_wts)\n",
    "\n",
    "new_wts_1 = optimizer.update_weights()\n",
    "new_wts_2 = optimizer.update_weights()\n",
    "\n",
    "print(f'SGD M: Wts after 1 iter\\n{new_wts_1}')\n",
    "print(f'SGD M: Wts after 2 iter\\n{new_wts_2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should be:\n",
    "\n",
    "    SGD M: Wts after 1 iter\n",
    "    [[ 1.6879486  0.3879897  0.9343517  2.2075258]\n",
    "     [ 1.7181501 -0.9567621  0.9187816 -0.0659476]\n",
    "     [ 0.1520801  0.3452366  0.0576     1.52849  ]]\n",
    "    SGD M: Wts after 2 iter\n",
    "    [[ 1.5661825  0.3685217  0.8633335  2.1541379]\n",
    "     [ 1.4790974 -0.9239367  0.8686908  0.0707077]\n",
    "     [ 0.5605585  0.2406577 -0.0807098  1.6472364]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Test Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam: Wts after 1 iter\n",
      "[[ 1.6640523  0.3001572  0.878738   2.1408932]\n",
      " [ 1.767558  -0.8772779  0.8500884 -0.0513572]\n",
      " [-0.0032189  0.3105985  0.0440436  1.5542735]]\n",
      "Adam: Wts after 2 iter\n",
      "[[ 1.5640523  0.2001572  0.778738   2.0408932]\n",
      " [ 1.667558  -0.7772779  0.7500884  0.0486428]\n",
      " [ 0.0967811  0.2105985 -0.0559564  1.6542735]]\n",
      "Adam: Wts after 3 iter\n",
      "[[ 1.4640523  0.1001572  0.678738   1.9408932]\n",
      " [ 1.567558  -0.6772779  0.6500884  0.1486428]\n",
      " [ 0.1967811  0.1105985 -0.1559564  1.7542735]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "wts = np.random.randn(3, 4)\n",
    "d_wts = np.random.randn(3, 4)\n",
    "\n",
    "optimizer = Adam(lr=0.1)\n",
    "optimizer.prepare(wts, d_wts)\n",
    "\n",
    "new_wts_1 = optimizer.update_weights()\n",
    "new_wts_2 = optimizer.update_weights()\n",
    "new_wts_3 = optimizer.update_weights()\n",
    "\n",
    "print(f'Adam: Wts after 1 iter\\n{new_wts_1}')\n",
    "print(f'Adam: Wts after 2 iter\\n{new_wts_2}')\n",
    "print(f'Adam: Wts after 3 iter\\n{new_wts_3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should be:\n",
    "\n",
    "    Adam: Wts after 1 iter\n",
    "    [[ 1.6640523  0.3001572  0.878738   2.1408932]\n",
    "     [ 1.767558  -0.8772779  0.8500884 -0.0513572]\n",
    "     [-0.0032189  0.3105985  0.0440436  1.5542735]]\n",
    "    Adam: Wts after 2 iter\n",
    "    [[ 1.5640523  0.2001572  0.778738   2.0408932]\n",
    "     [ 1.667558  -0.7772779  0.7500884  0.0486428]\n",
    "     [ 0.0967811  0.2105985 -0.0559564  1.6542735]]\n",
    "    Adam: Wts after 3 iter\n",
    "    [[ 1.4640523  0.1001572  0.678738   1.9408932]\n",
    "     [ 1.567558  -0.6772779  0.6500884  0.1486428]\n",
    "     [ 0.1967811  0.1105985 -0.1559564  1.7542735]]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5) Write network training methods\n",
    "\n",
    "Implement methods in `network.py` to actually train the network, using all the building blocks that you have created. The methods to implement are:\n",
    "\n",
    "- `predict`\n",
    "- `fit`. Add an optional parameter `print_every=1` that controls the frequency (in iterations) with which to wait before printing out the loss and iteration number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6) Overfitting a convolutional neural network\n",
    "\n",
    "Usually we try to prevent overfitting, but we can use it as a valuable debugging tool to test out a complex backprop-style neural network. Assuming everything is working, it is almost always the case that we should be able to overfit a tiny dataset with a huge model with tons of parameters (i.e. your CNN). You will use this strategy to verify that your network is working.\n",
    "\n",
    "Let's use a small amount of real data from STL-10. If everything is working properly, the network should overfit and you should see a significant drop in the loss from its starting value of ~2.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6a) Move your `preprocess_data.py` from the MLP project\n",
    "\n",
    "Make the one following change:\n",
    "\n",
    "- Re-arrange dimensions of `imgs` so that when it is returned, `shape=(Num imgs, RGB color chans, height, width)` (No longer flatten non-batch dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_stl10_dataset\n",
    "import preprocess_data\n",
    "from network import ConvNet4\n",
    "import optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b) Load in STL-10 at 16x16 resolution\n",
    "\n",
    "If you don't want to wait for STL-10 to download from the internet and resize, copy over your data and numpy folders from your MLP project.\n",
    "\n",
    "**Notes:**\n",
    "- You will need to download the new version of `load_stl10_dataset`.\n",
    "- The different train/test split here won't work if you hard coded the proportions in your `create_splits` implementation! *This isn't catastrophic, it just means that it will take longer to compute accuracy on the validation set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images are: (5000, 96, 96, 3)\n",
      "Labels are: (5000,)\n",
      "Resizing 5000 images to 16x16...Done!\n",
      "Saving Numpy arrays the images and labels to ./numpy...Done!\n",
      "imgs.shape (5000, 16, 16, 3)\n",
      "data.shape (5000, 768)\n",
      "Train data shape:  (4548, 768)\n",
      "Train labels shape:  (4548,)\n",
      "Test data shape:  (400, 768)\n",
      "Test labels shape:  (400,)\n",
      "Validation data shape:  (2, 768)\n",
      "Validation labels shape:  (2,)\n",
      "dev data shape:  (50, 768)\n",
      "dev labels shape:  (50,)\n"
     ]
    }
   ],
   "source": [
    "# Download the STL-10 dataset from the internet, convert it to Numpy ndarray, resize to 16x16\n",
    "# cache it locally on your computer for faster loading next time.\n",
    "load_stl10_dataset.purge_cached_dataset()\n",
    "stl_imgs, stl_labels = load_stl10_dataset.load(scale_fact=6)\n",
    "# preprocess\n",
    "stl_imgs, stl_labels = preprocess_data.preprocess_stl(stl_imgs, stl_labels)\n",
    "# create splits\n",
    "x_train, y_train, x_test, y_test, x_val, y_val, x_dev, y_dev = preprocess_data.create_splits(\n",
    "    stl_imgs, stl_labels, n_train_samps=4548, n_test_samps=400, n_valid_samps=2, n_dev_samps=50)\n",
    "\n",
    "print ('Train data shape: ', x_train.shape)\n",
    "print ('Train labels shape: ', y_train.shape)\n",
    "print ('Test data shape: ', x_test.shape)\n",
    "print ('Test labels shape: ', y_test.shape)\n",
    "print ('Validation data shape: ', x_val.shape)\n",
    "print ('Validation labels shape: ', y_val.shape)\n",
    "print ('dev data shape: ', x_dev.shape)\n",
    "print ('dev labels shape: ', y_dev.shape)\n",
    "\n",
    "classes = np.loadtxt(os.path.join('data', 'stl10_binary', 'class_names.txt'), dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6c) Train and overfit the network on a small STL-10 sample with each optimizer\n",
    "\n",
    "**Goal:** If your network works, you should see a drop in loss over epochs to 0.\n",
    "\n",
    "In 3 seperate cells below\n",
    "\n",
    "- Create 3 different `ConvNet4` networks.\n",
    "- Compile each with a different optimizer (each net uses a different optimizer).\n",
    "- Train each on the **dev** set and validate on the tiny validation set (we dont care about out-of-training-set performance here).\n",
    "\n",
    "You will be making plots demonstrating the overfitting for each optimizer below. **You should train the nets with the same number of epochs such that at least 2/3 of them clearly show loss convergence to a small value; one optimizer may not converge yet, and that's ok**. Cut off the simulations based on the 2/3 that do converge.\n",
    "\n",
    "Guidelines:\n",
    "\n",
    "- Weight scales and learning rates of `1e-2` should work well.\n",
    "- Start by testing the Adam optimizer.\n",
    "- Remember that the input shape is (3, 16, 16). You need to specify this to the network constructor.\n",
    "- The hyperparameters are up to you, though I wouldn't recommend a batch size that is too small (close to 1), otherwise it may be tricky to see whether the loss is actually decreasing on average.\n",
    "- Decreasing `acc_freq` will make the `fit` function evaluate the training and validation accuracy more often. This is a computationally intensive process, so small values come with an increase in training time. On the other hand, checking the accuracy too infrequently means you won't know whether the network is trending toward overfitting the training data, which is what you're checking for.\n",
    "- Each training session takes ~30 mins on my laptop.\n",
    "\n",
    "**Caveat emptor:** Training convolutional networks is notoriously computationally intensive. If you experiment with hyperparameters, each training session may take several hours. Use the loss/accuracy print outs to quickly gauge whether your hyperparameter choices are getting your network to decrease in loss. Monitor print outs and interrupt the Jupyter kernel if things are not trending in the right direction. Consider using the Davis 102 iMacs if this is running too slow on your laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_scale = 1e-2\n",
    "lr = 1e-2\n",
    "input_shape = (3, 16, 16)\n",
    "mini_batch_sz = 10\n",
    "n_epochs = 5\n",
    "\n",
    "#preprocessing flattened it when we actually wanted it not flattened.\n",
    "x_dev = x_dev.reshape(x_dev.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "x_val = x_val.reshape(x_val.shape[0], input_shape[0], input_shape[1], input_shape[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train...\n",
      "750 iterations. 5 iter/epoch.\n",
      "Iteration: 1/750.\n",
      "Time taken for iteration 0: 1.6660547256469727\n",
      "Estimated time to complete: 1249.5410442352295\n",
      "Iteration: 2/750.\n",
      "Iteration: 3/750.\n",
      "Iteration: 4/750.\n",
      "Iteration: 5/750.\n",
      "Iteration: 6/750.\n",
      "Iteration: 7/750.\n",
      "Iteration: 8/750.\n",
      "Iteration: 9/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.306944887611617\n",
      "Loss latest three: [2.2775698776140567, 2.271351335494903, 2.186149805570587]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 10/25.\n",
      "Iteration: 11/25.\n",
      "Iteration: 12/25.\n",
      "Iteration: 13/25.\n",
      "Iteration: 14/25.\n",
      "Iteration: 15/25.\n",
      "Iteration: 16/25.\n",
      "Iteration: 17/25.\n",
      "Iteration: 18/25.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.306944887611617\n",
      "Loss latest three: [2.240214794378763, 2.0907638896450584, 2.1254157689508006]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 19/25.\n",
      "Iteration: 20/25.\n",
      "Iteration: 21/25.\n",
      "Iteration: 22/25.\n",
      "Iteration: 23/25.\n",
      "Iteration: 24/25.\n",
      "Iteration: 25/25.\n",
      "\n",
      "\n",
      "----------------FINAL OUTPUT----------------\n",
      "Loss history last threee: [2.0737598605794227, 1.7079514241241882, 1.988837084745837]\n",
      "  Train acc: 0.22, Val acc: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Adam\n",
    "\n",
    "adam = ConvNet4(input_shape=input_shape, wt_scale=wt_scale, verbose=False)\n",
    "adam.compile('adam')\n",
    "adam.fit(x_dev, y_dev, x_val, y_val, mini_batch_sz=mini_batch_sz, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train...\n",
      "25 iterations. 5 iter/epoch.\n",
      "Iteration: 1/25.\n",
      "Time taken for iteration 0: 2.400736093521118\n",
      "Estimated time to complete: 60.018402338027954\n",
      "Iteration: 2/25.\n",
      "Iteration: 3/25.\n",
      "Iteration: 4/25.\n",
      "Iteration: 5/25.\n",
      "Iteration: 6/25.\n",
      "Iteration: 7/25.\n",
      "Iteration: 8/25.\n",
      "Iteration: 9/25.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.2973210948887424\n",
      "Loss latest three: [2.3018859109327003, 2.302756772104453, 2.30145137610562]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.06, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 10/25.\n",
      "Iteration: 11/25.\n",
      "Iteration: 12/25.\n",
      "Iteration: 13/25.\n",
      "Iteration: 14/25.\n",
      "Iteration: 15/25.\n",
      "Iteration: 16/25.\n",
      "Iteration: 17/25.\n",
      "Iteration: 18/25.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.2973210948887424\n",
      "Loss latest three: [2.307307883285965, 2.3052253566901633, 2.2986113658978335]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.06, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 19/25.\n",
      "Iteration: 20/25.\n",
      "Iteration: 21/25.\n",
      "Iteration: 22/25.\n",
      "Iteration: 23/25.\n",
      "Iteration: 24/25.\n",
      "Iteration: 25/25.\n",
      "\n",
      "\n",
      "----------------FINAL OUTPUT----------------\n",
      "Loss history last threee: [2.297494273910212, 2.3010333017691758, 2.292152180785262]\n",
      "  Train acc: 0.22, Val acc: 0.0\n"
     ]
    }
   ],
   "source": [
    "# SGD-M\n",
    "sgd_m = ConvNet4(input_shape=input_shape, wt_scale=wt_scale, verbose=False)\n",
    "sgd_m.compile('sgd_momentum')\n",
    "sgd_m.fit(x_dev, y_dev, x_val, y_val, mini_batch_sz=mini_batch_sz, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train...\n",
      "25 iterations. 5 iter/epoch.\n",
      "Iteration: 1/25.\n",
      "Time taken for iteration 0: 2.5022010803222656\n",
      "Estimated time to complete: 62.55502700805664\n",
      "Iteration: 2/25.\n",
      "Iteration: 3/25.\n",
      "Iteration: 4/25.\n",
      "Iteration: 5/25.\n",
      "Iteration: 6/25.\n",
      "Iteration: 7/25.\n",
      "Iteration: 8/25.\n",
      "Iteration: 9/25.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.308133619887886\n",
      "Loss latest three: [2.303152012045716, 2.3126533052386495, 2.2887390726173105]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 10/25.\n",
      "Iteration: 11/25.\n",
      "Iteration: 12/25.\n",
      "Iteration: 13/25.\n",
      "Iteration: 14/25.\n",
      "Iteration: 15/25.\n",
      "Iteration: 16/25.\n",
      "Iteration: 17/25.\n",
      "Iteration: 18/25.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.308133619887886\n",
      "Loss latest three: [2.2732551457016625, 2.2388021977921304, 2.2454276013109014]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 19/25.\n",
      "Iteration: 20/25.\n",
      "Iteration: 21/25.\n",
      "Iteration: 22/25.\n",
      "Iteration: 23/25.\n",
      "Iteration: 24/25.\n",
      "Iteration: 25/25.\n",
      "\n",
      "\n",
      "----------------FINAL OUTPUT----------------\n",
      "Loss history last threee: [2.2987172286185378, 2.265729310295075, 2.2955280398578743]\n",
      "  Train acc: 0.22, Val acc: 0.0\n"
     ]
    }
   ],
   "source": [
    "# SGD\n",
    "sgd = ConvNet4(input_shape=input_shape, wt_scale=wt_scale, verbose=False)\n",
    "sgd.compile('sgd')\n",
    "sgd.fit(x_dev, y_dev, x_val, y_val, mini_batch_sz=mini_batch_sz, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**: Why does decreasing the mini-batch size make the loss print-outs more erratic?\n",
    "\n",
    "**Answer**: Decreasing the mini-batch size makes the loss print-outs more erratic because there are fewer samples that have been able to propagate back through the network, so each sample has much more of an impact on what the loss value will be. There is an averaging over all samples that takes place, so if all of the samples in a very small mini-batch are predicted successfully, loss will be low, but it will be high if it performs poorly on the mini-batch. The smaller the mini-batch, the greater the chance that there is a skewed distribution of correct and incorrect predictions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6d) Evaluate the different optimizers\n",
    "\n",
    "Make 2 \"high quality\" plots showing the following\n",
    "\n",
    "- Plot the accuracy (y axis) for the three optimizers as a function of training epoch (x axis).\n",
    "- Plot the loss (y axis) for the three optimizers as a function of training iteration (x axis).\n",
    "\n",
    "A high quality plot consists of:\n",
    "- A useful title\n",
    "- X and Y axis labels\n",
    "- A legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ConvNet4' object has no attribute 'get_train_acc_history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-3d9ee43030e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#get accuracy values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0madam_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_acc_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msgd_m_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgd_m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_acc_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msgd_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_acc_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ConvNet4' object has no attribute 'get_train_acc_history'"
     ]
    }
   ],
   "source": [
    "#get accuracy values\n",
    "adam_acc = adam.get_train_acc_history()\n",
    "sgd_m_acc = sgd_m.get_train_acc_history()\n",
    "sgd_acc = sgd.get_train_acc_history()\n",
    "\n",
    "#plot\n",
    "plt.plot(adam_acc, label=\"adam\")\n",
    "plt.plot(sgd_m_acc, label=\"sgm_m\")\n",
    "plt.plot(sgd_acc, label=\"sgd\")\n",
    "\n",
    "#make words show up\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy Over Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Training epoch\")\n",
    "plt.show()\n",
    "\n",
    "#get loss values\n",
    "adam_loss = adam.get_loss_history()\n",
    "sgd_m_loss = sgd_m.get_loss_history()\n",
    "sgd_loss = sgd.get_loss_history()\n",
    "\n",
    "#plot\n",
    "plt.plot(adam_loss, label=\"adam\")\n",
    "plt.plot(sgd_m_loss, label=\"sgm_m\")\n",
    "plt.plot(sgd_loss, label=\"sgd\")\n",
    "\n",
    "#make words show up\n",
    "plt.legend()\n",
    "plt.title(\"Loss Over Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Training epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**: Which optimizer works best and why do think it is best?\n",
    "\n",
    "**Question 5**: What is happening with the training set accuracy and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Training convolutional neural network on STL-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7a) Load in STL-10 at 32x32 resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images are: (5000, 96, 96, 3)\n",
      "Labels are: (5000,)\n",
      "Resizing 5000 images to 32x32...Done!\n",
      "Saving Numpy arrays the images and labels to ./numpy...Done!\n",
      "imgs.shape (5000, 32, 32, 3)\n",
      "data.shape (5000, 3072)\n",
      "Train data shape:  (4548, 3072)\n",
      "Train labels shape:  (4548,)\n",
      "Test data shape:  (400, 3072)\n",
      "Test labels shape:  (400,)\n",
      "Validation data shape:  (2, 3072)\n",
      "Validation labels shape:  (2,)\n",
      "dev data shape:  (50, 3072)\n",
      "dev labels shape:  (50,)\n"
     ]
    }
   ],
   "source": [
    "# Download the STL-10 dataset from the internet, convert it to Numpy ndarray, resize to 32x32\n",
    "# cache it locally on your computer for faster loading next time.\n",
    "load_stl10_dataset.purge_cached_dataset()\n",
    "stl_imgs, stl_labels = load_stl10_dataset.load(scale_fact=3)\n",
    "# preprocess\n",
    "stl_imgs, stl_labels = preprocess_data.preprocess_stl(stl_imgs, stl_labels)\n",
    "# create splits\n",
    "x_train, y_train, x_test, y_test, x_val, y_val, x_dev, y_dev = preprocess_data.create_splits(\n",
    "    stl_imgs, stl_labels, n_train_samps=4548, n_test_samps=400, n_valid_samps=2, n_dev_samps=50)\n",
    "\n",
    "print ('Train data shape: ', x_train.shape)\n",
    "print ('Train labels shape: ', y_train.shape)\n",
    "print ('Test data shape: ', x_test.shape)\n",
    "print ('Test labels shape: ', y_test.shape)\n",
    "print ('Validation data shape: ', x_val.shape)\n",
    "print ('Validation labels shape: ', y_val.shape)\n",
    "print ('dev data shape: ', x_dev.shape)\n",
    "print ('dev labels shape: ', y_dev.shape)\n",
    "\n",
    "classes = np.loadtxt(os.path.join('data', 'stl10_binary', 'class_names.txt'), dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7b) Set up accelerated convolution and max pooling layers\n",
    "\n",
    "As you may have noticed, we had to downsize STL-10 to 16x16 resolution to train the network on the dev set (N=50) in a reasonable amount of time. The training set is N=4000, how will we ever manage to process that amount of data!?\n",
    "\n",
    "On one hand, this is an unfortunate inevitable reality of working with large (\"big\") datasets: you can easily find a dataset that is too time consuming to process for any computer, despite how fast/many CPU/GPUs it has.\n",
    "\n",
    "On the other hand, we can do better for this project and STL-10 :) If you were to time (profile) different parts of the training process, you'd notice that largest bottleneck is convolution and max pooling operations (both forward/backward). You implemented those operations intuitively, which does not always yield the best performance. **By swapping out forward/backward convolution and maxpooling for implementations that use different algorithms (im2col, reshaping) that are compiled to C code, we will speed up training up by several orders of magnitude**.\n",
    "\n",
    "Follow these steps to subsitute in the \"accelerated\" convolution and max pooling layers.\n",
    "\n",
    "- Install the `cython` python package: `pip3 install cython` (or `pip3 install cython --user` if working in Davis 102)\n",
    "- Dowload files `im2col_cython.pyx`, `accelerated_layer.py`, `setup.py` from the project website. Put them in your base project folder.\n",
    "- Open terminal, `cd` to Project directory.\n",
    "- Compile the im2col functions: `python3 setup.py build_ext --inplace`. A `.c` and `.so` file should have appeared in your project folder.\n",
    "- Restart Jupyter Notebook kernel\n",
    "- Create a class called `Conv4NetAccel` in `network.py` by copy-pasting the contents of `Conv4Net`. Import `accelerated_layer` at the top and replace the `Conv2D` and `MaxPool2D` layers with `Conv2DAccel` and `MaxPool2DAccel`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7c) Training convolutional neural network on STL-10\n",
    "\n",
    "You are now ready to train on the entire training set.\n",
    "\n",
    "- Create a `Conv4NetAccel` object with hyperparameters of your choice.\n",
    "- Your goal is to achieve 45% accuracy on the test and/or validation set.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- I suggest using your intuition about hyperparameters and over/underfitting to guide your choice, rather than a grid search. This should not be overly challenging.\n",
    "- Use the best / most efficient optimizer based on your prior analysis.\n",
    "- It should take on the order of 1 sec per training iteration. If that's way off, seek help as something could be wrong with running the acclerated code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network import ConvNet4Accel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4548, 3, 32, 32)\n",
      "(5000, 3, 32, 32)\n",
      "Starting to train...\n",
      "900 iterations. 45 iter/epoch.\n",
      "Iteration: 1/900.\n",
      "Time taken for iteration 0: 0.6499040126800537\n",
      "Estimated time to complete: 584.9136114120483\n",
      "Iteration: 2/900.\n",
      "Iteration: 3/900.\n",
      "Iteration: 4/900.\n",
      "Iteration: 5/900.\n",
      "Iteration: 6/900.\n",
      "Iteration: 7/900.\n",
      "Iteration: 8/900.\n",
      "Iteration: 9/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302473837733605\n",
      "Loss latest three: [2.2910129530336167, 2.2973661351700696, 2.2808237705433805]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.152, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 10/900.\n",
      "Iteration: 11/900.\n",
      "Iteration: 12/900.\n",
      "Iteration: 13/900.\n",
      "Iteration: 14/900.\n",
      "Iteration: 15/900.\n",
      "Iteration: 16/900.\n",
      "Iteration: 17/900.\n",
      "Iteration: 18/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302473837733605\n",
      "Loss latest three: [2.2141987294877468, 2.119124526340132, 2.1308535242342184]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.204, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 19/900.\n",
      "Iteration: 20/900.\n",
      "Iteration: 21/900.\n",
      "Iteration: 22/900.\n",
      "Iteration: 23/900.\n",
      "Iteration: 24/900.\n",
      "Iteration: 25/900.\n",
      "Iteration: 26/900.\n",
      "Iteration: 27/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302473837733605\n",
      "Loss latest three: [1.9628968845626866, 2.0230349640104244, 2.0793355754156866]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.232, Val acc: 0.0\n",
      "\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.1590602893676074, 1.31185675119782, 0.7630824680897652]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.54, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 109/750.\n",
      "Iteration: 110/750.\n",
      "Iteration: 111/750.\n",
      "Iteration: 112/750.\n",
      "Iteration: 113/750.\n",
      "Iteration: 114/750.\n",
      "Iteration: 115/750.\n",
      "Iteration: 116/750.\n",
      "Iteration: 117/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.1463413255311268, 0.8585563678333387, 1.010251243340975]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.62, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 118/750.\n",
      "Iteration: 119/750.\n",
      "Iteration: 120/750.\n",
      "Iteration: 121/750.\n",
      "Iteration: 122/750.\n",
      "Iteration: 123/750.\n",
      "Iteration: 124/750.\n",
      "Iteration: 125/750.\n",
      "Iteration: 126/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.9948293101744844, 0.6375138809069608, 0.9847764571024111]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.68, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 127/750.\n",
      "Iteration: 128/750.\n",
      "Iteration: 129/750.\n",
      "Iteration: 130/750.\n",
      "Iteration: 131/750.\n",
      "Iteration: 132/750.\n",
      "Iteration: 133/750.\n",
      "Iteration: 134/750.\n",
      "Iteration: 135/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.0188620864883335, 1.5110819357261827, 0.5282107176868567]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.72, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 136/750.\n",
      "Iteration: 137/750.\n",
      "Iteration: 138/750.\n",
      "Iteration: 139/750.\n",
      "Iteration: 140/750.\n",
      "Iteration: 141/750.\n",
      "Iteration: 142/750.\n",
      "Iteration: 143/750.\n",
      "Iteration: 144/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.0612828686712, 0.7893206818157843, 0.7050813317119973]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.88, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 145/750.\n",
      "Iteration: 146/750.\n",
      "Iteration: 147/750.\n",
      "Iteration: 148/750.\n",
      "Iteration: 149/750.\n",
      "Iteration: 150/750.\n",
      "Iteration: 151/750.\n",
      "Iteration: 152/750.\n",
      "Iteration: 153/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.7095368860314037, 0.7967863091393386, 0.34622555323144777]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.88, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 154/750.\n",
      "Iteration: 155/750.\n",
      "Iteration: 156/750.\n",
      "Iteration: 157/750.\n",
      "Iteration: 158/750.\n",
      "Iteration: 159/750.\n",
      "Iteration: 160/750.\n",
      "Iteration: 161/750.\n",
      "Iteration: 162/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.4147177801834292, 0.1976633903218503, 0.6149993686508091]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.88, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 163/750.\n",
      "Iteration: 164/750.\n",
      "Iteration: 165/750.\n",
      "Iteration: 166/750.\n",
      "Iteration: 167/750.\n",
      "Iteration: 168/750.\n",
      "Iteration: 169/750.\n",
      "Iteration: 170/750.\n",
      "Iteration: 171/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.5417135652175434, 0.5127860907579702, 0.4423694055698868]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.9, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 172/750.\n",
      "Iteration: 173/750.\n",
      "Iteration: 174/750.\n",
      "Iteration: 175/750.\n",
      "Iteration: 176/750.\n",
      "Iteration: 177/750.\n",
      "Iteration: 178/750.\n",
      "Iteration: 179/750.\n",
      "Iteration: 180/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.3257512884451925, 0.23361025284867254, 0.4208937622866595]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.9, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 181/750.\n",
      "Iteration: 182/750.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 183/750.\n",
      "Iteration: 184/750.\n",
      "Iteration: 185/750.\n",
      "Iteration: 186/750.\n",
      "Iteration: 187/750.\n",
      "Iteration: 188/750.\n",
      "Iteration: 189/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.2526039181014987, 0.2661376588799025, 0.29333143778269355]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.96, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 190/750.\n",
      "Iteration: 191/750.\n",
      "Iteration: 192/750.\n",
      "Iteration: 193/750.\n",
      "Iteration: 194/750.\n",
      "Iteration: 195/750.\n",
      "Iteration: 196/750.\n",
      "Iteration: 197/750.\n",
      "Iteration: 198/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.1112629225244871, 0.07248974144177063, 0.31392346579773295]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 199/750.\n",
      "Iteration: 200/750.\n",
      "Iteration: 201/750.\n",
      "Iteration: 202/750.\n",
      "Iteration: 203/750.\n",
      "Iteration: 204/750.\n",
      "Iteration: 205/750.\n",
      "Iteration: 206/750.\n",
      "Iteration: 207/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.10167808297320831, 0.07616059586943538, 0.17105394612346236]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 208/750.\n",
      "Iteration: 209/750.\n",
      "Iteration: 210/750.\n",
      "Iteration: 211/750.\n",
      "Iteration: 212/750.\n",
      "Iteration: 213/750.\n",
      "Iteration: 214/750.\n",
      "Iteration: 215/750.\n",
      "Iteration: 216/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.04604406308275605, 0.2700110150812488, 0.1730756502564659]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.98, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 217/750.\n",
      "Iteration: 218/750.\n",
      "Iteration: 219/750.\n",
      "Iteration: 220/750.\n",
      "Iteration: 221/750.\n",
      "Iteration: 222/750.\n",
      "Iteration: 223/750.\n",
      "Iteration: 224/750.\n",
      "Iteration: 225/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.198977290108442, 0.14312686463978938, 0.03261416902984667]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 226/750.\n",
      "Iteration: 227/750.\n",
      "Iteration: 228/750.\n",
      "Iteration: 229/750.\n",
      "Iteration: 230/750.\n",
      "Iteration: 231/750.\n",
      "Iteration: 232/750.\n",
      "Iteration: 233/750.\n",
      "Iteration: 234/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.12942711737335458, 0.07879379011385884, 0.062299998236475564]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 235/750.\n",
      "Iteration: 236/750.\n",
      "Iteration: 237/750.\n",
      "Iteration: 238/750.\n",
      "Iteration: 239/750.\n",
      "Iteration: 240/750.\n",
      "Iteration: 241/750.\n",
      "Iteration: 242/750.\n",
      "Iteration: 243/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0771401766704466, 0.061047090270442084, 0.07818698688785436]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 244/750.\n",
      "Iteration: 245/750.\n",
      "Iteration: 246/750.\n",
      "Iteration: 247/750.\n",
      "Iteration: 248/750.\n",
      "Iteration: 249/750.\n",
      "Iteration: 250/750.\n",
      "Iteration: 251/750.\n",
      "Iteration: 252/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.02739438570280992, 0.049484310869164085, 0.03895022424867601]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 253/750.\n",
      "Iteration: 254/750.\n",
      "Iteration: 255/750.\n",
      "Iteration: 256/750.\n",
      "Iteration: 257/750.\n",
      "Iteration: 258/750.\n",
      "Iteration: 259/750.\n",
      "Iteration: 260/750.\n",
      "Iteration: 261/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.05045440734712168, 0.032420542822017895, 0.06781557374825271]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 262/750.\n",
      "Iteration: 263/750.\n",
      "Iteration: 264/750.\n",
      "Iteration: 265/750.\n",
      "Iteration: 266/750.\n",
      "Iteration: 267/750.\n",
      "Iteration: 268/750.\n",
      "Iteration: 269/750.\n",
      "Iteration: 270/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.03012681363097762, 0.04319413731979136, 0.02004262906877935]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 271/750.\n",
      "Iteration: 272/750.\n",
      "Iteration: 273/750.\n",
      "Iteration: 274/750.\n",
      "Iteration: 275/750.\n",
      "Iteration: 276/750.\n",
      "Iteration: 277/750.\n",
      "Iteration: 278/750.\n",
      "Iteration: 279/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.03676389344140057, 0.06299340377472308, 0.03454798975959152]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 280/750.\n",
      "Iteration: 281/750.\n",
      "Iteration: 282/750.\n",
      "Iteration: 283/750.\n",
      "Iteration: 284/750.\n",
      "Iteration: 285/750.\n",
      "Iteration: 286/750.\n",
      "Iteration: 287/750.\n",
      "Iteration: 288/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.021970949590423193, 0.008230711804036533, 0.04246734441958538]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 289/750.\n",
      "Iteration: 290/750.\n",
      "Iteration: 291/750.\n",
      "Iteration: 292/750.\n",
      "Iteration: 293/750.\n",
      "Iteration: 294/750.\n",
      "Iteration: 295/750.\n",
      "Iteration: 296/750.\n",
      "Iteration: 297/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.024177658392888598, 0.017464338244271845, 0.022840108982812742]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 298/750.\n",
      "Iteration: 299/750.\n",
      "Iteration: 300/750.\n",
      "Iteration: 301/750.\n",
      "Iteration: 302/750.\n",
      "Iteration: 303/750.\n",
      "Iteration: 304/750.\n",
      "Iteration: 305/750.\n",
      "Iteration: 306/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.025275973834323114, 0.030583171943476435, 0.00984498326368509]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 307/750.\n",
      "Iteration: 308/750.\n",
      "Iteration: 309/750.\n",
      "Iteration: 310/750.\n",
      "Iteration: 311/750.\n",
      "Iteration: 312/750.\n",
      "Iteration: 313/750.\n",
      "Iteration: 314/750.\n",
      "Iteration: 315/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0203630879049658, 0.01757106751657544, 0.010262528385192541]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 316/750.\n",
      "Iteration: 317/750.\n",
      "Iteration: 318/750.\n",
      "Iteration: 319/750.\n",
      "Iteration: 320/750.\n",
      "Iteration: 321/750.\n",
      "Iteration: 322/750.\n",
      "Iteration: 323/750.\n",
      "Iteration: 324/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.02262841694298828, 0.01758433643867625, 0.012217754002525399]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 325/750.\n",
      "Iteration: 326/750.\n",
      "Iteration: 327/750.\n",
      "Iteration: 328/750.\n",
      "Iteration: 329/750.\n",
      "Iteration: 330/750.\n",
      "Iteration: 331/750.\n",
      "Iteration: 332/750.\n",
      "Iteration: 333/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.015909905810328133, 0.01982560712916763, 0.013691125194401766]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 334/750.\n",
      "Iteration: 335/750.\n",
      "Iteration: 336/750.\n",
      "Iteration: 337/750.\n",
      "Iteration: 338/750.\n",
      "Iteration: 339/750.\n",
      "Iteration: 340/750.\n",
      "Iteration: 341/750.\n",
      "Iteration: 342/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.015110213725853711, 0.009527124064834098, 0.009334541094691867]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 343/750.\n",
      "Iteration: 344/750.\n",
      "Iteration: 345/750.\n",
      "Iteration: 346/750.\n",
      "Iteration: 347/750.\n",
      "Iteration: 348/750.\n",
      "Iteration: 349/750.\n",
      "Iteration: 350/750.\n",
      "Iteration: 351/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.01383921247847794, 0.017888971550242515, 0.012172627766161605]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 352/750.\n",
      "Iteration: 353/750.\n",
      "Iteration: 354/750.\n",
      "Iteration: 355/750.\n",
      "Iteration: 356/750.\n",
      "Iteration: 357/750.\n",
      "Iteration: 358/750.\n",
      "Iteration: 359/750.\n",
      "Iteration: 360/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.007692151459628377, 0.00860071117621646, 0.01337183452254036]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 361/750.\n",
      "Iteration: 362/750.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 363/750.\n",
      "Iteration: 364/750.\n",
      "Iteration: 365/750.\n",
      "Iteration: 366/750.\n",
      "Iteration: 367/750.\n",
      "Iteration: 368/750.\n",
      "Iteration: 369/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.016828954981043625, 0.008532854220705712, 0.014868278238738564]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 370/750.\n",
      "Iteration: 371/750.\n",
      "Iteration: 372/750.\n",
      "Iteration: 373/750.\n",
      "Iteration: 374/750.\n",
      "Iteration: 375/750.\n",
      "Iteration: 376/750.\n",
      "Iteration: 377/750.\n",
      "Iteration: 378/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.009769343830962701, 0.014373982427621002, 0.00897708285199268]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 379/750.\n",
      "Iteration: 380/750.\n",
      "Iteration: 381/750.\n",
      "Iteration: 382/750.\n",
      "Iteration: 383/750.\n",
      "Iteration: 384/750.\n",
      "Iteration: 385/750.\n",
      "Iteration: 386/750.\n",
      "Iteration: 387/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0061601486759959275, 0.011447400788466693, 0.010592205644013353]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 388/750.\n",
      "Iteration: 389/750.\n",
      "Iteration: 390/750.\n",
      "Iteration: 391/750.\n",
      "Iteration: 392/750.\n",
      "Iteration: 393/750.\n",
      "Iteration: 394/750.\n",
      "Iteration: 395/750.\n",
      "Iteration: 396/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.01006021025842885, 0.007103090284885151, 0.008748900552573213]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 397/750.\n",
      "Iteration: 398/750.\n",
      "Iteration: 399/750.\n",
      "Iteration: 400/750.\n",
      "Iteration: 401/750.\n",
      "Iteration: 402/750.\n",
      "Iteration: 403/750.\n",
      "Iteration: 404/750.\n",
      "Iteration: 405/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.006573226155617962, 0.008800037481439274, 0.007748006161016941]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 406/750.\n",
      "Iteration: 407/750.\n",
      "Iteration: 408/750.\n",
      "Iteration: 409/750.\n",
      "Iteration: 410/750.\n",
      "Iteration: 411/750.\n",
      "Iteration: 412/750.\n",
      "Iteration: 413/750.\n",
      "Iteration: 414/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.00853616274950926, 0.010528373331639172, 0.007736132999175462]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 415/750.\n",
      "Iteration: 416/750.\n",
      "Iteration: 417/750.\n",
      "Iteration: 418/750.\n",
      "Iteration: 419/750.\n",
      "Iteration: 420/750.\n",
      "Iteration: 421/750.\n",
      "Iteration: 422/750.\n",
      "Iteration: 423/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.007007899223572968, 0.00644505755233859, 0.006488757529163384]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 424/750.\n",
      "Iteration: 425/750.\n",
      "Iteration: 426/750.\n",
      "Iteration: 427/750.\n",
      "Iteration: 428/750.\n",
      "Iteration: 429/750.\n",
      "Iteration: 430/750.\n",
      "Iteration: 431/750.\n",
      "Iteration: 432/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.006053104493819026, 0.006276737155629471, 0.004831008743959072]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 433/750.\n",
      "Iteration: 434/750.\n",
      "Iteration: 435/750.\n",
      "Iteration: 436/750.\n",
      "Iteration: 437/750.\n",
      "Iteration: 438/750.\n",
      "Iteration: 439/750.\n",
      "Iteration: 440/750.\n",
      "Iteration: 441/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0052529349195933744, 0.00732885029553593, 0.003807727914067893]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 442/750.\n",
      "Iteration: 443/750.\n",
      "Iteration: 444/750.\n",
      "Iteration: 445/750.\n",
      "Iteration: 446/750.\n",
      "Iteration: 447/750.\n",
      "Iteration: 448/750.\n",
      "Iteration: 449/750.\n",
      "Iteration: 450/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.002921542476523221, 0.003654111207226949, 0.0032846278323096566]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 451/750.\n",
      "Iteration: 452/750.\n",
      "Iteration: 453/750.\n",
      "Iteration: 454/750.\n",
      "Iteration: 455/750.\n",
      "Iteration: 456/750.\n",
      "Iteration: 457/750.\n",
      "Iteration: 458/750.\n",
      "Iteration: 459/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0037279718019347834, 0.010223271739306093, 0.005651504630691279]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 460/750.\n",
      "Iteration: 461/750.\n",
      "Iteration: 462/750.\n",
      "Iteration: 463/750.\n",
      "Iteration: 464/750.\n",
      "Iteration: 465/750.\n",
      "Iteration: 466/750.\n",
      "Iteration: 467/750.\n",
      "Iteration: 468/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.006245607454844204, 0.005093410584849283, 0.0015902946597379669]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 469/750.\n",
      "Iteration: 470/750.\n",
      "Iteration: 471/750.\n",
      "Iteration: 472/750.\n",
      "Iteration: 473/750.\n",
      "Iteration: 474/750.\n",
      "Iteration: 475/750.\n",
      "Iteration: 476/750.\n",
      "Iteration: 477/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.00439410620243234, 0.006613928781317931, 0.003303326800842834]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 478/750.\n",
      "Iteration: 479/750.\n",
      "Iteration: 480/750.\n",
      "Iteration: 481/750.\n",
      "Iteration: 482/750.\n",
      "Iteration: 483/750.\n",
      "Iteration: 484/750.\n",
      "Iteration: 485/750.\n",
      "Iteration: 486/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0031054786424553732, 0.004483647626745672, 0.005548467720664309]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 487/750.\n",
      "Iteration: 488/750.\n",
      "Iteration: 489/750.\n",
      "Iteration: 490/750.\n",
      "Iteration: 491/750.\n",
      "Iteration: 492/750.\n",
      "Iteration: 493/750.\n",
      "Iteration: 494/750.\n",
      "Iteration: 495/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.00414603240714461, 0.009087762031101948, 0.0044056132546797895]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 496/750.\n",
      "Iteration: 497/750.\n",
      "Iteration: 498/750.\n",
      "Iteration: 499/750.\n",
      "Iteration: 500/750.\n",
      "Iteration: 501/750.\n",
      "Iteration: 502/750.\n",
      "Iteration: 503/750.\n",
      "Iteration: 504/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.002901846718032038, 0.004217244810202035, 0.005485786030073221]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 505/750.\n",
      "Iteration: 506/750.\n",
      "Iteration: 507/750.\n",
      "Iteration: 508/750.\n",
      "Iteration: 509/750.\n",
      "Iteration: 510/750.\n",
      "Iteration: 511/750.\n",
      "Iteration: 512/750.\n",
      "Iteration: 513/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0026945877464181088, 0.003658771101894766, 0.006712447580500221]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 514/750.\n",
      "Iteration: 515/750.\n",
      "Iteration: 516/750.\n",
      "Iteration: 517/750.\n",
      "Iteration: 518/750.\n",
      "Iteration: 519/750.\n",
      "Iteration: 520/750.\n",
      "Iteration: 521/750.\n",
      "Iteration: 522/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.004699095964409763, 0.003251637175180632, 0.003271414183918524]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 523/750.\n",
      "Iteration: 524/750.\n",
      "Iteration: 525/750.\n",
      "Iteration: 526/750.\n",
      "Iteration: 527/750.\n",
      "Iteration: 528/750.\n",
      "Iteration: 529/750.\n",
      "Iteration: 530/750.\n",
      "Iteration: 531/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0042051060893134274, 0.0038884924191254335, 0.002962506320237182]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 532/750.\n",
      "Iteration: 533/750.\n",
      "Iteration: 534/750.\n",
      "Iteration: 535/750.\n",
      "Iteration: 536/750.\n",
      "Iteration: 537/750.\n",
      "Iteration: 538/750.\n",
      "Iteration: 539/750.\n",
      "Iteration: 540/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.00298678405030627, 0.0027571970161227623, 0.003610517848250861]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 541/750.\n",
      "Iteration: 542/750.\n",
      "Iteration: 543/750.\n",
      "Iteration: 544/750.\n",
      "Iteration: 545/750.\n",
      "Iteration: 546/750.\n",
      "Iteration: 547/750.\n",
      "Iteration: 548/750.\n",
      "Iteration: 549/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.002019017040326986, 0.003892469532275133, 0.0035536293901333015]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 550/750.\n",
      "Iteration: 551/750.\n",
      "Iteration: 552/750.\n",
      "Iteration: 553/750.\n",
      "Iteration: 554/750.\n",
      "Iteration: 555/750.\n",
      "Iteration: 556/750.\n",
      "Iteration: 557/750.\n",
      "Iteration: 558/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0033645742777648154, 0.0025017081009596756, 0.003921704589888853]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 559/750.\n",
      "Iteration: 560/750.\n",
      "Iteration: 561/750.\n",
      "Iteration: 562/750.\n",
      "Iteration: 563/750.\n",
      "Iteration: 564/750.\n",
      "Iteration: 565/750.\n",
      "Iteration: 566/750.\n",
      "Iteration: 567/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.002981434723113973, 0.0013819217487384989, 0.0032949836318237656]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 568/750.\n",
      "Iteration: 569/750.\n",
      "Iteration: 570/750.\n",
      "Iteration: 571/750.\n",
      "Iteration: 572/750.\n",
      "Iteration: 573/750.\n",
      "Iteration: 574/750.\n",
      "Iteration: 575/750.\n",
      "Iteration: 576/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.002842557068409717, 0.0024130196333980534, 0.003387120788490034]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 577/750.\n",
      "Iteration: 578/750.\n",
      "Iteration: 579/750.\n",
      "Iteration: 580/750.\n",
      "Iteration: 581/750.\n",
      "Iteration: 582/750.\n",
      "Iteration: 583/750.\n",
      "Iteration: 584/750.\n",
      "Iteration: 585/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.002065377079902587, 0.002318131956749458, 0.0026479241884973977]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 586/750.\n",
      "Iteration: 587/750.\n",
      "Iteration: 588/750.\n",
      "Iteration: 589/750.\n",
      "Iteration: 590/750.\n",
      "Iteration: 591/750.\n",
      "Iteration: 592/750.\n",
      "Iteration: 593/750.\n",
      "Iteration: 594/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0028781183348701484, 0.005051741188932144, 0.0037435946121089254]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 595/750.\n",
      "Iteration: 596/750.\n",
      "Iteration: 597/750.\n",
      "Iteration: 598/750.\n",
      "Iteration: 599/750.\n",
      "Iteration: 600/750.\n",
      "Iteration: 601/750.\n",
      "Iteration: 602/750.\n",
      "Iteration: 603/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.00299654626254093, 0.003778291377038163, 0.0020178715544124266]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 604/750.\n",
      "Iteration: 605/750.\n",
      "Iteration: 606/750.\n",
      "Iteration: 607/750.\n",
      "Iteration: 608/750.\n",
      "Iteration: 609/750.\n",
      "Iteration: 610/750.\n",
      "Iteration: 611/750.\n",
      "Iteration: 612/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0035613454363410397, 0.0020711168632821953, 0.0017361541606328416]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 613/750.\n",
      "Iteration: 614/750.\n",
      "Iteration: 615/750.\n",
      "Iteration: 616/750.\n",
      "Iteration: 617/750.\n",
      "Iteration: 618/750.\n",
      "Iteration: 619/750.\n",
      "Iteration: 620/750.\n",
      "Iteration: 621/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0023794135897436224, 0.002434979975880696, 0.002614170525187537]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 622/750.\n",
      "Iteration: 623/750.\n",
      "Iteration: 624/750.\n",
      "Iteration: 625/750.\n",
      "Iteration: 626/750.\n",
      "Iteration: 627/750.\n",
      "Iteration: 628/750.\n",
      "Iteration: 629/750.\n",
      "Iteration: 630/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.002221729193596045, 0.0031591962917994717, 0.0026405344122758093]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 631/750.\n",
      "Iteration: 632/750.\n",
      "Iteration: 633/750.\n",
      "Iteration: 634/750.\n",
      "Iteration: 635/750.\n",
      "Iteration: 636/750.\n",
      "Iteration: 637/750.\n",
      "Iteration: 638/750.\n",
      "Iteration: 639/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0017053509258779108, 0.002381196224160104, 0.0024068952505818864]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 640/750.\n",
      "Iteration: 641/750.\n",
      "Iteration: 642/750.\n",
      "Iteration: 643/750.\n",
      "Iteration: 644/750.\n",
      "Iteration: 645/750.\n",
      "Iteration: 646/750.\n",
      "Iteration: 647/750.\n",
      "Iteration: 648/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0020553921174259224, 0.0025360110659460853, 0.0015094830603940579]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 649/750.\n",
      "Iteration: 650/750.\n",
      "Iteration: 651/750.\n",
      "Iteration: 652/750.\n",
      "Iteration: 653/750.\n",
      "Iteration: 654/750.\n",
      "Iteration: 655/750.\n",
      "Iteration: 656/750.\n",
      "Iteration: 657/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0015051251377963782, 0.0017464836483383286, 0.0011076337311476472]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 658/750.\n",
      "Iteration: 659/750.\n",
      "Iteration: 660/750.\n",
      "Iteration: 661/750.\n",
      "Iteration: 662/750.\n",
      "Iteration: 663/750.\n",
      "Iteration: 664/750.\n",
      "Iteration: 665/750.\n",
      "Iteration: 666/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0017189487269323558, 0.0017419271948256071, 0.001517056077212622]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 667/750.\n",
      "Iteration: 668/750.\n",
      "Iteration: 669/750.\n",
      "Iteration: 670/750.\n",
      "Iteration: 671/750.\n",
      "Iteration: 672/750.\n",
      "Iteration: 673/750.\n",
      "Iteration: 674/750.\n",
      "Iteration: 675/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.002583472775782791, 0.003131088204767879, 0.0017153648706021698]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 676/750.\n",
      "Iteration: 677/750.\n",
      "Iteration: 678/750.\n",
      "Iteration: 679/750.\n",
      "Iteration: 680/750.\n",
      "Iteration: 681/750.\n",
      "Iteration: 682/750.\n",
      "Iteration: 683/750.\n",
      "Iteration: 684/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0015867622306953801, 0.0011906861619485998, 0.00190017199205851]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 685/750.\n",
      "Iteration: 686/750.\n",
      "Iteration: 687/750.\n",
      "Iteration: 688/750.\n",
      "Iteration: 689/750.\n",
      "Iteration: 690/750.\n",
      "Iteration: 691/750.\n",
      "Iteration: 692/750.\n",
      "Iteration: 693/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0019919869100415086, 0.0022330823709953945, 0.0027735989679547437]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 694/750.\n",
      "Iteration: 695/750.\n",
      "Iteration: 696/750.\n",
      "Iteration: 697/750.\n",
      "Iteration: 698/750.\n",
      "Iteration: 699/750.\n",
      "Iteration: 700/750.\n",
      "Iteration: 701/750.\n",
      "Iteration: 702/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0019377714647102334, 0.0021468643512156747, 0.001613401638587184]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 703/750.\n",
      "Iteration: 704/750.\n",
      "Iteration: 705/750.\n",
      "Iteration: 706/750.\n",
      "Iteration: 707/750.\n",
      "Iteration: 708/750.\n",
      "Iteration: 709/750.\n",
      "Iteration: 710/750.\n",
      "Iteration: 711/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.00086116675978332, 0.001959338149785799, 0.0013139268230490256]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 712/750.\n",
      "Iteration: 713/750.\n",
      "Iteration: 714/750.\n",
      "Iteration: 715/750.\n",
      "Iteration: 716/750.\n",
      "Iteration: 717/750.\n",
      "Iteration: 718/750.\n",
      "Iteration: 719/750.\n",
      "Iteration: 720/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.000922927035018237, 0.0017368995073886746, 0.0009850095962978867]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 721/750.\n",
      "Iteration: 722/750.\n",
      "Iteration: 723/750.\n",
      "Iteration: 724/750.\n",
      "Iteration: 725/750.\n",
      "Iteration: 726/750.\n",
      "Iteration: 727/750.\n",
      "Iteration: 728/750.\n",
      "Iteration: 729/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.00160438701564368, 0.0015889317417497155, 0.0020780690656655564]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 730/750.\n",
      "Iteration: 731/750.\n",
      "Iteration: 732/750.\n",
      "Iteration: 733/750.\n",
      "Iteration: 734/750.\n",
      "Iteration: 735/750.\n",
      "Iteration: 736/750.\n",
      "Iteration: 737/750.\n",
      "Iteration: 738/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0010388349009595177, 0.0013960343425225312, 0.0021584754233142122]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 739/750.\n",
      "Iteration: 740/750.\n",
      "Iteration: 741/750.\n",
      "Iteration: 742/750.\n",
      "Iteration: 743/750.\n",
      "Iteration: 744/750.\n",
      "Iteration: 745/750.\n",
      "Iteration: 746/750.\n",
      "Iteration: 747/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.001567330547213729, 0.0015021132826861594, 0.0016439893587461441]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 748/750.\n",
      "Iteration: 749/750.\n",
      "Iteration: 750/750.\n",
      "\n",
      "\n",
      "----------------FINAL OUTPUT----------------\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "Loss history: [2.3055575589923514, 2.300406899071696, 2.3029133221585023, 2.294747648840793, 2.288576523611883, 2.290180646017032, 2.2845796024885723, 2.2924845991086413, 2.2430353643670484, 2.206360550015477, 2.1353708524936774, 2.3061608483970777, 2.2316064461131218, 2.271748997353242, 2.13538884465873, 2.196307338513974, 2.1725589737331377, 1.8555820840431605, 2.2766782177253577, 1.934898385421089, 1.871502819358501, 1.9138400595387086, 1.7624269599259677, 2.4884701186691163, 2.312345181935326, 2.2428537670038167, 1.8153889792853417, 2.042472231906978, 2.0274354973933386, 2.064275502731321, 1.9516374136257122, 1.897432557211994, 1.6579292576131857, 1.7621807182453688, 1.833807183850169, 2.0549246832422936, 1.8723331847903255, 1.6442609969283204, 1.9184862164623917, 1.9046591136566846, 1.8139214361928027, 2.005325645143333, 2.1230847051097714, 2.00511916232943, 1.9732593373540852, 2.0184364213823445, 2.6499883370507757, 1.5295603987287203, 2.14307588096561, 1.6253938407019612, 1.8354510726197937, 1.8132958306345657, 2.0563964202738827, 1.6435401383740862, 1.8748368238786222, 1.828618938846007, 1.7541986087968249, 1.806783549190694, 2.1103791353341728, 1.8129516954502638, 2.1429283671619177, 1.3842210419573906, 1.8503751364566963, 1.6541310471876487, 1.7109102547436128, 1.5414166184470133, 2.0695831223441785, 1.8010124632203686, 1.7868103714244592, 1.4492693059040154, 1.8337719840693965, 1.788756308516624, 1.5214341048054703, 1.3312439993016554, 1.5178424544515476, 1.382287926451269, 1.6661837300331095, 1.511687800988327, 1.4360661578335743, 1.5099931642285551, 1.4284829401566097, 1.875279531052607, 1.3028650293485664, 2.0328953693539904, 1.1246929247966073, 1.0400213378264602, 1.0522843412446876, 1.3285889474860497, 1.476922730116448, 1.0004611806216006, 1.140158729501649, 1.219787349219302, 1.4518095314654245, 1.6059248107977853, 1.1818203407602288, 1.315681706105867, 1.4980733151569927, 1.2347177336803679, 1.2437637714007388, 0.9512570234349385, 1.2619418598021444, 1.1709905748167786, 1.2123945793386361, 0.9025242810217403, 1.2810415819546384, 1.1590602893676074, 1.31185675119782, 0.7630824680897652, 1.2410263152957888, 1.1463289555778127, 1.3242519901328729, 0.8602249197955484, 1.2629893396069092, 1.0838840137850903, 1.1463413255311268, 0.8585563678333387, 1.010251243340975, 0.9108188527342874, 1.1657749006634384, 0.8501433736954743, 1.158100675622405, 1.2207810975212778, 0.6518704700850245, 0.9948293101744844, 0.6375138809069608, 0.9847764571024111, 0.6513278452428268, 1.0415511171954228, 1.032705407100247, 0.6644553302503602, 0.9375853799071927, 1.0291258904932936, 1.0188620864883335, 1.5110819357261827, 0.5282107176868567, 0.7851462760323777, 0.6624157430401119, 0.6488930768763339, 0.4787559503043197, 0.5610481598343083, 0.8771298906666285, 1.0612828686712, 0.7893206818157843, 0.7050813317119973, 0.8077446501275436, 0.7209304969752148, 0.48393176482096134, 0.5696522016964861, 0.3705807523842548, 0.44725253533085835, 0.7095368860314037, 0.7967863091393386, 0.34622555323144777, 0.7874294421665962, 0.28001171706999456, 0.6417807190258413, 0.503012086297786, 0.40403112139353486, 0.7426523787714432, 0.4147177801834292, 0.1976633903218503, 0.6149993686508091, 0.33555014104080705, 0.4207246377423761, 0.7233705795063519, 0.5423455286862345, 0.2921541497299826, 0.395845769945578, 0.5417135652175434, 0.5127860907579702, 0.4423694055698868, 0.3236602867531084, 0.14447520925983015, 0.4420298535896451, 0.4239328888498195, 0.21793086374474135, 0.49037664023562694, 0.3257512884451925, 0.23361025284867254, 0.4208937622866595, 0.2746972798672402, 0.4300013296525805, 0.36844921168523465, 0.23540087771781135, 0.41544197284494255, 0.2823062465263835, 0.2526039181014987, 0.2661376588799025, 0.29333143778269355, 0.3487392255354627, 0.20258932667943813, 0.43918308371232995, 0.30036378963684185, 0.22148967050890503, 0.11776599062723939, 0.1112629225244871, 0.07248974144177063, 0.31392346579773295, 0.11642156820684864, 0.23277828595846, 0.27315102825750665, 0.10417861245265847, 0.19039342095246847, 0.1679942273827797, 0.10167808297320831, 0.07616059586943538, 0.17105394612346236, 0.12500151358681744, 0.23919722520178413, 0.07845111215034989, 0.1245876885694504, 0.08742772534386235, 0.09124494133054097, 0.04604406308275605, 0.2700110150812488, 0.1730756502564659, 0.1568864321992886, 0.301464783995552, 0.08661943850772261, 0.17291598426721763, 0.07844323760715582, 0.1762749192416052, 0.198977290108442, 0.14312686463978938, 0.03261416902984667, 0.03054111706547881, 0.037543319868447673, 0.08079612114761033, 0.08678542311860352, 0.060218295902046116, 0.11474317879204149, 0.12942711737335458, 0.07879379011385884, 0.062299998236475564, 0.03503384923598232, 0.05560824559122778, 0.1030194030756865, 0.0990760726210339, 0.13145477710517545, 0.01610179754280706, 0.0771401766704466, 0.061047090270442084, 0.07818698688785436, 0.0918949199397228, 0.09319227375868416, 0.08811378867269333, 0.0734461336023442, 0.03174724699505884, 0.06212261310142785, 0.02739438570280992, 0.049484310869164085, 0.03895022424867601, 0.04981987358064388, 0.05074515396988789, 0.03983638177250487, 0.08492574436301814, 0.05200150962079785, 0.030332530426612006, 0.05045440734712168, 0.032420542822017895, 0.06781557374825271, 0.07558561610917186, 0.027929277068080117, 0.05190464068235282, 0.012895986292345257, 0.03725704383043042, 0.02313330775136576, 0.03012681363097762, 0.04319413731979136, 0.02004262906877935, 0.02229894280837623, 0.02751672654121899, 0.035268494043214145, 0.039428610645982264, 0.03204125921046961, 0.02661427731158411, 0.03676389344140057, 0.06299340377472308, 0.03454798975959152, 0.030682535393103484, 0.04194412309936003, 0.015105433473034708, 0.027204625626327257, 0.040608940778716955, 0.023246911952024316, 0.021970949590423193, 0.008230711804036533, 0.04246734441958538, 0.030043344573878424, 0.01763246479153635, 0.0284837154134865, 0.03091560022319131, 0.016077278588038684, 0.026727421826212808, 0.024177658392888598, 0.017464338244271845, 0.022840108982812742, 0.017288535993820235, 0.02148621491454814, 0.028915874613482787, 0.011973472052999645, 0.021289459056199462, 0.012232264444989205, 0.025275973834323114, 0.030583171943476435, 0.00984498326368509, 0.028018216705707557, 0.023579674426418645, 0.02469447606856087, 0.014756764283264868, 0.029010128608519467, 0.0195356637239778, 0.0203630879049658, 0.01757106751657544, 0.010262528385192541, 0.013262062860265728, 0.021245177339018713, 0.021721729017228963, 0.03689103708585828, 0.01351651153150445, 0.009935061205575439, 0.02262841694298828, 0.01758433643867625, 0.012217754002525399, 0.01932559752139203, 0.014958402214521075, 0.027567374480082448, 0.018025596817403688, 0.010517139914541018, 0.017699444398169662, 0.015909905810328133, 0.01982560712916763, 0.013691125194401766, 0.016550432755662575, 0.009078229764578996, 0.007944549330278787, 0.01326805701525457, 0.025850371833740872, 0.013628392763211048, 0.015110213725853711, 0.009527124064834098, 0.009334541094691867, 0.006085630296447723, 0.017934603923836508, 0.009869102393588697, 0.01877156124176056, 0.010938960103062646, 0.009361092362202691, 0.01383921247847794, 0.017888971550242515, 0.012172627766161605, 0.012892033167378431, 0.008206894884965657, 0.00905059199327114, 0.017796027177664306, 0.009163779352044385, 0.009489457224419108, 0.007692151459628377, 0.00860071117621646, 0.01337183452254036, 0.01293385673719119, 0.008399997383587529, 0.01264211171765953, 0.013858170752131613, 0.004903633772470935, 0.009291095305653377, 0.016828954981043625, 0.008532854220705712, 0.014868278238738564, 0.009907119201207511, 0.015346446124348127, 0.011719231907154463, 0.004210578679004389, 0.014362194165312063, 0.013576512265913249, 0.009769343830962701, 0.014373982427621002, 0.00897708285199268, 0.007879795891827508, 0.009862970582605868, 0.007378127775919613, 0.007813068998589977, 0.006245027504013833, 0.008922686984725212, 0.0061601486759959275, 0.011447400788466693, 0.010592205644013353, 0.008779988807601034, 0.007384445939282525, 0.005231718442068179, 0.005374112996673978, 0.007218698803703233, 0.009120530892147403, 0.01006021025842885, 0.007103090284885151, 0.008748900552573213, 0.0035180300370962697, 0.007995185752619328, 0.006997362700971801, 0.009063252542229545, 0.00764816543458346, 0.010021398502396002, 0.006573226155617962, 0.008800037481439274, 0.007748006161016941, 0.010937734736951477, 0.006072712121079891, 0.00538165310937602, 0.007465023465125563, 0.004909194654504705, 0.0058572163199658075, 0.00853616274950926, 0.010528373331639172, 0.007736132999175462, 0.00718902707290607, 0.0029909235888185715, 0.010247816293518714, 0.006803988991259701, 0.0063724690003217335, 0.002905587914533317, 0.007007899223572968, 0.00644505755233859, 0.006488757529163384, 0.002572594387541828, 0.0044282391344211405, 0.006817876694734597, 0.004539010813511677, 0.004361713562243303, 0.0064384843265000965, 0.006053104493819026, 0.006276737155629471, 0.004831008743959072, 0.010958469366237057, 0.0060074596681144, 0.0029252938844696874, 0.0030119681989681257, 0.006741203732546441, 0.005249409183349996, 0.0052529349195933744, 0.00732885029553593, 0.003807727914067893, 0.0040358186911486175, 0.004833614462360483, 0.0047207717788715015, 0.003263436409534696, 0.00383955053305814, 0.009459019838161612, 0.002921542476523221, 0.003654111207226949, 0.0032846278323096566, 0.004885562824869446, 0.00508569049506098, 0.006373271890193562, 0.0050280810405093435, 0.006200431003390502, 0.0035311075063430976, 0.0037279718019347834, 0.010223271739306093, 0.005651504630691279, 0.005008483937049452, 0.006027997653171636, 0.006361605154960152, 0.0034867165884126454, 0.0029331952938917664, 0.004020571156771747, 0.006245607454844204, 0.005093410584849283, 0.0015902946597379669, 0.004266418964347722, 0.005104216601482122, 0.0033153953966085464, 0.002263779852399839, 0.007160398650149055, 0.006600048804114131, 0.00439410620243234, 0.006613928781317931, 0.003303326800842834, 0.004055481427072928, 0.006521513929952537, 0.004458408414755986, 0.005556058564968627, 0.0032512623862984338, 0.0029908740123645145, 0.0031054786424553732, 0.004483647626745672, 0.005548467720664309, 0.006621132659978442, 0.004664450628537602, 0.005383152499169567, 0.0044664641921742234, 0.006147528404749515, 0.004589144306922832, 0.00414603240714461, 0.009087762031101948, 0.0044056132546797895, 0.003909099621062447, 0.002374078828294372, 0.004199905211404633, 0.006057050601898557, 0.0032105797489554015, 0.0031396686273902057, 0.002901846718032038, 0.004217244810202035, 0.005485786030073221, 0.00456064989139799, 0.004663277704692785, 0.003895897854008306, 0.0035728899014997257, 0.0027536640593061232, 0.004219869830662639, 0.0026945877464181088, 0.003658771101894766, 0.006712447580500221, 0.0034696071602221797, 0.003184528785590471, 0.006798037894825511, 0.0049662754478161, 0.0027370586846505564, 0.004805179326626523, 0.004699095964409763, 0.003251637175180632, 0.003271414183918524, 0.005589838323135825, 0.0032441389877758903, 0.0067858601622701, 0.006118630688254335, 0.004131691877312017, 0.003082671607921422, 0.0042051060893134274, 0.0038884924191254335, 0.002962506320237182, 0.002076157311928311, 0.0020558838118918516, 0.003356738651684689, 0.0028620378255369678, 0.0029686248846644923, 0.00349765788185177, 0.00298678405030627, 0.0027571970161227623, 0.003610517848250861, 0.004466379683559644, 0.003580558601558644, 0.0022491600988507276, 0.0038862574565933887, 0.0020585755986470416, 0.003350119130455706, 0.002019017040326986, 0.003892469532275133, 0.0035536293901333015, 0.0032306450342034334, 0.002800186661059094, 0.003231323578908408, 0.002385128178917209, 0.0031244951981927393, 0.004250466780830907, 0.0033645742777648154, 0.0025017081009596756, 0.003921704589888853, 0.004116295634103122, 0.003620619447717498, 0.0017603315439187269, 0.002469395053677466, 0.0029986673784294093, 0.0032258431313620115, 0.002981434723113973, 0.0013819217487384989, 0.0032949836318237656, 0.004161248977183076, 0.0025140866251171495, 0.002230685022040397, 0.003939352093486844, 0.00295376376826937, 0.0030149312382889482, 0.002842557068409717, 0.0024130196333980534, 0.003387120788490034, 0.002919653903554604, 0.004610478841244241, 0.004830742754672676, 0.004849249306296692, 0.004092267674917679, 0.00323413799897149, 0.002065377079902587, 0.002318131956749458, 0.0026479241884973977, 0.002350889617063655, 0.002304473890369692, 0.004179157886665444, 0.00311171460734571, 0.002343209887573981, 0.0036532783388440442, 0.0028781183348701484, 0.005051741188932144, 0.0037435946121089254, 0.004010236480847776, 0.0017777984537796284, 0.0018778375051706848, 0.0035253957550480178, 0.004959555307564778, 0.001887589324937939, 0.00299654626254093, 0.003778291377038163, 0.0020178715544124266, 0.0030543333680973016, 0.002629996715882432, 0.00205269095445219, 0.0020654795626069322, 0.002139058456069485, 0.002084374852789543, 0.0035613454363410397, 0.0020711168632821953, 0.0017361541606328416, 0.002791846028621271, 0.0015494622708039484, 0.0026310909319199483, 0.0017459589478171595, 0.003877744793507751, 0.0026055125449147, 0.0023794135897436224, 0.002434979975880696, 0.002614170525187537, 0.002936776859233544, 0.001795591592682955, 0.002159181356940997, 0.0024298340813780532, 0.0022545333375030376, 0.0020952799545892044, 0.002221729193596045, 0.0031591962917994717, 0.0026405344122758093, 0.002253969437781386, 0.0020810698389622123, 0.002098951589179424, 0.0023879481220214368, 0.0024659611953409643, 0.0012442477664569135, 0.0017053509258779108, 0.002381196224160104, 0.0024068952505818864, 0.001942113174759474, 0.002249302904875942, 0.001991766694607963, 0.0021797384551068, 0.0024118015747329564, 0.0027592926374591935, 0.0020553921174259224, 0.0025360110659460853, 0.0015094830603940579, 0.0018396449015827107, 0.0019136425148368588, 0.0035232864151260052, 0.0037251412669029194, 0.002507815100855471, 0.0023615706936043353, 0.0015051251377963782, 0.0017464836483383286, 0.0011076337311476472, 0.0025276373927947977, 0.0019052993614125083, 0.0019418524240472032, 0.0017731584356773104, 0.0021402632026583635, 0.0022181837621589057, 0.0017189487269323558, 0.0017419271948256071, 0.001517056077212622, 0.0036580975340142043, 0.0021318460724613457, 0.0022641845365276372, 0.001735248554264855, 0.0023377994092444844, 0.0013196547302400724, 0.002583472775782791, 0.003131088204767879, 0.0017153648706021698, 0.0014351853360627023, 0.001400209395550599, 0.0024686255004100384, 0.0025826632567593215, 0.0022213754367474653, 0.0033098318179195334, 0.0015867622306953801, 0.0011906861619485998, 0.00190017199205851, 0.002267275949970542, 0.0017973148529912772, 0.0017866884705282319, 0.0015852220841670378, 0.002458232138383762, 0.0014244905431833356, 0.0019919869100415086, 0.0022330823709953945, 0.0027735989679547437, 0.0026790056440027204, 0.0016648337697080765, 0.0020633290652193647, 0.0018820694134797256, 0.0023719812136731443, 0.0019624419874155175, 0.0019377714647102334, 0.0021468643512156747, 0.001613401638587184, 0.0027210783105111533, 0.0013589544920751294, 0.000832850823144387, 0.0012942071864269781, 0.002052153088747626, 0.0018600081981792272, 0.00086116675978332, 0.001959338149785799, 0.0013139268230490256, 0.0014306314232718851, 0.0016959028832292259, 0.0017518964170656918, 0.0017331592446227701, 0.0015918302126712758, 0.0011052993354616284, 0.000922927035018237, 0.0017368995073886746, 0.0009850095962978867, 0.0011681268652901126, 0.0006304484831412962, 0.001487507723879396, 0.0026474813828503124, 0.001394117871138313, 0.0008204897749794848, 0.00160438701564368, 0.0015889317417497155, 0.0020780690656655564, 0.0010636636904031375, 0.0016175576179632877, 0.0008621424073317424, 0.0017633454725117536, 0.0019517233887101733, 0.0016230313448372285, 0.0010388349009595177, 0.0013960343425225312, 0.0021584754233142122, 0.0015085948267311207, 0.0015261316094420914, 0.0014562299200879358, 0.0021428721253259246, 0.0015409984000469296, 0.001926323120287074, 0.001567330547213729, 0.0015021132826861594, 0.0016439893587461441, 0.0016136400756901178, 0.0007363023758899155, 0.0013633331881789958]\n",
      "Accuracy history: [0.16, 0.24, 0.22, 0.24, 0.26, 0.42, 0.42, 0.42, 0.46, 0.46, 0.56, 0.54, 0.62, 0.68, 0.72, 0.88, 0.88, 0.88, 0.9, 0.9, 0.96, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Adam\n",
    "\n",
    "adam = ConvNet4(input_shape=input_shape, wt_scale=wt_scale, verbose=False)\n",
    "adam.compile('adam')\n",
    "adam.fit(x_dev, y_dev, x_val, y_val, mini_batch_sz=mini_batch_sz, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train...\n",
      "750 iterations. 5 iter/epoch.\n",
      "Iteration: 1/750.\n",
      "Time taken for iteration 0: 1.6611967086791992\n",
      "Estimated time to complete: 1245.8975315093994\n",
      "Iteration: 2/750.\n",
      "Iteration: 3/750.\n",
      "Iteration: 4/750.\n",
      "Iteration: 5/750.\n",
      "Iteration: 6/750.\n",
      "Iteration: 7/750.\n",
      "Iteration: 8/750.\n",
      "Iteration: 9/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3083244218204695, 2.305725695413475, 2.297431716772699]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.16, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 10/750.\n",
      "Iteration: 11/750.\n",
      "Iteration: 12/750.\n",
      "Iteration: 13/750.\n",
      "Iteration: 14/750.\n",
      "Iteration: 15/750.\n",
      "Iteration: 16/750.\n",
      "Iteration: 17/750.\n",
      "Iteration: 18/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3018450824131884, 2.3036695764710173, 2.3017066047698584]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.16, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 19/750.\n",
      "Iteration: 20/750.\n",
      "Iteration: 21/750.\n",
      "Iteration: 22/750.\n",
      "Iteration: 23/750.\n",
      "Iteration: 24/750.\n",
      "Iteration: 25/750.\n",
      "Iteration: 26/750.\n",
      "Iteration: 27/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.296826694035708, 2.2988054741544746, 2.298301305372676]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.16, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 28/750.\n",
      "Iteration: 29/750.\n",
      "Iteration: 30/750.\n",
      "Iteration: 31/750.\n",
      "Iteration: 32/750.\n",
      "Iteration: 33/750.\n",
      "Iteration: 34/750.\n",
      "Iteration: 35/750.\n",
      "Iteration: 36/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2966168260260056, 2.299502510069381, 2.297408434557621]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.16, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 37/750.\n",
      "Iteration: 38/750.\n",
      "Iteration: 39/750.\n",
      "Iteration: 40/750.\n",
      "Iteration: 41/750.\n",
      "Iteration: 42/750.\n",
      "Iteration: 43/750.\n",
      "Iteration: 44/750.\n",
      "Iteration: 45/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.298535173218746, 2.2969774199484214, 2.287743382443448]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.16, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 46/750.\n",
      "Iteration: 47/750.\n",
      "Iteration: 48/750.\n",
      "Iteration: 49/750.\n",
      "Iteration: 50/750.\n",
      "Iteration: 51/750.\n",
      "Iteration: 52/750.\n",
      "Iteration: 53/750.\n",
      "Iteration: 54/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2839536623842185, 2.2938137555028617, 2.302438358369893]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 55/750.\n",
      "Iteration: 56/750.\n",
      "Iteration: 57/750.\n",
      "Iteration: 58/750.\n",
      "Iteration: 59/750.\n",
      "Iteration: 60/750.\n",
      "Iteration: 61/750.\n",
      "Iteration: 62/750.\n",
      "Iteration: 63/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3025548231332373, 2.2845544381527216, 2.281798459737148]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 64/750.\n",
      "Iteration: 65/750.\n",
      "Iteration: 66/750.\n",
      "Iteration: 67/750.\n",
      "Iteration: 68/750.\n",
      "Iteration: 69/750.\n",
      "Iteration: 70/750.\n",
      "Iteration: 71/750.\n",
      "Iteration: 72/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2835935731873747, 2.2789792517783667, 2.2761709594171795]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 73/750.\n",
      "Iteration: 74/750.\n",
      "Iteration: 75/750.\n",
      "Iteration: 76/750.\n",
      "Iteration: 77/750.\n",
      "Iteration: 78/750.\n",
      "Iteration: 79/750.\n",
      "Iteration: 80/750.\n",
      "Iteration: 81/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.274863026115056, 2.294948740664123, 2.2861885214757716]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 82/750.\n",
      "Iteration: 83/750.\n",
      "Iteration: 84/750.\n",
      "Iteration: 85/750.\n",
      "Iteration: 86/750.\n",
      "Iteration: 87/750.\n",
      "Iteration: 88/750.\n",
      "Iteration: 89/750.\n",
      "Iteration: 90/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2580556523144146, 2.2770504059269747, 2.2811090608307416]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 91/750.\n",
      "Iteration: 92/750.\n",
      "Iteration: 93/750.\n",
      "Iteration: 94/750.\n",
      "Iteration: 95/750.\n",
      "Iteration: 96/750.\n",
      "Iteration: 97/750.\n",
      "Iteration: 98/750.\n",
      "Iteration: 99/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2767012191425837, 2.261673609481268, 2.272762194269741]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 100/750.\n",
      "Iteration: 101/750.\n",
      "Iteration: 102/750.\n",
      "Iteration: 103/750.\n",
      "Iteration: 104/750.\n",
      "Iteration: 105/750.\n",
      "Iteration: 106/750.\n",
      "Iteration: 107/750.\n",
      "Iteration: 108/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2664487825242845, 2.286639542318909, 2.247916495356978]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 109/750.\n",
      "Iteration: 110/750.\n",
      "Iteration: 111/750.\n",
      "Iteration: 112/750.\n",
      "Iteration: 113/750.\n",
      "Iteration: 114/750.\n",
      "Iteration: 115/750.\n",
      "Iteration: 116/750.\n",
      "Iteration: 117/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2729363999356873, 2.273953395587147, 2.2786906412783487]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 118/750.\n",
      "Iteration: 119/750.\n",
      "Iteration: 120/750.\n",
      "Iteration: 121/750.\n",
      "Iteration: 122/750.\n",
      "Iteration: 123/750.\n",
      "Iteration: 124/750.\n",
      "Iteration: 125/750.\n",
      "Iteration: 126/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2537882165991663, 2.278439164214905, 2.2486419643059565]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 127/750.\n",
      "Iteration: 128/750.\n",
      "Iteration: 129/750.\n",
      "Iteration: 130/750.\n",
      "Iteration: 131/750.\n",
      "Iteration: 132/750.\n",
      "Iteration: 133/750.\n",
      "Iteration: 134/750.\n",
      "Iteration: 135/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.32051304490723, 2.3152721759448656, 2.2505928687197856]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 136/750.\n",
      "Iteration: 137/750.\n",
      "Iteration: 138/750.\n",
      "Iteration: 139/750.\n",
      "Iteration: 140/750.\n",
      "Iteration: 141/750.\n",
      "Iteration: 142/750.\n",
      "Iteration: 143/750.\n",
      "Iteration: 144/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2918283532741106, 2.230395258419411, 2.2907653948807876]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 145/750.\n",
      "Iteration: 146/750.\n",
      "Iteration: 147/750.\n",
      "Iteration: 148/750.\n",
      "Iteration: 149/750.\n",
      "Iteration: 150/750.\n",
      "Iteration: 151/750.\n",
      "Iteration: 152/750.\n",
      "Iteration: 153/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.250193863582836, 2.2973584825330042, 2.268586391634377]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 154/750.\n",
      "Iteration: 155/750.\n",
      "Iteration: 156/750.\n",
      "Iteration: 157/750.\n",
      "Iteration: 158/750.\n",
      "Iteration: 159/750.\n",
      "Iteration: 160/750.\n",
      "Iteration: 161/750.\n",
      "Iteration: 162/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2824466859916486, 2.244650987574829, 2.2543351594952203]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 163/750.\n",
      "Iteration: 164/750.\n",
      "Iteration: 165/750.\n",
      "Iteration: 166/750.\n",
      "Iteration: 167/750.\n",
      "Iteration: 168/750.\n",
      "Iteration: 169/750.\n",
      "Iteration: 170/750.\n",
      "Iteration: 171/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.232717186500342, 2.2397897020179474, 2.2738842139177944]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 172/750.\n",
      "Iteration: 173/750.\n",
      "Iteration: 174/750.\n",
      "Iteration: 175/750.\n",
      "Iteration: 176/750.\n",
      "Iteration: 177/750.\n",
      "Iteration: 178/750.\n",
      "Iteration: 179/750.\n",
      "Iteration: 180/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2679793280689706, 2.28689228759865, 2.276174764319612]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 181/750.\n",
      "Iteration: 182/750.\n",
      "Iteration: 183/750.\n",
      "Iteration: 184/750.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 185/750.\n",
      "Iteration: 186/750.\n",
      "Iteration: 187/750.\n",
      "Iteration: 188/750.\n",
      "Iteration: 189/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2809092996558507, 2.2789688282647362, 2.28805322376837]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 190/750.\n",
      "Iteration: 191/750.\n",
      "Iteration: 192/750.\n",
      "Iteration: 193/750.\n",
      "Iteration: 194/750.\n",
      "Iteration: 195/750.\n",
      "Iteration: 196/750.\n",
      "Iteration: 197/750.\n",
      "Iteration: 198/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2551803876097742, 2.268825191182065, 2.2279083463692317]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 199/750.\n",
      "Iteration: 200/750.\n",
      "Iteration: 201/750.\n",
      "Iteration: 202/750.\n",
      "Iteration: 203/750.\n",
      "Iteration: 204/750.\n",
      "Iteration: 205/750.\n",
      "Iteration: 206/750.\n",
      "Iteration: 207/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.308294898786341, 2.2447185877161426, 2.2561030174177947]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 208/750.\n",
      "Iteration: 209/750.\n",
      "Iteration: 210/750.\n",
      "Iteration: 211/750.\n",
      "Iteration: 212/750.\n",
      "Iteration: 213/750.\n",
      "Iteration: 214/750.\n",
      "Iteration: 215/750.\n",
      "Iteration: 216/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.274031923422989, 2.292507221173354, 2.2214233061154625]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 217/750.\n",
      "Iteration: 218/750.\n",
      "Iteration: 219/750.\n",
      "Iteration: 220/750.\n",
      "Iteration: 221/750.\n",
      "Iteration: 222/750.\n",
      "Iteration: 223/750.\n",
      "Iteration: 224/750.\n",
      "Iteration: 225/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3173336547347247, 2.267883190300659, 2.216650036147166]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 226/750.\n",
      "Iteration: 227/750.\n",
      "Iteration: 228/750.\n",
      "Iteration: 229/750.\n",
      "Iteration: 230/750.\n",
      "Iteration: 231/750.\n",
      "Iteration: 232/750.\n",
      "Iteration: 233/750.\n",
      "Iteration: 234/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3015755984933626, 2.238681765961866, 2.2016660365529783]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 235/750.\n",
      "Iteration: 236/750.\n",
      "Iteration: 237/750.\n",
      "Iteration: 238/750.\n",
      "Iteration: 239/750.\n",
      "Iteration: 240/750.\n",
      "Iteration: 241/750.\n",
      "Iteration: 242/750.\n",
      "Iteration: 243/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3058068408717873, 2.2569811975613874, 2.269927435171348]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 244/750.\n",
      "Iteration: 245/750.\n",
      "Iteration: 246/750.\n",
      "Iteration: 247/750.\n",
      "Iteration: 248/750.\n",
      "Iteration: 249/750.\n",
      "Iteration: 250/750.\n",
      "Iteration: 251/750.\n",
      "Iteration: 252/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2473843431840854, 2.1699650093380507, 2.330035628675186]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 253/750.\n",
      "Iteration: 254/750.\n",
      "Iteration: 255/750.\n",
      "Iteration: 256/750.\n",
      "Iteration: 257/750.\n",
      "Iteration: 258/750.\n",
      "Iteration: 259/750.\n",
      "Iteration: 260/750.\n",
      "Iteration: 261/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.271419112296858, 2.201561180448635, 2.2923443997167285]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 262/750.\n",
      "Iteration: 263/750.\n",
      "Iteration: 264/750.\n",
      "Iteration: 265/750.\n",
      "Iteration: 266/750.\n",
      "Iteration: 267/750.\n",
      "Iteration: 268/750.\n",
      "Iteration: 269/750.\n",
      "Iteration: 270/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3423970038143236, 2.2769262981458995, 2.3062019756584777]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 271/750.\n",
      "Iteration: 272/750.\n",
      "Iteration: 273/750.\n",
      "Iteration: 274/750.\n",
      "Iteration: 275/750.\n",
      "Iteration: 276/750.\n",
      "Iteration: 277/750.\n",
      "Iteration: 278/750.\n",
      "Iteration: 279/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2407741533860976, 2.172899416933032, 2.1984333670970226]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 280/750.\n",
      "Iteration: 281/750.\n",
      "Iteration: 282/750.\n",
      "Iteration: 283/750.\n",
      "Iteration: 284/750.\n",
      "Iteration: 285/750.\n",
      "Iteration: 286/750.\n",
      "Iteration: 287/750.\n",
      "Iteration: 288/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2981574744501674, 2.2600462418147753, 2.2166681453444483]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 289/750.\n",
      "Iteration: 290/750.\n",
      "Iteration: 291/750.\n",
      "Iteration: 292/750.\n",
      "Iteration: 293/750.\n",
      "Iteration: 294/750.\n",
      "Iteration: 295/750.\n",
      "Iteration: 296/750.\n",
      "Iteration: 297/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2155338896642607, 2.197460097265643, 2.318388907515723]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 298/750.\n",
      "Iteration: 299/750.\n",
      "Iteration: 300/750.\n",
      "Iteration: 301/750.\n",
      "Iteration: 302/750.\n",
      "Iteration: 303/750.\n",
      "Iteration: 304/750.\n",
      "Iteration: 305/750.\n",
      "Iteration: 306/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.183530894514179, 2.1687370910024586, 2.280636857520292]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 307/750.\n",
      "Iteration: 308/750.\n",
      "Iteration: 309/750.\n",
      "Iteration: 310/750.\n",
      "Iteration: 311/750.\n",
      "Iteration: 312/750.\n",
      "Iteration: 313/750.\n",
      "Iteration: 314/750.\n",
      "Iteration: 315/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3083214739652798, 2.2727792010817764, 2.2652212535838214]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 316/750.\n",
      "Iteration: 317/750.\n",
      "Iteration: 318/750.\n",
      "Iteration: 319/750.\n",
      "Iteration: 320/750.\n",
      "Iteration: 321/750.\n",
      "Iteration: 322/750.\n",
      "Iteration: 323/750.\n",
      "Iteration: 324/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2743787708138794, 2.205430793999642, 2.216025726528513]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 325/750.\n",
      "Iteration: 326/750.\n",
      "Iteration: 327/750.\n",
      "Iteration: 328/750.\n",
      "Iteration: 329/750.\n",
      "Iteration: 330/750.\n",
      "Iteration: 331/750.\n",
      "Iteration: 332/750.\n",
      "Iteration: 333/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2523932586141244, 2.2818711031090086, 2.2729374546977894]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 334/750.\n",
      "Iteration: 335/750.\n",
      "Iteration: 336/750.\n",
      "Iteration: 337/750.\n",
      "Iteration: 338/750.\n",
      "Iteration: 339/750.\n",
      "Iteration: 340/750.\n",
      "Iteration: 341/750.\n",
      "Iteration: 342/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3870551622869107, 2.227611855833517, 2.2965227388282172]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 343/750.\n",
      "Iteration: 344/750.\n",
      "Iteration: 345/750.\n",
      "Iteration: 346/750.\n",
      "Iteration: 347/750.\n",
      "Iteration: 348/750.\n",
      "Iteration: 349/750.\n",
      "Iteration: 350/750.\n",
      "Iteration: 351/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1972767659635544, 2.1985326184695064, 2.2349970739214924]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 352/750.\n",
      "Iteration: 353/750.\n",
      "Iteration: 354/750.\n",
      "Iteration: 355/750.\n",
      "Iteration: 356/750.\n",
      "Iteration: 357/750.\n",
      "Iteration: 358/750.\n",
      "Iteration: 359/750.\n",
      "Iteration: 360/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1742901250718467, 2.0799756410980557, 2.2220841861419194]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 361/750.\n",
      "Iteration: 362/750.\n",
      "Iteration: 363/750.\n",
      "Iteration: 364/750.\n",
      "Iteration: 365/750.\n",
      "Iteration: 366/750.\n",
      "Iteration: 367/750.\n",
      "Iteration: 368/750.\n",
      "Iteration: 369/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1819677069929813, 2.2273059778822533, 2.1947998105505557]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 370/750.\n",
      "Iteration: 371/750.\n",
      "Iteration: 372/750.\n",
      "Iteration: 373/750.\n",
      "Iteration: 374/750.\n",
      "Iteration: 375/750.\n",
      "Iteration: 376/750.\n",
      "Iteration: 377/750.\n",
      "Iteration: 378/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.24355648076423, 2.176654193386759, 2.343633754517829]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 379/750.\n",
      "Iteration: 380/750.\n",
      "Iteration: 381/750.\n",
      "Iteration: 382/750.\n",
      "Iteration: 383/750.\n",
      "Iteration: 384/750.\n",
      "Iteration: 385/750.\n",
      "Iteration: 386/750.\n",
      "Iteration: 387/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.258344127286521, 2.287439493420776, 2.2160262565649242]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 388/750.\n",
      "Iteration: 389/750.\n",
      "Iteration: 390/750.\n",
      "Iteration: 391/750.\n",
      "Iteration: 392/750.\n",
      "Iteration: 393/750.\n",
      "Iteration: 394/750.\n",
      "Iteration: 395/750.\n",
      "Iteration: 396/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2432384498691653, 2.247009227711138, 2.3254272749122347]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 397/750.\n",
      "Iteration: 398/750.\n",
      "Iteration: 399/750.\n",
      "Iteration: 400/750.\n",
      "Iteration: 401/750.\n",
      "Iteration: 402/750.\n",
      "Iteration: 403/750.\n",
      "Iteration: 404/750.\n",
      "Iteration: 405/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.241971823851108, 2.1875700843869, 2.158207424004244]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 406/750.\n",
      "Iteration: 407/750.\n",
      "Iteration: 408/750.\n",
      "Iteration: 409/750.\n",
      "Iteration: 410/750.\n",
      "Iteration: 411/750.\n",
      "Iteration: 412/750.\n",
      "Iteration: 413/750.\n",
      "Iteration: 414/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2484347859408387, 2.3060932894119572, 2.2624999998559]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 415/750.\n",
      "Iteration: 416/750.\n",
      "Iteration: 417/750.\n",
      "Iteration: 418/750.\n",
      "Iteration: 419/750.\n",
      "Iteration: 420/750.\n",
      "Iteration: 421/750.\n",
      "Iteration: 422/750.\n",
      "Iteration: 423/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1886142866691873, 2.128478050810751, 2.311385551409236]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 424/750.\n",
      "Iteration: 425/750.\n",
      "Iteration: 426/750.\n",
      "Iteration: 427/750.\n",
      "Iteration: 428/750.\n",
      "Iteration: 429/750.\n",
      "Iteration: 430/750.\n",
      "Iteration: 431/750.\n",
      "Iteration: 432/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.194539654885159, 2.2900531022302424, 2.2121713336740445]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 433/750.\n",
      "Iteration: 434/750.\n",
      "Iteration: 435/750.\n",
      "Iteration: 436/750.\n",
      "Iteration: 437/750.\n",
      "Iteration: 438/750.\n",
      "Iteration: 439/750.\n",
      "Iteration: 440/750.\n",
      "Iteration: 441/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.134445198940538, 2.2699183887282217, 2.1507394401438193]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 442/750.\n",
      "Iteration: 443/750.\n",
      "Iteration: 444/750.\n",
      "Iteration: 445/750.\n",
      "Iteration: 446/750.\n",
      "Iteration: 447/750.\n",
      "Iteration: 448/750.\n",
      "Iteration: 449/750.\n",
      "Iteration: 450/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1917973532475976, 2.2950102348914974, 2.100521391053965]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 451/750.\n",
      "Iteration: 452/750.\n",
      "Iteration: 453/750.\n",
      "Iteration: 454/750.\n",
      "Iteration: 455/750.\n",
      "Iteration: 456/750.\n",
      "Iteration: 457/750.\n",
      "Iteration: 458/750.\n",
      "Iteration: 459/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.208949986017907, 2.302902253085499, 2.2244442228505825]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 460/750.\n",
      "Iteration: 461/750.\n",
      "Iteration: 462/750.\n",
      "Iteration: 463/750.\n",
      "Iteration: 464/750.\n",
      "Iteration: 465/750.\n",
      "Iteration: 466/750.\n",
      "Iteration: 467/750.\n",
      "Iteration: 468/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.311667349891113, 2.2451032878593913, 2.114113198046391]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 469/750.\n",
      "Iteration: 470/750.\n",
      "Iteration: 471/750.\n",
      "Iteration: 472/750.\n",
      "Iteration: 473/750.\n",
      "Iteration: 474/750.\n",
      "Iteration: 475/750.\n",
      "Iteration: 476/750.\n",
      "Iteration: 477/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.362584979184528, 2.1842563367266337, 2.2175075899941072]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 478/750.\n",
      "Iteration: 479/750.\n",
      "Iteration: 480/750.\n",
      "Iteration: 481/750.\n",
      "Iteration: 482/750.\n",
      "Iteration: 483/750.\n",
      "Iteration: 484/750.\n",
      "Iteration: 485/750.\n",
      "Iteration: 486/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.274318562637799, 2.065706641955925, 2.1479803585598147]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 487/750.\n",
      "Iteration: 488/750.\n",
      "Iteration: 489/750.\n",
      "Iteration: 490/750.\n",
      "Iteration: 491/750.\n",
      "Iteration: 492/750.\n",
      "Iteration: 493/750.\n",
      "Iteration: 494/750.\n",
      "Iteration: 495/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2761224118898045, 2.1852569399565933, 2.208590116865381]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 496/750.\n",
      "Iteration: 497/750.\n",
      "Iteration: 498/750.\n",
      "Iteration: 499/750.\n",
      "Iteration: 500/750.\n",
      "Iteration: 501/750.\n",
      "Iteration: 502/750.\n",
      "Iteration: 503/750.\n",
      "Iteration: 504/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.116283809246591, 2.2028335737626565, 2.2326783799474863]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 505/750.\n",
      "Iteration: 506/750.\n",
      "Iteration: 507/750.\n",
      "Iteration: 508/750.\n",
      "Iteration: 509/750.\n",
      "Iteration: 510/750.\n",
      "Iteration: 511/750.\n",
      "Iteration: 512/750.\n",
      "Iteration: 513/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1968076409861594, 2.274975264968471, 1.9855868075389775]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 514/750.\n",
      "Iteration: 515/750.\n",
      "Iteration: 516/750.\n",
      "Iteration: 517/750.\n",
      "Iteration: 518/750.\n",
      "Iteration: 519/750.\n",
      "Iteration: 520/750.\n",
      "Iteration: 521/750.\n",
      "Iteration: 522/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1953933392272265, 2.215828703238852, 2.3100137441845474]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 523/750.\n",
      "Iteration: 524/750.\n",
      "Iteration: 525/750.\n",
      "Iteration: 526/750.\n",
      "Iteration: 527/750.\n",
      "Iteration: 528/750.\n",
      "Iteration: 529/750.\n",
      "Iteration: 530/750.\n",
      "Iteration: 531/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.146009311380914, 2.2716558721394833, 2.2301757314982242]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 532/750.\n",
      "Iteration: 533/750.\n",
      "Iteration: 534/750.\n",
      "Iteration: 535/750.\n",
      "Iteration: 536/750.\n",
      "Iteration: 537/750.\n",
      "Iteration: 538/750.\n",
      "Iteration: 539/750.\n",
      "Iteration: 540/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.332063398600958, 2.28230492085212, 1.9736515415982288]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 541/750.\n",
      "Iteration: 542/750.\n",
      "Iteration: 543/750.\n",
      "Iteration: 544/750.\n",
      "Iteration: 545/750.\n",
      "Iteration: 546/750.\n",
      "Iteration: 547/750.\n",
      "Iteration: 548/750.\n",
      "Iteration: 549/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1829420813707525, 2.2437436638194823, 2.286485280603498]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 550/750.\n",
      "Iteration: 551/750.\n",
      "Iteration: 552/750.\n",
      "Iteration: 553/750.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 554/750.\n",
      "Iteration: 555/750.\n",
      "Iteration: 556/750.\n",
      "Iteration: 557/750.\n",
      "Iteration: 558/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1614771451312933, 2.3313536288551155, 2.166155356616962]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 559/750.\n",
      "Iteration: 560/750.\n",
      "Iteration: 561/750.\n",
      "Iteration: 562/750.\n",
      "Iteration: 563/750.\n",
      "Iteration: 564/750.\n",
      "Iteration: 565/750.\n",
      "Iteration: 566/750.\n",
      "Iteration: 567/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1527029769325354, 2.256560466136464, 2.298755900566209]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 568/750.\n",
      "Iteration: 569/750.\n",
      "Iteration: 570/750.\n",
      "Iteration: 571/750.\n",
      "Iteration: 572/750.\n",
      "Iteration: 573/750.\n",
      "Iteration: 574/750.\n",
      "Iteration: 575/750.\n",
      "Iteration: 576/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3348348069425224, 2.2370508707565415, 2.1151456857666564]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 577/750.\n",
      "Iteration: 578/750.\n",
      "Iteration: 579/750.\n",
      "Iteration: 580/750.\n",
      "Iteration: 581/750.\n",
      "Iteration: 582/750.\n",
      "Iteration: 583/750.\n",
      "Iteration: 584/750.\n",
      "Iteration: 585/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.250810597110293, 2.293818748703291, 2.020107784339772]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 586/750.\n",
      "Iteration: 587/750.\n",
      "Iteration: 588/750.\n",
      "Iteration: 589/750.\n",
      "Iteration: 590/750.\n",
      "Iteration: 591/750.\n",
      "Iteration: 592/750.\n",
      "Iteration: 593/750.\n",
      "Iteration: 594/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.206940417505602, 2.2418594893978243, 2.435203323547574]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 595/750.\n",
      "Iteration: 596/750.\n",
      "Iteration: 597/750.\n",
      "Iteration: 598/750.\n",
      "Iteration: 599/750.\n",
      "Iteration: 600/750.\n",
      "Iteration: 601/750.\n",
      "Iteration: 602/750.\n",
      "Iteration: 603/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2084709801104907, 2.303666300630048, 2.2445135191978163]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 604/750.\n",
      "Iteration: 605/750.\n",
      "Iteration: 606/750.\n",
      "Iteration: 607/750.\n",
      "Iteration: 608/750.\n",
      "Iteration: 609/750.\n",
      "Iteration: 610/750.\n",
      "Iteration: 611/750.\n",
      "Iteration: 612/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1552491626113706, 2.1625910894636866, 2.2808288405473975]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 613/750.\n",
      "Iteration: 614/750.\n",
      "Iteration: 615/750.\n",
      "Iteration: 616/750.\n",
      "Iteration: 617/750.\n",
      "Iteration: 618/750.\n",
      "Iteration: 619/750.\n",
      "Iteration: 620/750.\n",
      "Iteration: 621/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.221663604770886, 2.1665319540090957, 2.1961497029878596]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 622/750.\n",
      "Iteration: 623/750.\n",
      "Iteration: 624/750.\n",
      "Iteration: 625/750.\n",
      "Iteration: 626/750.\n",
      "Iteration: 627/750.\n",
      "Iteration: 628/750.\n",
      "Iteration: 629/750.\n",
      "Iteration: 630/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2191093408749887, 2.396593898677007, 2.171257568036445]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 631/750.\n",
      "Iteration: 632/750.\n",
      "Iteration: 633/750.\n",
      "Iteration: 634/750.\n",
      "Iteration: 635/750.\n",
      "Iteration: 636/750.\n",
      "Iteration: 637/750.\n",
      "Iteration: 638/750.\n",
      "Iteration: 639/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2136993033344794, 2.1609558943243043, 2.252445795017344]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 640/750.\n",
      "Iteration: 641/750.\n",
      "Iteration: 642/750.\n",
      "Iteration: 643/750.\n",
      "Iteration: 644/750.\n",
      "Iteration: 645/750.\n",
      "Iteration: 646/750.\n",
      "Iteration: 647/750.\n",
      "Iteration: 648/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.270517546564173, 2.077304121289031, 2.317812946959583]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 649/750.\n",
      "Iteration: 650/750.\n",
      "Iteration: 651/750.\n",
      "Iteration: 652/750.\n",
      "Iteration: 653/750.\n",
      "Iteration: 654/750.\n",
      "Iteration: 655/750.\n",
      "Iteration: 656/750.\n",
      "Iteration: 657/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.156857904148581, 2.1572597766780865, 2.3406381410100545]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 658/750.\n",
      "Iteration: 659/750.\n",
      "Iteration: 660/750.\n",
      "Iteration: 661/750.\n",
      "Iteration: 662/750.\n",
      "Iteration: 663/750.\n",
      "Iteration: 664/750.\n",
      "Iteration: 665/750.\n",
      "Iteration: 666/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3015316106955814, 2.286232444163045, 2.162189343749455]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 667/750.\n",
      "Iteration: 668/750.\n",
      "Iteration: 669/750.\n",
      "Iteration: 670/750.\n",
      "Iteration: 671/750.\n",
      "Iteration: 672/750.\n",
      "Iteration: 673/750.\n",
      "Iteration: 674/750.\n",
      "Iteration: 675/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2639214328186092, 2.2495302165354953, 2.3265115638659126]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 676/750.\n",
      "Iteration: 677/750.\n",
      "Iteration: 678/750.\n",
      "Iteration: 679/750.\n",
      "Iteration: 680/750.\n",
      "Iteration: 681/750.\n",
      "Iteration: 682/750.\n",
      "Iteration: 683/750.\n",
      "Iteration: 684/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.05322340943015, 2.2531749255380404, 2.166140649838978]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 685/750.\n",
      "Iteration: 686/750.\n",
      "Iteration: 687/750.\n",
      "Iteration: 688/750.\n",
      "Iteration: 689/750.\n",
      "Iteration: 690/750.\n",
      "Iteration: 691/750.\n",
      "Iteration: 692/750.\n",
      "Iteration: 693/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1525457102214616, 2.2363613751447224, 2.1949898306646323]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 694/750.\n",
      "Iteration: 695/750.\n",
      "Iteration: 696/750.\n",
      "Iteration: 697/750.\n",
      "Iteration: 698/750.\n",
      "Iteration: 699/750.\n",
      "Iteration: 700/750.\n",
      "Iteration: 701/750.\n",
      "Iteration: 702/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2611339353388433, 2.3471530584952562, 2.148841912322166]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 703/750.\n",
      "Iteration: 704/750.\n",
      "Iteration: 705/750.\n",
      "Iteration: 706/750.\n",
      "Iteration: 707/750.\n",
      "Iteration: 708/750.\n",
      "Iteration: 709/750.\n",
      "Iteration: 710/750.\n",
      "Iteration: 711/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.279490786046512, 2.3235368702024597, 2.1051195390993214]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 712/750.\n",
      "Iteration: 713/750.\n",
      "Iteration: 714/750.\n",
      "Iteration: 715/750.\n",
      "Iteration: 716/750.\n",
      "Iteration: 717/750.\n",
      "Iteration: 718/750.\n",
      "Iteration: 719/750.\n",
      "Iteration: 720/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2942693326887547, 2.2811890204616265, 2.1792211232871224]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 721/750.\n",
      "Iteration: 722/750.\n",
      "Iteration: 723/750.\n",
      "Iteration: 724/750.\n",
      "Iteration: 725/750.\n",
      "Iteration: 726/750.\n",
      "Iteration: 727/750.\n",
      "Iteration: 728/750.\n",
      "Iteration: 729/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.004881074775065, 2.165659208624099, 2.3316752411036696]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 730/750.\n",
      "Iteration: 731/750.\n",
      "Iteration: 732/750.\n",
      "Iteration: 733/750.\n",
      "Iteration: 734/750.\n",
      "Iteration: 735/750.\n",
      "Iteration: 736/750.\n",
      "Iteration: 737/750.\n",
      "Iteration: 738/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.254104787989307, 2.237855966679909, 2.157645520877249]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 739/750.\n",
      "Iteration: 740/750.\n",
      "Iteration: 741/750.\n",
      "Iteration: 742/750.\n",
      "Iteration: 743/750.\n",
      "Iteration: 744/750.\n",
      "Iteration: 745/750.\n",
      "Iteration: 746/750.\n",
      "Iteration: 747/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3353460865512017, 2.3626829171958854, 2.1615144716245003]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 748/750.\n",
      "Iteration: 749/750.\n",
      "Iteration: 750/750.\n",
      "\n",
      "\n",
      "----------------FINAL OUTPUT----------------\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "Loss history: [2.309333752799659, 2.3100475210286935, 2.302734681556495, 2.303403251768486, 2.3008919111426778, 2.3030688070130423, 2.3083244218204695, 2.305725695413475, 2.297431716772699, 2.2971185583164577, 2.295047037977388, 2.3075030230866163, 2.2950493262693032, 2.302557765275875, 2.302703579304868, 2.3018450824131884, 2.3036695764710173, 2.3017066047698584, 2.3026332858630787, 2.3012372917903128, 2.3057936665043886, 2.300111243151785, 2.304657471487298, 2.294923033034066, 2.296826694035708, 2.2988054741544746, 2.298301305372676, 2.308902546197689, 2.293720393659111, 2.3003195881098004, 2.289276288094972, 2.3027816559633787, 2.2924948880619533, 2.2966168260260056, 2.299502510069381, 2.297408434557621, 2.2952866492919974, 2.2881488785226263, 2.303345064379366, 2.2877893250629198, 2.3013919178983135, 2.2975664705523684, 2.298535173218746, 2.2969774199484214, 2.287743382443448, 2.280444956983327, 2.2919499877728744, 2.298898649446118, 2.3153907594305907, 2.3021109911026945, 2.3012461177238124, 2.2839536623842185, 2.2938137555028617, 2.302438358369893, 2.2965300061329255, 2.294813767132136, 2.280370548936137, 2.2904263157753526, 2.286187019459692, 2.2942040775522634, 2.3025548231332373, 2.2845544381527216, 2.281798459737148, 2.297068291329227, 2.2617424650288287, 2.2809299499204694, 2.2538198629755417, 2.271909689382298, 2.2776021913092346, 2.2835935731873747, 2.2789792517783667, 2.2761709594171795, 2.3143959334197306, 2.3177621604066103, 2.2884189660660303, 2.311392910466611, 2.275393185958308, 2.286765087652806, 2.274863026115056, 2.294948740664123, 2.2861885214757716, 2.2871390892288956, 2.2743610652958326, 2.263448886273185, 2.289666590899042, 2.2975086874459687, 2.2790189825112743, 2.2580556523144146, 2.2770504059269747, 2.2811090608307416, 2.2757719949998716, 2.2933836279835105, 2.290496385895961, 2.2614085040767975, 2.28383061215381, 2.277935653130013, 2.2767012191425837, 2.261673609481268, 2.272762194269741, 2.2714647010266664, 2.290487473501098, 2.2658902911532883, 2.2773112089049175, 2.2770148055491357, 2.2772368045511544, 2.2664487825242845, 2.286639542318909, 2.247916495356978, 2.305542273864441, 2.2512646507169696, 2.2485440855344465, 2.272688105860993, 2.260213811437478, 2.2630233761375496, 2.2729363999356873, 2.273953395587147, 2.2786906412783487, 2.3192567603996523, 2.281512004316321, 2.262797660909111, 2.2803547558436756, 2.3100277732334074, 2.2617148656766113, 2.2537882165991663, 2.278439164214905, 2.2486419643059565, 2.289976849054879, 2.2591118473935636, 2.246622518184403, 2.2485539756996356, 2.2714022353054437, 2.270946773985217, 2.32051304490723, 2.3152721759448656, 2.2505928687197856, 2.271279483717185, 2.2719340757255693, 2.2735338698687357, 2.2513331669232333, 2.258728522823861, 2.31674771521857, 2.2918283532741106, 2.230395258419411, 2.2907653948807876, 2.3037061892965354, 2.28888440081966, 2.277688608511672, 2.288715704807792, 2.2466543833451955, 2.2653392763452334, 2.250193863582836, 2.2973584825330042, 2.268586391634377, 2.222452286381358, 2.3213260910671365, 2.2621099588300413, 2.2481418825953106, 2.261055779220247, 2.269432519652733, 2.2824466859916486, 2.244650987574829, 2.2543351594952203, 2.292507476194037, 2.2723139494422697, 2.2869218409553285, 2.279710771930238, 2.266694714661016, 2.2587020252489243, 2.232717186500342, 2.2397897020179474, 2.2738842139177944, 2.3130731927395285, 2.2755739239371615, 2.277945426675915, 2.243454052034584, 2.2645562990858603, 2.2133727421426292, 2.2679793280689706, 2.28689228759865, 2.276174764319612, 2.2236739566899186, 2.298363742985818, 2.2729309430404605, 2.2563693135874368, 2.240465637311098, 2.2955739515634828, 2.2809092996558507, 2.2789688282647362, 2.28805322376837, 2.3166855884055906, 2.2527713544474053, 2.2731325887768667, 2.2651127106971516, 2.2173477328947846, 2.2098539807486257, 2.2551803876097742, 2.268825191182065, 2.2279083463692317, 2.301795306412694, 2.276189958551281, 2.3712431846473616, 2.2868328038979064, 2.225494902506178, 2.2219202034810803, 2.308294898786341, 2.2447185877161426, 2.2561030174177947, 2.2817544299604884, 2.3094376965185885, 2.3225717067902347, 2.2474790805137927, 2.226260243316582, 2.2742641512258435, 2.274031923422989, 2.292507221173354, 2.2214233061154625, 2.3038294789458624, 2.2006161740230277, 2.3114010439995507, 2.2498249385676092, 2.2499307261201498, 2.26822350700755, 2.3173336547347247, 2.267883190300659, 2.216650036147166, 2.242851204365984, 2.308774772880274, 2.208584330916785, 2.251177796565325, 2.2929429254504283, 2.2742531109980155, 2.3015755984933626, 2.238681765961866, 2.2016660365529783, 2.2414976699522264, 2.2339527682828093, 2.2419668631047154, 2.22561029152744, 2.2696078286368095, 2.289551143034933, 2.3058068408717873, 2.2569811975613874, 2.269927435171348, 2.26324520377211, 2.1625316569152084, 2.2589574526575413, 2.2686326588972605, 2.277235929152427, 2.316094084563435, 2.2473843431840854, 2.1699650093380507, 2.330035628675186, 2.236761831921097, 2.2167136729817942, 2.1725469404099202, 2.3115513746023284, 2.2496455469082437, 2.2959816020938013, 2.271419112296858, 2.201561180448635, 2.2923443997167285, 2.2735742312378213, 2.25518484706304, 2.2075910110023926, 2.245772882471312, 2.2168554586432294, 2.1799703079039072, 2.3423970038143236, 2.2769262981458995, 2.3062019756584777, 2.179473352098399, 2.3465312122995425, 2.2788156964679245, 2.2823152490006624, 2.291660751892977, 2.273853471694125, 2.2407741533860976, 2.172899416933032, 2.1984333670970226, 2.178768841035081, 2.260190263650482, 2.194242320129877, 2.301754317479586, 2.1379310461165866, 2.1671659270788104, 2.2981574744501674, 2.2600462418147753, 2.2166681453444483, 2.3116682133374518, 2.27308235347993, 2.315415519813724, 2.2454148717035594, 2.2420022123738086, 2.2460575891304244, 2.2155338896642607, 2.197460097265643, 2.318388907515723, 2.3021859078807085, 2.286161200886601, 2.339669902771893, 2.197040245797655, 2.282705282487249, 2.1695848944874228, 2.183530894514179, 2.1687370910024586, 2.280636857520292, 2.176898355230328, 2.1658383758779904, 2.2287497642505523, 2.184245077130056, 2.254430533214847, 2.296480705564703, 2.3083214739652798, 2.2727792010817764, 2.2652212535838214, 2.3004397483350063, 2.2647615090690967, 2.1270535971968676, 2.234923844557901, 2.3556544580008727, 2.320327028296227, 2.2743787708138794, 2.205430793999642, 2.216025726528513, 2.2339827155221434, 2.1774274007415815, 2.2233050871556332, 2.2044080679554936, 2.2527007936336836, 2.2318030781141664, 2.2523932586141244, 2.2818711031090086, 2.2729374546977894, 2.2574072904957827, 2.3021979157163655, 2.2759473621417485, 2.204111729271655, 2.189252558882519, 2.237886234675441, 2.3870551622869107, 2.227611855833517, 2.2965227388282172, 2.174574405759914, 2.2223644710891475, 2.39034188002874, 2.1858808523300532, 2.296386752140095, 2.2378239480935656, 2.1972767659635544, 2.1985326184695064, 2.2349970739214924, 2.1900170750618506, 2.1709146497469116, 2.1477515443108333, 2.179970896350563, 2.24916708009886, 2.2343122848263763, 2.1742901250718467, 2.0799756410980557, 2.2220841861419194, 2.247956174372676, 2.1803141868710143, 2.1836113036816145, 2.1633272251408706, 2.225359362065844, 2.2165446854092736, 2.1819677069929813, 2.2273059778822533, 2.1947998105505557, 2.2708157561509386, 2.157979293178159, 2.174449023672463, 2.139562600685668, 2.235704364427927, 2.156121229015864, 2.24355648076423, 2.176654193386759, 2.343633754517829, 2.292389334541857, 2.276347535490325, 2.211671708181607, 2.2094181486726217, 2.185462330048158, 2.205960619547817, 2.258344127286521, 2.287439493420776, 2.2160262565649242, 2.203038506014433, 2.171591728644529, 2.251956833505868, 2.2446382834380154, 2.3595602557073363, 2.2099139511800328, 2.2432384498691653, 2.247009227711138, 2.3254272749122347, 2.301744621986177, 2.2252482918131613, 2.2291867398624254, 2.113158742224389, 2.189558115279206, 2.2351086865998013, 2.241971823851108, 2.1875700843869, 2.158207424004244, 2.236749938926907, 2.226051608823242, 2.2771237789620407, 2.27284293806041, 2.2980481552943437, 2.176719451395235, 2.2484347859408387, 2.3060932894119572, 2.2624999998559, 2.1618495127116626, 2.3237064027617147, 2.240239675948818, 2.2147104791774455, 2.2982272673545237, 2.3358278030850026, 2.1886142866691873, 2.128478050810751, 2.311385551409236, 2.2407913353908095, 2.2632245758216527, 2.2672079876550835, 2.252928202675109, 2.220774177698773, 2.2748848137560786, 2.194539654885159, 2.2900531022302424, 2.2121713336740445, 2.0913593308298855, 2.2703914639860456, 2.1523936948973397, 2.2416866824376345, 2.21617490475356, 2.246419970940201, 2.134445198940538, 2.2699183887282217, 2.1507394401438193, 2.261806802927795, 2.2308286221768205, 2.281423296231043, 2.31822669922193, 2.144160823199504, 2.1513400802171367, 2.1917973532475976, 2.2950102348914974, 2.100521391053965, 2.2267112330326637, 2.1420273375967542, 2.239265776719974, 2.240059615437333, 2.1599768407343682, 2.2681748784584044, 2.208949986017907, 2.302902253085499, 2.2244442228505825, 2.263264839591967, 2.1429665401663955, 2.1882899043981214, 2.246000540413417, 2.334004475609229, 2.2200255863545557, 2.311667349891113, 2.2451032878593913, 2.114113198046391, 2.1379800774131756, 2.220794836257581, 2.254915990121502, 2.1857252690471274, 2.1547434933653054, 2.3005650631533237, 2.362584979184528, 2.1842563367266337, 2.2175075899941072, 2.3064659369084306, 2.2739830313540934, 2.2070774735280523, 2.263569936190832, 2.257398307509753, 2.2027435879602897, 2.274318562637799, 2.065706641955925, 2.1479803585598147, 2.116617104891958, 2.190731908778447, 2.1471997685634467, 2.072579382669971, 2.3231734245159856, 2.23225608264937, 2.2761224118898045, 2.1852569399565933, 2.208590116865381, 2.2565222860804144, 2.0983197982911714, 2.3377358893142426, 2.182478165500148, 2.1482805444065542, 2.31464717537705, 2.116283809246591, 2.2028335737626565, 2.2326783799474863, 2.265263127103594, 2.2437697487925976, 2.167442523067767, 2.2645192117642092, 2.216527928137511, 2.187732284931041, 2.1968076409861594, 2.274975264968471, 1.9855868075389775, 2.2597717458141573, 2.2821159178218937, 2.198963079030357, 2.1891945104577792, 2.190512050266871, 2.244300651790686, 2.1953933392272265, 2.215828703238852, 2.3100137441845474, 2.3159477531534285, 2.266643430772907, 2.3032593686739, 2.394652653544321, 2.254032433202731, 2.1311157260927445, 2.146009311380914, 2.2716558721394833, 2.2301757314982242, 2.2678814937265264, 2.213838422499859, 2.1760596121838343, 2.273506105418976, 2.251507362888061, 2.34512189411497, 2.332063398600958, 2.28230492085212, 1.9736515415982288, 2.1189227626523808, 2.114206787754031, 2.151698611244652, 2.2705814786836984, 2.177990638800233, 2.149726026830303, 2.1829420813707525, 2.2437436638194823, 2.286485280603498, 2.24980885955483, 2.1938046741035966, 2.166350240360741, 2.2258846532623258, 2.34285458114567, 2.1929663692256596, 2.1614771451312933, 2.3313536288551155, 2.166155356616962, 2.307011222268517, 2.205694221413372, 2.2193724314442504, 2.2778575348161674, 2.179588781404833, 2.0986308429095795, 2.1527029769325354, 2.256560466136464, 2.298755900566209, 2.1344338590413137, 2.2271608822114453, 2.284828067580046, 2.2062007773753085, 2.2073675067406073, 2.2657247281164135, 2.3348348069425224, 2.2370508707565415, 2.1151456857666564, 2.2414551117609363, 2.242918937566582, 2.0946004292917, 2.011953260774737, 2.3388613365973963, 2.2385748500313736, 2.250810597110293, 2.293818748703291, 2.020107784339772, 2.28195275115603, 2.255223923374266, 2.14869507768359, 2.2096173142820246, 2.12365362130284, 2.165925894978741, 2.206940417505602, 2.2418594893978243, 2.435203323547574, 2.20321458119135, 2.070248470128996, 2.2187936744119288, 2.161224616410725, 2.10497447919098, 2.3018433928622675, 2.2084709801104907, 2.303666300630048, 2.2445135191978163, 2.14649496794392, 2.0165073561594746, 2.2256396388575146, 2.169012236912601, 2.2624673734805483, 2.2824335920666723, 2.1552491626113706, 2.1625910894636866, 2.2808288405473975, 2.2439545751133325, 2.2382588261264305, 2.0898953426194247, 2.281819890727656, 2.1287797113418496, 2.052971964456318, 2.221663604770886, 2.1665319540090957, 2.1961497029878596, 2.243790478326919, 2.293736071871257, 2.0943813936038187, 2.1252794685292478, 2.196465547035758, 2.134140944633871, 2.2191093408749887, 2.396593898677007, 2.171257568036445, 2.155883916476155, 2.1104135783744593, 2.0524650095148735, 2.2488262598015436, 2.0561069788442414, 2.1472249905974543, 2.2136993033344794, 2.1609558943243043, 2.252445795017344, 2.2339208167457225, 2.2423719595775986, 2.1862903605124084, 2.2134832077203015, 2.174377667422002, 2.1115971157864943, 2.270517546564173, 2.077304121289031, 2.317812946959583, 2.268672642916388, 2.3044974800034406, 2.239222797193361, 2.0344313210390585, 2.2635853161527493, 2.328787323928576, 2.156857904148581, 2.1572597766780865, 2.3406381410100545, 2.2613898090021998, 2.220838742755532, 2.127641741219276, 2.2885573407925426, 2.3954004390646886, 2.1131574528161963, 2.3015316106955814, 2.286232444163045, 2.162189343749455, 2.2110244404878454, 2.282304150695663, 2.2319069197630688, 2.0955823281878074, 2.2538220790481094, 2.309866398808077, 2.2639214328186092, 2.2495302165354953, 2.3265115638659126, 2.2928396386392604, 2.1773725694758985, 2.2069998234792862, 2.230035014498944, 2.2812789349642024, 2.2869453871015626, 2.05322340943015, 2.2531749255380404, 2.166140649838978, 2.160660281549361, 2.129177684204914, 2.1192023458260607, 2.2709323561132626, 2.1739322421048892, 2.3058492710722223, 2.1525457102214616, 2.2363613751447224, 2.1949898306646323, 2.2208228374884667, 2.1805919721518943, 2.2467683982408158, 2.01945280802785, 2.324196217749735, 2.219503405308274, 2.2611339353388433, 2.3471530584952562, 2.148841912322166, 2.2105145700352185, 2.10299896001295, 2.2452306411883876, 2.2659922834657737, 2.0824698025658726, 2.296669271511598, 2.279490786046512, 2.3235368702024597, 2.1051195390993214, 2.1919482176571354, 2.3903194487425594, 2.274765857341968, 2.319324972765244, 2.1576878470952674, 2.2717888821075, 2.2942693326887547, 2.2811890204616265, 2.1792211232871224, 2.345779364711508, 2.0234519079395117, 2.1855523912559702, 2.241816207815938, 2.352432464466387, 2.2271404793424554, 2.004881074775065, 2.165659208624099, 2.3316752411036696, 2.1963508768128817, 2.0938752581510442, 2.37773310826539, 2.224772979726787, 2.2509349442974904, 2.10176376249588, 2.254104787989307, 2.237855966679909, 2.157645520877249, 2.090897439914133, 2.0999701055851396, 2.1815854494495666, 2.0745644609802456, 2.21998773174819, 2.0882375961335344, 2.3353460865512017, 2.3626829171958854, 2.1615144716245003, 2.0949775769564627, 2.1786421785573125, 2.2810184967981635]\n",
      "Accuracy history: [0.16, 0.16, 0.16, 0.16, 0.16, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22]\n"
     ]
    }
   ],
   "source": [
    "# SGD-M\n",
    "sgd_m = ConvNet4(input_shape=input_shape, wt_scale=wt_scale, verbose=False)\n",
    "sgd_m.compile('sgd_momentum')\n",
    "sgd_m.fit(x_dev, y_dev, x_val, y_val, mini_batch_sz=mini_batch_sz, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train...\n",
      "750 iterations. 5 iter/epoch.\n",
      "Iteration: 1/750.\n",
      "Time taken for iteration 0: 1.6547000408172607\n",
      "Estimated time to complete: 1241.0250306129456\n",
      "Iteration: 2/750.\n",
      "Iteration: 3/750.\n",
      "Iteration: 4/750.\n",
      "Iteration: 5/750.\n",
      "Iteration: 6/750.\n",
      "Iteration: 7/750.\n",
      "Iteration: 8/750.\n",
      "Iteration: 9/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.2667797754626706, 2.275256568294465, 2.2556253335188514]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 10/750.\n",
      "Iteration: 11/750.\n",
      "Iteration: 12/750.\n",
      "Iteration: 13/750.\n",
      "Iteration: 14/750.\n",
      "Iteration: 15/750.\n",
      "Iteration: 16/750.\n",
      "Iteration: 17/750.\n",
      "Iteration: 18/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.2727310367417854, 2.204103899788421, 2.258343837284645]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 19/750.\n",
      "Iteration: 20/750.\n",
      "Iteration: 21/750.\n",
      "Iteration: 22/750.\n",
      "Iteration: 23/750.\n",
      "Iteration: 24/750.\n",
      "Iteration: 25/750.\n",
      "Iteration: 26/750.\n",
      "Iteration: 27/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.1944675031511447, 2.2390716805564996, 2.281966984895512]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 28/750.\n",
      "Iteration: 29/750.\n",
      "Iteration: 30/750.\n",
      "Iteration: 31/750.\n",
      "Iteration: 32/750.\n",
      "Iteration: 33/750.\n",
      "Iteration: 34/750.\n",
      "Iteration: 35/750.\n",
      "Iteration: 36/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.2104394749579153, 2.186158200387568, 2.314671231617274]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 37/750.\n",
      "Iteration: 38/750.\n",
      "Iteration: 39/750.\n",
      "Iteration: 40/750.\n",
      "Iteration: 41/750.\n",
      "Iteration: 42/750.\n",
      "Iteration: 43/750.\n",
      "Iteration: 44/750.\n",
      "Iteration: 45/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.262181205808585, 2.1475236738810057, 2.1927888638495125]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 46/750.\n",
      "Iteration: 47/750.\n",
      "Iteration: 48/750.\n",
      "Iteration: 49/750.\n",
      "Iteration: 50/750.\n",
      "Iteration: 51/750.\n",
      "Iteration: 52/750.\n",
      "Iteration: 53/750.\n",
      "Iteration: 54/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.1454005350969774, 2.3007085399330136, 2.254623388762617]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 55/750.\n",
      "Iteration: 56/750.\n",
      "Iteration: 57/750.\n",
      "Iteration: 58/750.\n",
      "Iteration: 59/750.\n",
      "Iteration: 60/750.\n",
      "Iteration: 61/750.\n",
      "Iteration: 62/750.\n",
      "Iteration: 63/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.2224804656011212, 2.132319431106546, 2.160046661484646]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 64/750.\n",
      "Iteration: 65/750.\n",
      "Iteration: 66/750.\n",
      "Iteration: 67/750.\n",
      "Iteration: 68/750.\n",
      "Iteration: 69/750.\n",
      "Iteration: 70/750.\n",
      "Iteration: 71/750.\n",
      "Iteration: 72/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.062876644831836, 2.1102064822282625, 2.342895680648545]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 73/750.\n",
      "Iteration: 74/750.\n",
      "Iteration: 75/750.\n",
      "Iteration: 76/750.\n",
      "Iteration: 77/750.\n",
      "Iteration: 78/750.\n",
      "Iteration: 79/750.\n",
      "Iteration: 80/750.\n",
      "Iteration: 81/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.245438425118237, 2.1422264296376468, 2.328747695698904]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 82/750.\n",
      "Iteration: 83/750.\n",
      "Iteration: 84/750.\n",
      "Iteration: 85/750.\n",
      "Iteration: 86/750.\n",
      "Iteration: 87/750.\n",
      "Iteration: 88/750.\n",
      "Iteration: 89/750.\n",
      "Iteration: 90/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.2147544396521504, 2.182436801416794, 2.3272995062125292]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 91/750.\n",
      "Iteration: 92/750.\n",
      "Iteration: 93/750.\n",
      "Iteration: 94/750.\n",
      "Iteration: 95/750.\n",
      "Iteration: 96/750.\n",
      "Iteration: 97/750.\n",
      "Iteration: 98/750.\n",
      "Iteration: 99/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.141941328348959, 2.112567311006279, 2.2184563897284577]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 100/750.\n",
      "Iteration: 101/750.\n",
      "Iteration: 102/750.\n",
      "Iteration: 103/750.\n",
      "Iteration: 104/750.\n",
      "Iteration: 105/750.\n",
      "Iteration: 106/750.\n",
      "Iteration: 107/750.\n",
      "Iteration: 108/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.280583389083929, 2.0969955815327004, 1.8145079639294601]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 109/750.\n",
      "Iteration: 110/750.\n",
      "Iteration: 111/750.\n",
      "Iteration: 112/750.\n",
      "Iteration: 113/750.\n",
      "Iteration: 114/750.\n",
      "Iteration: 115/750.\n",
      "Iteration: 116/750.\n",
      "Iteration: 117/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.9084026833757428, 2.1167140453380213, 2.3740182895421293]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 118/750.\n",
      "Iteration: 119/750.\n",
      "Iteration: 120/750.\n",
      "Iteration: 121/750.\n",
      "Iteration: 122/750.\n",
      "Iteration: 123/750.\n",
      "Iteration: 124/750.\n",
      "Iteration: 125/750.\n",
      "Iteration: 126/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.346263581167927, 1.8869907552364524, 2.461087123313006]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.24, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 127/750.\n",
      "Iteration: 128/750.\n",
      "Iteration: 129/750.\n",
      "Iteration: 130/750.\n",
      "Iteration: 131/750.\n",
      "Iteration: 132/750.\n",
      "Iteration: 133/750.\n",
      "Iteration: 134/750.\n",
      "Iteration: 135/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.810131401373492, 2.1380569646358207, 2.21698438382967]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.24, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 136/750.\n",
      "Iteration: 137/750.\n",
      "Iteration: 138/750.\n",
      "Iteration: 139/750.\n",
      "Iteration: 140/750.\n",
      "Iteration: 141/750.\n",
      "Iteration: 142/750.\n",
      "Iteration: 143/750.\n",
      "Iteration: 144/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.8719002668156919, 2.0312890920937923, 2.0585284689321237]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.28, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 145/750.\n",
      "Iteration: 146/750.\n",
      "Iteration: 147/750.\n",
      "Iteration: 148/750.\n",
      "Iteration: 149/750.\n",
      "Iteration: 150/750.\n",
      "Iteration: 151/750.\n",
      "Iteration: 152/750.\n",
      "Iteration: 153/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.7564821912496176, 1.9865067476393694, 1.943536227476239]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.32, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 154/750.\n",
      "Iteration: 155/750.\n",
      "Iteration: 156/750.\n",
      "Iteration: 157/750.\n",
      "Iteration: 158/750.\n",
      "Iteration: 159/750.\n",
      "Iteration: 160/750.\n",
      "Iteration: 161/750.\n",
      "Iteration: 162/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.8245981558917728, 1.9881851896046179, 1.9763830868347005]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.36, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 163/750.\n",
      "Iteration: 164/750.\n",
      "Iteration: 165/750.\n",
      "Iteration: 166/750.\n",
      "Iteration: 167/750.\n",
      "Iteration: 168/750.\n",
      "Iteration: 169/750.\n",
      "Iteration: 170/750.\n",
      "Iteration: 171/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.080556145782496, 1.624516182196767, 1.592757630272275]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.32, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 172/750.\n",
      "Iteration: 173/750.\n",
      "Iteration: 174/750.\n",
      "Iteration: 175/750.\n",
      "Iteration: 176/750.\n",
      "Iteration: 177/750.\n",
      "Iteration: 178/750.\n",
      "Iteration: 179/750.\n",
      "Iteration: 180/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.69090942977038, 1.9665592430540089, 1.8076948352084192]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.38, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 181/750.\n",
      "Iteration: 182/750.\n",
      "Iteration: 183/750.\n",
      "Iteration: 184/750.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 185/750.\n",
      "Iteration: 186/750.\n",
      "Iteration: 187/750.\n",
      "Iteration: 188/750.\n",
      "Iteration: 189/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.7687937906780133, 1.5499046858392924, 1.7933401927001027]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.44, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 190/750.\n",
      "Iteration: 191/750.\n",
      "Iteration: 192/750.\n",
      "Iteration: 193/750.\n",
      "Iteration: 194/750.\n",
      "Iteration: 195/750.\n",
      "Iteration: 196/750.\n",
      "Iteration: 197/750.\n",
      "Iteration: 198/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.8762247744837355, 1.8434306913313045, 1.646268527799408]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.4, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 199/750.\n",
      "Iteration: 200/750.\n",
      "Iteration: 201/750.\n",
      "Iteration: 202/750.\n",
      "Iteration: 203/750.\n",
      "Iteration: 204/750.\n",
      "Iteration: 205/750.\n",
      "Iteration: 206/750.\n",
      "Iteration: 207/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.5854351235733253, 1.6089370888972951, 2.058576125229221]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.36, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 208/750.\n",
      "Iteration: 209/750.\n",
      "Iteration: 210/750.\n",
      "Iteration: 211/750.\n",
      "Iteration: 212/750.\n",
      "Iteration: 213/750.\n",
      "Iteration: 214/750.\n",
      "Iteration: 215/750.\n",
      "Iteration: 216/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.784510600880739, 1.7362789713859994, 1.446772620741509]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.46, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 217/750.\n",
      "Iteration: 218/750.\n",
      "Iteration: 219/750.\n",
      "Iteration: 220/750.\n",
      "Iteration: 221/750.\n",
      "Iteration: 222/750.\n",
      "Iteration: 223/750.\n",
      "Iteration: 224/750.\n",
      "Iteration: 225/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.367081370829135, 1.7940175672363345, 1.0815554925170499]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.32, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 226/750.\n",
      "Iteration: 227/750.\n",
      "Iteration: 228/750.\n",
      "Iteration: 229/750.\n",
      "Iteration: 230/750.\n",
      "Iteration: 231/750.\n",
      "Iteration: 232/750.\n",
      "Iteration: 233/750.\n",
      "Iteration: 234/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.5370650534071886, 1.4994189511311506, 1.2382007588936237]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.34, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 235/750.\n",
      "Iteration: 236/750.\n",
      "Iteration: 237/750.\n",
      "Iteration: 238/750.\n",
      "Iteration: 239/750.\n",
      "Iteration: 240/750.\n",
      "Iteration: 241/750.\n",
      "Iteration: 242/750.\n",
      "Iteration: 243/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.2365533918378877, 1.4200319681257227, 1.8429169176312614]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.52, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 244/750.\n",
      "Iteration: 245/750.\n",
      "Iteration: 246/750.\n",
      "Iteration: 247/750.\n",
      "Iteration: 248/750.\n",
      "Iteration: 249/750.\n",
      "Iteration: 250/750.\n",
      "Iteration: 251/750.\n",
      "Iteration: 252/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.565128906395955, 0.9596664751092816, 1.229862366192645]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.58, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 253/750.\n",
      "Iteration: 254/750.\n",
      "Iteration: 255/750.\n",
      "Iteration: 256/750.\n",
      "Iteration: 257/750.\n",
      "Iteration: 258/750.\n",
      "Iteration: 259/750.\n",
      "Iteration: 260/750.\n",
      "Iteration: 261/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.9498037380592687, 1.0349733756213138, 1.866582896709912]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.62, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 262/750.\n",
      "Iteration: 263/750.\n",
      "Iteration: 264/750.\n",
      "Iteration: 265/750.\n",
      "Iteration: 266/750.\n",
      "Iteration: 267/750.\n",
      "Iteration: 268/750.\n",
      "Iteration: 269/750.\n",
      "Iteration: 270/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.1245383727738474, 1.0784979611253103, 1.2490428991528333]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.56, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 271/750.\n",
      "Iteration: 272/750.\n",
      "Iteration: 273/750.\n",
      "Iteration: 274/750.\n",
      "Iteration: 275/750.\n",
      "Iteration: 276/750.\n",
      "Iteration: 277/750.\n",
      "Iteration: 278/750.\n",
      "Iteration: 279/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.6562065181910253, 0.6959926880204463, 0.3588537720135193]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.62, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 280/750.\n",
      "Iteration: 281/750.\n",
      "Iteration: 282/750.\n",
      "Iteration: 283/750.\n",
      "Iteration: 284/750.\n",
      "Iteration: 285/750.\n",
      "Iteration: 286/750.\n",
      "Iteration: 287/750.\n",
      "Iteration: 288/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.7398576115325761, 0.9311622601760052, 1.2553016971047823]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.7, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 289/750.\n",
      "Iteration: 290/750.\n",
      "Iteration: 291/750.\n",
      "Iteration: 292/750.\n",
      "Iteration: 293/750.\n",
      "Iteration: 294/750.\n",
      "Iteration: 295/750.\n",
      "Iteration: 296/750.\n",
      "Iteration: 297/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.6652714303068812, 0.8960706763006234, 0.9986283064194643]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.7, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 298/750.\n",
      "Iteration: 299/750.\n",
      "Iteration: 300/750.\n",
      "Iteration: 301/750.\n",
      "Iteration: 302/750.\n",
      "Iteration: 303/750.\n",
      "Iteration: 304/750.\n",
      "Iteration: 305/750.\n",
      "Iteration: 306/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.7121118251675107, 0.7926670437404754, 0.42816989121324867]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.8, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 307/750.\n",
      "Iteration: 308/750.\n",
      "Iteration: 309/750.\n",
      "Iteration: 310/750.\n",
      "Iteration: 311/750.\n",
      "Iteration: 312/750.\n",
      "Iteration: 313/750.\n",
      "Iteration: 314/750.\n",
      "Iteration: 315/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.5999820458657846, 0.5932785082964959, 0.726994351994561]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.7, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 316/750.\n",
      "Iteration: 317/750.\n",
      "Iteration: 318/750.\n",
      "Iteration: 319/750.\n",
      "Iteration: 320/750.\n",
      "Iteration: 321/750.\n",
      "Iteration: 322/750.\n",
      "Iteration: 323/750.\n",
      "Iteration: 324/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.5185312490896163, 0.6175317543712656, 0.8038231033579081]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.84, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 325/750.\n",
      "Iteration: 326/750.\n",
      "Iteration: 327/750.\n",
      "Iteration: 328/750.\n",
      "Iteration: 329/750.\n",
      "Iteration: 330/750.\n",
      "Iteration: 331/750.\n",
      "Iteration: 332/750.\n",
      "Iteration: 333/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.3721801906432122, 0.48718576388629914, 0.7962355987553126]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.76, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 334/750.\n",
      "Iteration: 335/750.\n",
      "Iteration: 336/750.\n",
      "Iteration: 337/750.\n",
      "Iteration: 338/750.\n",
      "Iteration: 339/750.\n",
      "Iteration: 340/750.\n",
      "Iteration: 341/750.\n",
      "Iteration: 342/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.45703582118824787, 0.4455654487389473, 0.6131551995768093]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.86, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 343/750.\n",
      "Iteration: 344/750.\n",
      "Iteration: 345/750.\n",
      "Iteration: 346/750.\n",
      "Iteration: 347/750.\n",
      "Iteration: 348/750.\n",
      "Iteration: 349/750.\n",
      "Iteration: 350/750.\n",
      "Iteration: 351/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.3600096173785579, 0.24084408281141323, 0.44647209122066905]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.92, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 352/750.\n",
      "Iteration: 353/750.\n",
      "Iteration: 354/750.\n",
      "Iteration: 355/750.\n",
      "Iteration: 356/750.\n",
      "Iteration: 357/750.\n",
      "Iteration: 358/750.\n",
      "Iteration: 359/750.\n",
      "Iteration: 360/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.21452174186284392, 0.7726704370448986, 0.45634574429319485]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.88, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 361/750.\n",
      "Iteration: 362/750.\n",
      "Iteration: 363/750.\n",
      "Iteration: 364/750.\n",
      "Iteration: 365/750.\n",
      "Iteration: 366/750.\n",
      "Iteration: 367/750.\n",
      "Iteration: 368/750.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 369/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.3295559392454376, 0.5024100581564189, 0.4312812816763762]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.92, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 370/750.\n",
      "Iteration: 371/750.\n",
      "Iteration: 372/750.\n",
      "Iteration: 373/750.\n",
      "Iteration: 374/750.\n",
      "Iteration: 375/750.\n",
      "Iteration: 376/750.\n",
      "Iteration: 377/750.\n",
      "Iteration: 378/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.17926499542018026, 0.08146063120636833, 0.14776578401052776]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.96, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 379/750.\n",
      "Iteration: 380/750.\n",
      "Iteration: 381/750.\n",
      "Iteration: 382/750.\n",
      "Iteration: 383/750.\n",
      "Iteration: 384/750.\n",
      "Iteration: 385/750.\n",
      "Iteration: 386/750.\n",
      "Iteration: 387/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.12690574396394314, 0.14626360697760457, 0.27095333847671027]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.96, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 388/750.\n",
      "Iteration: 389/750.\n",
      "Iteration: 390/750.\n",
      "Iteration: 391/750.\n",
      "Iteration: 392/750.\n",
      "Iteration: 393/750.\n",
      "Iteration: 394/750.\n",
      "Iteration: 395/750.\n",
      "Iteration: 396/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.05127357695541305, 0.03077601974795683, 0.09978465409029391]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.94, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 397/750.\n",
      "Iteration: 398/750.\n",
      "Iteration: 399/750.\n",
      "Iteration: 400/750.\n",
      "Iteration: 401/750.\n",
      "Iteration: 402/750.\n",
      "Iteration: 403/750.\n",
      "Iteration: 404/750.\n",
      "Iteration: 405/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.7968578466671137, 0.12050352961368434, 0.13951782523844286]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.86, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 406/750.\n",
      "Iteration: 407/750.\n",
      "Iteration: 408/750.\n",
      "Iteration: 409/750.\n",
      "Iteration: 410/750.\n",
      "Iteration: 411/750.\n",
      "Iteration: 412/750.\n",
      "Iteration: 413/750.\n",
      "Iteration: 414/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.0725913969922685, 0.05298564169811315, 0.11045412407002203]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.94, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 415/750.\n",
      "Iteration: 416/750.\n",
      "Iteration: 417/750.\n",
      "Iteration: 418/750.\n",
      "Iteration: 419/750.\n",
      "Iteration: 420/750.\n",
      "Iteration: 421/750.\n",
      "Iteration: 422/750.\n",
      "Iteration: 423/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.4215869937130474, 0.10524540448488609, 0.06936716339445756]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.98, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 424/750.\n",
      "Iteration: 425/750.\n",
      "Iteration: 426/750.\n",
      "Iteration: 427/750.\n",
      "Iteration: 428/750.\n",
      "Iteration: 429/750.\n",
      "Iteration: 430/750.\n",
      "Iteration: 431/750.\n",
      "Iteration: 432/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.12616707832464547, 0.045277642345445346, 0.057425751483152626]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 433/750.\n",
      "Iteration: 434/750.\n",
      "Iteration: 435/750.\n",
      "Iteration: 436/750.\n",
      "Iteration: 437/750.\n",
      "Iteration: 438/750.\n",
      "Iteration: 439/750.\n",
      "Iteration: 440/750.\n",
      "Iteration: 441/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.06250334432634279, 0.06565315499960024, 0.09353956286963783]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 442/750.\n",
      "Iteration: 443/750.\n",
      "Iteration: 444/750.\n",
      "Iteration: 445/750.\n",
      "Iteration: 446/750.\n",
      "Iteration: 447/750.\n",
      "Iteration: 448/750.\n",
      "Iteration: 449/750.\n",
      "Iteration: 450/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.04103252661290321, 0.023090117515521657, 0.02139307379917506]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 451/750.\n",
      "Iteration: 452/750.\n",
      "Iteration: 453/750.\n",
      "Iteration: 454/750.\n",
      "Iteration: 455/750.\n",
      "Iteration: 456/750.\n",
      "Iteration: 457/750.\n",
      "Iteration: 458/750.\n",
      "Iteration: 459/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.061574340908845206, 0.03061612113091562, 0.0361981308188108]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 460/750.\n",
      "Iteration: 461/750.\n",
      "Iteration: 462/750.\n",
      "Iteration: 463/750.\n",
      "Iteration: 464/750.\n",
      "Iteration: 465/750.\n",
      "Iteration: 466/750.\n",
      "Iteration: 467/750.\n",
      "Iteration: 468/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.04146044511708326, 0.04634376549811827, 0.02264088468215882]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 469/750.\n",
      "Iteration: 470/750.\n",
      "Iteration: 471/750.\n",
      "Iteration: 472/750.\n",
      "Iteration: 473/750.\n",
      "Iteration: 474/750.\n",
      "Iteration: 475/750.\n",
      "Iteration: 476/750.\n",
      "Iteration: 477/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.0529215937297416, 0.015122620732716927, 0.016567539291492402]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 478/750.\n",
      "Iteration: 479/750.\n",
      "Iteration: 480/750.\n",
      "Iteration: 481/750.\n",
      "Iteration: 482/750.\n",
      "Iteration: 483/750.\n",
      "Iteration: 484/750.\n",
      "Iteration: 485/750.\n",
      "Iteration: 486/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.024942579891443245, 0.014512102097624636, 0.017145682969261618]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 487/750.\n",
      "Iteration: 488/750.\n",
      "Iteration: 489/750.\n",
      "Iteration: 490/750.\n",
      "Iteration: 491/750.\n",
      "Iteration: 492/750.\n",
      "Iteration: 493/750.\n",
      "Iteration: 494/750.\n",
      "Iteration: 495/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.01651922213586791, 0.0232326344336274, 0.012857308652123273]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 496/750.\n",
      "Iteration: 497/750.\n",
      "Iteration: 498/750.\n",
      "Iteration: 499/750.\n",
      "Iteration: 500/750.\n",
      "Iteration: 501/750.\n",
      "Iteration: 502/750.\n",
      "Iteration: 503/750.\n",
      "Iteration: 504/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.01592147211275496, 0.009630283114641458, 0.019034746972734173]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 505/750.\n",
      "Iteration: 506/750.\n",
      "Iteration: 507/750.\n",
      "Iteration: 508/750.\n",
      "Iteration: 509/750.\n",
      "Iteration: 510/750.\n",
      "Iteration: 511/750.\n",
      "Iteration: 512/750.\n",
      "Iteration: 513/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.01378188759272413, 0.015363995120761626, 0.015868681066423168]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 514/750.\n",
      "Iteration: 515/750.\n",
      "Iteration: 516/750.\n",
      "Iteration: 517/750.\n",
      "Iteration: 518/750.\n",
      "Iteration: 519/750.\n",
      "Iteration: 520/750.\n",
      "Iteration: 521/750.\n",
      "Iteration: 522/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.009422600680863272, 0.012448477097153542, 0.011722024897707557]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 523/750.\n",
      "Iteration: 524/750.\n",
      "Iteration: 525/750.\n",
      "Iteration: 526/750.\n",
      "Iteration: 527/750.\n",
      "Iteration: 528/750.\n",
      "Iteration: 529/750.\n",
      "Iteration: 530/750.\n",
      "Iteration: 531/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.014234979332353945, 0.01259274166389674, 0.010110019908980701]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 532/750.\n",
      "Iteration: 533/750.\n",
      "Iteration: 534/750.\n",
      "Iteration: 535/750.\n",
      "Iteration: 536/750.\n",
      "Iteration: 537/750.\n",
      "Iteration: 538/750.\n",
      "Iteration: 539/750.\n",
      "Iteration: 540/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.011304716332335658, 0.01568272955105841, 0.009243550549294243]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 541/750.\n",
      "Iteration: 542/750.\n",
      "Iteration: 543/750.\n",
      "Iteration: 544/750.\n",
      "Iteration: 545/750.\n",
      "Iteration: 546/750.\n",
      "Iteration: 547/750.\n",
      "Iteration: 548/750.\n",
      "Iteration: 549/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.009461286540340778, 0.015398113683632223, 0.007788799428446233]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 550/750.\n",
      "Iteration: 551/750.\n",
      "Iteration: 552/750.\n",
      "Iteration: 553/750.\n",
      "Iteration: 554/750.\n",
      "Iteration: 555/750.\n",
      "Iteration: 556/750.\n",
      "Iteration: 557/750.\n",
      "Iteration: 558/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.023462035444168056, 0.0072876130475124055, 0.007217936771467708]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 559/750.\n",
      "Iteration: 560/750.\n",
      "Iteration: 561/750.\n",
      "Iteration: 562/750.\n",
      "Iteration: 563/750.\n",
      "Iteration: 564/750.\n",
      "Iteration: 565/750.\n",
      "Iteration: 566/750.\n",
      "Iteration: 567/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.011486809869905357, 0.014080469611488039, 0.006042325705134973]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 568/750.\n",
      "Iteration: 569/750.\n",
      "Iteration: 570/750.\n",
      "Iteration: 571/750.\n",
      "Iteration: 572/750.\n",
      "Iteration: 573/750.\n",
      "Iteration: 574/750.\n",
      "Iteration: 575/750.\n",
      "Iteration: 576/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.00531625625296299, 0.008090467558170616, 0.007376739239527481]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 577/750.\n",
      "Iteration: 578/750.\n",
      "Iteration: 579/750.\n",
      "Iteration: 580/750.\n",
      "Iteration: 581/750.\n",
      "Iteration: 582/750.\n",
      "Iteration: 583/750.\n",
      "Iteration: 584/750.\n",
      "Iteration: 585/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.011827042260566472, 0.004132437648233549, 0.00841132813573615]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 586/750.\n",
      "Iteration: 587/750.\n",
      "Iteration: 588/750.\n",
      "Iteration: 589/750.\n",
      "Iteration: 590/750.\n",
      "Iteration: 591/750.\n",
      "Iteration: 592/750.\n",
      "Iteration: 593/750.\n",
      "Iteration: 594/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.012874743552425738, 0.006268778404724124, 0.008973090645707517]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 595/750.\n",
      "Iteration: 596/750.\n",
      "Iteration: 597/750.\n",
      "Iteration: 598/750.\n",
      "Iteration: 599/750.\n",
      "Iteration: 600/750.\n",
      "Iteration: 601/750.\n",
      "Iteration: 602/750.\n",
      "Iteration: 603/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.01861776886689462, 0.00892701187826333, 0.0037573490446317624]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 604/750.\n",
      "Iteration: 605/750.\n",
      "Iteration: 606/750.\n",
      "Iteration: 607/750.\n",
      "Iteration: 608/750.\n",
      "Iteration: 609/750.\n",
      "Iteration: 610/750.\n",
      "Iteration: 611/750.\n",
      "Iteration: 612/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.01449660654662139, 0.01239944670797838, 0.019480864669444573]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 613/750.\n",
      "Iteration: 614/750.\n",
      "Iteration: 615/750.\n",
      "Iteration: 616/750.\n",
      "Iteration: 617/750.\n",
      "Iteration: 618/750.\n",
      "Iteration: 619/750.\n",
      "Iteration: 620/750.\n",
      "Iteration: 621/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.007348764169813485, 0.005429214354160296, 0.009889597384216563]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 622/750.\n",
      "Iteration: 623/750.\n",
      "Iteration: 624/750.\n",
      "Iteration: 625/750.\n",
      "Iteration: 626/750.\n",
      "Iteration: 627/750.\n",
      "Iteration: 628/750.\n",
      "Iteration: 629/750.\n",
      "Iteration: 630/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.007102762083355663, 0.006198899235130445, 0.006963071672403594]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 631/750.\n",
      "Iteration: 632/750.\n",
      "Iteration: 633/750.\n",
      "Iteration: 634/750.\n",
      "Iteration: 635/750.\n",
      "Iteration: 636/750.\n",
      "Iteration: 637/750.\n",
      "Iteration: 638/750.\n",
      "Iteration: 639/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.010189974498328942, 0.006265418445138659, 0.009006211923601624]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 640/750.\n",
      "Iteration: 641/750.\n",
      "Iteration: 642/750.\n",
      "Iteration: 643/750.\n",
      "Iteration: 644/750.\n",
      "Iteration: 645/750.\n",
      "Iteration: 646/750.\n",
      "Iteration: 647/750.\n",
      "Iteration: 648/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.009768225769362357, 0.004369052790291455, 0.009330586000170749]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 649/750.\n",
      "Iteration: 650/750.\n",
      "Iteration: 651/750.\n",
      "Iteration: 652/750.\n",
      "Iteration: 653/750.\n",
      "Iteration: 654/750.\n",
      "Iteration: 655/750.\n",
      "Iteration: 656/750.\n",
      "Iteration: 657/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.006903764936367092, 0.008378244942623849, 0.006026925378267827]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 658/750.\n",
      "Iteration: 659/750.\n",
      "Iteration: 660/750.\n",
      "Iteration: 661/750.\n",
      "Iteration: 662/750.\n",
      "Iteration: 663/750.\n",
      "Iteration: 664/750.\n",
      "Iteration: 665/750.\n",
      "Iteration: 666/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.005647861855906821, 0.006373796991816481, 0.0036909966583682525]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 667/750.\n",
      "Iteration: 668/750.\n",
      "Iteration: 669/750.\n",
      "Iteration: 670/750.\n",
      "Iteration: 671/750.\n",
      "Iteration: 672/750.\n",
      "Iteration: 673/750.\n",
      "Iteration: 674/750.\n",
      "Iteration: 675/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.0035338558313725097, 0.004852368511403443, 0.006987997809365887]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 676/750.\n",
      "Iteration: 677/750.\n",
      "Iteration: 678/750.\n",
      "Iteration: 679/750.\n",
      "Iteration: 680/750.\n",
      "Iteration: 681/750.\n",
      "Iteration: 682/750.\n",
      "Iteration: 683/750.\n",
      "Iteration: 684/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.004503677118881455, 0.004347344683964638, 0.005801303881045078]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 685/750.\n",
      "Iteration: 686/750.\n",
      "Iteration: 687/750.\n",
      "Iteration: 688/750.\n",
      "Iteration: 689/750.\n",
      "Iteration: 690/750.\n",
      "Iteration: 691/750.\n",
      "Iteration: 692/750.\n",
      "Iteration: 693/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.008569560094122864, 0.004063908132902382, 0.00772795803548363]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 694/750.\n",
      "Iteration: 695/750.\n",
      "Iteration: 696/750.\n",
      "Iteration: 697/750.\n",
      "Iteration: 698/750.\n",
      "Iteration: 699/750.\n",
      "Iteration: 700/750.\n",
      "Iteration: 701/750.\n",
      "Iteration: 702/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.004720701108118153, 0.004657498544578502, 0.006138915366123302]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 703/750.\n",
      "Iteration: 704/750.\n",
      "Iteration: 705/750.\n",
      "Iteration: 706/750.\n",
      "Iteration: 707/750.\n",
      "Iteration: 708/750.\n",
      "Iteration: 709/750.\n",
      "Iteration: 710/750.\n",
      "Iteration: 711/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.003572075049107347, 0.0035199080476802816, 0.004912580903612035]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 712/750.\n",
      "Iteration: 713/750.\n",
      "Iteration: 714/750.\n",
      "Iteration: 715/750.\n",
      "Iteration: 716/750.\n",
      "Iteration: 717/750.\n",
      "Iteration: 718/750.\n",
      "Iteration: 719/750.\n",
      "Iteration: 720/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.010649138571592715, 0.0077419436390885115, 0.007040307072026977]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 721/750.\n",
      "Iteration: 722/750.\n",
      "Iteration: 723/750.\n",
      "Iteration: 724/750.\n",
      "Iteration: 725/750.\n",
      "Iteration: 726/750.\n",
      "Iteration: 727/750.\n",
      "Iteration: 728/750.\n",
      "Iteration: 729/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.005056200145948542, 0.0028941767692891984, 0.0044539151251743285]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 730/750.\n",
      "Iteration: 731/750.\n",
      "Iteration: 732/750.\n",
      "Iteration: 733/750.\n",
      "Iteration: 734/750.\n",
      "Iteration: 735/750.\n",
      "Iteration: 736/750.\n",
      "Iteration: 737/750.\n",
      "Iteration: 738/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.005925159649311415, 0.005571045170012228, 0.002954864165883129]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 739/750.\n",
      "Iteration: 740/750.\n",
      "Iteration: 741/750.\n",
      "Iteration: 742/750.\n",
      "Iteration: 743/750.\n",
      "Iteration: 744/750.\n",
      "Iteration: 745/750.\n",
      "Iteration: 746/750.\n",
      "Iteration: 747/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.00718462533908763, 0.004472624064811412, 0.005578008915382949]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 748/750.\n",
      "Iteration: 749/750.\n",
      "Iteration: 750/750.\n",
      "\n",
      "\n",
      "----------------FINAL OUTPUT----------------\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "Loss history: [2.304373733952622, 2.3001941262256884, 2.303508335898844, 2.2942834211171674, 2.302257539684362, 2.2846940314338897, 2.2667797754626706, 2.275256568294465, 2.2556253335188514, 2.255724245474301, 2.343866761998466, 2.273996881707418, 2.277199410984449, 2.2972349987580034, 2.237717600372464, 2.2727310367417854, 2.204103899788421, 2.258343837284645, 2.2590217321572386, 2.2410365036727793, 2.21648210922612, 2.257682756751057, 2.1638925675409975, 2.3051962858235155, 2.1944675031511447, 2.2390716805564996, 2.281966984895512, 2.2260888371900878, 2.3167035067377824, 2.1894941797773226, 2.2825784282113477, 2.3185600416673253, 2.2054036260114587, 2.2104394749579153, 2.186158200387568, 2.314671231617274, 2.1841281528840213, 2.2162186641313677, 2.2318659828023284, 2.215329853793867, 2.2706347464264036, 2.238357849701217, 2.262181205808585, 2.1475236738810057, 2.1927888638495125, 2.1374337344344143, 2.334798103876355, 2.314091974017603, 2.100504717556598, 2.310267870364869, 2.08967420692944, 2.1454005350969774, 2.3007085399330136, 2.254623388762617, 2.2115336061775284, 2.237354164274268, 2.2526442310975536, 2.1206935324215714, 2.091221586124789, 2.2756473590375514, 2.2224804656011212, 2.132319431106546, 2.160046661484646, 2.1065756443487866, 2.294840035022363, 2.304341258435041, 2.260400324063611, 2.1889218481103163, 2.1682860883507287, 2.062876644831836, 2.1102064822282625, 2.342895680648545, 2.272264401800523, 2.2724597301609637, 2.2953864638718358, 2.2955029132038853, 2.027709232618005, 2.335128325909235, 2.245438425118237, 2.1422264296376468, 2.328747695698904, 2.1241701530745263, 2.211319922999345, 2.404273097629641, 2.1776793791750038, 2.311223172066713, 2.253595040493041, 2.2147544396521504, 2.182436801416794, 2.3272995062125292, 2.14755198939707, 2.224311403857144, 2.193804558545494, 2.27463863108589, 1.983868980764864, 2.0552797263934353, 2.141941328348959, 2.112567311006279, 2.2184563897284577, 2.2253812604806753, 2.155493279442576, 2.04488320323522, 2.208044770196635, 2.2270848300181565, 2.0493456358370006, 2.280583389083929, 2.0969955815327004, 1.8145079639294601, 1.733432267376918, 2.361115924278763, 2.011099680766377, 2.1170719359633607, 2.021971444499357, 2.1079372889774723, 1.9084026833757428, 2.1167140453380213, 2.3740182895421293, 1.8632060517607802, 2.076402464514721, 2.215907711831237, 2.4446407943538606, 2.1757815782797603, 2.1450355930985054, 2.346263581167927, 1.8869907552364524, 2.461087123313006, 2.0273978330067357, 2.1504613281145346, 2.088402859836269, 2.198812654945001, 2.2531395007423036, 2.059688649164976, 1.810131401373492, 2.1380569646358207, 2.21698438382967, 2.4823956220819454, 2.1195679167362993, 1.971004143341482, 2.0001928766222687, 2.0518658433650567, 1.8909377547150426, 1.8719002668156919, 2.0312890920937923, 2.0585284689321237, 1.9562310562304412, 2.2622172964686587, 2.3135086353729846, 1.7294282976062227, 2.1329809755411775, 1.6555943980913044, 1.7564821912496176, 1.9865067476393694, 1.943536227476239, 1.978874337334997, 2.2721219696167485, 1.3500510895461262, 1.945111821369137, 2.128804782670874, 2.0435319259775824, 1.8245981558917728, 1.9881851896046179, 1.9763830868347005, 1.410153978585237, 1.9877138408190764, 1.5592077802792934, 2.1382530196473235, 1.6794143710999903, 1.7629319662927774, 2.080556145782496, 1.624516182196767, 1.592757630272275, 1.6688359778036816, 2.1488719483329546, 1.749666852250499, 1.8956478752597332, 1.9571766940492932, 1.8259836749279672, 1.69090942977038, 1.9665592430540089, 1.8076948352084192, 1.7684841448798214, 1.9426014940154408, 1.7010945896422616, 1.7521388139652931, 1.6375481266407115, 1.6952798468192982, 1.7687937906780133, 1.5499046858392924, 1.7933401927001027, 1.6386323965508027, 1.6667082364842258, 1.5691874815949882, 1.6214483501900927, 1.6512104077564604, 1.3213007010113835, 1.8762247744837355, 1.8434306913313045, 1.646268527799408, 1.9997997873481772, 1.6309667060129955, 1.1701267890877338, 1.4891833294811843, 1.70317424237858, 1.5404069149122528, 1.5854351235733253, 1.6089370888972951, 2.058576125229221, 1.4337536442625005, 1.6372614896313955, 1.8904958727682826, 1.607664259219223, 1.7799635780008747, 1.7339694812403939, 1.784510600880739, 1.7362789713859994, 1.446772620741509, 1.7854874993092509, 1.7322336368741515, 1.4470217991003047, 1.287346335620474, 1.589470404148197, 1.4931802740816207, 1.367081370829135, 1.7940175672363345, 1.0815554925170499, 1.7961263543101298, 1.6367526511272754, 1.5904461673912726, 1.2824299904658127, 1.4529797833257037, 1.8568912108204836, 1.5370650534071886, 1.4994189511311506, 1.2382007588936237, 1.8971348537323285, 1.6130488323359, 1.1399157682988508, 1.6915364333186673, 1.1268907625369757, 1.7847238692913756, 1.2365533918378877, 1.4200319681257227, 1.8429169176312614, 1.3573146631657107, 1.3613831185322685, 1.4558154294226406, 1.2780111208324212, 0.7886692002199776, 1.2513134697770254, 1.565128906395955, 0.9596664751092816, 1.229862366192645, 1.081924466891789, 1.111507314381988, 0.9787962615669147, 0.6116717571822116, 0.9510060655520807, 1.1903071320327847, 0.9498037380592687, 1.0349733756213138, 1.866582896709912, 1.0865466015454803, 0.6267400342107526, 0.9140208774985443, 0.9798408473607394, 1.1648825935370135, 1.2159846140092239, 1.1245383727738474, 1.0784979611253103, 1.2490428991528333, 0.6407967725801771, 1.068668324933873, 0.6870142273109042, 0.927031672449401, 0.4454871539636873, 0.5097538075567264, 0.6562065181910253, 0.6959926880204463, 0.3588537720135193, 0.6224377220658721, 1.7307132444984323, 2.3000145835268153, 1.327206831417534, 0.9257733388814313, 1.2153523303143385, 0.7398576115325761, 0.9311622601760052, 1.2553016971047823, 1.133802533321011, 0.5773086413340248, 0.8756150802856881, 1.0928458184275962, 1.0155751885345552, 0.8983073758017677, 0.6652714303068812, 0.8960706763006234, 0.9986283064194643, 0.44583114642604027, 1.0500690080684956, 0.8119711476721613, 1.0797259989022492, 1.1591392809881758, 0.5589498619539535, 0.7121118251675107, 0.7926670437404754, 0.42816989121324867, 1.3000331944944883, 0.7871258827220289, 0.6142199270606485, 0.7738938012672615, 0.7036560755282376, 0.7959690693617248, 0.5999820458657846, 0.5932785082964959, 0.726994351994561, 0.3667169119883454, 0.40103360430521057, 1.1649978002926777, 0.6092126977255794, 0.35891871619390314, 0.4949443402085074, 0.5185312490896163, 0.6175317543712656, 0.8038231033579081, 0.2553670226039733, 0.3339115350747408, 0.5362680466660616, 0.39771775162361667, 0.5402554749542119, 0.3149396882057357, 0.3721801906432122, 0.48718576388629914, 0.7962355987553126, 0.4918489195598988, 0.25226505624543255, 0.15026628045480792, 0.7376997211704017, 0.7590797499739401, 0.38793019759373837, 0.45703582118824787, 0.4455654487389473, 0.6131551995768093, 0.7531816428423523, 0.41108880104674034, 0.29370583260422584, 0.5912291931437593, 0.21482833435145687, 0.3093758810872063, 0.3600096173785579, 0.24084408281141323, 0.44647209122066905, 0.19964547106471958, 0.29215732521324317, 0.3130263662296953, 0.24575520068006115, 0.13825717262552809, 0.36514773550099505, 0.21452174186284392, 0.7726704370448986, 0.45634574429319485, 0.3122368050781844, 0.05776697602024016, 0.1854773946131284, 0.5503131811487881, 0.28743680936224414, 0.3771210398268423, 0.3295559392454376, 0.5024100581564189, 0.4312812816763762, 0.16620777183144497, 0.3508810148044679, 0.16407267229376166, 0.24960096171718807, 0.1099984918803749, 0.25971806004965553, 0.17926499542018026, 0.08146063120636833, 0.14776578401052776, 0.21537143605947007, 0.2795480336964628, 0.09655567892633884, 0.12237956574301555, 0.16769773791632528, 0.2053869851110839, 0.12690574396394314, 0.14626360697760457, 0.27095333847671027, 0.08534698567233853, 0.11189195969800379, 0.04948403915152692, 0.04686323923690525, 0.025367597061111982, 0.09769466920014397, 0.05127357695541305, 0.03077601974795683, 0.09978465409029391, 0.08285593039943791, 0.08336358672151922, 0.296778078865661, 0.17747115931833968, 0.1652346467031926, 0.9328940050886166, 0.7968578466671137, 0.12050352961368434, 0.13951782523844286, 1.5305899356410402, 0.5357295783816367, 0.25324114847254797, 0.20073981006510697, 0.2233964466265701, 0.24295233857099802, 0.0725913969922685, 0.05298564169811315, 0.11045412407002203, 0.4968842232084023, 0.4371184149349949, 1.4942854079244765, 0.14652136624419634, 0.15948199664138601, 0.31005253162862445, 0.4215869937130474, 0.10524540448488609, 0.06936716339445756, 0.028366359470039727, 0.06622668476741339, 0.05638199354172865, 0.09400144409042956, 0.07466870217471178, 0.06626108132385396, 0.12616707832464547, 0.045277642345445346, 0.057425751483152626, 0.04994955847014086, 0.0357417098933736, 0.05729720134239927, 0.0380212057132703, 0.022473503359104828, 0.10765829459673788, 0.06250334432634279, 0.06565315499960024, 0.09353956286963783, 0.06560212981643271, 0.08354932861172695, 0.04462981999921014, 0.035550513887743794, 0.04394705129940501, 0.02369816594578828, 0.04103252661290321, 0.023090117515521657, 0.02139307379917506, 0.027697651054030034, 0.047769814246892944, 0.030182028669400757, 0.0102742852100776, 0.059926868062292596, 0.04037808543071978, 0.061574340908845206, 0.03061612113091562, 0.0361981308188108, 0.11871876214480453, 0.02711514343001913, 0.030976357804806915, 0.02748553665434398, 0.021510635515774104, 0.019245058776419233, 0.04146044511708326, 0.04634376549811827, 0.02264088468215882, 0.0392049201174265, 0.006836150385772681, 0.020894789802302166, 0.007139786195501827, 0.02259755622753597, 0.027050669788486522, 0.0529215937297416, 0.015122620732716927, 0.016567539291492402, 0.02348200079721816, 0.047905095082202344, 0.039325259914641225, 0.024334853338468125, 0.021740722869688995, 0.023134379727173526, 0.024942579891443245, 0.014512102097624636, 0.017145682969261618, 0.014833737849948887, 0.030325040465723686, 0.025440286233534438, 0.013319141922429036, 0.034172089968108166, 0.017870379722329913, 0.01651922213586791, 0.0232326344336274, 0.012857308652123273, 0.015452327329549577, 0.011569529579973335, 0.01606499371361354, 0.0254439479702484, 0.005483964579962343, 0.03755204699226703, 0.01592147211275496, 0.009630283114641458, 0.019034746972734173, 0.026398567195622516, 0.03537987162931496, 0.027986562536791812, 0.02010668388338037, 0.016697632867202675, 0.009339512873053887, 0.01378188759272413, 0.015363995120761626, 0.015868681066423168, 0.01925600705433446, 0.0162695367607405, 0.011625233886545794, 0.01253275354471131, 0.011914555606381896, 0.01619025255379878, 0.009422600680863272, 0.012448477097153542, 0.011722024897707557, 0.010980889587049568, 0.019698072511830034, 0.008106453248236514, 0.024261969313031276, 0.01664952295875847, 0.02209273164248876, 0.014234979332353945, 0.01259274166389674, 0.010110019908980701, 0.01836947418802377, 0.004577332326274213, 0.015563620312384625, 0.010464749807813395, 0.031001429885089412, 0.013934688045500736, 0.011304716332335658, 0.01568272955105841, 0.009243550549294243, 0.015496693887549928, 0.009004373868964697, 0.012918905489206253, 0.014236148726334837, 0.007616032784086871, 0.011923181484998463, 0.009461286540340778, 0.015398113683632223, 0.007788799428446233, 0.015113599583182646, 0.021553783004299915, 0.01639392292162116, 0.008440645614175579, 0.012806029385153225, 0.00874927098586356, 0.023462035444168056, 0.0072876130475124055, 0.007217936771467708, 0.01782194198233033, 0.01207155237968858, 0.011752139959465968, 0.0062419636601818256, 0.013935065942190003, 0.004752039193712535, 0.011486809869905357, 0.014080469611488039, 0.006042325705134973, 0.011840332992193676, 0.007672160077540881, 0.014259665348858825, 0.015814291732097942, 0.00996384980651853, 0.009753477038422397, 0.00531625625296299, 0.008090467558170616, 0.007376739239527481, 0.007561630080063654, 0.0048067482718969325, 0.01455321646510775, 0.009534030105478314, 0.006823118986963241, 0.012258925685638085, 0.011827042260566472, 0.004132437648233549, 0.00841132813573615, 0.01163982120993098, 0.008515131479418538, 0.01037824653241514, 0.011728255494906156, 0.008709818568441973, 0.009080710544627063, 0.012874743552425738, 0.006268778404724124, 0.008973090645707517, 0.00789153802921903, 0.010576438959029565, 0.01370747626669494, 0.010576139975443058, 0.008087506534542226, 0.008060868983640241, 0.01861776886689462, 0.00892701187826333, 0.0037573490446317624, 0.016570516146758602, 0.01005173608680383, 0.0073664631730625445, 0.011563938003768235, 0.005498498875273184, 0.007866180621534696, 0.01449660654662139, 0.01239944670797838, 0.019480864669444573, 0.008557679950982921, 0.008240917611021543, 0.007697114025691263, 0.004959484933526304, 0.0076770077535636624, 0.005585870863360915, 0.007348764169813485, 0.005429214354160296, 0.009889597384216563, 0.005898542378264898, 0.006844108812315968, 0.008954046744797191, 0.006692399897707854, 0.0068495914899186355, 0.0038659909177255446, 0.007102762083355663, 0.006198899235130445, 0.006963071672403594, 0.009938492067863744, 0.005734729090470414, 0.010965489489377032, 0.00696197330801953, 0.009014750912451984, 0.005234961588205492, 0.010189974498328942, 0.006265418445138659, 0.009006211923601624, 0.0069112952326092275, 0.006952217059568428, 0.0060525169198034105, 0.004624616378595277, 0.006300349436876483, 0.006327778553716645, 0.009768225769362357, 0.004369052790291455, 0.009330586000170749, 0.011916091975691092, 0.009587720872694959, 0.009000985560248923, 0.009036224419451375, 0.008854067363251455, 0.0062079453437354085, 0.006903764936367092, 0.008378244942623849, 0.006026925378267827, 0.0070854818870063315, 0.003817874057583822, 0.006060431005049817, 0.006294697868466508, 0.004406882469618212, 0.00513493240171714, 0.005647861855906821, 0.006373796991816481, 0.0036909966583682525, 0.003103439946741614, 0.007847740087956221, 0.0050946230776867584, 0.00548313360983361, 0.0066137318919213045, 0.008360881951082057, 0.0035338558313725097, 0.004852368511403443, 0.006987997809365887, 0.006896849101987068, 0.002169801300467622, 0.003442951170971006, 0.009987380424426477, 0.005124334851919167, 0.009748315281390522, 0.004503677118881455, 0.004347344683964638, 0.005801303881045078, 0.0054503464532295424, 0.00651982963991027, 0.006015332376606099, 0.005741210110396739, 0.004278929573893216, 0.0029834098257500016, 0.008569560094122864, 0.004063908132902382, 0.00772795803548363, 0.0061756214695268055, 0.00395160928194057, 0.011982435101879504, 0.010200359248933318, 0.006216088906124246, 0.004598530518563445, 0.004720701108118153, 0.004657498544578502, 0.006138915366123302, 0.0013014428460153586, 0.004593745286225871, 0.007867314830581736, 0.00706103713156039, 0.0047291814880302055, 0.004435165978522119, 0.003572075049107347, 0.0035199080476802816, 0.004912580903612035, 0.010671017101633795, 0.005025775275609579, 0.006725925348585075, 0.004400996638160431, 0.0057950247019024625, 0.003918046208421628, 0.010649138571592715, 0.0077419436390885115, 0.007040307072026977, 0.004485089963919566, 0.0025554286710430685, 0.004692542015776534, 0.004412933727764303, 0.0028848586911630805, 0.0043098954352949245, 0.005056200145948542, 0.0028941767692891984, 0.0044539151251743285, 0.0033704169158858266, 0.004387385607255327, 0.005898412790377338, 0.0033261656118733825, 0.005787752550088934, 0.003084066630380062, 0.005925159649311415, 0.005571045170012228, 0.002954864165883129, 0.0047262716933875164, 0.005992546982146556, 0.006148414847011039, 0.002562720094109994, 0.007561353489565804, 0.004224458399807254, 0.00718462533908763, 0.004472624064811412, 0.005578008915382949, 0.0051878711302764564, 0.007220312662910829, 0.004471873677363599]\n",
      "Accuracy history: [0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.24, 0.24, 0.28, 0.32, 0.36, 0.32, 0.38, 0.44, 0.4, 0.36, 0.46, 0.32, 0.34, 0.52, 0.58, 0.62, 0.56, 0.62, 0.7, 0.7, 0.8, 0.7, 0.84, 0.76, 0.86, 0.92, 0.88, 0.92, 0.96, 0.96, 0.94, 0.86, 0.94, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# SGD\n",
    "sgd = ConvNet4(input_shape=input_shape, wt_scale=wt_scale, verbose=False)\n",
    "sgd.compile('sgd')\n",
    "sgd.fit(x_dev, y_dev, x_val, y_val, mini_batch_sz=mini_batch_sz, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**: Why does decreasing the mini-batch size make the loss print-outs more erratic?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6d) Evaluate the different optimizers\n",
    "\n",
    "Make 2 \"high quality\" plots showing the following\n",
    "\n",
    "- Plot the accuracy (y axis) for the three optimizers as a function of training epoch (x axis).\n",
    "- Plot the loss (y axis) for the three optimizers as a function of training iteration (x axis).\n",
    "\n",
    "A high quality plot consists of:\n",
    "- A useful title\n",
    "- X and Y axis labels\n",
    "- A legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get accuracy values\n",
    "adam_acc = adam.train_acc_history\n",
    "sgd_m_acc = sgd_m.train_acc_history\n",
    "sgd_acc = sgd.train_acc_history\n",
    "\n",
    "#plot\n",
    "plt.plot(adam_acc, label=\"adam\")\n",
    "plt.plot(sgd_m_acc, label=\"sgd_m\")\n",
    "plt.plot(sgd_acc, label=\"sgd\")\n",
    "\n",
    "#make words show up\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy Over Iterations\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Training Iterations\")\n",
    "plt.show()\n",
    "\n",
    "#get loss values\n",
    "adam_loss = adam.loss_history\n",
    "sgd_m_loss = sgd_m.loss_history\n",
    "sgd_loss = sgd.loss_history\n",
    "\n",
    "#plot\n",
    "plt.plot(adam_loss, label=\"adam\")\n",
    "plt.plot(sgd_m_loss, label=\"sgd_m\")\n",
    "plt.plot(sgd_loss, label=\"sgd\")\n",
    "\n",
    "#make words show up\n",
    "plt.legend()\n",
    "plt.title(\"Loss Over Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Training Iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**: Which optimizer works best and why do think it is best?\n",
    "The adam optimizer works the best. It does so because it dynamically updates learning rate.\n",
    "\n",
    "**Question 5**: What is happening with the training set accuracy and why?\n",
    "The training set accuracy is overfitting. It is basically memorizing the features about each image, or memorizing the dataset. Therefore, it has perfect accuracy while looking at its own training set, but once it has a non-training image, it has horrible accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Training convolutional neural network on STL-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7a) Load in STL-10 at 32x32 resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the STL-10 dataset from the internet, convert it to Numpy ndarray, resize to 32x32\n",
    "# cache it locally on your computer for faster loading next time.\n",
    "load_stl10_dataset.purge_cached_dataset()\n",
    "stl_imgs, stl_labels = load_stl10_dataset.load(scale_fact=3)\n",
    "# preprocess\n",
    "stl_imgs, stl_labels = preprocess_data.preprocess_stl(stl_imgs, stl_labels)\n",
    "# create splits\n",
    "x_train, y_train, x_test, y_test, x_val, y_val, x_dev, y_dev = preprocess_data.create_splits(\n",
    "    stl_imgs, stl_labels, n_train_samps=4548, n_test_samps=400, n_valid_samps=2, n_dev_samps=50)\n",
    "\n",
    "print ('Train data shape: ', x_train.shape)\n",
    "print ('Train labels shape: ', y_train.shape)\n",
    "print ('Test data shape: ', x_test.shape)\n",
    "print ('Test labels shape: ', y_test.shape)\n",
    "print ('Validation data shape: ', x_val.shape)\n",
    "print ('Validation labels shape: ', y_val.shape)\n",
    "print ('dev data shape: ', x_dev.shape)\n",
    "print ('dev labels shape: ', y_dev.shape)\n",
    "\n",
    "classes = np.loadtxt(os.path.join('data', 'stl10_binary', 'class_names.txt'), dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7b) Set up accelerated convolution and max pooling layers\n",
    "\n",
    "As you may have noticed, we had to downsize STL-10 to 16x16 resolution to train the network on the dev set (N=50) in a reasonable amount of time. The training set is N=4000, how will we ever manage to process that amount of data!?\n",
    "\n",
    "On one hand, this is an unfortunate inevitable reality of working with large (\"big\") datasets: you can easily find a dataset that is too time consuming to process for any computer, despite how fast/many CPU/GPUs it has.\n",
    "\n",
    "On the other hand, we can do better for this project and STL-10 :) If you were to time (profile) different parts of the training process, you'd notice that largest bottleneck is convolution and max pooling operations (both forward/backward). You implemented those operations intuitively, which does not always yield the best performance. **By swapping out forward/backward convolution and maxpooling for implementations that use different algorithms (im2col, reshaping) that are compiled to C code, we will speed up training up by several orders of magnitude**.\n",
    "\n",
    "Follow these steps to subsitute in the \"accelerated\" convolution and max pooling layers.\n",
    "\n",
    "- Install the `cython` python package: `pip3 install cython` (or `pip3 install cython --user` if working in Davis 102)\n",
    "- Dowload files `im2col_cython.pyx`, `accelerated_layer.py`, `setup.py` from the project website. Put them in your base project folder.\n",
    "- Open terminal, `cd` to Project directory.\n",
    "- Compile the im2col functions: `python3 setup.py build_ext --inplace`. A `.c` and `.so` file should have appeared in your project folder.\n",
    "- Restart Jupyter Notebook kernel\n",
    "- Create a class called `Conv4NetAccel` in `network.py` by copy-pasting the contents of `Conv4Net`. Import `accelerated_layer` at the top and replace the `Conv2D` and `MaxPool2D` layers with `Conv2DAccel` and `MaxPool2DAccel`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7c) Training convolutional neural network on STL-10\n",
    "\n",
    "You are now ready to train on the entire training set.\n",
    "\n",
    "- Create a `Conv4NetAccel` object with hyperparameters of your choice.\n",
    "- Your goal is to achieve 45% accuracy on the test and/or validation set.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- I suggest using your intuition about hyperparameters and over/underfitting to guide your choice, rather than a grid search. This should not be overly challenging.\n",
    "- Use the best / most efficient optimizer based on your prior analysis.\n",
    "- It should take on the order of 1 sec per training iteration. If that's way off, seek help as something could be wrong with running the acclerated code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network import ConvNet4Accel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7d) Analysis of STL-10 training quality\n",
    "\n",
    "Use your trained network that achieves 45%+ accuracy on the test set to make \"high quality\" plots showing the following \n",
    "\n",
    "- Plot the accuracy of the training and validation sets as a function of training epoch. You may have to convert iterations to epochs.\n",
    "- Plot the loss as a function of training iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(netT.validation_acc_history)\n",
    "plt.plot(netT.train_acc_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(netT.loss_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7f) Visualize layer weights\n",
    "\n",
    "Run the following code and submit the inline image of the weight visualization of the 1st layer (convolutional layer) of the network.\n",
    "\n",
    "**Note:**\n",
    "- Setting optional parameter to `True` will let you save a .PNG file in your project folder of your weights. I'd suggest setting it to `False` unless look at your weights and they look like they are worth saving. You don't want a training run that produces undesirable weights to overwrite your good looking results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights(wts, saveFig=True, filename='convWts_adam_overfit.png'):\n",
    "    grid_sz = int(np.sqrt(len(wts)))\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for x in range(grid_sz):\n",
    "        for y in range(grid_sz):\n",
    "            lin_ind = np.ravel_multi_index((x, y), dims=(grid_sz, grid_sz))\n",
    "            plt.subplot(grid_sz, grid_sz, lin_ind+1)\n",
    "            currImg = wts[lin_ind]\n",
    "            low, high = np.min(currImg), np.max(currImg)\n",
    "            currImg = 255*(currImg - low) / (high - low)\n",
    "            currImg = currImg.astype('uint8')\n",
    "            plt.imshow(currImg)\n",
    "            plt.gca().axis('off')\n",
    "    if saveFig:\n",
    "        plt.savefig('convWts_adam_overfit.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsitute your trained network below\n",
    "# netT is my network's name\n",
    "# You shouldn't see RGB noise\n",
    "plot_weights(netT.layers[0].wts.transpose(0, 2, 3, 1), saveFig=False, filename='convWts_adam_train_20epoch.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:** What do the learned filters look like? Does this make sense to you / is this what you expected? In which area of the brain do these filters resemble cell receptive fields?\n",
    "\n",
    "Note: you should not see RGB \"noise\". If you do, and you pass the \"overfit\" test with the Adam optimizer, you probably need to increase the number of training epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "**General advice:** When making modifications for extensions, make small changes, then check to make sure you pass test code. Also, test out the network runtime on small examples before/after the changes. If you're not careful, the simulation time can become intractable really quickly!\n",
    "\n",
    "**Remember:** One thorough extension usually is worth more than several \"shallow\" extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Pedal to the metal: achieve high accuracy on STL-10\n",
    "\n",
    "You can achieve higher (>50%) classification accuracy on the STL-10 test set. Find the hyperparameters to achieve this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Experiment with different network architectures.\n",
    "\n",
    "The design of the `Network` class is modular. As long as you're careful about shapes, adding/removing network layers (e.g. `Conv2D`, `Dense`, etc.) should be straight forward. Experiment with adding another sequence of `Conv2D` and `MaxPooling2D` layers. Add another `Dense` hidden layer before the output layer. How do the changes affect classification accuracy and loss? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Experiment with different network hyperparameters.\n",
    "\n",
    "Explore the affect one or more change below has on classification. Be careful about how the hyperparameters may affect the shape of network layers. Thorough analysis will get you more points (not try a few ad hoc values).\n",
    "\n",
    "- Experiment with different numbers of hidden units in the Dense layers.\n",
    "- Experiment different max pooling window sizes and strides.\n",
    "- Experiment with kernel sizes (not 7x7). Can you get away with smaller ones? Do they perform just as well? What is the change in runtime like? What is the impact on their visualized appearance?\n",
    "- Experiment with number of kernels in the convolutional layer. Is more/fewer better? What is the impact on their visualized appearance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. Add and test some training bells and whistles\n",
    "\n",
    "Add features like early stopping, learning rate decay (learning rate at the end of an epoch becomes some fraction of its former value), etc and assess how they affect training loss convergence and accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Additional optimizers\n",
    "\n",
    "Research other optimizers used in backpropogation and implement one or more of them within the model structure. Compare its performance to ones you have implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Optimize your algorithms\n",
    "\n",
    "Find the main performance bottlenecks in the network and improve your code to reduce runtime (e.g. reduce explicit for loops, increase vectorization, etc). Research faster algorithms to do operations like convolution and implement them. Given the complexity of the network, I suggest focusing on one area at a time and make sure everything you change passes the test code before proceeding. Quantify and discuss your performance improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Additional loss functions\n",
    "\n",
    "Implement support for sigmoid, or another activation functions and associated losses. Test it out and compare with softmax/cross entropy. Make sure any necessary changes to the layer's gradient are made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Additional datasets\n",
    "\n",
    "Do classification and analyxe the results with an image dataset of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Performance analysis\n",
    "\n",
    "Do a thorough comparative analysis of the non-accelerated network and accelerated networks with respect to runtime."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
