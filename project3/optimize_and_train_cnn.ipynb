{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cole Turner and Ethan Seal**\n",
    "\n",
    "Fall 2019\n",
    "\n",
    "CS343: Neural Networks\n",
    "\n",
    "Project 3: Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(['seaborn-colorblind', 'seaborn-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=7)\n",
    "\n",
    "# Automatically reload your external source code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global note: Make sure any debug printouts do not appear if `verbose=False`!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4) Implement weight optimizers for gradient descent\n",
    "\n",
    "To change the weights during training, we need an optimization algorithm to have our loss decrease over epochs as we learn the structure of the input patterns. Until now, we used **Stochastic gradient descent (SGD)**, which is the simplest algorithm. We will implement 3 popular algorithms:\n",
    "\n",
    "- `SGD` (stochastic gradient descent)\n",
    "- `SGD_Momentum` (stochastic gradient descent with momentum)\n",
    "- `Adam` (Adaptive Moment Estimation)\n",
    "\n",
    "Implement each of these according to the update equations (in `optimizer.py::update_weights` in each subclass). Let's use $w_t$ in the math below to represent the weights in a layer at time step $t$, $dw$ to represent the gradient of the weights in a layer, and $\\eta$ represent the learning rate. We use vectorized notation below (update applies to all weights element-wise). Then:\n",
    "\n",
    "**SGD**: \n",
    "\n",
    "$w_{t} = w_{t-1} - \\eta \\times dw$\n",
    "\n",
    "**SGD (momentum)**:\n",
    "\n",
    "$v_{t} = m \\times v_{t-1} - \\eta \\times dw$\n",
    "\n",
    "$w_{t} = w_{t-1} + v_t$\n",
    "\n",
    "where $v_t$ is called the `velocity` at time $t$. At the first time step (0), velocity should be set to all zeros and have the same shape as $w$. $m$ is a constant that determines how much of the gradient obtained on the previous time step should factor into the weight update for the current time step.\n",
    "\n",
    "\n",
    "**Adam**:\n",
    "\n",
    "$m_{t} = \\beta_1 \\times m_{t-1} + (1 - \\beta_1)\\times dw$\n",
    "\n",
    "$v_{t} = \\beta_2 \\times v_{t-1} + (1 - \\beta_2)\\times dw^2$\n",
    "\n",
    "$n = m_{t} / \\left (1-(\\beta_1^t) \\right )$\n",
    "\n",
    "$u = v_{t} / \\left (1-(\\beta_2^t) \\right )$\n",
    "\n",
    "$w_{t} = w_{t-1} - \\left ( \\eta \\times n \\right ) / \\left ( \\sqrt(u) + \\epsilon \\right ) $\n",
    "\n",
    "\n",
    "Like SGD (momentum), Adam records momentum terms $m$ and $v$. At time step 0, you should initialize them to zeros in an array equal in size to the weights. $n$ and $u$ are variables computed on each time step. The remaining quantities are constants. Note that $t$ keeps track of the integer time step, and needs to be incremented on each update. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimizer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Test SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD: Wts after 1 iter [-3.1764052 -2.0400157 -1.0978738 -0.2240893  0.8132442  2.0977278]\n",
      "SGD: Wts after 2 iter [-3.3528105 -2.0800314 -1.1957476 -0.4481786  0.6264884  2.1954556]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "wts = np.arange(-3, 3, dtype=np.float64)\n",
    "d_wts = np.random.randn(len(wts))\n",
    "\n",
    "optimizer = SGD()\n",
    "optimizer.prepare(wts, d_wts)\n",
    "\n",
    "new_wts_1 = optimizer.update_weights()\n",
    "new_wts_2 = optimizer.update_weights()\n",
    "\n",
    "print(f'SGD: Wts after 1 iter {new_wts_1}')\n",
    "print(f'SGD: Wts after 2 iter {new_wts_2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should be:\n",
    "\n",
    "    SGD: Wts after 1 iter [-3.1764052 -2.0400157 -1.0978738 -0.2240893  0.8132442  2.0977278]\n",
    "    SGD: Wts after 2 iter [-3.3528105 -2.0800314 -1.1957476 -0.4481786  0.6264884  2.1954556]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Test SGD_Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD M: Wts after 1 iter\n",
      "[[ 1.6879486  0.3879897  0.9343517  2.2075258]\n",
      " [ 1.7181501 -0.9567621  0.9187816 -0.0659476]\n",
      " [ 0.1520801  0.3452366  0.0576     1.52849  ]]\n",
      "SGD M: Wts after 2 iter\n",
      "[[ 1.5661825  0.3685217  0.8633335  2.1541379]\n",
      " [ 1.4790974 -0.9239367  0.8686908  0.0707077]\n",
      " [ 0.5605585  0.2406577 -0.0807098  1.6472364]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "wts = np.random.randn(3, 4)\n",
    "d_wts = np.random.randn(3, 4)\n",
    "\n",
    "optimizer = SGD_Momentum(lr=0.1, m=0.6)\n",
    "optimizer.prepare(wts, d_wts)\n",
    "\n",
    "new_wts_1 = optimizer.update_weights()\n",
    "new_wts_2 = optimizer.update_weights()\n",
    "\n",
    "print(f'SGD M: Wts after 1 iter\\n{new_wts_1}')\n",
    "print(f'SGD M: Wts after 2 iter\\n{new_wts_2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should be:\n",
    "\n",
    "    SGD M: Wts after 1 iter\n",
    "    [[ 1.6879486  0.3879897  0.9343517  2.2075258]\n",
    "     [ 1.7181501 -0.9567621  0.9187816 -0.0659476]\n",
    "     [ 0.1520801  0.3452366  0.0576     1.52849  ]]\n",
    "    SGD M: Wts after 2 iter\n",
    "    [[ 1.5661825  0.3685217  0.8633335  2.1541379]\n",
    "     [ 1.4790974 -0.9239367  0.8686908  0.0707077]\n",
    "     [ 0.5605585  0.2406577 -0.0807098  1.6472364]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Test Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam: Wts after 1 iter\n",
      "[[ 1.6640523  0.3001572  0.878738   2.1408932]\n",
      " [ 1.767558  -0.8772779  0.8500884 -0.0513572]\n",
      " [-0.0032189  0.3105985  0.0440436  1.5542735]]\n",
      "Adam: Wts after 2 iter\n",
      "[[ 1.5640523  0.2001572  0.778738   2.0408932]\n",
      " [ 1.667558  -0.7772779  0.7500884  0.0486428]\n",
      " [ 0.0967811  0.2105985 -0.0559564  1.6542735]]\n",
      "Adam: Wts after 3 iter\n",
      "[[ 1.4640523  0.1001572  0.678738   1.9408932]\n",
      " [ 1.567558  -0.6772779  0.6500884  0.1486428]\n",
      " [ 0.1967811  0.1105985 -0.1559564  1.7542735]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "wts = np.random.randn(3, 4)\n",
    "d_wts = np.random.randn(3, 4)\n",
    "\n",
    "optimizer = Adam(lr=0.1)\n",
    "optimizer.prepare(wts, d_wts)\n",
    "\n",
    "new_wts_1 = optimizer.update_weights()\n",
    "new_wts_2 = optimizer.update_weights()\n",
    "new_wts_3 = optimizer.update_weights()\n",
    "\n",
    "print(f'Adam: Wts after 1 iter\\n{new_wts_1}')\n",
    "print(f'Adam: Wts after 2 iter\\n{new_wts_2}')\n",
    "print(f'Adam: Wts after 3 iter\\n{new_wts_3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should be:\n",
    "\n",
    "    Adam: Wts after 1 iter\n",
    "    [[ 1.6640523  0.3001572  0.878738   2.1408932]\n",
    "     [ 1.767558  -0.8772779  0.8500884 -0.0513572]\n",
    "     [-0.0032189  0.3105985  0.0440436  1.5542735]]\n",
    "    Adam: Wts after 2 iter\n",
    "    [[ 1.5640523  0.2001572  0.778738   2.0408932]\n",
    "     [ 1.667558  -0.7772779  0.7500884  0.0486428]\n",
    "     [ 0.0967811  0.2105985 -0.0559564  1.6542735]]\n",
    "    Adam: Wts after 3 iter\n",
    "    [[ 1.4640523  0.1001572  0.678738   1.9408932]\n",
    "     [ 1.567558  -0.6772779  0.6500884  0.1486428]\n",
    "     [ 0.1967811  0.1105985 -0.1559564  1.7542735]]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5) Write network training methods\n",
    "\n",
    "Implement methods in `network.py` to actually train the network, using all the building blocks that you have created. The methods to implement are:\n",
    "\n",
    "- `predict`\n",
    "- `fit`. Add an optional parameter `print_every=1` that controls the frequency (in iterations) with which to wait before printing out the loss and iteration number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6) Overfitting a convolutional neural network\n",
    "\n",
    "Usually we try to prevent overfitting, but we can use it as a valuable debugging tool to test out a complex backprop-style neural network. Assuming everything is working, it is almost always the case that we should be able to overfit a tiny dataset with a huge model with tons of parameters (i.e. your CNN). You will use this strategy to verify that your network is working.\n",
    "\n",
    "Let's use a small amount of real data from STL-10. If everything is working properly, the network should overfit and you should see a significant drop in the loss from its starting value of ~2.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6a) Move your `preprocess_data.py` from the MLP project\n",
    "\n",
    "Make the one following change:\n",
    "\n",
    "- Re-arrange dimensions of `imgs` so that when it is returned, `shape=(Num imgs, RGB color chans, height, width)` (No longer flatten non-batch dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_stl10_dataset\n",
    "import preprocess_data\n",
    "from network import ConvNet4\n",
    "import optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b) Load in STL-10 at 16x16 resolution\n",
    "\n",
    "If you don't want to wait for STL-10 to download from the internet and resize, copy over your data and numpy folders from your MLP project.\n",
    "\n",
    "**Notes:**\n",
    "- You will need to download the new version of `load_stl10_dataset`.\n",
    "- The different train/test split here won't work if you hard coded the proportions in your `create_splits` implementation! *This isn't catastrophic, it just means that it will take longer to compute accuracy on the validation set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images are: (5000, 96, 96, 3)\n",
      "Labels are: (5000,)\n",
      "Resizing 5000 images to 16x16...Done!\n",
      "Saving Numpy arrays the images and labels to ./numpy...Done!\n",
      "imgs.shape (5000, 16, 16, 3)\n",
      "data.shape (5000, 768)\n",
      "Train data shape:  (4548, 768)\n",
      "Train labels shape:  (4548,)\n",
      "Test data shape:  (400, 768)\n",
      "Test labels shape:  (400,)\n",
      "Validation data shape:  (2, 768)\n",
      "Validation labels shape:  (2,)\n",
      "dev data shape:  (50, 768)\n",
      "dev labels shape:  (50,)\n"
     ]
    }
   ],
   "source": [
    "# Download the STL-10 dataset from the internet, convert it to Numpy ndarray, resize to 16x16\n",
    "# cache it locally on your computer for faster loading next time.\n",
    "load_stl10_dataset.purge_cached_dataset()\n",
    "stl_imgs, stl_labels = load_stl10_dataset.load(scale_fact=6)\n",
    "# preprocess\n",
    "stl_imgs, stl_labels = preprocess_data.preprocess_stl(stl_imgs, stl_labels)\n",
    "# create splits\n",
    "x_train, y_train, x_test, y_test, x_val, y_val, x_dev, y_dev = preprocess_data.create_splits(\n",
    "    stl_imgs, stl_labels, n_train_samps=4548, n_test_samps=400, n_valid_samps=2, n_dev_samps=50)\n",
    "\n",
    "print ('Train data shape: ', x_train.shape)\n",
    "print ('Train labels shape: ', y_train.shape)\n",
    "print ('Test data shape: ', x_test.shape)\n",
    "print ('Test labels shape: ', y_test.shape)\n",
    "print ('Validation data shape: ', x_val.shape)\n",
    "print ('Validation labels shape: ', y_val.shape)\n",
    "print ('dev data shape: ', x_dev.shape)\n",
    "print ('dev labels shape: ', y_dev.shape)\n",
    "\n",
    "classes = np.loadtxt(os.path.join('data', 'stl10_binary', 'class_names.txt'), dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6c) Train and overfit the network on a small STL-10 sample with each optimizer\n",
    "\n",
    "**Goal:** If your network works, you should see a drop in loss over epochs to 0.\n",
    "\n",
    "In 3 seperate cells below\n",
    "\n",
    "- Create 3 different `ConvNet4` networks.\n",
    "- Compile each with a different optimizer (each net uses a different optimizer).\n",
    "- Train each on the **dev** set and validate on the tiny validation set (we dont care about out-of-training-set performance here).\n",
    "\n",
    "You will be making plots demonstrating the overfitting for each optimizer below. **You should train the nets with the same number of epochs such that at least 2/3 of them clearly show loss convergence to a small value; one optimizer may not converge yet, and that's ok**. Cut off the simulations based on the 2/3 that do converge.\n",
    "\n",
    "Guidelines:\n",
    "\n",
    "- Weight scales and learning rates of `1e-2` should work well.\n",
    "- Start by testing the Adam optimizer.\n",
    "- Remember that the input shape is (3, 16, 16). You need to specify this to the network constructor.\n",
    "- The hyperparameters are up to you, though I wouldn't recommend a batch size that is too small (close to 1), otherwise it may be tricky to see whether the loss is actually decreasing on average.\n",
    "- Decreasing `acc_freq` will make the `fit` function evaluate the training and validation accuracy more often. This is a computationally intensive process, so small values come with an increase in training time. On the other hand, checking the accuracy too infrequently means you won't know whether the network is trending toward overfitting the training data, which is what you're checking for.\n",
    "- Each training session takes ~30 mins on my laptop.\n",
    "\n",
    "**Caveat emptor:** Training convolutional networks is notoriously computationally intensive. If you experiment with hyperparameters, each training session may take several hours. Use the loss/accuracy print outs to quickly gauge whether your hyperparameter choices are getting your network to decrease in loss. Monitor print outs and interrupt the Jupyter kernel if things are not trending in the right direction. Consider using the Davis 102 iMacs if this is running too slow on your laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_scale = 1e-2\n",
    "lr = 1e-2\n",
    "input_shape = (3, 16, 16)\n",
    "mini_batch_sz = 10\n",
    "n_epochs = 150\n",
    "\n",
    "#preprocessing flattened it when we actually wanted it not flattened.\n",
    "x_dev = x_dev.reshape(x_dev.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "x_val = x_val.reshape(x_val.shape[0], input_shape[0], input_shape[1], input_shape[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train...\n",
      "750 iterations. 5 iter/epoch.\n",
      "Iteration: 1/750.\n",
      "Time taken for iteration 0: 1.6660547256469727\n",
      "Estimated time to complete: 1249.5410442352295\n",
      "Iteration: 2/750.\n",
      "Iteration: 3/750.\n",
      "Iteration: 4/750.\n",
      "Iteration: 5/750.\n",
      "Iteration: 6/750.\n",
      "Iteration: 7/750.\n",
      "Iteration: 8/750.\n",
      "Iteration: 9/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [2.2845796024885723, 2.2924845991086413, 2.2430353643670484]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.16, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 10/750.\n",
      "Iteration: 11/750.\n",
      "Iteration: 12/750.\n",
      "Iteration: 13/750.\n",
      "Iteration: 14/750.\n",
      "Iteration: 15/750.\n",
      "Iteration: 16/750.\n",
      "Iteration: 17/750.\n",
      "Iteration: 18/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [2.196307338513974, 2.1725589737331377, 1.8555820840431605]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.24, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 19/750.\n",
      "Iteration: 20/750.\n",
      "Iteration: 21/750.\n",
      "Iteration: 22/750.\n",
      "Iteration: 23/750.\n",
      "Iteration: 24/750.\n",
      "Iteration: 25/750.\n",
      "Iteration: 26/750.\n",
      "Iteration: 27/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [2.312345181935326, 2.2428537670038167, 1.8153889792853417]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 28/750.\n",
      "Iteration: 29/750.\n",
      "Iteration: 30/750.\n",
      "Iteration: 31/750.\n",
      "Iteration: 32/750.\n",
      "Iteration: 33/750.\n",
      "Iteration: 34/750.\n",
      "Iteration: 35/750.\n",
      "Iteration: 36/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.7621807182453688, 1.833807183850169, 2.0549246832422936]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.24, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 37/750.\n",
      "Iteration: 38/750.\n",
      "Iteration: 39/750.\n",
      "Iteration: 40/750.\n",
      "Iteration: 41/750.\n",
      "Iteration: 42/750.\n",
      "Iteration: 43/750.\n",
      "Iteration: 44/750.\n",
      "Iteration: 45/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [2.1230847051097714, 2.00511916232943, 1.9732593373540852]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.26, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 46/750.\n",
      "Iteration: 47/750.\n",
      "Iteration: 48/750.\n",
      "Iteration: 49/750.\n",
      "Iteration: 50/750.\n",
      "Iteration: 51/750.\n",
      "Iteration: 52/750.\n",
      "Iteration: 53/750.\n",
      "Iteration: 54/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.8132958306345657, 2.0563964202738827, 1.6435401383740862]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.42, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 55/750.\n",
      "Iteration: 56/750.\n",
      "Iteration: 57/750.\n",
      "Iteration: 58/750.\n",
      "Iteration: 59/750.\n",
      "Iteration: 60/750.\n",
      "Iteration: 61/750.\n",
      "Iteration: 62/750.\n",
      "Iteration: 63/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [2.1429283671619177, 1.3842210419573906, 1.8503751364566963]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.42, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 64/750.\n",
      "Iteration: 65/750.\n",
      "Iteration: 66/750.\n",
      "Iteration: 67/750.\n",
      "Iteration: 68/750.\n",
      "Iteration: 69/750.\n",
      "Iteration: 70/750.\n",
      "Iteration: 71/750.\n",
      "Iteration: 72/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.4492693059040154, 1.8337719840693965, 1.788756308516624]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.42, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 73/750.\n",
      "Iteration: 74/750.\n",
      "Iteration: 75/750.\n",
      "Iteration: 76/750.\n",
      "Iteration: 77/750.\n",
      "Iteration: 78/750.\n",
      "Iteration: 79/750.\n",
      "Iteration: 80/750.\n",
      "Iteration: 81/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.4360661578335743, 1.5099931642285551, 1.4284829401566097]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.46, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 82/750.\n",
      "Iteration: 83/750.\n",
      "Iteration: 84/750.\n",
      "Iteration: 85/750.\n",
      "Iteration: 86/750.\n",
      "Iteration: 87/750.\n",
      "Iteration: 88/750.\n",
      "Iteration: 89/750.\n",
      "Iteration: 90/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.3285889474860497, 1.476922730116448, 1.0004611806216006]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.46, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 91/750.\n",
      "Iteration: 92/750.\n",
      "Iteration: 93/750.\n",
      "Iteration: 94/750.\n",
      "Iteration: 95/750.\n",
      "Iteration: 96/750.\n",
      "Iteration: 97/750.\n",
      "Iteration: 98/750.\n",
      "Iteration: 99/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.4980733151569927, 1.2347177336803679, 1.2437637714007388]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.56, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 100/750.\n",
      "Iteration: 101/750.\n",
      "Iteration: 102/750.\n",
      "Iteration: 103/750.\n",
      "Iteration: 104/750.\n",
      "Iteration: 105/750.\n",
      "Iteration: 106/750.\n",
      "Iteration: 107/750.\n",
      "Iteration: 108/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.1590602893676074, 1.31185675119782, 0.7630824680897652]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.54, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 109/750.\n",
      "Iteration: 110/750.\n",
      "Iteration: 111/750.\n",
      "Iteration: 112/750.\n",
      "Iteration: 113/750.\n",
      "Iteration: 114/750.\n",
      "Iteration: 115/750.\n",
      "Iteration: 116/750.\n",
      "Iteration: 117/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.1463413255311268, 0.8585563678333387, 1.010251243340975]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.62, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 118/750.\n",
      "Iteration: 119/750.\n",
      "Iteration: 120/750.\n",
      "Iteration: 121/750.\n",
      "Iteration: 122/750.\n",
      "Iteration: 123/750.\n",
      "Iteration: 124/750.\n",
      "Iteration: 125/750.\n",
      "Iteration: 126/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.9948293101744844, 0.6375138809069608, 0.9847764571024111]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.68, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 127/750.\n",
      "Iteration: 128/750.\n",
      "Iteration: 129/750.\n",
      "Iteration: 130/750.\n",
      "Iteration: 131/750.\n",
      "Iteration: 132/750.\n",
      "Iteration: 133/750.\n",
      "Iteration: 134/750.\n",
      "Iteration: 135/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.0188620864883335, 1.5110819357261827, 0.5282107176868567]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.72, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 136/750.\n",
      "Iteration: 137/750.\n",
      "Iteration: 138/750.\n",
      "Iteration: 139/750.\n",
      "Iteration: 140/750.\n",
      "Iteration: 141/750.\n",
      "Iteration: 142/750.\n",
      "Iteration: 143/750.\n",
      "Iteration: 144/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [1.0612828686712, 0.7893206818157843, 0.7050813317119973]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.88, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 145/750.\n",
      "Iteration: 146/750.\n",
      "Iteration: 147/750.\n",
      "Iteration: 148/750.\n",
      "Iteration: 149/750.\n",
      "Iteration: 150/750.\n",
      "Iteration: 151/750.\n",
      "Iteration: 152/750.\n",
      "Iteration: 153/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.7095368860314037, 0.7967863091393386, 0.34622555323144777]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.88, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 154/750.\n",
      "Iteration: 155/750.\n",
      "Iteration: 156/750.\n",
      "Iteration: 157/750.\n",
      "Iteration: 158/750.\n",
      "Iteration: 159/750.\n",
      "Iteration: 160/750.\n",
      "Iteration: 161/750.\n",
      "Iteration: 162/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.4147177801834292, 0.1976633903218503, 0.6149993686508091]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.88, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 163/750.\n",
      "Iteration: 164/750.\n",
      "Iteration: 165/750.\n",
      "Iteration: 166/750.\n",
      "Iteration: 167/750.\n",
      "Iteration: 168/750.\n",
      "Iteration: 169/750.\n",
      "Iteration: 170/750.\n",
      "Iteration: 171/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.5417135652175434, 0.5127860907579702, 0.4423694055698868]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.9, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 172/750.\n",
      "Iteration: 173/750.\n",
      "Iteration: 174/750.\n",
      "Iteration: 175/750.\n",
      "Iteration: 176/750.\n",
      "Iteration: 177/750.\n",
      "Iteration: 178/750.\n",
      "Iteration: 179/750.\n",
      "Iteration: 180/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.3257512884451925, 0.23361025284867254, 0.4208937622866595]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.9, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 181/750.\n",
      "Iteration: 182/750.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 183/750.\n",
      "Iteration: 184/750.\n",
      "Iteration: 185/750.\n",
      "Iteration: 186/750.\n",
      "Iteration: 187/750.\n",
      "Iteration: 188/750.\n",
      "Iteration: 189/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.2526039181014987, 0.2661376588799025, 0.29333143778269355]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.96, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 190/750.\n",
      "Iteration: 191/750.\n",
      "Iteration: 192/750.\n",
      "Iteration: 193/750.\n",
      "Iteration: 194/750.\n",
      "Iteration: 195/750.\n",
      "Iteration: 196/750.\n",
      "Iteration: 197/750.\n",
      "Iteration: 198/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.1112629225244871, 0.07248974144177063, 0.31392346579773295]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 199/750.\n",
      "Iteration: 200/750.\n",
      "Iteration: 201/750.\n",
      "Iteration: 202/750.\n",
      "Iteration: 203/750.\n",
      "Iteration: 204/750.\n",
      "Iteration: 205/750.\n",
      "Iteration: 206/750.\n",
      "Iteration: 207/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.10167808297320831, 0.07616059586943538, 0.17105394612346236]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 208/750.\n",
      "Iteration: 209/750.\n",
      "Iteration: 210/750.\n",
      "Iteration: 211/750.\n",
      "Iteration: 212/750.\n",
      "Iteration: 213/750.\n",
      "Iteration: 214/750.\n",
      "Iteration: 215/750.\n",
      "Iteration: 216/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.04604406308275605, 0.2700110150812488, 0.1730756502564659]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.98, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 217/750.\n",
      "Iteration: 218/750.\n",
      "Iteration: 219/750.\n",
      "Iteration: 220/750.\n",
      "Iteration: 221/750.\n",
      "Iteration: 222/750.\n",
      "Iteration: 223/750.\n",
      "Iteration: 224/750.\n",
      "Iteration: 225/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.198977290108442, 0.14312686463978938, 0.03261416902984667]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 226/750.\n",
      "Iteration: 227/750.\n",
      "Iteration: 228/750.\n",
      "Iteration: 229/750.\n",
      "Iteration: 230/750.\n",
      "Iteration: 231/750.\n",
      "Iteration: 232/750.\n",
      "Iteration: 233/750.\n",
      "Iteration: 234/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.12942711737335458, 0.07879379011385884, 0.062299998236475564]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 235/750.\n",
      "Iteration: 236/750.\n",
      "Iteration: 237/750.\n",
      "Iteration: 238/750.\n",
      "Iteration: 239/750.\n",
      "Iteration: 240/750.\n",
      "Iteration: 241/750.\n",
      "Iteration: 242/750.\n",
      "Iteration: 243/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0771401766704466, 0.061047090270442084, 0.07818698688785436]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 244/750.\n",
      "Iteration: 245/750.\n",
      "Iteration: 246/750.\n",
      "Iteration: 247/750.\n",
      "Iteration: 248/750.\n",
      "Iteration: 249/750.\n",
      "Iteration: 250/750.\n",
      "Iteration: 251/750.\n",
      "Iteration: 252/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.02739438570280992, 0.049484310869164085, 0.03895022424867601]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 253/750.\n",
      "Iteration: 254/750.\n",
      "Iteration: 255/750.\n",
      "Iteration: 256/750.\n",
      "Iteration: 257/750.\n",
      "Iteration: 258/750.\n",
      "Iteration: 259/750.\n",
      "Iteration: 260/750.\n",
      "Iteration: 261/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.05045440734712168, 0.032420542822017895, 0.06781557374825271]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 262/750.\n",
      "Iteration: 263/750.\n",
      "Iteration: 264/750.\n",
      "Iteration: 265/750.\n",
      "Iteration: 266/750.\n",
      "Iteration: 267/750.\n",
      "Iteration: 268/750.\n",
      "Iteration: 269/750.\n",
      "Iteration: 270/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.03012681363097762, 0.04319413731979136, 0.02004262906877935]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 271/750.\n",
      "Iteration: 272/750.\n",
      "Iteration: 273/750.\n",
      "Iteration: 274/750.\n",
      "Iteration: 275/750.\n",
      "Iteration: 276/750.\n",
      "Iteration: 277/750.\n",
      "Iteration: 278/750.\n",
      "Iteration: 279/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.03676389344140057, 0.06299340377472308, 0.03454798975959152]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 280/750.\n",
      "Iteration: 281/750.\n",
      "Iteration: 282/750.\n",
      "Iteration: 283/750.\n",
      "Iteration: 284/750.\n",
      "Iteration: 285/750.\n",
      "Iteration: 286/750.\n",
      "Iteration: 287/750.\n",
      "Iteration: 288/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.021970949590423193, 0.008230711804036533, 0.04246734441958538]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 289/750.\n",
      "Iteration: 290/750.\n",
      "Iteration: 291/750.\n",
      "Iteration: 292/750.\n",
      "Iteration: 293/750.\n",
      "Iteration: 294/750.\n",
      "Iteration: 295/750.\n",
      "Iteration: 296/750.\n",
      "Iteration: 297/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.024177658392888598, 0.017464338244271845, 0.022840108982812742]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 298/750.\n",
      "Iteration: 299/750.\n",
      "Iteration: 300/750.\n",
      "Iteration: 301/750.\n",
      "Iteration: 302/750.\n",
      "Iteration: 303/750.\n",
      "Iteration: 304/750.\n",
      "Iteration: 305/750.\n",
      "Iteration: 306/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.025275973834323114, 0.030583171943476435, 0.00984498326368509]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 307/750.\n",
      "Iteration: 308/750.\n",
      "Iteration: 309/750.\n",
      "Iteration: 310/750.\n",
      "Iteration: 311/750.\n",
      "Iteration: 312/750.\n",
      "Iteration: 313/750.\n",
      "Iteration: 314/750.\n",
      "Iteration: 315/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0203630879049658, 0.01757106751657544, 0.010262528385192541]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 316/750.\n",
      "Iteration: 317/750.\n",
      "Iteration: 318/750.\n",
      "Iteration: 319/750.\n",
      "Iteration: 320/750.\n",
      "Iteration: 321/750.\n",
      "Iteration: 322/750.\n",
      "Iteration: 323/750.\n",
      "Iteration: 324/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.02262841694298828, 0.01758433643867625, 0.012217754002525399]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 325/750.\n",
      "Iteration: 326/750.\n",
      "Iteration: 327/750.\n",
      "Iteration: 328/750.\n",
      "Iteration: 329/750.\n",
      "Iteration: 330/750.\n",
      "Iteration: 331/750.\n",
      "Iteration: 332/750.\n",
      "Iteration: 333/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.015909905810328133, 0.01982560712916763, 0.013691125194401766]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 334/750.\n",
      "Iteration: 335/750.\n",
      "Iteration: 336/750.\n",
      "Iteration: 337/750.\n",
      "Iteration: 338/750.\n",
      "Iteration: 339/750.\n",
      "Iteration: 340/750.\n",
      "Iteration: 341/750.\n",
      "Iteration: 342/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.015110213725853711, 0.009527124064834098, 0.009334541094691867]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 343/750.\n",
      "Iteration: 344/750.\n",
      "Iteration: 345/750.\n",
      "Iteration: 346/750.\n",
      "Iteration: 347/750.\n",
      "Iteration: 348/750.\n",
      "Iteration: 349/750.\n",
      "Iteration: 350/750.\n",
      "Iteration: 351/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.01383921247847794, 0.017888971550242515, 0.012172627766161605]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 352/750.\n",
      "Iteration: 353/750.\n",
      "Iteration: 354/750.\n",
      "Iteration: 355/750.\n",
      "Iteration: 356/750.\n",
      "Iteration: 357/750.\n",
      "Iteration: 358/750.\n",
      "Iteration: 359/750.\n",
      "Iteration: 360/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.007692151459628377, 0.00860071117621646, 0.01337183452254036]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 361/750.\n",
      "Iteration: 362/750.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 363/750.\n",
      "Iteration: 364/750.\n",
      "Iteration: 365/750.\n",
      "Iteration: 366/750.\n",
      "Iteration: 367/750.\n",
      "Iteration: 368/750.\n",
      "Iteration: 369/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.016828954981043625, 0.008532854220705712, 0.014868278238738564]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 370/750.\n",
      "Iteration: 371/750.\n",
      "Iteration: 372/750.\n",
      "Iteration: 373/750.\n",
      "Iteration: 374/750.\n",
      "Iteration: 375/750.\n",
      "Iteration: 376/750.\n",
      "Iteration: 377/750.\n",
      "Iteration: 378/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.009769343830962701, 0.014373982427621002, 0.00897708285199268]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 379/750.\n",
      "Iteration: 380/750.\n",
      "Iteration: 381/750.\n",
      "Iteration: 382/750.\n",
      "Iteration: 383/750.\n",
      "Iteration: 384/750.\n",
      "Iteration: 385/750.\n",
      "Iteration: 386/750.\n",
      "Iteration: 387/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0061601486759959275, 0.011447400788466693, 0.010592205644013353]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 388/750.\n",
      "Iteration: 389/750.\n",
      "Iteration: 390/750.\n",
      "Iteration: 391/750.\n",
      "Iteration: 392/750.\n",
      "Iteration: 393/750.\n",
      "Iteration: 394/750.\n",
      "Iteration: 395/750.\n",
      "Iteration: 396/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.01006021025842885, 0.007103090284885151, 0.008748900552573213]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 397/750.\n",
      "Iteration: 398/750.\n",
      "Iteration: 399/750.\n",
      "Iteration: 400/750.\n",
      "Iteration: 401/750.\n",
      "Iteration: 402/750.\n",
      "Iteration: 403/750.\n",
      "Iteration: 404/750.\n",
      "Iteration: 405/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.006573226155617962, 0.008800037481439274, 0.007748006161016941]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 406/750.\n",
      "Iteration: 407/750.\n",
      "Iteration: 408/750.\n",
      "Iteration: 409/750.\n",
      "Iteration: 410/750.\n",
      "Iteration: 411/750.\n",
      "Iteration: 412/750.\n",
      "Iteration: 413/750.\n",
      "Iteration: 414/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.00853616274950926, 0.010528373331639172, 0.007736132999175462]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 415/750.\n",
      "Iteration: 416/750.\n",
      "Iteration: 417/750.\n",
      "Iteration: 418/750.\n",
      "Iteration: 419/750.\n",
      "Iteration: 420/750.\n",
      "Iteration: 421/750.\n",
      "Iteration: 422/750.\n",
      "Iteration: 423/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.007007899223572968, 0.00644505755233859, 0.006488757529163384]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 424/750.\n",
      "Iteration: 425/750.\n",
      "Iteration: 426/750.\n",
      "Iteration: 427/750.\n",
      "Iteration: 428/750.\n",
      "Iteration: 429/750.\n",
      "Iteration: 430/750.\n",
      "Iteration: 431/750.\n",
      "Iteration: 432/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.006053104493819026, 0.006276737155629471, 0.004831008743959072]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 433/750.\n",
      "Iteration: 434/750.\n",
      "Iteration: 435/750.\n",
      "Iteration: 436/750.\n",
      "Iteration: 437/750.\n",
      "Iteration: 438/750.\n",
      "Iteration: 439/750.\n",
      "Iteration: 440/750.\n",
      "Iteration: 441/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0052529349195933744, 0.00732885029553593, 0.003807727914067893]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 442/750.\n",
      "Iteration: 443/750.\n",
      "Iteration: 444/750.\n",
      "Iteration: 445/750.\n",
      "Iteration: 446/750.\n",
      "Iteration: 447/750.\n",
      "Iteration: 448/750.\n",
      "Iteration: 449/750.\n",
      "Iteration: 450/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.002921542476523221, 0.003654111207226949, 0.0032846278323096566]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 451/750.\n",
      "Iteration: 452/750.\n",
      "Iteration: 453/750.\n",
      "Iteration: 454/750.\n",
      "Iteration: 455/750.\n",
      "Iteration: 456/750.\n",
      "Iteration: 457/750.\n",
      "Iteration: 458/750.\n",
      "Iteration: 459/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0037279718019347834, 0.010223271739306093, 0.005651504630691279]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 460/750.\n",
      "Iteration: 461/750.\n",
      "Iteration: 462/750.\n",
      "Iteration: 463/750.\n",
      "Iteration: 464/750.\n",
      "Iteration: 465/750.\n",
      "Iteration: 466/750.\n",
      "Iteration: 467/750.\n",
      "Iteration: 468/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.006245607454844204, 0.005093410584849283, 0.0015902946597379669]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 469/750.\n",
      "Iteration: 470/750.\n",
      "Iteration: 471/750.\n",
      "Iteration: 472/750.\n",
      "Iteration: 473/750.\n",
      "Iteration: 474/750.\n",
      "Iteration: 475/750.\n",
      "Iteration: 476/750.\n",
      "Iteration: 477/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.00439410620243234, 0.006613928781317931, 0.003303326800842834]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 478/750.\n",
      "Iteration: 479/750.\n",
      "Iteration: 480/750.\n",
      "Iteration: 481/750.\n",
      "Iteration: 482/750.\n",
      "Iteration: 483/750.\n",
      "Iteration: 484/750.\n",
      "Iteration: 485/750.\n",
      "Iteration: 486/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0031054786424553732, 0.004483647626745672, 0.005548467720664309]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 487/750.\n",
      "Iteration: 488/750.\n",
      "Iteration: 489/750.\n",
      "Iteration: 490/750.\n",
      "Iteration: 491/750.\n",
      "Iteration: 492/750.\n",
      "Iteration: 493/750.\n",
      "Iteration: 494/750.\n",
      "Iteration: 495/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.00414603240714461, 0.009087762031101948, 0.0044056132546797895]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 496/750.\n",
      "Iteration: 497/750.\n",
      "Iteration: 498/750.\n",
      "Iteration: 499/750.\n",
      "Iteration: 500/750.\n",
      "Iteration: 501/750.\n",
      "Iteration: 502/750.\n",
      "Iteration: 503/750.\n",
      "Iteration: 504/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.002901846718032038, 0.004217244810202035, 0.005485786030073221]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 505/750.\n",
      "Iteration: 506/750.\n",
      "Iteration: 507/750.\n",
      "Iteration: 508/750.\n",
      "Iteration: 509/750.\n",
      "Iteration: 510/750.\n",
      "Iteration: 511/750.\n",
      "Iteration: 512/750.\n",
      "Iteration: 513/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0026945877464181088, 0.003658771101894766, 0.006712447580500221]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 514/750.\n",
      "Iteration: 515/750.\n",
      "Iteration: 516/750.\n",
      "Iteration: 517/750.\n",
      "Iteration: 518/750.\n",
      "Iteration: 519/750.\n",
      "Iteration: 520/750.\n",
      "Iteration: 521/750.\n",
      "Iteration: 522/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.004699095964409763, 0.003251637175180632, 0.003271414183918524]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 523/750.\n",
      "Iteration: 524/750.\n",
      "Iteration: 525/750.\n",
      "Iteration: 526/750.\n",
      "Iteration: 527/750.\n",
      "Iteration: 528/750.\n",
      "Iteration: 529/750.\n",
      "Iteration: 530/750.\n",
      "Iteration: 531/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0042051060893134274, 0.0038884924191254335, 0.002962506320237182]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 532/750.\n",
      "Iteration: 533/750.\n",
      "Iteration: 534/750.\n",
      "Iteration: 535/750.\n",
      "Iteration: 536/750.\n",
      "Iteration: 537/750.\n",
      "Iteration: 538/750.\n",
      "Iteration: 539/750.\n",
      "Iteration: 540/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.00298678405030627, 0.0027571970161227623, 0.003610517848250861]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 541/750.\n",
      "Iteration: 542/750.\n",
      "Iteration: 543/750.\n",
      "Iteration: 544/750.\n",
      "Iteration: 545/750.\n",
      "Iteration: 546/750.\n",
      "Iteration: 547/750.\n",
      "Iteration: 548/750.\n",
      "Iteration: 549/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.002019017040326986, 0.003892469532275133, 0.0035536293901333015]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 550/750.\n",
      "Iteration: 551/750.\n",
      "Iteration: 552/750.\n",
      "Iteration: 553/750.\n",
      "Iteration: 554/750.\n",
      "Iteration: 555/750.\n",
      "Iteration: 556/750.\n",
      "Iteration: 557/750.\n",
      "Iteration: 558/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0033645742777648154, 0.0025017081009596756, 0.003921704589888853]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 559/750.\n",
      "Iteration: 560/750.\n",
      "Iteration: 561/750.\n",
      "Iteration: 562/750.\n",
      "Iteration: 563/750.\n",
      "Iteration: 564/750.\n",
      "Iteration: 565/750.\n",
      "Iteration: 566/750.\n",
      "Iteration: 567/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.002981434723113973, 0.0013819217487384989, 0.0032949836318237656]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 568/750.\n",
      "Iteration: 569/750.\n",
      "Iteration: 570/750.\n",
      "Iteration: 571/750.\n",
      "Iteration: 572/750.\n",
      "Iteration: 573/750.\n",
      "Iteration: 574/750.\n",
      "Iteration: 575/750.\n",
      "Iteration: 576/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.002842557068409717, 0.0024130196333980534, 0.003387120788490034]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 577/750.\n",
      "Iteration: 578/750.\n",
      "Iteration: 579/750.\n",
      "Iteration: 580/750.\n",
      "Iteration: 581/750.\n",
      "Iteration: 582/750.\n",
      "Iteration: 583/750.\n",
      "Iteration: 584/750.\n",
      "Iteration: 585/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.002065377079902587, 0.002318131956749458, 0.0026479241884973977]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 586/750.\n",
      "Iteration: 587/750.\n",
      "Iteration: 588/750.\n",
      "Iteration: 589/750.\n",
      "Iteration: 590/750.\n",
      "Iteration: 591/750.\n",
      "Iteration: 592/750.\n",
      "Iteration: 593/750.\n",
      "Iteration: 594/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0028781183348701484, 0.005051741188932144, 0.0037435946121089254]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 595/750.\n",
      "Iteration: 596/750.\n",
      "Iteration: 597/750.\n",
      "Iteration: 598/750.\n",
      "Iteration: 599/750.\n",
      "Iteration: 600/750.\n",
      "Iteration: 601/750.\n",
      "Iteration: 602/750.\n",
      "Iteration: 603/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.00299654626254093, 0.003778291377038163, 0.0020178715544124266]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 604/750.\n",
      "Iteration: 605/750.\n",
      "Iteration: 606/750.\n",
      "Iteration: 607/750.\n",
      "Iteration: 608/750.\n",
      "Iteration: 609/750.\n",
      "Iteration: 610/750.\n",
      "Iteration: 611/750.\n",
      "Iteration: 612/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0035613454363410397, 0.0020711168632821953, 0.0017361541606328416]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 613/750.\n",
      "Iteration: 614/750.\n",
      "Iteration: 615/750.\n",
      "Iteration: 616/750.\n",
      "Iteration: 617/750.\n",
      "Iteration: 618/750.\n",
      "Iteration: 619/750.\n",
      "Iteration: 620/750.\n",
      "Iteration: 621/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0023794135897436224, 0.002434979975880696, 0.002614170525187537]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 622/750.\n",
      "Iteration: 623/750.\n",
      "Iteration: 624/750.\n",
      "Iteration: 625/750.\n",
      "Iteration: 626/750.\n",
      "Iteration: 627/750.\n",
      "Iteration: 628/750.\n",
      "Iteration: 629/750.\n",
      "Iteration: 630/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.002221729193596045, 0.0031591962917994717, 0.0026405344122758093]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 631/750.\n",
      "Iteration: 632/750.\n",
      "Iteration: 633/750.\n",
      "Iteration: 634/750.\n",
      "Iteration: 635/750.\n",
      "Iteration: 636/750.\n",
      "Iteration: 637/750.\n",
      "Iteration: 638/750.\n",
      "Iteration: 639/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0017053509258779108, 0.002381196224160104, 0.0024068952505818864]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 640/750.\n",
      "Iteration: 641/750.\n",
      "Iteration: 642/750.\n",
      "Iteration: 643/750.\n",
      "Iteration: 644/750.\n",
      "Iteration: 645/750.\n",
      "Iteration: 646/750.\n",
      "Iteration: 647/750.\n",
      "Iteration: 648/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0020553921174259224, 0.0025360110659460853, 0.0015094830603940579]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 649/750.\n",
      "Iteration: 650/750.\n",
      "Iteration: 651/750.\n",
      "Iteration: 652/750.\n",
      "Iteration: 653/750.\n",
      "Iteration: 654/750.\n",
      "Iteration: 655/750.\n",
      "Iteration: 656/750.\n",
      "Iteration: 657/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0015051251377963782, 0.0017464836483383286, 0.0011076337311476472]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 658/750.\n",
      "Iteration: 659/750.\n",
      "Iteration: 660/750.\n",
      "Iteration: 661/750.\n",
      "Iteration: 662/750.\n",
      "Iteration: 663/750.\n",
      "Iteration: 664/750.\n",
      "Iteration: 665/750.\n",
      "Iteration: 666/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0017189487269323558, 0.0017419271948256071, 0.001517056077212622]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 667/750.\n",
      "Iteration: 668/750.\n",
      "Iteration: 669/750.\n",
      "Iteration: 670/750.\n",
      "Iteration: 671/750.\n",
      "Iteration: 672/750.\n",
      "Iteration: 673/750.\n",
      "Iteration: 674/750.\n",
      "Iteration: 675/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.002583472775782791, 0.003131088204767879, 0.0017153648706021698]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 676/750.\n",
      "Iteration: 677/750.\n",
      "Iteration: 678/750.\n",
      "Iteration: 679/750.\n",
      "Iteration: 680/750.\n",
      "Iteration: 681/750.\n",
      "Iteration: 682/750.\n",
      "Iteration: 683/750.\n",
      "Iteration: 684/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0015867622306953801, 0.0011906861619485998, 0.00190017199205851]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 685/750.\n",
      "Iteration: 686/750.\n",
      "Iteration: 687/750.\n",
      "Iteration: 688/750.\n",
      "Iteration: 689/750.\n",
      "Iteration: 690/750.\n",
      "Iteration: 691/750.\n",
      "Iteration: 692/750.\n",
      "Iteration: 693/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0019919869100415086, 0.0022330823709953945, 0.0027735989679547437]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 694/750.\n",
      "Iteration: 695/750.\n",
      "Iteration: 696/750.\n",
      "Iteration: 697/750.\n",
      "Iteration: 698/750.\n",
      "Iteration: 699/750.\n",
      "Iteration: 700/750.\n",
      "Iteration: 701/750.\n",
      "Iteration: 702/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0019377714647102334, 0.0021468643512156747, 0.001613401638587184]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 703/750.\n",
      "Iteration: 704/750.\n",
      "Iteration: 705/750.\n",
      "Iteration: 706/750.\n",
      "Iteration: 707/750.\n",
      "Iteration: 708/750.\n",
      "Iteration: 709/750.\n",
      "Iteration: 710/750.\n",
      "Iteration: 711/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.00086116675978332, 0.001959338149785799, 0.0013139268230490256]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 712/750.\n",
      "Iteration: 713/750.\n",
      "Iteration: 714/750.\n",
      "Iteration: 715/750.\n",
      "Iteration: 716/750.\n",
      "Iteration: 717/750.\n",
      "Iteration: 718/750.\n",
      "Iteration: 719/750.\n",
      "Iteration: 720/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.000922927035018237, 0.0017368995073886746, 0.0009850095962978867]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 721/750.\n",
      "Iteration: 722/750.\n",
      "Iteration: 723/750.\n",
      "Iteration: 724/750.\n",
      "Iteration: 725/750.\n",
      "Iteration: 726/750.\n",
      "Iteration: 727/750.\n",
      "Iteration: 728/750.\n",
      "Iteration: 729/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.00160438701564368, 0.0015889317417497155, 0.0020780690656655564]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 730/750.\n",
      "Iteration: 731/750.\n",
      "Iteration: 732/750.\n",
      "Iteration: 733/750.\n",
      "Iteration: 734/750.\n",
      "Iteration: 735/750.\n",
      "Iteration: 736/750.\n",
      "Iteration: 737/750.\n",
      "Iteration: 738/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.0010388349009595177, 0.0013960343425225312, 0.0021584754233142122]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 739/750.\n",
      "Iteration: 740/750.\n",
      "Iteration: 741/750.\n",
      "Iteration: 742/750.\n",
      "Iteration: 743/750.\n",
      "Iteration: 744/750.\n",
      "Iteration: 745/750.\n",
      "Iteration: 746/750.\n",
      "Iteration: 747/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3055575589923514\n",
      "Loss latest three: [0.001567330547213729, 0.0015021132826861594, 0.0016439893587461441]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 748/750.\n",
      "Iteration: 749/750.\n",
      "Iteration: 750/750.\n",
      "\n",
      "\n",
      "----------------FINAL OUTPUT----------------\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "Loss history: [2.3055575589923514, 2.300406899071696, 2.3029133221585023, 2.294747648840793, 2.288576523611883, 2.290180646017032, 2.2845796024885723, 2.2924845991086413, 2.2430353643670484, 2.206360550015477, 2.1353708524936774, 2.3061608483970777, 2.2316064461131218, 2.271748997353242, 2.13538884465873, 2.196307338513974, 2.1725589737331377, 1.8555820840431605, 2.2766782177253577, 1.934898385421089, 1.871502819358501, 1.9138400595387086, 1.7624269599259677, 2.4884701186691163, 2.312345181935326, 2.2428537670038167, 1.8153889792853417, 2.042472231906978, 2.0274354973933386, 2.064275502731321, 1.9516374136257122, 1.897432557211994, 1.6579292576131857, 1.7621807182453688, 1.833807183850169, 2.0549246832422936, 1.8723331847903255, 1.6442609969283204, 1.9184862164623917, 1.9046591136566846, 1.8139214361928027, 2.005325645143333, 2.1230847051097714, 2.00511916232943, 1.9732593373540852, 2.0184364213823445, 2.6499883370507757, 1.5295603987287203, 2.14307588096561, 1.6253938407019612, 1.8354510726197937, 1.8132958306345657, 2.0563964202738827, 1.6435401383740862, 1.8748368238786222, 1.828618938846007, 1.7541986087968249, 1.806783549190694, 2.1103791353341728, 1.8129516954502638, 2.1429283671619177, 1.3842210419573906, 1.8503751364566963, 1.6541310471876487, 1.7109102547436128, 1.5414166184470133, 2.0695831223441785, 1.8010124632203686, 1.7868103714244592, 1.4492693059040154, 1.8337719840693965, 1.788756308516624, 1.5214341048054703, 1.3312439993016554, 1.5178424544515476, 1.382287926451269, 1.6661837300331095, 1.511687800988327, 1.4360661578335743, 1.5099931642285551, 1.4284829401566097, 1.875279531052607, 1.3028650293485664, 2.0328953693539904, 1.1246929247966073, 1.0400213378264602, 1.0522843412446876, 1.3285889474860497, 1.476922730116448, 1.0004611806216006, 1.140158729501649, 1.219787349219302, 1.4518095314654245, 1.6059248107977853, 1.1818203407602288, 1.315681706105867, 1.4980733151569927, 1.2347177336803679, 1.2437637714007388, 0.9512570234349385, 1.2619418598021444, 1.1709905748167786, 1.2123945793386361, 0.9025242810217403, 1.2810415819546384, 1.1590602893676074, 1.31185675119782, 0.7630824680897652, 1.2410263152957888, 1.1463289555778127, 1.3242519901328729, 0.8602249197955484, 1.2629893396069092, 1.0838840137850903, 1.1463413255311268, 0.8585563678333387, 1.010251243340975, 0.9108188527342874, 1.1657749006634384, 0.8501433736954743, 1.158100675622405, 1.2207810975212778, 0.6518704700850245, 0.9948293101744844, 0.6375138809069608, 0.9847764571024111, 0.6513278452428268, 1.0415511171954228, 1.032705407100247, 0.6644553302503602, 0.9375853799071927, 1.0291258904932936, 1.0188620864883335, 1.5110819357261827, 0.5282107176868567, 0.7851462760323777, 0.6624157430401119, 0.6488930768763339, 0.4787559503043197, 0.5610481598343083, 0.8771298906666285, 1.0612828686712, 0.7893206818157843, 0.7050813317119973, 0.8077446501275436, 0.7209304969752148, 0.48393176482096134, 0.5696522016964861, 0.3705807523842548, 0.44725253533085835, 0.7095368860314037, 0.7967863091393386, 0.34622555323144777, 0.7874294421665962, 0.28001171706999456, 0.6417807190258413, 0.503012086297786, 0.40403112139353486, 0.7426523787714432, 0.4147177801834292, 0.1976633903218503, 0.6149993686508091, 0.33555014104080705, 0.4207246377423761, 0.7233705795063519, 0.5423455286862345, 0.2921541497299826, 0.395845769945578, 0.5417135652175434, 0.5127860907579702, 0.4423694055698868, 0.3236602867531084, 0.14447520925983015, 0.4420298535896451, 0.4239328888498195, 0.21793086374474135, 0.49037664023562694, 0.3257512884451925, 0.23361025284867254, 0.4208937622866595, 0.2746972798672402, 0.4300013296525805, 0.36844921168523465, 0.23540087771781135, 0.41544197284494255, 0.2823062465263835, 0.2526039181014987, 0.2661376588799025, 0.29333143778269355, 0.3487392255354627, 0.20258932667943813, 0.43918308371232995, 0.30036378963684185, 0.22148967050890503, 0.11776599062723939, 0.1112629225244871, 0.07248974144177063, 0.31392346579773295, 0.11642156820684864, 0.23277828595846, 0.27315102825750665, 0.10417861245265847, 0.19039342095246847, 0.1679942273827797, 0.10167808297320831, 0.07616059586943538, 0.17105394612346236, 0.12500151358681744, 0.23919722520178413, 0.07845111215034989, 0.1245876885694504, 0.08742772534386235, 0.09124494133054097, 0.04604406308275605, 0.2700110150812488, 0.1730756502564659, 0.1568864321992886, 0.301464783995552, 0.08661943850772261, 0.17291598426721763, 0.07844323760715582, 0.1762749192416052, 0.198977290108442, 0.14312686463978938, 0.03261416902984667, 0.03054111706547881, 0.037543319868447673, 0.08079612114761033, 0.08678542311860352, 0.060218295902046116, 0.11474317879204149, 0.12942711737335458, 0.07879379011385884, 0.062299998236475564, 0.03503384923598232, 0.05560824559122778, 0.1030194030756865, 0.0990760726210339, 0.13145477710517545, 0.01610179754280706, 0.0771401766704466, 0.061047090270442084, 0.07818698688785436, 0.0918949199397228, 0.09319227375868416, 0.08811378867269333, 0.0734461336023442, 0.03174724699505884, 0.06212261310142785, 0.02739438570280992, 0.049484310869164085, 0.03895022424867601, 0.04981987358064388, 0.05074515396988789, 0.03983638177250487, 0.08492574436301814, 0.05200150962079785, 0.030332530426612006, 0.05045440734712168, 0.032420542822017895, 0.06781557374825271, 0.07558561610917186, 0.027929277068080117, 0.05190464068235282, 0.012895986292345257, 0.03725704383043042, 0.02313330775136576, 0.03012681363097762, 0.04319413731979136, 0.02004262906877935, 0.02229894280837623, 0.02751672654121899, 0.035268494043214145, 0.039428610645982264, 0.03204125921046961, 0.02661427731158411, 0.03676389344140057, 0.06299340377472308, 0.03454798975959152, 0.030682535393103484, 0.04194412309936003, 0.015105433473034708, 0.027204625626327257, 0.040608940778716955, 0.023246911952024316, 0.021970949590423193, 0.008230711804036533, 0.04246734441958538, 0.030043344573878424, 0.01763246479153635, 0.0284837154134865, 0.03091560022319131, 0.016077278588038684, 0.026727421826212808, 0.024177658392888598, 0.017464338244271845, 0.022840108982812742, 0.017288535993820235, 0.02148621491454814, 0.028915874613482787, 0.011973472052999645, 0.021289459056199462, 0.012232264444989205, 0.025275973834323114, 0.030583171943476435, 0.00984498326368509, 0.028018216705707557, 0.023579674426418645, 0.02469447606856087, 0.014756764283264868, 0.029010128608519467, 0.0195356637239778, 0.0203630879049658, 0.01757106751657544, 0.010262528385192541, 0.013262062860265728, 0.021245177339018713, 0.021721729017228963, 0.03689103708585828, 0.01351651153150445, 0.009935061205575439, 0.02262841694298828, 0.01758433643867625, 0.012217754002525399, 0.01932559752139203, 0.014958402214521075, 0.027567374480082448, 0.018025596817403688, 0.010517139914541018, 0.017699444398169662, 0.015909905810328133, 0.01982560712916763, 0.013691125194401766, 0.016550432755662575, 0.009078229764578996, 0.007944549330278787, 0.01326805701525457, 0.025850371833740872, 0.013628392763211048, 0.015110213725853711, 0.009527124064834098, 0.009334541094691867, 0.006085630296447723, 0.017934603923836508, 0.009869102393588697, 0.01877156124176056, 0.010938960103062646, 0.009361092362202691, 0.01383921247847794, 0.017888971550242515, 0.012172627766161605, 0.012892033167378431, 0.008206894884965657, 0.00905059199327114, 0.017796027177664306, 0.009163779352044385, 0.009489457224419108, 0.007692151459628377, 0.00860071117621646, 0.01337183452254036, 0.01293385673719119, 0.008399997383587529, 0.01264211171765953, 0.013858170752131613, 0.004903633772470935, 0.009291095305653377, 0.016828954981043625, 0.008532854220705712, 0.014868278238738564, 0.009907119201207511, 0.015346446124348127, 0.011719231907154463, 0.004210578679004389, 0.014362194165312063, 0.013576512265913249, 0.009769343830962701, 0.014373982427621002, 0.00897708285199268, 0.007879795891827508, 0.009862970582605868, 0.007378127775919613, 0.007813068998589977, 0.006245027504013833, 0.008922686984725212, 0.0061601486759959275, 0.011447400788466693, 0.010592205644013353, 0.008779988807601034, 0.007384445939282525, 0.005231718442068179, 0.005374112996673978, 0.007218698803703233, 0.009120530892147403, 0.01006021025842885, 0.007103090284885151, 0.008748900552573213, 0.0035180300370962697, 0.007995185752619328, 0.006997362700971801, 0.009063252542229545, 0.00764816543458346, 0.010021398502396002, 0.006573226155617962, 0.008800037481439274, 0.007748006161016941, 0.010937734736951477, 0.006072712121079891, 0.00538165310937602, 0.007465023465125563, 0.004909194654504705, 0.0058572163199658075, 0.00853616274950926, 0.010528373331639172, 0.007736132999175462, 0.00718902707290607, 0.0029909235888185715, 0.010247816293518714, 0.006803988991259701, 0.0063724690003217335, 0.002905587914533317, 0.007007899223572968, 0.00644505755233859, 0.006488757529163384, 0.002572594387541828, 0.0044282391344211405, 0.006817876694734597, 0.004539010813511677, 0.004361713562243303, 0.0064384843265000965, 0.006053104493819026, 0.006276737155629471, 0.004831008743959072, 0.010958469366237057, 0.0060074596681144, 0.0029252938844696874, 0.0030119681989681257, 0.006741203732546441, 0.005249409183349996, 0.0052529349195933744, 0.00732885029553593, 0.003807727914067893, 0.0040358186911486175, 0.004833614462360483, 0.0047207717788715015, 0.003263436409534696, 0.00383955053305814, 0.009459019838161612, 0.002921542476523221, 0.003654111207226949, 0.0032846278323096566, 0.004885562824869446, 0.00508569049506098, 0.006373271890193562, 0.0050280810405093435, 0.006200431003390502, 0.0035311075063430976, 0.0037279718019347834, 0.010223271739306093, 0.005651504630691279, 0.005008483937049452, 0.006027997653171636, 0.006361605154960152, 0.0034867165884126454, 0.0029331952938917664, 0.004020571156771747, 0.006245607454844204, 0.005093410584849283, 0.0015902946597379669, 0.004266418964347722, 0.005104216601482122, 0.0033153953966085464, 0.002263779852399839, 0.007160398650149055, 0.006600048804114131, 0.00439410620243234, 0.006613928781317931, 0.003303326800842834, 0.004055481427072928, 0.006521513929952537, 0.004458408414755986, 0.005556058564968627, 0.0032512623862984338, 0.0029908740123645145, 0.0031054786424553732, 0.004483647626745672, 0.005548467720664309, 0.006621132659978442, 0.004664450628537602, 0.005383152499169567, 0.0044664641921742234, 0.006147528404749515, 0.004589144306922832, 0.00414603240714461, 0.009087762031101948, 0.0044056132546797895, 0.003909099621062447, 0.002374078828294372, 0.004199905211404633, 0.006057050601898557, 0.0032105797489554015, 0.0031396686273902057, 0.002901846718032038, 0.004217244810202035, 0.005485786030073221, 0.00456064989139799, 0.004663277704692785, 0.003895897854008306, 0.0035728899014997257, 0.0027536640593061232, 0.004219869830662639, 0.0026945877464181088, 0.003658771101894766, 0.006712447580500221, 0.0034696071602221797, 0.003184528785590471, 0.006798037894825511, 0.0049662754478161, 0.0027370586846505564, 0.004805179326626523, 0.004699095964409763, 0.003251637175180632, 0.003271414183918524, 0.005589838323135825, 0.0032441389877758903, 0.0067858601622701, 0.006118630688254335, 0.004131691877312017, 0.003082671607921422, 0.0042051060893134274, 0.0038884924191254335, 0.002962506320237182, 0.002076157311928311, 0.0020558838118918516, 0.003356738651684689, 0.0028620378255369678, 0.0029686248846644923, 0.00349765788185177, 0.00298678405030627, 0.0027571970161227623, 0.003610517848250861, 0.004466379683559644, 0.003580558601558644, 0.0022491600988507276, 0.0038862574565933887, 0.0020585755986470416, 0.003350119130455706, 0.002019017040326986, 0.003892469532275133, 0.0035536293901333015, 0.0032306450342034334, 0.002800186661059094, 0.003231323578908408, 0.002385128178917209, 0.0031244951981927393, 0.004250466780830907, 0.0033645742777648154, 0.0025017081009596756, 0.003921704589888853, 0.004116295634103122, 0.003620619447717498, 0.0017603315439187269, 0.002469395053677466, 0.0029986673784294093, 0.0032258431313620115, 0.002981434723113973, 0.0013819217487384989, 0.0032949836318237656, 0.004161248977183076, 0.0025140866251171495, 0.002230685022040397, 0.003939352093486844, 0.00295376376826937, 0.0030149312382889482, 0.002842557068409717, 0.0024130196333980534, 0.003387120788490034, 0.002919653903554604, 0.004610478841244241, 0.004830742754672676, 0.004849249306296692, 0.004092267674917679, 0.00323413799897149, 0.002065377079902587, 0.002318131956749458, 0.0026479241884973977, 0.002350889617063655, 0.002304473890369692, 0.004179157886665444, 0.00311171460734571, 0.002343209887573981, 0.0036532783388440442, 0.0028781183348701484, 0.005051741188932144, 0.0037435946121089254, 0.004010236480847776, 0.0017777984537796284, 0.0018778375051706848, 0.0035253957550480178, 0.004959555307564778, 0.001887589324937939, 0.00299654626254093, 0.003778291377038163, 0.0020178715544124266, 0.0030543333680973016, 0.002629996715882432, 0.00205269095445219, 0.0020654795626069322, 0.002139058456069485, 0.002084374852789543, 0.0035613454363410397, 0.0020711168632821953, 0.0017361541606328416, 0.002791846028621271, 0.0015494622708039484, 0.0026310909319199483, 0.0017459589478171595, 0.003877744793507751, 0.0026055125449147, 0.0023794135897436224, 0.002434979975880696, 0.002614170525187537, 0.002936776859233544, 0.001795591592682955, 0.002159181356940997, 0.0024298340813780532, 0.0022545333375030376, 0.0020952799545892044, 0.002221729193596045, 0.0031591962917994717, 0.0026405344122758093, 0.002253969437781386, 0.0020810698389622123, 0.002098951589179424, 0.0023879481220214368, 0.0024659611953409643, 0.0012442477664569135, 0.0017053509258779108, 0.002381196224160104, 0.0024068952505818864, 0.001942113174759474, 0.002249302904875942, 0.001991766694607963, 0.0021797384551068, 0.0024118015747329564, 0.0027592926374591935, 0.0020553921174259224, 0.0025360110659460853, 0.0015094830603940579, 0.0018396449015827107, 0.0019136425148368588, 0.0035232864151260052, 0.0037251412669029194, 0.002507815100855471, 0.0023615706936043353, 0.0015051251377963782, 0.0017464836483383286, 0.0011076337311476472, 0.0025276373927947977, 0.0019052993614125083, 0.0019418524240472032, 0.0017731584356773104, 0.0021402632026583635, 0.0022181837621589057, 0.0017189487269323558, 0.0017419271948256071, 0.001517056077212622, 0.0036580975340142043, 0.0021318460724613457, 0.0022641845365276372, 0.001735248554264855, 0.0023377994092444844, 0.0013196547302400724, 0.002583472775782791, 0.003131088204767879, 0.0017153648706021698, 0.0014351853360627023, 0.001400209395550599, 0.0024686255004100384, 0.0025826632567593215, 0.0022213754367474653, 0.0033098318179195334, 0.0015867622306953801, 0.0011906861619485998, 0.00190017199205851, 0.002267275949970542, 0.0017973148529912772, 0.0017866884705282319, 0.0015852220841670378, 0.002458232138383762, 0.0014244905431833356, 0.0019919869100415086, 0.0022330823709953945, 0.0027735989679547437, 0.0026790056440027204, 0.0016648337697080765, 0.0020633290652193647, 0.0018820694134797256, 0.0023719812136731443, 0.0019624419874155175, 0.0019377714647102334, 0.0021468643512156747, 0.001613401638587184, 0.0027210783105111533, 0.0013589544920751294, 0.000832850823144387, 0.0012942071864269781, 0.002052153088747626, 0.0018600081981792272, 0.00086116675978332, 0.001959338149785799, 0.0013139268230490256, 0.0014306314232718851, 0.0016959028832292259, 0.0017518964170656918, 0.0017331592446227701, 0.0015918302126712758, 0.0011052993354616284, 0.000922927035018237, 0.0017368995073886746, 0.0009850095962978867, 0.0011681268652901126, 0.0006304484831412962, 0.001487507723879396, 0.0026474813828503124, 0.001394117871138313, 0.0008204897749794848, 0.00160438701564368, 0.0015889317417497155, 0.0020780690656655564, 0.0010636636904031375, 0.0016175576179632877, 0.0008621424073317424, 0.0017633454725117536, 0.0019517233887101733, 0.0016230313448372285, 0.0010388349009595177, 0.0013960343425225312, 0.0021584754233142122, 0.0015085948267311207, 0.0015261316094420914, 0.0014562299200879358, 0.0021428721253259246, 0.0015409984000469296, 0.001926323120287074, 0.001567330547213729, 0.0015021132826861594, 0.0016439893587461441, 0.0016136400756901178, 0.0007363023758899155, 0.0013633331881789958]\n",
      "Accuracy history: [0.16, 0.24, 0.22, 0.24, 0.26, 0.42, 0.42, 0.42, 0.46, 0.46, 0.56, 0.54, 0.62, 0.68, 0.72, 0.88, 0.88, 0.88, 0.9, 0.9, 0.96, 1.0, 1.0, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Adam\n",
    "\n",
    "adam = ConvNet4(input_shape=input_shape, wt_scale=wt_scale, verbose=False)\n",
    "adam.compile('adam')\n",
    "adam.fit(x_dev, y_dev, x_val, y_val, mini_batch_sz=mini_batch_sz, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train...\n",
      "750 iterations. 5 iter/epoch.\n",
      "Iteration: 1/750.\n",
      "Time taken for iteration 0: 1.6611967086791992\n",
      "Estimated time to complete: 1245.8975315093994\n",
      "Iteration: 2/750.\n",
      "Iteration: 3/750.\n",
      "Iteration: 4/750.\n",
      "Iteration: 5/750.\n",
      "Iteration: 6/750.\n",
      "Iteration: 7/750.\n",
      "Iteration: 8/750.\n",
      "Iteration: 9/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3083244218204695, 2.305725695413475, 2.297431716772699]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.16, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 10/750.\n",
      "Iteration: 11/750.\n",
      "Iteration: 12/750.\n",
      "Iteration: 13/750.\n",
      "Iteration: 14/750.\n",
      "Iteration: 15/750.\n",
      "Iteration: 16/750.\n",
      "Iteration: 17/750.\n",
      "Iteration: 18/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3018450824131884, 2.3036695764710173, 2.3017066047698584]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.16, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 19/750.\n",
      "Iteration: 20/750.\n",
      "Iteration: 21/750.\n",
      "Iteration: 22/750.\n",
      "Iteration: 23/750.\n",
      "Iteration: 24/750.\n",
      "Iteration: 25/750.\n",
      "Iteration: 26/750.\n",
      "Iteration: 27/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.296826694035708, 2.2988054741544746, 2.298301305372676]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.16, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 28/750.\n",
      "Iteration: 29/750.\n",
      "Iteration: 30/750.\n",
      "Iteration: 31/750.\n",
      "Iteration: 32/750.\n",
      "Iteration: 33/750.\n",
      "Iteration: 34/750.\n",
      "Iteration: 35/750.\n",
      "Iteration: 36/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2966168260260056, 2.299502510069381, 2.297408434557621]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.16, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 37/750.\n",
      "Iteration: 38/750.\n",
      "Iteration: 39/750.\n",
      "Iteration: 40/750.\n",
      "Iteration: 41/750.\n",
      "Iteration: 42/750.\n",
      "Iteration: 43/750.\n",
      "Iteration: 44/750.\n",
      "Iteration: 45/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.298535173218746, 2.2969774199484214, 2.287743382443448]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.16, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 46/750.\n",
      "Iteration: 47/750.\n",
      "Iteration: 48/750.\n",
      "Iteration: 49/750.\n",
      "Iteration: 50/750.\n",
      "Iteration: 51/750.\n",
      "Iteration: 52/750.\n",
      "Iteration: 53/750.\n",
      "Iteration: 54/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2839536623842185, 2.2938137555028617, 2.302438358369893]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 55/750.\n",
      "Iteration: 56/750.\n",
      "Iteration: 57/750.\n",
      "Iteration: 58/750.\n",
      "Iteration: 59/750.\n",
      "Iteration: 60/750.\n",
      "Iteration: 61/750.\n",
      "Iteration: 62/750.\n",
      "Iteration: 63/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3025548231332373, 2.2845544381527216, 2.281798459737148]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 64/750.\n",
      "Iteration: 65/750.\n",
      "Iteration: 66/750.\n",
      "Iteration: 67/750.\n",
      "Iteration: 68/750.\n",
      "Iteration: 69/750.\n",
      "Iteration: 70/750.\n",
      "Iteration: 71/750.\n",
      "Iteration: 72/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2835935731873747, 2.2789792517783667, 2.2761709594171795]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 73/750.\n",
      "Iteration: 74/750.\n",
      "Iteration: 75/750.\n",
      "Iteration: 76/750.\n",
      "Iteration: 77/750.\n",
      "Iteration: 78/750.\n",
      "Iteration: 79/750.\n",
      "Iteration: 80/750.\n",
      "Iteration: 81/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.274863026115056, 2.294948740664123, 2.2861885214757716]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 82/750.\n",
      "Iteration: 83/750.\n",
      "Iteration: 84/750.\n",
      "Iteration: 85/750.\n",
      "Iteration: 86/750.\n",
      "Iteration: 87/750.\n",
      "Iteration: 88/750.\n",
      "Iteration: 89/750.\n",
      "Iteration: 90/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2580556523144146, 2.2770504059269747, 2.2811090608307416]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 91/750.\n",
      "Iteration: 92/750.\n",
      "Iteration: 93/750.\n",
      "Iteration: 94/750.\n",
      "Iteration: 95/750.\n",
      "Iteration: 96/750.\n",
      "Iteration: 97/750.\n",
      "Iteration: 98/750.\n",
      "Iteration: 99/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2767012191425837, 2.261673609481268, 2.272762194269741]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 100/750.\n",
      "Iteration: 101/750.\n",
      "Iteration: 102/750.\n",
      "Iteration: 103/750.\n",
      "Iteration: 104/750.\n",
      "Iteration: 105/750.\n",
      "Iteration: 106/750.\n",
      "Iteration: 107/750.\n",
      "Iteration: 108/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2664487825242845, 2.286639542318909, 2.247916495356978]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 109/750.\n",
      "Iteration: 110/750.\n",
      "Iteration: 111/750.\n",
      "Iteration: 112/750.\n",
      "Iteration: 113/750.\n",
      "Iteration: 114/750.\n",
      "Iteration: 115/750.\n",
      "Iteration: 116/750.\n",
      "Iteration: 117/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2729363999356873, 2.273953395587147, 2.2786906412783487]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 118/750.\n",
      "Iteration: 119/750.\n",
      "Iteration: 120/750.\n",
      "Iteration: 121/750.\n",
      "Iteration: 122/750.\n",
      "Iteration: 123/750.\n",
      "Iteration: 124/750.\n",
      "Iteration: 125/750.\n",
      "Iteration: 126/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2537882165991663, 2.278439164214905, 2.2486419643059565]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 127/750.\n",
      "Iteration: 128/750.\n",
      "Iteration: 129/750.\n",
      "Iteration: 130/750.\n",
      "Iteration: 131/750.\n",
      "Iteration: 132/750.\n",
      "Iteration: 133/750.\n",
      "Iteration: 134/750.\n",
      "Iteration: 135/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.32051304490723, 2.3152721759448656, 2.2505928687197856]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 136/750.\n",
      "Iteration: 137/750.\n",
      "Iteration: 138/750.\n",
      "Iteration: 139/750.\n",
      "Iteration: 140/750.\n",
      "Iteration: 141/750.\n",
      "Iteration: 142/750.\n",
      "Iteration: 143/750.\n",
      "Iteration: 144/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2918283532741106, 2.230395258419411, 2.2907653948807876]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 145/750.\n",
      "Iteration: 146/750.\n",
      "Iteration: 147/750.\n",
      "Iteration: 148/750.\n",
      "Iteration: 149/750.\n",
      "Iteration: 150/750.\n",
      "Iteration: 151/750.\n",
      "Iteration: 152/750.\n",
      "Iteration: 153/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.250193863582836, 2.2973584825330042, 2.268586391634377]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 154/750.\n",
      "Iteration: 155/750.\n",
      "Iteration: 156/750.\n",
      "Iteration: 157/750.\n",
      "Iteration: 158/750.\n",
      "Iteration: 159/750.\n",
      "Iteration: 160/750.\n",
      "Iteration: 161/750.\n",
      "Iteration: 162/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2824466859916486, 2.244650987574829, 2.2543351594952203]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 163/750.\n",
      "Iteration: 164/750.\n",
      "Iteration: 165/750.\n",
      "Iteration: 166/750.\n",
      "Iteration: 167/750.\n",
      "Iteration: 168/750.\n",
      "Iteration: 169/750.\n",
      "Iteration: 170/750.\n",
      "Iteration: 171/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.232717186500342, 2.2397897020179474, 2.2738842139177944]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 172/750.\n",
      "Iteration: 173/750.\n",
      "Iteration: 174/750.\n",
      "Iteration: 175/750.\n",
      "Iteration: 176/750.\n",
      "Iteration: 177/750.\n",
      "Iteration: 178/750.\n",
      "Iteration: 179/750.\n",
      "Iteration: 180/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2679793280689706, 2.28689228759865, 2.276174764319612]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 181/750.\n",
      "Iteration: 182/750.\n",
      "Iteration: 183/750.\n",
      "Iteration: 184/750.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 185/750.\n",
      "Iteration: 186/750.\n",
      "Iteration: 187/750.\n",
      "Iteration: 188/750.\n",
      "Iteration: 189/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2809092996558507, 2.2789688282647362, 2.28805322376837]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 190/750.\n",
      "Iteration: 191/750.\n",
      "Iteration: 192/750.\n",
      "Iteration: 193/750.\n",
      "Iteration: 194/750.\n",
      "Iteration: 195/750.\n",
      "Iteration: 196/750.\n",
      "Iteration: 197/750.\n",
      "Iteration: 198/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2551803876097742, 2.268825191182065, 2.2279083463692317]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 199/750.\n",
      "Iteration: 200/750.\n",
      "Iteration: 201/750.\n",
      "Iteration: 202/750.\n",
      "Iteration: 203/750.\n",
      "Iteration: 204/750.\n",
      "Iteration: 205/750.\n",
      "Iteration: 206/750.\n",
      "Iteration: 207/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.308294898786341, 2.2447185877161426, 2.2561030174177947]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 208/750.\n",
      "Iteration: 209/750.\n",
      "Iteration: 210/750.\n",
      "Iteration: 211/750.\n",
      "Iteration: 212/750.\n",
      "Iteration: 213/750.\n",
      "Iteration: 214/750.\n",
      "Iteration: 215/750.\n",
      "Iteration: 216/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.274031923422989, 2.292507221173354, 2.2214233061154625]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 217/750.\n",
      "Iteration: 218/750.\n",
      "Iteration: 219/750.\n",
      "Iteration: 220/750.\n",
      "Iteration: 221/750.\n",
      "Iteration: 222/750.\n",
      "Iteration: 223/750.\n",
      "Iteration: 224/750.\n",
      "Iteration: 225/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3173336547347247, 2.267883190300659, 2.216650036147166]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 226/750.\n",
      "Iteration: 227/750.\n",
      "Iteration: 228/750.\n",
      "Iteration: 229/750.\n",
      "Iteration: 230/750.\n",
      "Iteration: 231/750.\n",
      "Iteration: 232/750.\n",
      "Iteration: 233/750.\n",
      "Iteration: 234/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3015755984933626, 2.238681765961866, 2.2016660365529783]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 235/750.\n",
      "Iteration: 236/750.\n",
      "Iteration: 237/750.\n",
      "Iteration: 238/750.\n",
      "Iteration: 239/750.\n",
      "Iteration: 240/750.\n",
      "Iteration: 241/750.\n",
      "Iteration: 242/750.\n",
      "Iteration: 243/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3058068408717873, 2.2569811975613874, 2.269927435171348]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 244/750.\n",
      "Iteration: 245/750.\n",
      "Iteration: 246/750.\n",
      "Iteration: 247/750.\n",
      "Iteration: 248/750.\n",
      "Iteration: 249/750.\n",
      "Iteration: 250/750.\n",
      "Iteration: 251/750.\n",
      "Iteration: 252/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2473843431840854, 2.1699650093380507, 2.330035628675186]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 253/750.\n",
      "Iteration: 254/750.\n",
      "Iteration: 255/750.\n",
      "Iteration: 256/750.\n",
      "Iteration: 257/750.\n",
      "Iteration: 258/750.\n",
      "Iteration: 259/750.\n",
      "Iteration: 260/750.\n",
      "Iteration: 261/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.271419112296858, 2.201561180448635, 2.2923443997167285]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 262/750.\n",
      "Iteration: 263/750.\n",
      "Iteration: 264/750.\n",
      "Iteration: 265/750.\n",
      "Iteration: 266/750.\n",
      "Iteration: 267/750.\n",
      "Iteration: 268/750.\n",
      "Iteration: 269/750.\n",
      "Iteration: 270/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3423970038143236, 2.2769262981458995, 2.3062019756584777]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 271/750.\n",
      "Iteration: 272/750.\n",
      "Iteration: 273/750.\n",
      "Iteration: 274/750.\n",
      "Iteration: 275/750.\n",
      "Iteration: 276/750.\n",
      "Iteration: 277/750.\n",
      "Iteration: 278/750.\n",
      "Iteration: 279/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2407741533860976, 2.172899416933032, 2.1984333670970226]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 280/750.\n",
      "Iteration: 281/750.\n",
      "Iteration: 282/750.\n",
      "Iteration: 283/750.\n",
      "Iteration: 284/750.\n",
      "Iteration: 285/750.\n",
      "Iteration: 286/750.\n",
      "Iteration: 287/750.\n",
      "Iteration: 288/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2981574744501674, 2.2600462418147753, 2.2166681453444483]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 289/750.\n",
      "Iteration: 290/750.\n",
      "Iteration: 291/750.\n",
      "Iteration: 292/750.\n",
      "Iteration: 293/750.\n",
      "Iteration: 294/750.\n",
      "Iteration: 295/750.\n",
      "Iteration: 296/750.\n",
      "Iteration: 297/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2155338896642607, 2.197460097265643, 2.318388907515723]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 298/750.\n",
      "Iteration: 299/750.\n",
      "Iteration: 300/750.\n",
      "Iteration: 301/750.\n",
      "Iteration: 302/750.\n",
      "Iteration: 303/750.\n",
      "Iteration: 304/750.\n",
      "Iteration: 305/750.\n",
      "Iteration: 306/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.183530894514179, 2.1687370910024586, 2.280636857520292]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 307/750.\n",
      "Iteration: 308/750.\n",
      "Iteration: 309/750.\n",
      "Iteration: 310/750.\n",
      "Iteration: 311/750.\n",
      "Iteration: 312/750.\n",
      "Iteration: 313/750.\n",
      "Iteration: 314/750.\n",
      "Iteration: 315/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3083214739652798, 2.2727792010817764, 2.2652212535838214]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 316/750.\n",
      "Iteration: 317/750.\n",
      "Iteration: 318/750.\n",
      "Iteration: 319/750.\n",
      "Iteration: 320/750.\n",
      "Iteration: 321/750.\n",
      "Iteration: 322/750.\n",
      "Iteration: 323/750.\n",
      "Iteration: 324/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2743787708138794, 2.205430793999642, 2.216025726528513]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 325/750.\n",
      "Iteration: 326/750.\n",
      "Iteration: 327/750.\n",
      "Iteration: 328/750.\n",
      "Iteration: 329/750.\n",
      "Iteration: 330/750.\n",
      "Iteration: 331/750.\n",
      "Iteration: 332/750.\n",
      "Iteration: 333/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2523932586141244, 2.2818711031090086, 2.2729374546977894]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 334/750.\n",
      "Iteration: 335/750.\n",
      "Iteration: 336/750.\n",
      "Iteration: 337/750.\n",
      "Iteration: 338/750.\n",
      "Iteration: 339/750.\n",
      "Iteration: 340/750.\n",
      "Iteration: 341/750.\n",
      "Iteration: 342/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3870551622869107, 2.227611855833517, 2.2965227388282172]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 343/750.\n",
      "Iteration: 344/750.\n",
      "Iteration: 345/750.\n",
      "Iteration: 346/750.\n",
      "Iteration: 347/750.\n",
      "Iteration: 348/750.\n",
      "Iteration: 349/750.\n",
      "Iteration: 350/750.\n",
      "Iteration: 351/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1972767659635544, 2.1985326184695064, 2.2349970739214924]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 352/750.\n",
      "Iteration: 353/750.\n",
      "Iteration: 354/750.\n",
      "Iteration: 355/750.\n",
      "Iteration: 356/750.\n",
      "Iteration: 357/750.\n",
      "Iteration: 358/750.\n",
      "Iteration: 359/750.\n",
      "Iteration: 360/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1742901250718467, 2.0799756410980557, 2.2220841861419194]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 361/750.\n",
      "Iteration: 362/750.\n",
      "Iteration: 363/750.\n",
      "Iteration: 364/750.\n",
      "Iteration: 365/750.\n",
      "Iteration: 366/750.\n",
      "Iteration: 367/750.\n",
      "Iteration: 368/750.\n",
      "Iteration: 369/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1819677069929813, 2.2273059778822533, 2.1947998105505557]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 370/750.\n",
      "Iteration: 371/750.\n",
      "Iteration: 372/750.\n",
      "Iteration: 373/750.\n",
      "Iteration: 374/750.\n",
      "Iteration: 375/750.\n",
      "Iteration: 376/750.\n",
      "Iteration: 377/750.\n",
      "Iteration: 378/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.24355648076423, 2.176654193386759, 2.343633754517829]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 379/750.\n",
      "Iteration: 380/750.\n",
      "Iteration: 381/750.\n",
      "Iteration: 382/750.\n",
      "Iteration: 383/750.\n",
      "Iteration: 384/750.\n",
      "Iteration: 385/750.\n",
      "Iteration: 386/750.\n",
      "Iteration: 387/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.258344127286521, 2.287439493420776, 2.2160262565649242]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 388/750.\n",
      "Iteration: 389/750.\n",
      "Iteration: 390/750.\n",
      "Iteration: 391/750.\n",
      "Iteration: 392/750.\n",
      "Iteration: 393/750.\n",
      "Iteration: 394/750.\n",
      "Iteration: 395/750.\n",
      "Iteration: 396/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2432384498691653, 2.247009227711138, 2.3254272749122347]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 397/750.\n",
      "Iteration: 398/750.\n",
      "Iteration: 399/750.\n",
      "Iteration: 400/750.\n",
      "Iteration: 401/750.\n",
      "Iteration: 402/750.\n",
      "Iteration: 403/750.\n",
      "Iteration: 404/750.\n",
      "Iteration: 405/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.241971823851108, 2.1875700843869, 2.158207424004244]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 406/750.\n",
      "Iteration: 407/750.\n",
      "Iteration: 408/750.\n",
      "Iteration: 409/750.\n",
      "Iteration: 410/750.\n",
      "Iteration: 411/750.\n",
      "Iteration: 412/750.\n",
      "Iteration: 413/750.\n",
      "Iteration: 414/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2484347859408387, 2.3060932894119572, 2.2624999998559]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 415/750.\n",
      "Iteration: 416/750.\n",
      "Iteration: 417/750.\n",
      "Iteration: 418/750.\n",
      "Iteration: 419/750.\n",
      "Iteration: 420/750.\n",
      "Iteration: 421/750.\n",
      "Iteration: 422/750.\n",
      "Iteration: 423/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1886142866691873, 2.128478050810751, 2.311385551409236]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 424/750.\n",
      "Iteration: 425/750.\n",
      "Iteration: 426/750.\n",
      "Iteration: 427/750.\n",
      "Iteration: 428/750.\n",
      "Iteration: 429/750.\n",
      "Iteration: 430/750.\n",
      "Iteration: 431/750.\n",
      "Iteration: 432/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.194539654885159, 2.2900531022302424, 2.2121713336740445]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 433/750.\n",
      "Iteration: 434/750.\n",
      "Iteration: 435/750.\n",
      "Iteration: 436/750.\n",
      "Iteration: 437/750.\n",
      "Iteration: 438/750.\n",
      "Iteration: 439/750.\n",
      "Iteration: 440/750.\n",
      "Iteration: 441/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.134445198940538, 2.2699183887282217, 2.1507394401438193]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 442/750.\n",
      "Iteration: 443/750.\n",
      "Iteration: 444/750.\n",
      "Iteration: 445/750.\n",
      "Iteration: 446/750.\n",
      "Iteration: 447/750.\n",
      "Iteration: 448/750.\n",
      "Iteration: 449/750.\n",
      "Iteration: 450/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1917973532475976, 2.2950102348914974, 2.100521391053965]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 451/750.\n",
      "Iteration: 452/750.\n",
      "Iteration: 453/750.\n",
      "Iteration: 454/750.\n",
      "Iteration: 455/750.\n",
      "Iteration: 456/750.\n",
      "Iteration: 457/750.\n",
      "Iteration: 458/750.\n",
      "Iteration: 459/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.208949986017907, 2.302902253085499, 2.2244442228505825]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 460/750.\n",
      "Iteration: 461/750.\n",
      "Iteration: 462/750.\n",
      "Iteration: 463/750.\n",
      "Iteration: 464/750.\n",
      "Iteration: 465/750.\n",
      "Iteration: 466/750.\n",
      "Iteration: 467/750.\n",
      "Iteration: 468/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.311667349891113, 2.2451032878593913, 2.114113198046391]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 469/750.\n",
      "Iteration: 470/750.\n",
      "Iteration: 471/750.\n",
      "Iteration: 472/750.\n",
      "Iteration: 473/750.\n",
      "Iteration: 474/750.\n",
      "Iteration: 475/750.\n",
      "Iteration: 476/750.\n",
      "Iteration: 477/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.362584979184528, 2.1842563367266337, 2.2175075899941072]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 478/750.\n",
      "Iteration: 479/750.\n",
      "Iteration: 480/750.\n",
      "Iteration: 481/750.\n",
      "Iteration: 482/750.\n",
      "Iteration: 483/750.\n",
      "Iteration: 484/750.\n",
      "Iteration: 485/750.\n",
      "Iteration: 486/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.274318562637799, 2.065706641955925, 2.1479803585598147]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 487/750.\n",
      "Iteration: 488/750.\n",
      "Iteration: 489/750.\n",
      "Iteration: 490/750.\n",
      "Iteration: 491/750.\n",
      "Iteration: 492/750.\n",
      "Iteration: 493/750.\n",
      "Iteration: 494/750.\n",
      "Iteration: 495/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2761224118898045, 2.1852569399565933, 2.208590116865381]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 496/750.\n",
      "Iteration: 497/750.\n",
      "Iteration: 498/750.\n",
      "Iteration: 499/750.\n",
      "Iteration: 500/750.\n",
      "Iteration: 501/750.\n",
      "Iteration: 502/750.\n",
      "Iteration: 503/750.\n",
      "Iteration: 504/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.116283809246591, 2.2028335737626565, 2.2326783799474863]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 505/750.\n",
      "Iteration: 506/750.\n",
      "Iteration: 507/750.\n",
      "Iteration: 508/750.\n",
      "Iteration: 509/750.\n",
      "Iteration: 510/750.\n",
      "Iteration: 511/750.\n",
      "Iteration: 512/750.\n",
      "Iteration: 513/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1968076409861594, 2.274975264968471, 1.9855868075389775]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 514/750.\n",
      "Iteration: 515/750.\n",
      "Iteration: 516/750.\n",
      "Iteration: 517/750.\n",
      "Iteration: 518/750.\n",
      "Iteration: 519/750.\n",
      "Iteration: 520/750.\n",
      "Iteration: 521/750.\n",
      "Iteration: 522/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1953933392272265, 2.215828703238852, 2.3100137441845474]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 523/750.\n",
      "Iteration: 524/750.\n",
      "Iteration: 525/750.\n",
      "Iteration: 526/750.\n",
      "Iteration: 527/750.\n",
      "Iteration: 528/750.\n",
      "Iteration: 529/750.\n",
      "Iteration: 530/750.\n",
      "Iteration: 531/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.146009311380914, 2.2716558721394833, 2.2301757314982242]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 532/750.\n",
      "Iteration: 533/750.\n",
      "Iteration: 534/750.\n",
      "Iteration: 535/750.\n",
      "Iteration: 536/750.\n",
      "Iteration: 537/750.\n",
      "Iteration: 538/750.\n",
      "Iteration: 539/750.\n",
      "Iteration: 540/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.332063398600958, 2.28230492085212, 1.9736515415982288]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 541/750.\n",
      "Iteration: 542/750.\n",
      "Iteration: 543/750.\n",
      "Iteration: 544/750.\n",
      "Iteration: 545/750.\n",
      "Iteration: 546/750.\n",
      "Iteration: 547/750.\n",
      "Iteration: 548/750.\n",
      "Iteration: 549/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1829420813707525, 2.2437436638194823, 2.286485280603498]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 550/750.\n",
      "Iteration: 551/750.\n",
      "Iteration: 552/750.\n",
      "Iteration: 553/750.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 554/750.\n",
      "Iteration: 555/750.\n",
      "Iteration: 556/750.\n",
      "Iteration: 557/750.\n",
      "Iteration: 558/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1614771451312933, 2.3313536288551155, 2.166155356616962]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 559/750.\n",
      "Iteration: 560/750.\n",
      "Iteration: 561/750.\n",
      "Iteration: 562/750.\n",
      "Iteration: 563/750.\n",
      "Iteration: 564/750.\n",
      "Iteration: 565/750.\n",
      "Iteration: 566/750.\n",
      "Iteration: 567/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1527029769325354, 2.256560466136464, 2.298755900566209]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 568/750.\n",
      "Iteration: 569/750.\n",
      "Iteration: 570/750.\n",
      "Iteration: 571/750.\n",
      "Iteration: 572/750.\n",
      "Iteration: 573/750.\n",
      "Iteration: 574/750.\n",
      "Iteration: 575/750.\n",
      "Iteration: 576/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3348348069425224, 2.2370508707565415, 2.1151456857666564]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 577/750.\n",
      "Iteration: 578/750.\n",
      "Iteration: 579/750.\n",
      "Iteration: 580/750.\n",
      "Iteration: 581/750.\n",
      "Iteration: 582/750.\n",
      "Iteration: 583/750.\n",
      "Iteration: 584/750.\n",
      "Iteration: 585/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.250810597110293, 2.293818748703291, 2.020107784339772]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 586/750.\n",
      "Iteration: 587/750.\n",
      "Iteration: 588/750.\n",
      "Iteration: 589/750.\n",
      "Iteration: 590/750.\n",
      "Iteration: 591/750.\n",
      "Iteration: 592/750.\n",
      "Iteration: 593/750.\n",
      "Iteration: 594/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.206940417505602, 2.2418594893978243, 2.435203323547574]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 595/750.\n",
      "Iteration: 596/750.\n",
      "Iteration: 597/750.\n",
      "Iteration: 598/750.\n",
      "Iteration: 599/750.\n",
      "Iteration: 600/750.\n",
      "Iteration: 601/750.\n",
      "Iteration: 602/750.\n",
      "Iteration: 603/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2084709801104907, 2.303666300630048, 2.2445135191978163]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 604/750.\n",
      "Iteration: 605/750.\n",
      "Iteration: 606/750.\n",
      "Iteration: 607/750.\n",
      "Iteration: 608/750.\n",
      "Iteration: 609/750.\n",
      "Iteration: 610/750.\n",
      "Iteration: 611/750.\n",
      "Iteration: 612/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1552491626113706, 2.1625910894636866, 2.2808288405473975]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 613/750.\n",
      "Iteration: 614/750.\n",
      "Iteration: 615/750.\n",
      "Iteration: 616/750.\n",
      "Iteration: 617/750.\n",
      "Iteration: 618/750.\n",
      "Iteration: 619/750.\n",
      "Iteration: 620/750.\n",
      "Iteration: 621/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.221663604770886, 2.1665319540090957, 2.1961497029878596]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 622/750.\n",
      "Iteration: 623/750.\n",
      "Iteration: 624/750.\n",
      "Iteration: 625/750.\n",
      "Iteration: 626/750.\n",
      "Iteration: 627/750.\n",
      "Iteration: 628/750.\n",
      "Iteration: 629/750.\n",
      "Iteration: 630/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2191093408749887, 2.396593898677007, 2.171257568036445]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 631/750.\n",
      "Iteration: 632/750.\n",
      "Iteration: 633/750.\n",
      "Iteration: 634/750.\n",
      "Iteration: 635/750.\n",
      "Iteration: 636/750.\n",
      "Iteration: 637/750.\n",
      "Iteration: 638/750.\n",
      "Iteration: 639/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2136993033344794, 2.1609558943243043, 2.252445795017344]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 640/750.\n",
      "Iteration: 641/750.\n",
      "Iteration: 642/750.\n",
      "Iteration: 643/750.\n",
      "Iteration: 644/750.\n",
      "Iteration: 645/750.\n",
      "Iteration: 646/750.\n",
      "Iteration: 647/750.\n",
      "Iteration: 648/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.270517546564173, 2.077304121289031, 2.317812946959583]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 649/750.\n",
      "Iteration: 650/750.\n",
      "Iteration: 651/750.\n",
      "Iteration: 652/750.\n",
      "Iteration: 653/750.\n",
      "Iteration: 654/750.\n",
      "Iteration: 655/750.\n",
      "Iteration: 656/750.\n",
      "Iteration: 657/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.156857904148581, 2.1572597766780865, 2.3406381410100545]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 658/750.\n",
      "Iteration: 659/750.\n",
      "Iteration: 660/750.\n",
      "Iteration: 661/750.\n",
      "Iteration: 662/750.\n",
      "Iteration: 663/750.\n",
      "Iteration: 664/750.\n",
      "Iteration: 665/750.\n",
      "Iteration: 666/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3015316106955814, 2.286232444163045, 2.162189343749455]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 667/750.\n",
      "Iteration: 668/750.\n",
      "Iteration: 669/750.\n",
      "Iteration: 670/750.\n",
      "Iteration: 671/750.\n",
      "Iteration: 672/750.\n",
      "Iteration: 673/750.\n",
      "Iteration: 674/750.\n",
      "Iteration: 675/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2639214328186092, 2.2495302165354953, 2.3265115638659126]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 676/750.\n",
      "Iteration: 677/750.\n",
      "Iteration: 678/750.\n",
      "Iteration: 679/750.\n",
      "Iteration: 680/750.\n",
      "Iteration: 681/750.\n",
      "Iteration: 682/750.\n",
      "Iteration: 683/750.\n",
      "Iteration: 684/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.05322340943015, 2.2531749255380404, 2.166140649838978]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 685/750.\n",
      "Iteration: 686/750.\n",
      "Iteration: 687/750.\n",
      "Iteration: 688/750.\n",
      "Iteration: 689/750.\n",
      "Iteration: 690/750.\n",
      "Iteration: 691/750.\n",
      "Iteration: 692/750.\n",
      "Iteration: 693/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.1525457102214616, 2.2363613751447224, 2.1949898306646323]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 694/750.\n",
      "Iteration: 695/750.\n",
      "Iteration: 696/750.\n",
      "Iteration: 697/750.\n",
      "Iteration: 698/750.\n",
      "Iteration: 699/750.\n",
      "Iteration: 700/750.\n",
      "Iteration: 701/750.\n",
      "Iteration: 702/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2611339353388433, 2.3471530584952562, 2.148841912322166]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 703/750.\n",
      "Iteration: 704/750.\n",
      "Iteration: 705/750.\n",
      "Iteration: 706/750.\n",
      "Iteration: 707/750.\n",
      "Iteration: 708/750.\n",
      "Iteration: 709/750.\n",
      "Iteration: 710/750.\n",
      "Iteration: 711/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.279490786046512, 2.3235368702024597, 2.1051195390993214]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 712/750.\n",
      "Iteration: 713/750.\n",
      "Iteration: 714/750.\n",
      "Iteration: 715/750.\n",
      "Iteration: 716/750.\n",
      "Iteration: 717/750.\n",
      "Iteration: 718/750.\n",
      "Iteration: 719/750.\n",
      "Iteration: 720/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.2942693326887547, 2.2811890204616265, 2.1792211232871224]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 721/750.\n",
      "Iteration: 722/750.\n",
      "Iteration: 723/750.\n",
      "Iteration: 724/750.\n",
      "Iteration: 725/750.\n",
      "Iteration: 726/750.\n",
      "Iteration: 727/750.\n",
      "Iteration: 728/750.\n",
      "Iteration: 729/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.004881074775065, 2.165659208624099, 2.3316752411036696]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 730/750.\n",
      "Iteration: 731/750.\n",
      "Iteration: 732/750.\n",
      "Iteration: 733/750.\n",
      "Iteration: 734/750.\n",
      "Iteration: 735/750.\n",
      "Iteration: 736/750.\n",
      "Iteration: 737/750.\n",
      "Iteration: 738/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.254104787989307, 2.237855966679909, 2.157645520877249]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 739/750.\n",
      "Iteration: 740/750.\n",
      "Iteration: 741/750.\n",
      "Iteration: 742/750.\n",
      "Iteration: 743/750.\n",
      "Iteration: 744/750.\n",
      "Iteration: 745/750.\n",
      "Iteration: 746/750.\n",
      "Iteration: 747/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.309333752799659\n",
      "Loss latest three: [2.3353460865512017, 2.3626829171958854, 2.1615144716245003]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 748/750.\n",
      "Iteration: 749/750.\n",
      "Iteration: 750/750.\n",
      "\n",
      "\n",
      "----------------FINAL OUTPUT----------------\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "Loss history: [2.309333752799659, 2.3100475210286935, 2.302734681556495, 2.303403251768486, 2.3008919111426778, 2.3030688070130423, 2.3083244218204695, 2.305725695413475, 2.297431716772699, 2.2971185583164577, 2.295047037977388, 2.3075030230866163, 2.2950493262693032, 2.302557765275875, 2.302703579304868, 2.3018450824131884, 2.3036695764710173, 2.3017066047698584, 2.3026332858630787, 2.3012372917903128, 2.3057936665043886, 2.300111243151785, 2.304657471487298, 2.294923033034066, 2.296826694035708, 2.2988054741544746, 2.298301305372676, 2.308902546197689, 2.293720393659111, 2.3003195881098004, 2.289276288094972, 2.3027816559633787, 2.2924948880619533, 2.2966168260260056, 2.299502510069381, 2.297408434557621, 2.2952866492919974, 2.2881488785226263, 2.303345064379366, 2.2877893250629198, 2.3013919178983135, 2.2975664705523684, 2.298535173218746, 2.2969774199484214, 2.287743382443448, 2.280444956983327, 2.2919499877728744, 2.298898649446118, 2.3153907594305907, 2.3021109911026945, 2.3012461177238124, 2.2839536623842185, 2.2938137555028617, 2.302438358369893, 2.2965300061329255, 2.294813767132136, 2.280370548936137, 2.2904263157753526, 2.286187019459692, 2.2942040775522634, 2.3025548231332373, 2.2845544381527216, 2.281798459737148, 2.297068291329227, 2.2617424650288287, 2.2809299499204694, 2.2538198629755417, 2.271909689382298, 2.2776021913092346, 2.2835935731873747, 2.2789792517783667, 2.2761709594171795, 2.3143959334197306, 2.3177621604066103, 2.2884189660660303, 2.311392910466611, 2.275393185958308, 2.286765087652806, 2.274863026115056, 2.294948740664123, 2.2861885214757716, 2.2871390892288956, 2.2743610652958326, 2.263448886273185, 2.289666590899042, 2.2975086874459687, 2.2790189825112743, 2.2580556523144146, 2.2770504059269747, 2.2811090608307416, 2.2757719949998716, 2.2933836279835105, 2.290496385895961, 2.2614085040767975, 2.28383061215381, 2.277935653130013, 2.2767012191425837, 2.261673609481268, 2.272762194269741, 2.2714647010266664, 2.290487473501098, 2.2658902911532883, 2.2773112089049175, 2.2770148055491357, 2.2772368045511544, 2.2664487825242845, 2.286639542318909, 2.247916495356978, 2.305542273864441, 2.2512646507169696, 2.2485440855344465, 2.272688105860993, 2.260213811437478, 2.2630233761375496, 2.2729363999356873, 2.273953395587147, 2.2786906412783487, 2.3192567603996523, 2.281512004316321, 2.262797660909111, 2.2803547558436756, 2.3100277732334074, 2.2617148656766113, 2.2537882165991663, 2.278439164214905, 2.2486419643059565, 2.289976849054879, 2.2591118473935636, 2.246622518184403, 2.2485539756996356, 2.2714022353054437, 2.270946773985217, 2.32051304490723, 2.3152721759448656, 2.2505928687197856, 2.271279483717185, 2.2719340757255693, 2.2735338698687357, 2.2513331669232333, 2.258728522823861, 2.31674771521857, 2.2918283532741106, 2.230395258419411, 2.2907653948807876, 2.3037061892965354, 2.28888440081966, 2.277688608511672, 2.288715704807792, 2.2466543833451955, 2.2653392763452334, 2.250193863582836, 2.2973584825330042, 2.268586391634377, 2.222452286381358, 2.3213260910671365, 2.2621099588300413, 2.2481418825953106, 2.261055779220247, 2.269432519652733, 2.2824466859916486, 2.244650987574829, 2.2543351594952203, 2.292507476194037, 2.2723139494422697, 2.2869218409553285, 2.279710771930238, 2.266694714661016, 2.2587020252489243, 2.232717186500342, 2.2397897020179474, 2.2738842139177944, 2.3130731927395285, 2.2755739239371615, 2.277945426675915, 2.243454052034584, 2.2645562990858603, 2.2133727421426292, 2.2679793280689706, 2.28689228759865, 2.276174764319612, 2.2236739566899186, 2.298363742985818, 2.2729309430404605, 2.2563693135874368, 2.240465637311098, 2.2955739515634828, 2.2809092996558507, 2.2789688282647362, 2.28805322376837, 2.3166855884055906, 2.2527713544474053, 2.2731325887768667, 2.2651127106971516, 2.2173477328947846, 2.2098539807486257, 2.2551803876097742, 2.268825191182065, 2.2279083463692317, 2.301795306412694, 2.276189958551281, 2.3712431846473616, 2.2868328038979064, 2.225494902506178, 2.2219202034810803, 2.308294898786341, 2.2447185877161426, 2.2561030174177947, 2.2817544299604884, 2.3094376965185885, 2.3225717067902347, 2.2474790805137927, 2.226260243316582, 2.2742641512258435, 2.274031923422989, 2.292507221173354, 2.2214233061154625, 2.3038294789458624, 2.2006161740230277, 2.3114010439995507, 2.2498249385676092, 2.2499307261201498, 2.26822350700755, 2.3173336547347247, 2.267883190300659, 2.216650036147166, 2.242851204365984, 2.308774772880274, 2.208584330916785, 2.251177796565325, 2.2929429254504283, 2.2742531109980155, 2.3015755984933626, 2.238681765961866, 2.2016660365529783, 2.2414976699522264, 2.2339527682828093, 2.2419668631047154, 2.22561029152744, 2.2696078286368095, 2.289551143034933, 2.3058068408717873, 2.2569811975613874, 2.269927435171348, 2.26324520377211, 2.1625316569152084, 2.2589574526575413, 2.2686326588972605, 2.277235929152427, 2.316094084563435, 2.2473843431840854, 2.1699650093380507, 2.330035628675186, 2.236761831921097, 2.2167136729817942, 2.1725469404099202, 2.3115513746023284, 2.2496455469082437, 2.2959816020938013, 2.271419112296858, 2.201561180448635, 2.2923443997167285, 2.2735742312378213, 2.25518484706304, 2.2075910110023926, 2.245772882471312, 2.2168554586432294, 2.1799703079039072, 2.3423970038143236, 2.2769262981458995, 2.3062019756584777, 2.179473352098399, 2.3465312122995425, 2.2788156964679245, 2.2823152490006624, 2.291660751892977, 2.273853471694125, 2.2407741533860976, 2.172899416933032, 2.1984333670970226, 2.178768841035081, 2.260190263650482, 2.194242320129877, 2.301754317479586, 2.1379310461165866, 2.1671659270788104, 2.2981574744501674, 2.2600462418147753, 2.2166681453444483, 2.3116682133374518, 2.27308235347993, 2.315415519813724, 2.2454148717035594, 2.2420022123738086, 2.2460575891304244, 2.2155338896642607, 2.197460097265643, 2.318388907515723, 2.3021859078807085, 2.286161200886601, 2.339669902771893, 2.197040245797655, 2.282705282487249, 2.1695848944874228, 2.183530894514179, 2.1687370910024586, 2.280636857520292, 2.176898355230328, 2.1658383758779904, 2.2287497642505523, 2.184245077130056, 2.254430533214847, 2.296480705564703, 2.3083214739652798, 2.2727792010817764, 2.2652212535838214, 2.3004397483350063, 2.2647615090690967, 2.1270535971968676, 2.234923844557901, 2.3556544580008727, 2.320327028296227, 2.2743787708138794, 2.205430793999642, 2.216025726528513, 2.2339827155221434, 2.1774274007415815, 2.2233050871556332, 2.2044080679554936, 2.2527007936336836, 2.2318030781141664, 2.2523932586141244, 2.2818711031090086, 2.2729374546977894, 2.2574072904957827, 2.3021979157163655, 2.2759473621417485, 2.204111729271655, 2.189252558882519, 2.237886234675441, 2.3870551622869107, 2.227611855833517, 2.2965227388282172, 2.174574405759914, 2.2223644710891475, 2.39034188002874, 2.1858808523300532, 2.296386752140095, 2.2378239480935656, 2.1972767659635544, 2.1985326184695064, 2.2349970739214924, 2.1900170750618506, 2.1709146497469116, 2.1477515443108333, 2.179970896350563, 2.24916708009886, 2.2343122848263763, 2.1742901250718467, 2.0799756410980557, 2.2220841861419194, 2.247956174372676, 2.1803141868710143, 2.1836113036816145, 2.1633272251408706, 2.225359362065844, 2.2165446854092736, 2.1819677069929813, 2.2273059778822533, 2.1947998105505557, 2.2708157561509386, 2.157979293178159, 2.174449023672463, 2.139562600685668, 2.235704364427927, 2.156121229015864, 2.24355648076423, 2.176654193386759, 2.343633754517829, 2.292389334541857, 2.276347535490325, 2.211671708181607, 2.2094181486726217, 2.185462330048158, 2.205960619547817, 2.258344127286521, 2.287439493420776, 2.2160262565649242, 2.203038506014433, 2.171591728644529, 2.251956833505868, 2.2446382834380154, 2.3595602557073363, 2.2099139511800328, 2.2432384498691653, 2.247009227711138, 2.3254272749122347, 2.301744621986177, 2.2252482918131613, 2.2291867398624254, 2.113158742224389, 2.189558115279206, 2.2351086865998013, 2.241971823851108, 2.1875700843869, 2.158207424004244, 2.236749938926907, 2.226051608823242, 2.2771237789620407, 2.27284293806041, 2.2980481552943437, 2.176719451395235, 2.2484347859408387, 2.3060932894119572, 2.2624999998559, 2.1618495127116626, 2.3237064027617147, 2.240239675948818, 2.2147104791774455, 2.2982272673545237, 2.3358278030850026, 2.1886142866691873, 2.128478050810751, 2.311385551409236, 2.2407913353908095, 2.2632245758216527, 2.2672079876550835, 2.252928202675109, 2.220774177698773, 2.2748848137560786, 2.194539654885159, 2.2900531022302424, 2.2121713336740445, 2.0913593308298855, 2.2703914639860456, 2.1523936948973397, 2.2416866824376345, 2.21617490475356, 2.246419970940201, 2.134445198940538, 2.2699183887282217, 2.1507394401438193, 2.261806802927795, 2.2308286221768205, 2.281423296231043, 2.31822669922193, 2.144160823199504, 2.1513400802171367, 2.1917973532475976, 2.2950102348914974, 2.100521391053965, 2.2267112330326637, 2.1420273375967542, 2.239265776719974, 2.240059615437333, 2.1599768407343682, 2.2681748784584044, 2.208949986017907, 2.302902253085499, 2.2244442228505825, 2.263264839591967, 2.1429665401663955, 2.1882899043981214, 2.246000540413417, 2.334004475609229, 2.2200255863545557, 2.311667349891113, 2.2451032878593913, 2.114113198046391, 2.1379800774131756, 2.220794836257581, 2.254915990121502, 2.1857252690471274, 2.1547434933653054, 2.3005650631533237, 2.362584979184528, 2.1842563367266337, 2.2175075899941072, 2.3064659369084306, 2.2739830313540934, 2.2070774735280523, 2.263569936190832, 2.257398307509753, 2.2027435879602897, 2.274318562637799, 2.065706641955925, 2.1479803585598147, 2.116617104891958, 2.190731908778447, 2.1471997685634467, 2.072579382669971, 2.3231734245159856, 2.23225608264937, 2.2761224118898045, 2.1852569399565933, 2.208590116865381, 2.2565222860804144, 2.0983197982911714, 2.3377358893142426, 2.182478165500148, 2.1482805444065542, 2.31464717537705, 2.116283809246591, 2.2028335737626565, 2.2326783799474863, 2.265263127103594, 2.2437697487925976, 2.167442523067767, 2.2645192117642092, 2.216527928137511, 2.187732284931041, 2.1968076409861594, 2.274975264968471, 1.9855868075389775, 2.2597717458141573, 2.2821159178218937, 2.198963079030357, 2.1891945104577792, 2.190512050266871, 2.244300651790686, 2.1953933392272265, 2.215828703238852, 2.3100137441845474, 2.3159477531534285, 2.266643430772907, 2.3032593686739, 2.394652653544321, 2.254032433202731, 2.1311157260927445, 2.146009311380914, 2.2716558721394833, 2.2301757314982242, 2.2678814937265264, 2.213838422499859, 2.1760596121838343, 2.273506105418976, 2.251507362888061, 2.34512189411497, 2.332063398600958, 2.28230492085212, 1.9736515415982288, 2.1189227626523808, 2.114206787754031, 2.151698611244652, 2.2705814786836984, 2.177990638800233, 2.149726026830303, 2.1829420813707525, 2.2437436638194823, 2.286485280603498, 2.24980885955483, 2.1938046741035966, 2.166350240360741, 2.2258846532623258, 2.34285458114567, 2.1929663692256596, 2.1614771451312933, 2.3313536288551155, 2.166155356616962, 2.307011222268517, 2.205694221413372, 2.2193724314442504, 2.2778575348161674, 2.179588781404833, 2.0986308429095795, 2.1527029769325354, 2.256560466136464, 2.298755900566209, 2.1344338590413137, 2.2271608822114453, 2.284828067580046, 2.2062007773753085, 2.2073675067406073, 2.2657247281164135, 2.3348348069425224, 2.2370508707565415, 2.1151456857666564, 2.2414551117609363, 2.242918937566582, 2.0946004292917, 2.011953260774737, 2.3388613365973963, 2.2385748500313736, 2.250810597110293, 2.293818748703291, 2.020107784339772, 2.28195275115603, 2.255223923374266, 2.14869507768359, 2.2096173142820246, 2.12365362130284, 2.165925894978741, 2.206940417505602, 2.2418594893978243, 2.435203323547574, 2.20321458119135, 2.070248470128996, 2.2187936744119288, 2.161224616410725, 2.10497447919098, 2.3018433928622675, 2.2084709801104907, 2.303666300630048, 2.2445135191978163, 2.14649496794392, 2.0165073561594746, 2.2256396388575146, 2.169012236912601, 2.2624673734805483, 2.2824335920666723, 2.1552491626113706, 2.1625910894636866, 2.2808288405473975, 2.2439545751133325, 2.2382588261264305, 2.0898953426194247, 2.281819890727656, 2.1287797113418496, 2.052971964456318, 2.221663604770886, 2.1665319540090957, 2.1961497029878596, 2.243790478326919, 2.293736071871257, 2.0943813936038187, 2.1252794685292478, 2.196465547035758, 2.134140944633871, 2.2191093408749887, 2.396593898677007, 2.171257568036445, 2.155883916476155, 2.1104135783744593, 2.0524650095148735, 2.2488262598015436, 2.0561069788442414, 2.1472249905974543, 2.2136993033344794, 2.1609558943243043, 2.252445795017344, 2.2339208167457225, 2.2423719595775986, 2.1862903605124084, 2.2134832077203015, 2.174377667422002, 2.1115971157864943, 2.270517546564173, 2.077304121289031, 2.317812946959583, 2.268672642916388, 2.3044974800034406, 2.239222797193361, 2.0344313210390585, 2.2635853161527493, 2.328787323928576, 2.156857904148581, 2.1572597766780865, 2.3406381410100545, 2.2613898090021998, 2.220838742755532, 2.127641741219276, 2.2885573407925426, 2.3954004390646886, 2.1131574528161963, 2.3015316106955814, 2.286232444163045, 2.162189343749455, 2.2110244404878454, 2.282304150695663, 2.2319069197630688, 2.0955823281878074, 2.2538220790481094, 2.309866398808077, 2.2639214328186092, 2.2495302165354953, 2.3265115638659126, 2.2928396386392604, 2.1773725694758985, 2.2069998234792862, 2.230035014498944, 2.2812789349642024, 2.2869453871015626, 2.05322340943015, 2.2531749255380404, 2.166140649838978, 2.160660281549361, 2.129177684204914, 2.1192023458260607, 2.2709323561132626, 2.1739322421048892, 2.3058492710722223, 2.1525457102214616, 2.2363613751447224, 2.1949898306646323, 2.2208228374884667, 2.1805919721518943, 2.2467683982408158, 2.01945280802785, 2.324196217749735, 2.219503405308274, 2.2611339353388433, 2.3471530584952562, 2.148841912322166, 2.2105145700352185, 2.10299896001295, 2.2452306411883876, 2.2659922834657737, 2.0824698025658726, 2.296669271511598, 2.279490786046512, 2.3235368702024597, 2.1051195390993214, 2.1919482176571354, 2.3903194487425594, 2.274765857341968, 2.319324972765244, 2.1576878470952674, 2.2717888821075, 2.2942693326887547, 2.2811890204616265, 2.1792211232871224, 2.345779364711508, 2.0234519079395117, 2.1855523912559702, 2.241816207815938, 2.352432464466387, 2.2271404793424554, 2.004881074775065, 2.165659208624099, 2.3316752411036696, 2.1963508768128817, 2.0938752581510442, 2.37773310826539, 2.224772979726787, 2.2509349442974904, 2.10176376249588, 2.254104787989307, 2.237855966679909, 2.157645520877249, 2.090897439914133, 2.0999701055851396, 2.1815854494495666, 2.0745644609802456, 2.21998773174819, 2.0882375961335344, 2.3353460865512017, 2.3626829171958854, 2.1615144716245003, 2.0949775769564627, 2.1786421785573125, 2.2810184967981635]\n",
      "Accuracy history: [0.16, 0.16, 0.16, 0.16, 0.16, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22]\n"
     ]
    }
   ],
   "source": [
    "# SGD-M\n",
    "sgd_m = ConvNet4(input_shape=input_shape, wt_scale=wt_scale, verbose=False)\n",
    "sgd_m.compile('sgd_momentum')\n",
    "sgd_m.fit(x_dev, y_dev, x_val, y_val, mini_batch_sz=mini_batch_sz, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train...\n",
      "750 iterations. 5 iter/epoch.\n",
      "Iteration: 1/750.\n",
      "Time taken for iteration 0: 1.6547000408172607\n",
      "Estimated time to complete: 1241.0250306129456\n",
      "Iteration: 2/750.\n",
      "Iteration: 3/750.\n",
      "Iteration: 4/750.\n",
      "Iteration: 5/750.\n",
      "Iteration: 6/750.\n",
      "Iteration: 7/750.\n",
      "Iteration: 8/750.\n",
      "Iteration: 9/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.2667797754626706, 2.275256568294465, 2.2556253335188514]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 10/750.\n",
      "Iteration: 11/750.\n",
      "Iteration: 12/750.\n",
      "Iteration: 13/750.\n",
      "Iteration: 14/750.\n",
      "Iteration: 15/750.\n",
      "Iteration: 16/750.\n",
      "Iteration: 17/750.\n",
      "Iteration: 18/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.2727310367417854, 2.204103899788421, 2.258343837284645]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 19/750.\n",
      "Iteration: 20/750.\n",
      "Iteration: 21/750.\n",
      "Iteration: 22/750.\n",
      "Iteration: 23/750.\n",
      "Iteration: 24/750.\n",
      "Iteration: 25/750.\n",
      "Iteration: 26/750.\n",
      "Iteration: 27/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.1944675031511447, 2.2390716805564996, 2.281966984895512]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 28/750.\n",
      "Iteration: 29/750.\n",
      "Iteration: 30/750.\n",
      "Iteration: 31/750.\n",
      "Iteration: 32/750.\n",
      "Iteration: 33/750.\n",
      "Iteration: 34/750.\n",
      "Iteration: 35/750.\n",
      "Iteration: 36/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.2104394749579153, 2.186158200387568, 2.314671231617274]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 37/750.\n",
      "Iteration: 38/750.\n",
      "Iteration: 39/750.\n",
      "Iteration: 40/750.\n",
      "Iteration: 41/750.\n",
      "Iteration: 42/750.\n",
      "Iteration: 43/750.\n",
      "Iteration: 44/750.\n",
      "Iteration: 45/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.262181205808585, 2.1475236738810057, 2.1927888638495125]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 46/750.\n",
      "Iteration: 47/750.\n",
      "Iteration: 48/750.\n",
      "Iteration: 49/750.\n",
      "Iteration: 50/750.\n",
      "Iteration: 51/750.\n",
      "Iteration: 52/750.\n",
      "Iteration: 53/750.\n",
      "Iteration: 54/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.1454005350969774, 2.3007085399330136, 2.254623388762617]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 55/750.\n",
      "Iteration: 56/750.\n",
      "Iteration: 57/750.\n",
      "Iteration: 58/750.\n",
      "Iteration: 59/750.\n",
      "Iteration: 60/750.\n",
      "Iteration: 61/750.\n",
      "Iteration: 62/750.\n",
      "Iteration: 63/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.2224804656011212, 2.132319431106546, 2.160046661484646]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 64/750.\n",
      "Iteration: 65/750.\n",
      "Iteration: 66/750.\n",
      "Iteration: 67/750.\n",
      "Iteration: 68/750.\n",
      "Iteration: 69/750.\n",
      "Iteration: 70/750.\n",
      "Iteration: 71/750.\n",
      "Iteration: 72/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.062876644831836, 2.1102064822282625, 2.342895680648545]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 73/750.\n",
      "Iteration: 74/750.\n",
      "Iteration: 75/750.\n",
      "Iteration: 76/750.\n",
      "Iteration: 77/750.\n",
      "Iteration: 78/750.\n",
      "Iteration: 79/750.\n",
      "Iteration: 80/750.\n",
      "Iteration: 81/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.245438425118237, 2.1422264296376468, 2.328747695698904]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 82/750.\n",
      "Iteration: 83/750.\n",
      "Iteration: 84/750.\n",
      "Iteration: 85/750.\n",
      "Iteration: 86/750.\n",
      "Iteration: 87/750.\n",
      "Iteration: 88/750.\n",
      "Iteration: 89/750.\n",
      "Iteration: 90/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.2147544396521504, 2.182436801416794, 2.3272995062125292]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 91/750.\n",
      "Iteration: 92/750.\n",
      "Iteration: 93/750.\n",
      "Iteration: 94/750.\n",
      "Iteration: 95/750.\n",
      "Iteration: 96/750.\n",
      "Iteration: 97/750.\n",
      "Iteration: 98/750.\n",
      "Iteration: 99/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.141941328348959, 2.112567311006279, 2.2184563897284577]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 100/750.\n",
      "Iteration: 101/750.\n",
      "Iteration: 102/750.\n",
      "Iteration: 103/750.\n",
      "Iteration: 104/750.\n",
      "Iteration: 105/750.\n",
      "Iteration: 106/750.\n",
      "Iteration: 107/750.\n",
      "Iteration: 108/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.280583389083929, 2.0969955815327004, 1.8145079639294601]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 109/750.\n",
      "Iteration: 110/750.\n",
      "Iteration: 111/750.\n",
      "Iteration: 112/750.\n",
      "Iteration: 113/750.\n",
      "Iteration: 114/750.\n",
      "Iteration: 115/750.\n",
      "Iteration: 116/750.\n",
      "Iteration: 117/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.9084026833757428, 2.1167140453380213, 2.3740182895421293]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 118/750.\n",
      "Iteration: 119/750.\n",
      "Iteration: 120/750.\n",
      "Iteration: 121/750.\n",
      "Iteration: 122/750.\n",
      "Iteration: 123/750.\n",
      "Iteration: 124/750.\n",
      "Iteration: 125/750.\n",
      "Iteration: 126/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.346263581167927, 1.8869907552364524, 2.461087123313006]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.24, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 127/750.\n",
      "Iteration: 128/750.\n",
      "Iteration: 129/750.\n",
      "Iteration: 130/750.\n",
      "Iteration: 131/750.\n",
      "Iteration: 132/750.\n",
      "Iteration: 133/750.\n",
      "Iteration: 134/750.\n",
      "Iteration: 135/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.810131401373492, 2.1380569646358207, 2.21698438382967]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.24, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 136/750.\n",
      "Iteration: 137/750.\n",
      "Iteration: 138/750.\n",
      "Iteration: 139/750.\n",
      "Iteration: 140/750.\n",
      "Iteration: 141/750.\n",
      "Iteration: 142/750.\n",
      "Iteration: 143/750.\n",
      "Iteration: 144/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.8719002668156919, 2.0312890920937923, 2.0585284689321237]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.28, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 145/750.\n",
      "Iteration: 146/750.\n",
      "Iteration: 147/750.\n",
      "Iteration: 148/750.\n",
      "Iteration: 149/750.\n",
      "Iteration: 150/750.\n",
      "Iteration: 151/750.\n",
      "Iteration: 152/750.\n",
      "Iteration: 153/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.7564821912496176, 1.9865067476393694, 1.943536227476239]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.32, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 154/750.\n",
      "Iteration: 155/750.\n",
      "Iteration: 156/750.\n",
      "Iteration: 157/750.\n",
      "Iteration: 158/750.\n",
      "Iteration: 159/750.\n",
      "Iteration: 160/750.\n",
      "Iteration: 161/750.\n",
      "Iteration: 162/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.8245981558917728, 1.9881851896046179, 1.9763830868347005]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.36, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 163/750.\n",
      "Iteration: 164/750.\n",
      "Iteration: 165/750.\n",
      "Iteration: 166/750.\n",
      "Iteration: 167/750.\n",
      "Iteration: 168/750.\n",
      "Iteration: 169/750.\n",
      "Iteration: 170/750.\n",
      "Iteration: 171/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [2.080556145782496, 1.624516182196767, 1.592757630272275]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.32, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 172/750.\n",
      "Iteration: 173/750.\n",
      "Iteration: 174/750.\n",
      "Iteration: 175/750.\n",
      "Iteration: 176/750.\n",
      "Iteration: 177/750.\n",
      "Iteration: 178/750.\n",
      "Iteration: 179/750.\n",
      "Iteration: 180/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.69090942977038, 1.9665592430540089, 1.8076948352084192]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.38, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 181/750.\n",
      "Iteration: 182/750.\n",
      "Iteration: 183/750.\n",
      "Iteration: 184/750.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 185/750.\n",
      "Iteration: 186/750.\n",
      "Iteration: 187/750.\n",
      "Iteration: 188/750.\n",
      "Iteration: 189/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.7687937906780133, 1.5499046858392924, 1.7933401927001027]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.44, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 190/750.\n",
      "Iteration: 191/750.\n",
      "Iteration: 192/750.\n",
      "Iteration: 193/750.\n",
      "Iteration: 194/750.\n",
      "Iteration: 195/750.\n",
      "Iteration: 196/750.\n",
      "Iteration: 197/750.\n",
      "Iteration: 198/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.8762247744837355, 1.8434306913313045, 1.646268527799408]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.4, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 199/750.\n",
      "Iteration: 200/750.\n",
      "Iteration: 201/750.\n",
      "Iteration: 202/750.\n",
      "Iteration: 203/750.\n",
      "Iteration: 204/750.\n",
      "Iteration: 205/750.\n",
      "Iteration: 206/750.\n",
      "Iteration: 207/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.5854351235733253, 1.6089370888972951, 2.058576125229221]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.36, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 208/750.\n",
      "Iteration: 209/750.\n",
      "Iteration: 210/750.\n",
      "Iteration: 211/750.\n",
      "Iteration: 212/750.\n",
      "Iteration: 213/750.\n",
      "Iteration: 214/750.\n",
      "Iteration: 215/750.\n",
      "Iteration: 216/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.784510600880739, 1.7362789713859994, 1.446772620741509]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.46, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 217/750.\n",
      "Iteration: 218/750.\n",
      "Iteration: 219/750.\n",
      "Iteration: 220/750.\n",
      "Iteration: 221/750.\n",
      "Iteration: 222/750.\n",
      "Iteration: 223/750.\n",
      "Iteration: 224/750.\n",
      "Iteration: 225/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.367081370829135, 1.7940175672363345, 1.0815554925170499]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.32, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 226/750.\n",
      "Iteration: 227/750.\n",
      "Iteration: 228/750.\n",
      "Iteration: 229/750.\n",
      "Iteration: 230/750.\n",
      "Iteration: 231/750.\n",
      "Iteration: 232/750.\n",
      "Iteration: 233/750.\n",
      "Iteration: 234/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.5370650534071886, 1.4994189511311506, 1.2382007588936237]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.34, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 235/750.\n",
      "Iteration: 236/750.\n",
      "Iteration: 237/750.\n",
      "Iteration: 238/750.\n",
      "Iteration: 239/750.\n",
      "Iteration: 240/750.\n",
      "Iteration: 241/750.\n",
      "Iteration: 242/750.\n",
      "Iteration: 243/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.2365533918378877, 1.4200319681257227, 1.8429169176312614]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.52, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 244/750.\n",
      "Iteration: 245/750.\n",
      "Iteration: 246/750.\n",
      "Iteration: 247/750.\n",
      "Iteration: 248/750.\n",
      "Iteration: 249/750.\n",
      "Iteration: 250/750.\n",
      "Iteration: 251/750.\n",
      "Iteration: 252/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.565128906395955, 0.9596664751092816, 1.229862366192645]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.58, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 253/750.\n",
      "Iteration: 254/750.\n",
      "Iteration: 255/750.\n",
      "Iteration: 256/750.\n",
      "Iteration: 257/750.\n",
      "Iteration: 258/750.\n",
      "Iteration: 259/750.\n",
      "Iteration: 260/750.\n",
      "Iteration: 261/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.9498037380592687, 1.0349733756213138, 1.866582896709912]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.62, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 262/750.\n",
      "Iteration: 263/750.\n",
      "Iteration: 264/750.\n",
      "Iteration: 265/750.\n",
      "Iteration: 266/750.\n",
      "Iteration: 267/750.\n",
      "Iteration: 268/750.\n",
      "Iteration: 269/750.\n",
      "Iteration: 270/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [1.1245383727738474, 1.0784979611253103, 1.2490428991528333]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.56, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 271/750.\n",
      "Iteration: 272/750.\n",
      "Iteration: 273/750.\n",
      "Iteration: 274/750.\n",
      "Iteration: 275/750.\n",
      "Iteration: 276/750.\n",
      "Iteration: 277/750.\n",
      "Iteration: 278/750.\n",
      "Iteration: 279/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.6562065181910253, 0.6959926880204463, 0.3588537720135193]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.62, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 280/750.\n",
      "Iteration: 281/750.\n",
      "Iteration: 282/750.\n",
      "Iteration: 283/750.\n",
      "Iteration: 284/750.\n",
      "Iteration: 285/750.\n",
      "Iteration: 286/750.\n",
      "Iteration: 287/750.\n",
      "Iteration: 288/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.7398576115325761, 0.9311622601760052, 1.2553016971047823]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.7, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 289/750.\n",
      "Iteration: 290/750.\n",
      "Iteration: 291/750.\n",
      "Iteration: 292/750.\n",
      "Iteration: 293/750.\n",
      "Iteration: 294/750.\n",
      "Iteration: 295/750.\n",
      "Iteration: 296/750.\n",
      "Iteration: 297/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.6652714303068812, 0.8960706763006234, 0.9986283064194643]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.7, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 298/750.\n",
      "Iteration: 299/750.\n",
      "Iteration: 300/750.\n",
      "Iteration: 301/750.\n",
      "Iteration: 302/750.\n",
      "Iteration: 303/750.\n",
      "Iteration: 304/750.\n",
      "Iteration: 305/750.\n",
      "Iteration: 306/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.7121118251675107, 0.7926670437404754, 0.42816989121324867]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.8, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 307/750.\n",
      "Iteration: 308/750.\n",
      "Iteration: 309/750.\n",
      "Iteration: 310/750.\n",
      "Iteration: 311/750.\n",
      "Iteration: 312/750.\n",
      "Iteration: 313/750.\n",
      "Iteration: 314/750.\n",
      "Iteration: 315/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.5999820458657846, 0.5932785082964959, 0.726994351994561]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.7, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 316/750.\n",
      "Iteration: 317/750.\n",
      "Iteration: 318/750.\n",
      "Iteration: 319/750.\n",
      "Iteration: 320/750.\n",
      "Iteration: 321/750.\n",
      "Iteration: 322/750.\n",
      "Iteration: 323/750.\n",
      "Iteration: 324/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.5185312490896163, 0.6175317543712656, 0.8038231033579081]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.84, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 325/750.\n",
      "Iteration: 326/750.\n",
      "Iteration: 327/750.\n",
      "Iteration: 328/750.\n",
      "Iteration: 329/750.\n",
      "Iteration: 330/750.\n",
      "Iteration: 331/750.\n",
      "Iteration: 332/750.\n",
      "Iteration: 333/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.3721801906432122, 0.48718576388629914, 0.7962355987553126]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.76, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 334/750.\n",
      "Iteration: 335/750.\n",
      "Iteration: 336/750.\n",
      "Iteration: 337/750.\n",
      "Iteration: 338/750.\n",
      "Iteration: 339/750.\n",
      "Iteration: 340/750.\n",
      "Iteration: 341/750.\n",
      "Iteration: 342/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.45703582118824787, 0.4455654487389473, 0.6131551995768093]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.86, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 343/750.\n",
      "Iteration: 344/750.\n",
      "Iteration: 345/750.\n",
      "Iteration: 346/750.\n",
      "Iteration: 347/750.\n",
      "Iteration: 348/750.\n",
      "Iteration: 349/750.\n",
      "Iteration: 350/750.\n",
      "Iteration: 351/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.3600096173785579, 0.24084408281141323, 0.44647209122066905]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.92, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 352/750.\n",
      "Iteration: 353/750.\n",
      "Iteration: 354/750.\n",
      "Iteration: 355/750.\n",
      "Iteration: 356/750.\n",
      "Iteration: 357/750.\n",
      "Iteration: 358/750.\n",
      "Iteration: 359/750.\n",
      "Iteration: 360/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.21452174186284392, 0.7726704370448986, 0.45634574429319485]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.88, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 361/750.\n",
      "Iteration: 362/750.\n",
      "Iteration: 363/750.\n",
      "Iteration: 364/750.\n",
      "Iteration: 365/750.\n",
      "Iteration: 366/750.\n",
      "Iteration: 367/750.\n",
      "Iteration: 368/750.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 369/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.3295559392454376, 0.5024100581564189, 0.4312812816763762]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.92, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 370/750.\n",
      "Iteration: 371/750.\n",
      "Iteration: 372/750.\n",
      "Iteration: 373/750.\n",
      "Iteration: 374/750.\n",
      "Iteration: 375/750.\n",
      "Iteration: 376/750.\n",
      "Iteration: 377/750.\n",
      "Iteration: 378/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.17926499542018026, 0.08146063120636833, 0.14776578401052776]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.96, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 379/750.\n",
      "Iteration: 380/750.\n",
      "Iteration: 381/750.\n",
      "Iteration: 382/750.\n",
      "Iteration: 383/750.\n",
      "Iteration: 384/750.\n",
      "Iteration: 385/750.\n",
      "Iteration: 386/750.\n",
      "Iteration: 387/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.12690574396394314, 0.14626360697760457, 0.27095333847671027]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.96, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 388/750.\n",
      "Iteration: 389/750.\n",
      "Iteration: 390/750.\n",
      "Iteration: 391/750.\n",
      "Iteration: 392/750.\n",
      "Iteration: 393/750.\n",
      "Iteration: 394/750.\n",
      "Iteration: 395/750.\n",
      "Iteration: 396/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.05127357695541305, 0.03077601974795683, 0.09978465409029391]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.94, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 397/750.\n",
      "Iteration: 398/750.\n",
      "Iteration: 399/750.\n",
      "Iteration: 400/750.\n",
      "Iteration: 401/750.\n",
      "Iteration: 402/750.\n",
      "Iteration: 403/750.\n",
      "Iteration: 404/750.\n",
      "Iteration: 405/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.7968578466671137, 0.12050352961368434, 0.13951782523844286]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.86, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 406/750.\n",
      "Iteration: 407/750.\n",
      "Iteration: 408/750.\n",
      "Iteration: 409/750.\n",
      "Iteration: 410/750.\n",
      "Iteration: 411/750.\n",
      "Iteration: 412/750.\n",
      "Iteration: 413/750.\n",
      "Iteration: 414/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.0725913969922685, 0.05298564169811315, 0.11045412407002203]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.94, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 415/750.\n",
      "Iteration: 416/750.\n",
      "Iteration: 417/750.\n",
      "Iteration: 418/750.\n",
      "Iteration: 419/750.\n",
      "Iteration: 420/750.\n",
      "Iteration: 421/750.\n",
      "Iteration: 422/750.\n",
      "Iteration: 423/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.4215869937130474, 0.10524540448488609, 0.06936716339445756]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.98, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 424/750.\n",
      "Iteration: 425/750.\n",
      "Iteration: 426/750.\n",
      "Iteration: 427/750.\n",
      "Iteration: 428/750.\n",
      "Iteration: 429/750.\n",
      "Iteration: 430/750.\n",
      "Iteration: 431/750.\n",
      "Iteration: 432/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.12616707832464547, 0.045277642345445346, 0.057425751483152626]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 433/750.\n",
      "Iteration: 434/750.\n",
      "Iteration: 435/750.\n",
      "Iteration: 436/750.\n",
      "Iteration: 437/750.\n",
      "Iteration: 438/750.\n",
      "Iteration: 439/750.\n",
      "Iteration: 440/750.\n",
      "Iteration: 441/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.06250334432634279, 0.06565315499960024, 0.09353956286963783]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 442/750.\n",
      "Iteration: 443/750.\n",
      "Iteration: 444/750.\n",
      "Iteration: 445/750.\n",
      "Iteration: 446/750.\n",
      "Iteration: 447/750.\n",
      "Iteration: 448/750.\n",
      "Iteration: 449/750.\n",
      "Iteration: 450/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.04103252661290321, 0.023090117515521657, 0.02139307379917506]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 451/750.\n",
      "Iteration: 452/750.\n",
      "Iteration: 453/750.\n",
      "Iteration: 454/750.\n",
      "Iteration: 455/750.\n",
      "Iteration: 456/750.\n",
      "Iteration: 457/750.\n",
      "Iteration: 458/750.\n",
      "Iteration: 459/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.061574340908845206, 0.03061612113091562, 0.0361981308188108]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 460/750.\n",
      "Iteration: 461/750.\n",
      "Iteration: 462/750.\n",
      "Iteration: 463/750.\n",
      "Iteration: 464/750.\n",
      "Iteration: 465/750.\n",
      "Iteration: 466/750.\n",
      "Iteration: 467/750.\n",
      "Iteration: 468/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.04146044511708326, 0.04634376549811827, 0.02264088468215882]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 469/750.\n",
      "Iteration: 470/750.\n",
      "Iteration: 471/750.\n",
      "Iteration: 472/750.\n",
      "Iteration: 473/750.\n",
      "Iteration: 474/750.\n",
      "Iteration: 475/750.\n",
      "Iteration: 476/750.\n",
      "Iteration: 477/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.0529215937297416, 0.015122620732716927, 0.016567539291492402]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 478/750.\n",
      "Iteration: 479/750.\n",
      "Iteration: 480/750.\n",
      "Iteration: 481/750.\n",
      "Iteration: 482/750.\n",
      "Iteration: 483/750.\n",
      "Iteration: 484/750.\n",
      "Iteration: 485/750.\n",
      "Iteration: 486/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.024942579891443245, 0.014512102097624636, 0.017145682969261618]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 487/750.\n",
      "Iteration: 488/750.\n",
      "Iteration: 489/750.\n",
      "Iteration: 490/750.\n",
      "Iteration: 491/750.\n",
      "Iteration: 492/750.\n",
      "Iteration: 493/750.\n",
      "Iteration: 494/750.\n",
      "Iteration: 495/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.01651922213586791, 0.0232326344336274, 0.012857308652123273]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 496/750.\n",
      "Iteration: 497/750.\n",
      "Iteration: 498/750.\n",
      "Iteration: 499/750.\n",
      "Iteration: 500/750.\n",
      "Iteration: 501/750.\n",
      "Iteration: 502/750.\n",
      "Iteration: 503/750.\n",
      "Iteration: 504/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.01592147211275496, 0.009630283114641458, 0.019034746972734173]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 505/750.\n",
      "Iteration: 506/750.\n",
      "Iteration: 507/750.\n",
      "Iteration: 508/750.\n",
      "Iteration: 509/750.\n",
      "Iteration: 510/750.\n",
      "Iteration: 511/750.\n",
      "Iteration: 512/750.\n",
      "Iteration: 513/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.01378188759272413, 0.015363995120761626, 0.015868681066423168]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 514/750.\n",
      "Iteration: 515/750.\n",
      "Iteration: 516/750.\n",
      "Iteration: 517/750.\n",
      "Iteration: 518/750.\n",
      "Iteration: 519/750.\n",
      "Iteration: 520/750.\n",
      "Iteration: 521/750.\n",
      "Iteration: 522/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.009422600680863272, 0.012448477097153542, 0.011722024897707557]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 523/750.\n",
      "Iteration: 524/750.\n",
      "Iteration: 525/750.\n",
      "Iteration: 526/750.\n",
      "Iteration: 527/750.\n",
      "Iteration: 528/750.\n",
      "Iteration: 529/750.\n",
      "Iteration: 530/750.\n",
      "Iteration: 531/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.014234979332353945, 0.01259274166389674, 0.010110019908980701]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 532/750.\n",
      "Iteration: 533/750.\n",
      "Iteration: 534/750.\n",
      "Iteration: 535/750.\n",
      "Iteration: 536/750.\n",
      "Iteration: 537/750.\n",
      "Iteration: 538/750.\n",
      "Iteration: 539/750.\n",
      "Iteration: 540/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.011304716332335658, 0.01568272955105841, 0.009243550549294243]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 541/750.\n",
      "Iteration: 542/750.\n",
      "Iteration: 543/750.\n",
      "Iteration: 544/750.\n",
      "Iteration: 545/750.\n",
      "Iteration: 546/750.\n",
      "Iteration: 547/750.\n",
      "Iteration: 548/750.\n",
      "Iteration: 549/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.009461286540340778, 0.015398113683632223, 0.007788799428446233]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 550/750.\n",
      "Iteration: 551/750.\n",
      "Iteration: 552/750.\n",
      "Iteration: 553/750.\n",
      "Iteration: 554/750.\n",
      "Iteration: 555/750.\n",
      "Iteration: 556/750.\n",
      "Iteration: 557/750.\n",
      "Iteration: 558/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.023462035444168056, 0.0072876130475124055, 0.007217936771467708]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 559/750.\n",
      "Iteration: 560/750.\n",
      "Iteration: 561/750.\n",
      "Iteration: 562/750.\n",
      "Iteration: 563/750.\n",
      "Iteration: 564/750.\n",
      "Iteration: 565/750.\n",
      "Iteration: 566/750.\n",
      "Iteration: 567/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.011486809869905357, 0.014080469611488039, 0.006042325705134973]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 568/750.\n",
      "Iteration: 569/750.\n",
      "Iteration: 570/750.\n",
      "Iteration: 571/750.\n",
      "Iteration: 572/750.\n",
      "Iteration: 573/750.\n",
      "Iteration: 574/750.\n",
      "Iteration: 575/750.\n",
      "Iteration: 576/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.00531625625296299, 0.008090467558170616, 0.007376739239527481]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 577/750.\n",
      "Iteration: 578/750.\n",
      "Iteration: 579/750.\n",
      "Iteration: 580/750.\n",
      "Iteration: 581/750.\n",
      "Iteration: 582/750.\n",
      "Iteration: 583/750.\n",
      "Iteration: 584/750.\n",
      "Iteration: 585/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.011827042260566472, 0.004132437648233549, 0.00841132813573615]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 586/750.\n",
      "Iteration: 587/750.\n",
      "Iteration: 588/750.\n",
      "Iteration: 589/750.\n",
      "Iteration: 590/750.\n",
      "Iteration: 591/750.\n",
      "Iteration: 592/750.\n",
      "Iteration: 593/750.\n",
      "Iteration: 594/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.012874743552425738, 0.006268778404724124, 0.008973090645707517]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 595/750.\n",
      "Iteration: 596/750.\n",
      "Iteration: 597/750.\n",
      "Iteration: 598/750.\n",
      "Iteration: 599/750.\n",
      "Iteration: 600/750.\n",
      "Iteration: 601/750.\n",
      "Iteration: 602/750.\n",
      "Iteration: 603/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.01861776886689462, 0.00892701187826333, 0.0037573490446317624]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 604/750.\n",
      "Iteration: 605/750.\n",
      "Iteration: 606/750.\n",
      "Iteration: 607/750.\n",
      "Iteration: 608/750.\n",
      "Iteration: 609/750.\n",
      "Iteration: 610/750.\n",
      "Iteration: 611/750.\n",
      "Iteration: 612/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.01449660654662139, 0.01239944670797838, 0.019480864669444573]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 613/750.\n",
      "Iteration: 614/750.\n",
      "Iteration: 615/750.\n",
      "Iteration: 616/750.\n",
      "Iteration: 617/750.\n",
      "Iteration: 618/750.\n",
      "Iteration: 619/750.\n",
      "Iteration: 620/750.\n",
      "Iteration: 621/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.007348764169813485, 0.005429214354160296, 0.009889597384216563]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 622/750.\n",
      "Iteration: 623/750.\n",
      "Iteration: 624/750.\n",
      "Iteration: 625/750.\n",
      "Iteration: 626/750.\n",
      "Iteration: 627/750.\n",
      "Iteration: 628/750.\n",
      "Iteration: 629/750.\n",
      "Iteration: 630/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.007102762083355663, 0.006198899235130445, 0.006963071672403594]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 631/750.\n",
      "Iteration: 632/750.\n",
      "Iteration: 633/750.\n",
      "Iteration: 634/750.\n",
      "Iteration: 635/750.\n",
      "Iteration: 636/750.\n",
      "Iteration: 637/750.\n",
      "Iteration: 638/750.\n",
      "Iteration: 639/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.010189974498328942, 0.006265418445138659, 0.009006211923601624]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 640/750.\n",
      "Iteration: 641/750.\n",
      "Iteration: 642/750.\n",
      "Iteration: 643/750.\n",
      "Iteration: 644/750.\n",
      "Iteration: 645/750.\n",
      "Iteration: 646/750.\n",
      "Iteration: 647/750.\n",
      "Iteration: 648/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.009768225769362357, 0.004369052790291455, 0.009330586000170749]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 649/750.\n",
      "Iteration: 650/750.\n",
      "Iteration: 651/750.\n",
      "Iteration: 652/750.\n",
      "Iteration: 653/750.\n",
      "Iteration: 654/750.\n",
      "Iteration: 655/750.\n",
      "Iteration: 656/750.\n",
      "Iteration: 657/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.006903764936367092, 0.008378244942623849, 0.006026925378267827]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 658/750.\n",
      "Iteration: 659/750.\n",
      "Iteration: 660/750.\n",
      "Iteration: 661/750.\n",
      "Iteration: 662/750.\n",
      "Iteration: 663/750.\n",
      "Iteration: 664/750.\n",
      "Iteration: 665/750.\n",
      "Iteration: 666/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.005647861855906821, 0.006373796991816481, 0.0036909966583682525]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 667/750.\n",
      "Iteration: 668/750.\n",
      "Iteration: 669/750.\n",
      "Iteration: 670/750.\n",
      "Iteration: 671/750.\n",
      "Iteration: 672/750.\n",
      "Iteration: 673/750.\n",
      "Iteration: 674/750.\n",
      "Iteration: 675/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.0035338558313725097, 0.004852368511403443, 0.006987997809365887]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 676/750.\n",
      "Iteration: 677/750.\n",
      "Iteration: 678/750.\n",
      "Iteration: 679/750.\n",
      "Iteration: 680/750.\n",
      "Iteration: 681/750.\n",
      "Iteration: 682/750.\n",
      "Iteration: 683/750.\n",
      "Iteration: 684/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.004503677118881455, 0.004347344683964638, 0.005801303881045078]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 685/750.\n",
      "Iteration: 686/750.\n",
      "Iteration: 687/750.\n",
      "Iteration: 688/750.\n",
      "Iteration: 689/750.\n",
      "Iteration: 690/750.\n",
      "Iteration: 691/750.\n",
      "Iteration: 692/750.\n",
      "Iteration: 693/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.008569560094122864, 0.004063908132902382, 0.00772795803548363]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 694/750.\n",
      "Iteration: 695/750.\n",
      "Iteration: 696/750.\n",
      "Iteration: 697/750.\n",
      "Iteration: 698/750.\n",
      "Iteration: 699/750.\n",
      "Iteration: 700/750.\n",
      "Iteration: 701/750.\n",
      "Iteration: 702/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.004720701108118153, 0.004657498544578502, 0.006138915366123302]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 703/750.\n",
      "Iteration: 704/750.\n",
      "Iteration: 705/750.\n",
      "Iteration: 706/750.\n",
      "Iteration: 707/750.\n",
      "Iteration: 708/750.\n",
      "Iteration: 709/750.\n",
      "Iteration: 710/750.\n",
      "Iteration: 711/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.003572075049107347, 0.0035199080476802816, 0.004912580903612035]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 712/750.\n",
      "Iteration: 713/750.\n",
      "Iteration: 714/750.\n",
      "Iteration: 715/750.\n",
      "Iteration: 716/750.\n",
      "Iteration: 717/750.\n",
      "Iteration: 718/750.\n",
      "Iteration: 719/750.\n",
      "Iteration: 720/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.010649138571592715, 0.0077419436390885115, 0.007040307072026977]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 721/750.\n",
      "Iteration: 722/750.\n",
      "Iteration: 723/750.\n",
      "Iteration: 724/750.\n",
      "Iteration: 725/750.\n",
      "Iteration: 726/750.\n",
      "Iteration: 727/750.\n",
      "Iteration: 728/750.\n",
      "Iteration: 729/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.005056200145948542, 0.0028941767692891984, 0.0044539151251743285]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 730/750.\n",
      "Iteration: 731/750.\n",
      "Iteration: 732/750.\n",
      "Iteration: 733/750.\n",
      "Iteration: 734/750.\n",
      "Iteration: 735/750.\n",
      "Iteration: 736/750.\n",
      "Iteration: 737/750.\n",
      "Iteration: 738/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.005925159649311415, 0.005571045170012228, 0.002954864165883129]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 739/750.\n",
      "Iteration: 740/750.\n",
      "Iteration: 741/750.\n",
      "Iteration: 742/750.\n",
      "Iteration: 743/750.\n",
      "Iteration: 744/750.\n",
      "Iteration: 745/750.\n",
      "Iteration: 746/750.\n",
      "Iteration: 747/750.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.304373733952622\n",
      "Loss latest three: [0.00718462533908763, 0.004472624064811412, 0.005578008915382949]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "\n",
      "\n",
      "Iteration: 748/750.\n",
      "Iteration: 749/750.\n",
      "Iteration: 750/750.\n",
      "\n",
      "\n",
      "----------------FINAL OUTPUT----------------\n",
      "  Train acc: 1.0, Val acc: 0.0\n",
      "Loss history: [2.304373733952622, 2.3001941262256884, 2.303508335898844, 2.2942834211171674, 2.302257539684362, 2.2846940314338897, 2.2667797754626706, 2.275256568294465, 2.2556253335188514, 2.255724245474301, 2.343866761998466, 2.273996881707418, 2.277199410984449, 2.2972349987580034, 2.237717600372464, 2.2727310367417854, 2.204103899788421, 2.258343837284645, 2.2590217321572386, 2.2410365036727793, 2.21648210922612, 2.257682756751057, 2.1638925675409975, 2.3051962858235155, 2.1944675031511447, 2.2390716805564996, 2.281966984895512, 2.2260888371900878, 2.3167035067377824, 2.1894941797773226, 2.2825784282113477, 2.3185600416673253, 2.2054036260114587, 2.2104394749579153, 2.186158200387568, 2.314671231617274, 2.1841281528840213, 2.2162186641313677, 2.2318659828023284, 2.215329853793867, 2.2706347464264036, 2.238357849701217, 2.262181205808585, 2.1475236738810057, 2.1927888638495125, 2.1374337344344143, 2.334798103876355, 2.314091974017603, 2.100504717556598, 2.310267870364869, 2.08967420692944, 2.1454005350969774, 2.3007085399330136, 2.254623388762617, 2.2115336061775284, 2.237354164274268, 2.2526442310975536, 2.1206935324215714, 2.091221586124789, 2.2756473590375514, 2.2224804656011212, 2.132319431106546, 2.160046661484646, 2.1065756443487866, 2.294840035022363, 2.304341258435041, 2.260400324063611, 2.1889218481103163, 2.1682860883507287, 2.062876644831836, 2.1102064822282625, 2.342895680648545, 2.272264401800523, 2.2724597301609637, 2.2953864638718358, 2.2955029132038853, 2.027709232618005, 2.335128325909235, 2.245438425118237, 2.1422264296376468, 2.328747695698904, 2.1241701530745263, 2.211319922999345, 2.404273097629641, 2.1776793791750038, 2.311223172066713, 2.253595040493041, 2.2147544396521504, 2.182436801416794, 2.3272995062125292, 2.14755198939707, 2.224311403857144, 2.193804558545494, 2.27463863108589, 1.983868980764864, 2.0552797263934353, 2.141941328348959, 2.112567311006279, 2.2184563897284577, 2.2253812604806753, 2.155493279442576, 2.04488320323522, 2.208044770196635, 2.2270848300181565, 2.0493456358370006, 2.280583389083929, 2.0969955815327004, 1.8145079639294601, 1.733432267376918, 2.361115924278763, 2.011099680766377, 2.1170719359633607, 2.021971444499357, 2.1079372889774723, 1.9084026833757428, 2.1167140453380213, 2.3740182895421293, 1.8632060517607802, 2.076402464514721, 2.215907711831237, 2.4446407943538606, 2.1757815782797603, 2.1450355930985054, 2.346263581167927, 1.8869907552364524, 2.461087123313006, 2.0273978330067357, 2.1504613281145346, 2.088402859836269, 2.198812654945001, 2.2531395007423036, 2.059688649164976, 1.810131401373492, 2.1380569646358207, 2.21698438382967, 2.4823956220819454, 2.1195679167362993, 1.971004143341482, 2.0001928766222687, 2.0518658433650567, 1.8909377547150426, 1.8719002668156919, 2.0312890920937923, 2.0585284689321237, 1.9562310562304412, 2.2622172964686587, 2.3135086353729846, 1.7294282976062227, 2.1329809755411775, 1.6555943980913044, 1.7564821912496176, 1.9865067476393694, 1.943536227476239, 1.978874337334997, 2.2721219696167485, 1.3500510895461262, 1.945111821369137, 2.128804782670874, 2.0435319259775824, 1.8245981558917728, 1.9881851896046179, 1.9763830868347005, 1.410153978585237, 1.9877138408190764, 1.5592077802792934, 2.1382530196473235, 1.6794143710999903, 1.7629319662927774, 2.080556145782496, 1.624516182196767, 1.592757630272275, 1.6688359778036816, 2.1488719483329546, 1.749666852250499, 1.8956478752597332, 1.9571766940492932, 1.8259836749279672, 1.69090942977038, 1.9665592430540089, 1.8076948352084192, 1.7684841448798214, 1.9426014940154408, 1.7010945896422616, 1.7521388139652931, 1.6375481266407115, 1.6952798468192982, 1.7687937906780133, 1.5499046858392924, 1.7933401927001027, 1.6386323965508027, 1.6667082364842258, 1.5691874815949882, 1.6214483501900927, 1.6512104077564604, 1.3213007010113835, 1.8762247744837355, 1.8434306913313045, 1.646268527799408, 1.9997997873481772, 1.6309667060129955, 1.1701267890877338, 1.4891833294811843, 1.70317424237858, 1.5404069149122528, 1.5854351235733253, 1.6089370888972951, 2.058576125229221, 1.4337536442625005, 1.6372614896313955, 1.8904958727682826, 1.607664259219223, 1.7799635780008747, 1.7339694812403939, 1.784510600880739, 1.7362789713859994, 1.446772620741509, 1.7854874993092509, 1.7322336368741515, 1.4470217991003047, 1.287346335620474, 1.589470404148197, 1.4931802740816207, 1.367081370829135, 1.7940175672363345, 1.0815554925170499, 1.7961263543101298, 1.6367526511272754, 1.5904461673912726, 1.2824299904658127, 1.4529797833257037, 1.8568912108204836, 1.5370650534071886, 1.4994189511311506, 1.2382007588936237, 1.8971348537323285, 1.6130488323359, 1.1399157682988508, 1.6915364333186673, 1.1268907625369757, 1.7847238692913756, 1.2365533918378877, 1.4200319681257227, 1.8429169176312614, 1.3573146631657107, 1.3613831185322685, 1.4558154294226406, 1.2780111208324212, 0.7886692002199776, 1.2513134697770254, 1.565128906395955, 0.9596664751092816, 1.229862366192645, 1.081924466891789, 1.111507314381988, 0.9787962615669147, 0.6116717571822116, 0.9510060655520807, 1.1903071320327847, 0.9498037380592687, 1.0349733756213138, 1.866582896709912, 1.0865466015454803, 0.6267400342107526, 0.9140208774985443, 0.9798408473607394, 1.1648825935370135, 1.2159846140092239, 1.1245383727738474, 1.0784979611253103, 1.2490428991528333, 0.6407967725801771, 1.068668324933873, 0.6870142273109042, 0.927031672449401, 0.4454871539636873, 0.5097538075567264, 0.6562065181910253, 0.6959926880204463, 0.3588537720135193, 0.6224377220658721, 1.7307132444984323, 2.3000145835268153, 1.327206831417534, 0.9257733388814313, 1.2153523303143385, 0.7398576115325761, 0.9311622601760052, 1.2553016971047823, 1.133802533321011, 0.5773086413340248, 0.8756150802856881, 1.0928458184275962, 1.0155751885345552, 0.8983073758017677, 0.6652714303068812, 0.8960706763006234, 0.9986283064194643, 0.44583114642604027, 1.0500690080684956, 0.8119711476721613, 1.0797259989022492, 1.1591392809881758, 0.5589498619539535, 0.7121118251675107, 0.7926670437404754, 0.42816989121324867, 1.3000331944944883, 0.7871258827220289, 0.6142199270606485, 0.7738938012672615, 0.7036560755282376, 0.7959690693617248, 0.5999820458657846, 0.5932785082964959, 0.726994351994561, 0.3667169119883454, 0.40103360430521057, 1.1649978002926777, 0.6092126977255794, 0.35891871619390314, 0.4949443402085074, 0.5185312490896163, 0.6175317543712656, 0.8038231033579081, 0.2553670226039733, 0.3339115350747408, 0.5362680466660616, 0.39771775162361667, 0.5402554749542119, 0.3149396882057357, 0.3721801906432122, 0.48718576388629914, 0.7962355987553126, 0.4918489195598988, 0.25226505624543255, 0.15026628045480792, 0.7376997211704017, 0.7590797499739401, 0.38793019759373837, 0.45703582118824787, 0.4455654487389473, 0.6131551995768093, 0.7531816428423523, 0.41108880104674034, 0.29370583260422584, 0.5912291931437593, 0.21482833435145687, 0.3093758810872063, 0.3600096173785579, 0.24084408281141323, 0.44647209122066905, 0.19964547106471958, 0.29215732521324317, 0.3130263662296953, 0.24575520068006115, 0.13825717262552809, 0.36514773550099505, 0.21452174186284392, 0.7726704370448986, 0.45634574429319485, 0.3122368050781844, 0.05776697602024016, 0.1854773946131284, 0.5503131811487881, 0.28743680936224414, 0.3771210398268423, 0.3295559392454376, 0.5024100581564189, 0.4312812816763762, 0.16620777183144497, 0.3508810148044679, 0.16407267229376166, 0.24960096171718807, 0.1099984918803749, 0.25971806004965553, 0.17926499542018026, 0.08146063120636833, 0.14776578401052776, 0.21537143605947007, 0.2795480336964628, 0.09655567892633884, 0.12237956574301555, 0.16769773791632528, 0.2053869851110839, 0.12690574396394314, 0.14626360697760457, 0.27095333847671027, 0.08534698567233853, 0.11189195969800379, 0.04948403915152692, 0.04686323923690525, 0.025367597061111982, 0.09769466920014397, 0.05127357695541305, 0.03077601974795683, 0.09978465409029391, 0.08285593039943791, 0.08336358672151922, 0.296778078865661, 0.17747115931833968, 0.1652346467031926, 0.9328940050886166, 0.7968578466671137, 0.12050352961368434, 0.13951782523844286, 1.5305899356410402, 0.5357295783816367, 0.25324114847254797, 0.20073981006510697, 0.2233964466265701, 0.24295233857099802, 0.0725913969922685, 0.05298564169811315, 0.11045412407002203, 0.4968842232084023, 0.4371184149349949, 1.4942854079244765, 0.14652136624419634, 0.15948199664138601, 0.31005253162862445, 0.4215869937130474, 0.10524540448488609, 0.06936716339445756, 0.028366359470039727, 0.06622668476741339, 0.05638199354172865, 0.09400144409042956, 0.07466870217471178, 0.06626108132385396, 0.12616707832464547, 0.045277642345445346, 0.057425751483152626, 0.04994955847014086, 0.0357417098933736, 0.05729720134239927, 0.0380212057132703, 0.022473503359104828, 0.10765829459673788, 0.06250334432634279, 0.06565315499960024, 0.09353956286963783, 0.06560212981643271, 0.08354932861172695, 0.04462981999921014, 0.035550513887743794, 0.04394705129940501, 0.02369816594578828, 0.04103252661290321, 0.023090117515521657, 0.02139307379917506, 0.027697651054030034, 0.047769814246892944, 0.030182028669400757, 0.0102742852100776, 0.059926868062292596, 0.04037808543071978, 0.061574340908845206, 0.03061612113091562, 0.0361981308188108, 0.11871876214480453, 0.02711514343001913, 0.030976357804806915, 0.02748553665434398, 0.021510635515774104, 0.019245058776419233, 0.04146044511708326, 0.04634376549811827, 0.02264088468215882, 0.0392049201174265, 0.006836150385772681, 0.020894789802302166, 0.007139786195501827, 0.02259755622753597, 0.027050669788486522, 0.0529215937297416, 0.015122620732716927, 0.016567539291492402, 0.02348200079721816, 0.047905095082202344, 0.039325259914641225, 0.024334853338468125, 0.021740722869688995, 0.023134379727173526, 0.024942579891443245, 0.014512102097624636, 0.017145682969261618, 0.014833737849948887, 0.030325040465723686, 0.025440286233534438, 0.013319141922429036, 0.034172089968108166, 0.017870379722329913, 0.01651922213586791, 0.0232326344336274, 0.012857308652123273, 0.015452327329549577, 0.011569529579973335, 0.01606499371361354, 0.0254439479702484, 0.005483964579962343, 0.03755204699226703, 0.01592147211275496, 0.009630283114641458, 0.019034746972734173, 0.026398567195622516, 0.03537987162931496, 0.027986562536791812, 0.02010668388338037, 0.016697632867202675, 0.009339512873053887, 0.01378188759272413, 0.015363995120761626, 0.015868681066423168, 0.01925600705433446, 0.0162695367607405, 0.011625233886545794, 0.01253275354471131, 0.011914555606381896, 0.01619025255379878, 0.009422600680863272, 0.012448477097153542, 0.011722024897707557, 0.010980889587049568, 0.019698072511830034, 0.008106453248236514, 0.024261969313031276, 0.01664952295875847, 0.02209273164248876, 0.014234979332353945, 0.01259274166389674, 0.010110019908980701, 0.01836947418802377, 0.004577332326274213, 0.015563620312384625, 0.010464749807813395, 0.031001429885089412, 0.013934688045500736, 0.011304716332335658, 0.01568272955105841, 0.009243550549294243, 0.015496693887549928, 0.009004373868964697, 0.012918905489206253, 0.014236148726334837, 0.007616032784086871, 0.011923181484998463, 0.009461286540340778, 0.015398113683632223, 0.007788799428446233, 0.015113599583182646, 0.021553783004299915, 0.01639392292162116, 0.008440645614175579, 0.012806029385153225, 0.00874927098586356, 0.023462035444168056, 0.0072876130475124055, 0.007217936771467708, 0.01782194198233033, 0.01207155237968858, 0.011752139959465968, 0.0062419636601818256, 0.013935065942190003, 0.004752039193712535, 0.011486809869905357, 0.014080469611488039, 0.006042325705134973, 0.011840332992193676, 0.007672160077540881, 0.014259665348858825, 0.015814291732097942, 0.00996384980651853, 0.009753477038422397, 0.00531625625296299, 0.008090467558170616, 0.007376739239527481, 0.007561630080063654, 0.0048067482718969325, 0.01455321646510775, 0.009534030105478314, 0.006823118986963241, 0.012258925685638085, 0.011827042260566472, 0.004132437648233549, 0.00841132813573615, 0.01163982120993098, 0.008515131479418538, 0.01037824653241514, 0.011728255494906156, 0.008709818568441973, 0.009080710544627063, 0.012874743552425738, 0.006268778404724124, 0.008973090645707517, 0.00789153802921903, 0.010576438959029565, 0.01370747626669494, 0.010576139975443058, 0.008087506534542226, 0.008060868983640241, 0.01861776886689462, 0.00892701187826333, 0.0037573490446317624, 0.016570516146758602, 0.01005173608680383, 0.0073664631730625445, 0.011563938003768235, 0.005498498875273184, 0.007866180621534696, 0.01449660654662139, 0.01239944670797838, 0.019480864669444573, 0.008557679950982921, 0.008240917611021543, 0.007697114025691263, 0.004959484933526304, 0.0076770077535636624, 0.005585870863360915, 0.007348764169813485, 0.005429214354160296, 0.009889597384216563, 0.005898542378264898, 0.006844108812315968, 0.008954046744797191, 0.006692399897707854, 0.0068495914899186355, 0.0038659909177255446, 0.007102762083355663, 0.006198899235130445, 0.006963071672403594, 0.009938492067863744, 0.005734729090470414, 0.010965489489377032, 0.00696197330801953, 0.009014750912451984, 0.005234961588205492, 0.010189974498328942, 0.006265418445138659, 0.009006211923601624, 0.0069112952326092275, 0.006952217059568428, 0.0060525169198034105, 0.004624616378595277, 0.006300349436876483, 0.006327778553716645, 0.009768225769362357, 0.004369052790291455, 0.009330586000170749, 0.011916091975691092, 0.009587720872694959, 0.009000985560248923, 0.009036224419451375, 0.008854067363251455, 0.0062079453437354085, 0.006903764936367092, 0.008378244942623849, 0.006026925378267827, 0.0070854818870063315, 0.003817874057583822, 0.006060431005049817, 0.006294697868466508, 0.004406882469618212, 0.00513493240171714, 0.005647861855906821, 0.006373796991816481, 0.0036909966583682525, 0.003103439946741614, 0.007847740087956221, 0.0050946230776867584, 0.00548313360983361, 0.0066137318919213045, 0.008360881951082057, 0.0035338558313725097, 0.004852368511403443, 0.006987997809365887, 0.006896849101987068, 0.002169801300467622, 0.003442951170971006, 0.009987380424426477, 0.005124334851919167, 0.009748315281390522, 0.004503677118881455, 0.004347344683964638, 0.005801303881045078, 0.0054503464532295424, 0.00651982963991027, 0.006015332376606099, 0.005741210110396739, 0.004278929573893216, 0.0029834098257500016, 0.008569560094122864, 0.004063908132902382, 0.00772795803548363, 0.0061756214695268055, 0.00395160928194057, 0.011982435101879504, 0.010200359248933318, 0.006216088906124246, 0.004598530518563445, 0.004720701108118153, 0.004657498544578502, 0.006138915366123302, 0.0013014428460153586, 0.004593745286225871, 0.007867314830581736, 0.00706103713156039, 0.0047291814880302055, 0.004435165978522119, 0.003572075049107347, 0.0035199080476802816, 0.004912580903612035, 0.010671017101633795, 0.005025775275609579, 0.006725925348585075, 0.004400996638160431, 0.0057950247019024625, 0.003918046208421628, 0.010649138571592715, 0.0077419436390885115, 0.007040307072026977, 0.004485089963919566, 0.0025554286710430685, 0.004692542015776534, 0.004412933727764303, 0.0028848586911630805, 0.0043098954352949245, 0.005056200145948542, 0.0028941767692891984, 0.0044539151251743285, 0.0033704169158858266, 0.004387385607255327, 0.005898412790377338, 0.0033261656118733825, 0.005787752550088934, 0.003084066630380062, 0.005925159649311415, 0.005571045170012228, 0.002954864165883129, 0.0047262716933875164, 0.005992546982146556, 0.006148414847011039, 0.002562720094109994, 0.007561353489565804, 0.004224458399807254, 0.00718462533908763, 0.004472624064811412, 0.005578008915382949, 0.0051878711302764564, 0.007220312662910829, 0.004471873677363599]\n",
      "Accuracy history: [0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.22, 0.24, 0.24, 0.28, 0.32, 0.36, 0.32, 0.38, 0.44, 0.4, 0.36, 0.46, 0.32, 0.34, 0.52, 0.58, 0.62, 0.56, 0.62, 0.7, 0.7, 0.8, 0.7, 0.84, 0.76, 0.86, 0.92, 0.88, 0.92, 0.96, 0.96, 0.94, 0.86, 0.94, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# SGD\n",
    "sgd = ConvNet4(input_shape=input_shape, wt_scale=wt_scale, verbose=False)\n",
    "sgd.compile('sgd')\n",
    "sgd.fit(x_dev, y_dev, x_val, y_val, mini_batch_sz=mini_batch_sz, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**: Why does decreasing the mini-batch size make the loss print-outs more erratic?\n",
    "\n",
    "Answer: Decreasing the mini-batch size makes the loss print-outs more erratic because there are fewer samples that have been able to propagate back through the network, so each sample has much more of an impact on what the loss value will be. There is an averaging over all samples that takes place, so if all of the samples in a very small mini-batch are predicted successfully, loss will be low, but it will be high if it performs poorly on the mini-batch. The smaller the mini-batch, the greater the chance that there is a skewed distribution of correct and incorrect predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6d) Evaluate the different optimizers\n",
    "\n",
    "Make 2 \"high quality\" plots showing the following\n",
    "\n",
    "- Plot the accuracy (y axis) for the three optimizers as a function of training epoch (x axis).\n",
    "- Plot the loss (y axis) for the three optimizers as a function of training iteration (x axis).\n",
    "\n",
    "A high quality plot consists of:\n",
    "- A useful title\n",
    "- X and Y axis labels\n",
    "- A legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get accuracy values\n",
    "adam_acc = adam.train_acc_history\n",
    "sgd_m_acc = sgd_m.train_acc_history\n",
    "sgd_acc = sgd.train_acc_history\n",
    "\n",
    "#plot\n",
    "plt.plot(adam_acc, label=\"adam\")\n",
    "plt.plot(sgd_m_acc, label=\"sgd_m\")\n",
    "plt.plot(sgd_acc, label=\"sgd\")\n",
    "\n",
    "#make words show up\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy Over Iterations\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Training Iterations\")\n",
    "plt.show()\n",
    "\n",
    "#get loss values\n",
    "adam_loss = adam.loss_history\n",
    "sgd_m_loss = sgd_m.loss_history\n",
    "sgd_loss = sgd.loss_history\n",
    "\n",
    "#plot\n",
    "plt.plot(adam_loss, label=\"adam\")\n",
    "plt.plot(sgd_m_loss, label=\"sgd_m\")\n",
    "plt.plot(sgd_loss, label=\"sgd\")\n",
    "\n",
    "#make words show up\n",
    "plt.legend()\n",
    "plt.title(\"Loss Over Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Training Iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**: Which optimizer works best and why do think it is best?\n",
    "\n",
    "The adam optimizer works the best. It does so because it dynamically updates learning rate.\n",
    "\n",
    "**Question 5**: What is happening with the training set accuracy and why?\n",
    "\n",
    "The training set accuracy is overfitting. It is basically memorizing the features about each image, or memorizing the dataset. Therefore, it has perfect accuracy while looking at its own training set, but once it has a non-training image, it has horrible accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Training convolutional neural network on STL-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7a) Load in STL-10 at 32x32 resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images are: (5000, 96, 96, 3)\n",
      "Labels are: (5000,)\n",
      "Resizing 5000 images to 32x32...Done!\n",
      "Saving Numpy arrays the images and labels to ./numpy...Done!\n",
      "imgs.shape (5000, 32, 32, 3)\n",
      "data.shape (5000, 3072)\n",
      "Train data shape:  (4548, 3072)\n",
      "Train labels shape:  (4548,)\n",
      "Test data shape:  (400, 3072)\n",
      "Test labels shape:  (400,)\n",
      "Validation data shape:  (2, 3072)\n",
      "Validation labels shape:  (2,)\n",
      "dev data shape:  (50, 3072)\n",
      "dev labels shape:  (50,)\n"
     ]
    }
   ],
   "source": [
    "# Download the STL-10 dataset from the internet, convert it to Numpy ndarray, resize to 32x32\n",
    "# cache it locally on your computer for faster loading next time.\n",
    "load_stl10_dataset.purge_cached_dataset()\n",
    "stl_imgs, stl_labels = load_stl10_dataset.load(scale_fact=3)\n",
    "# preprocess\n",
    "stl_imgs, stl_labels = preprocess_data.preprocess_stl(stl_imgs, stl_labels)\n",
    "# create splits\n",
    "x_train, y_train, x_test, y_test, x_val, y_val, x_dev, y_dev = preprocess_data.create_splits(\n",
    "    stl_imgs, stl_labels, n_train_samps=4548, n_test_samps=400, n_valid_samps=2, n_dev_samps=50)\n",
    "\n",
    "print ('Train data shape: ', x_train.shape)\n",
    "print ('Train labels shape: ', y_train.shape)\n",
    "print ('Test data shape: ', x_test.shape)\n",
    "print ('Test labels shape: ', y_test.shape)\n",
    "print ('Validation data shape: ', x_val.shape)\n",
    "print ('Validation labels shape: ', y_val.shape)\n",
    "print ('dev data shape: ', x_dev.shape)\n",
    "print ('dev labels shape: ', y_dev.shape)\n",
    "\n",
    "classes = np.loadtxt(os.path.join('data', 'stl10_binary', 'class_names.txt'), dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7b) Set up accelerated convolution and max pooling layers\n",
    "\n",
    "As you may have noticed, we had to downsize STL-10 to 16x16 resolution to train the network on the dev set (N=50) in a reasonable amount of time. The training set is N=4000, how will we ever manage to process that amount of data!?\n",
    "\n",
    "On one hand, this is an unfortunate inevitable reality of working with large (\"big\") datasets: you can easily find a dataset that is too time consuming to process for any computer, despite how fast/many CPU/GPUs it has.\n",
    "\n",
    "On the other hand, we can do better for this project and STL-10 :) If you were to time (profile) different parts of the training process, you'd notice that largest bottleneck is convolution and max pooling operations (both forward/backward). You implemented those operations intuitively, which does not always yield the best performance. **By swapping out forward/backward convolution and maxpooling for implementations that use different algorithms (im2col, reshaping) that are compiled to C code, we will speed up training up by several orders of magnitude**.\n",
    "\n",
    "Follow these steps to subsitute in the \"accelerated\" convolution and max pooling layers.\n",
    "\n",
    "- Install the `cython` python package: `pip3 install cython` (or `pip3 install cython --user` if working in Davis 102)\n",
    "- Dowload files `im2col_cython.pyx`, `accelerated_layer.py`, `setup.py` from the project website. Put them in your base project folder.\n",
    "- Open terminal, `cd` to Project directory.\n",
    "- Compile the im2col functions: `python3 setup.py build_ext --inplace`. A `.c` and `.so` file should have appeared in your project folder.\n",
    "- Restart Jupyter Notebook kernel\n",
    "- Create a class called `Conv4NetAccel` in `network.py` by copy-pasting the contents of `Conv4Net`. Import `accelerated_layer` at the top and replace the `Conv2D` and `MaxPool2D` layers with `Conv2DAccel` and `MaxPool2DAccel`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7c) Training convolutional neural network on STL-10\n",
    "\n",
    "You are now ready to train on the entire training set.\n",
    "\n",
    "- Create a `Conv4NetAccel` object with hyperparameters of your choice.\n",
    "- Your goal is to achieve 45% accuracy on the test and/or validation set.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- I suggest using your intuition about hyperparameters and over/underfitting to guide your choice, rather than a grid search. This should not be overly challenging.\n",
    "- Use the best / most efficient optimizer based on your prior analysis.\n",
    "- It should take on the order of 1 sec per training iteration. If that's way off, seek help as something could be wrong with running the acclerated code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network import ConvNet4Accel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4548, 3, 32, 32)\n",
      "(5000, 3, 32, 32)\n",
      "Starting to train...\n",
      "900 iterations. 45 iter/epoch.\n",
      "Iteration: 1/900.\n",
      "Time taken for iteration 0: 1.0902049541473389\n",
      "Estimated time to complete: 981.184458732605\n",
      "Iteration: 2/900.\n",
      "Iteration: 3/900.\n",
      "Iteration: 4/900.\n",
      "Iteration: 5/900.\n",
      "Iteration: 6/900.\n",
      "Iteration: 7/900.\n",
      "Iteration: 8/900.\n",
      "Iteration: 9/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [2.3023472197184054, 2.3015962716512393, 2.300246923982207]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.074, Val acc: 0.06\n",
      "\n",
      "\n",
      "Iteration: 10/900.\n",
      "Iteration: 11/900.\n",
      "Iteration: 12/900.\n",
      "Iteration: 13/900.\n",
      "Iteration: 14/900.\n",
      "Iteration: 15/900.\n",
      "Iteration: 16/900.\n",
      "Iteration: 17/900.\n",
      "Iteration: 18/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [2.2834916877801397, 2.181072621319521, 2.224486294256639]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.156, Val acc: 0.08\n",
      "\n",
      "\n",
      "Iteration: 19/900.\n",
      "Iteration: 20/900.\n",
      "Iteration: 21/900.\n",
      "Iteration: 22/900.\n",
      "Iteration: 23/900.\n",
      "Iteration: 24/900.\n",
      "Iteration: 25/900.\n",
      "Iteration: 26/900.\n",
      "Iteration: 27/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [2.2700961926678915, 2.165698444113134, 2.2467884203546866]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.168, Val acc: 0.08\n",
      "\n",
      "\n",
      "Iteration: 28/900.\n",
      "Iteration: 29/900.\n",
      "Iteration: 30/900.\n",
      "Iteration: 31/900.\n",
      "Iteration: 32/900.\n",
      "Iteration: 33/900.\n",
      "Iteration: 34/900.\n",
      "Iteration: 35/900.\n",
      "Iteration: 36/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [2.2047044796823663, 2.092912992117795, 2.1518044880726364]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.188, Val acc: 0.14\n",
      "\n",
      "\n",
      "Iteration: 37/900.\n",
      "Iteration: 38/900.\n",
      "Iteration: 39/900.\n",
      "Iteration: 40/900.\n",
      "Iteration: 41/900.\n",
      "Iteration: 42/900.\n",
      "Iteration: 43/900.\n",
      "Iteration: 44/900.\n",
      "Iteration: 45/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [2.1368221835032823, 2.033727887405248, 2.206101089998645]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.218, Val acc: 0.16\n",
      "\n",
      "\n",
      "Iteration: 46/900.\n",
      "Iteration: 47/900.\n",
      "Iteration: 48/900.\n",
      "Iteration: 49/900.\n",
      "Iteration: 50/900.\n",
      "Iteration: 51/900.\n",
      "Iteration: 52/900.\n",
      "Iteration: 53/900.\n",
      "Iteration: 54/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [2.0404426808205707, 2.06843584438159, 2.03447305582438]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.244, Val acc: 0.12\n",
      "\n",
      "\n",
      "Iteration: 55/900.\n",
      "Iteration: 56/900.\n",
      "Iteration: 57/900.\n",
      "Iteration: 58/900.\n",
      "Iteration: 59/900.\n",
      "Iteration: 60/900.\n",
      "Iteration: 61/900.\n",
      "Iteration: 62/900.\n",
      "Iteration: 63/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [2.009086863948008, 1.9111959531377714, 1.8807404241732095]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.214, Val acc: 0.2\n",
      "\n",
      "\n",
      "Iteration: 64/900.\n",
      "Iteration: 65/900.\n",
      "Iteration: 66/900.\n",
      "Iteration: 67/900.\n",
      "Iteration: 68/900.\n",
      "Iteration: 69/900.\n",
      "Iteration: 70/900.\n",
      "Iteration: 71/900.\n",
      "Iteration: 72/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.9936949660723624, 2.065563280592904, 1.9937547963476032]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.222, Val acc: 0.22\n",
      "\n",
      "\n",
      "Iteration: 73/900.\n",
      "Iteration: 74/900.\n",
      "Iteration: 75/900.\n",
      "Iteration: 76/900.\n",
      "Iteration: 77/900.\n",
      "Iteration: 78/900.\n",
      "Iteration: 79/900.\n",
      "Iteration: 80/900.\n",
      "Iteration: 81/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.9744192765772055, 1.9470627710401467, 2.0332638280596256]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.274, Val acc: 0.18\n",
      "\n",
      "\n",
      "Iteration: 82/900.\n",
      "Iteration: 83/900.\n",
      "Iteration: 84/900.\n",
      "Iteration: 85/900.\n",
      "Iteration: 86/900.\n",
      "Iteration: 87/900.\n",
      "Iteration: 88/900.\n",
      "Iteration: 89/900.\n",
      "Iteration: 90/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [2.059381430015757, 2.045533823096536, 1.9890677354590691]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.28, Val acc: 0.24\n",
      "\n",
      "\n",
      "Iteration: 91/900.\n",
      "Iteration: 92/900.\n",
      "Iteration: 93/900.\n",
      "Iteration: 94/900.\n",
      "Iteration: 95/900.\n",
      "Iteration: 96/900.\n",
      "Iteration: 97/900.\n",
      "Iteration: 98/900.\n",
      "Iteration: 99/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.9732560627335378, 1.809160540705878, 1.943813617854872]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.268, Val acc: 0.16\n",
      "\n",
      "\n",
      "Iteration: 100/900.\n",
      "Iteration: 101/900.\n",
      "Iteration: 102/900.\n",
      "Iteration: 103/900.\n",
      "Iteration: 104/900.\n",
      "Iteration: 105/900.\n",
      "Iteration: 106/900.\n",
      "Iteration: 107/900.\n",
      "Iteration: 108/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.9226663704739166, 1.9475437194684773, 1.8862642675050745]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.222, Val acc: 0.26\n",
      "\n",
      "\n",
      "Iteration: 109/900.\n",
      "Iteration: 110/900.\n",
      "Iteration: 111/900.\n",
      "Iteration: 112/900.\n",
      "Iteration: 113/900.\n",
      "Iteration: 114/900.\n",
      "Iteration: 115/900.\n",
      "Iteration: 116/900.\n",
      "Iteration: 117/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.958998760345062, 1.8015100696238517, 1.8513453433722358]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.324, Val acc: 0.22\n",
      "\n",
      "\n",
      "Iteration: 118/900.\n",
      "Iteration: 119/900.\n",
      "Iteration: 120/900.\n",
      "Iteration: 121/900.\n",
      "Iteration: 122/900.\n",
      "Iteration: 123/900.\n",
      "Iteration: 124/900.\n",
      "Iteration: 125/900.\n",
      "Iteration: 126/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.805372379654562, 1.8557016964002022, 1.9795760593066385]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.272, Val acc: 0.22\n",
      "\n",
      "\n",
      "Iteration: 127/900.\n",
      "Iteration: 128/900.\n",
      "Iteration: 129/900.\n",
      "Iteration: 130/900.\n",
      "Iteration: 131/900.\n",
      "Iteration: 132/900.\n",
      "Iteration: 133/900.\n",
      "Iteration: 134/900.\n",
      "Iteration: 135/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.8129983176985607, 1.820793861204638, 1.8538514961285861]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.288, Val acc: 0.14\n",
      "\n",
      "\n",
      "Iteration: 136/900.\n",
      "Iteration: 137/900.\n",
      "Iteration: 138/900.\n",
      "Iteration: 139/900.\n",
      "Iteration: 140/900.\n",
      "Iteration: 141/900.\n",
      "Iteration: 142/900.\n",
      "Iteration: 143/900.\n",
      "Iteration: 144/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.6525537215843111, 1.7073029149298822, 1.841767098471278]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.316, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 145/900.\n",
      "Iteration: 146/900.\n",
      "Iteration: 147/900.\n",
      "Iteration: 148/900.\n",
      "Iteration: 149/900.\n",
      "Iteration: 150/900.\n",
      "Iteration: 151/900.\n",
      "Iteration: 152/900.\n",
      "Iteration: 153/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.7227366822573238, 1.7838534974350757, 1.6778771973789937]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.306, Val acc: 0.2\n",
      "\n",
      "\n",
      "Iteration: 154/900.\n",
      "Iteration: 155/900.\n",
      "Iteration: 156/900.\n",
      "Iteration: 157/900.\n",
      "Iteration: 158/900.\n",
      "Iteration: 159/900.\n",
      "Iteration: 160/900.\n",
      "Iteration: 161/900.\n",
      "Iteration: 162/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.7398923816407998, 1.7944429321990651, 1.77007997579471]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.298, Val acc: 0.24\n",
      "\n",
      "\n",
      "Iteration: 163/900.\n",
      "Iteration: 164/900.\n",
      "Iteration: 165/900.\n",
      "Iteration: 166/900.\n",
      "Iteration: 167/900.\n",
      "Iteration: 168/900.\n",
      "Iteration: 169/900.\n",
      "Iteration: 170/900.\n",
      "Iteration: 171/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.6020448058118362, 1.6802248240840525, 1.566286540275584]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.324, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 172/900.\n",
      "Iteration: 173/900.\n",
      "Iteration: 174/900.\n",
      "Iteration: 175/900.\n",
      "Iteration: 176/900.\n",
      "Iteration: 177/900.\n",
      "Iteration: 178/900.\n",
      "Iteration: 179/900.\n",
      "Iteration: 180/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.6682152743129277, 1.6686885782248493, 1.7125876688300727]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.262, Val acc: 0.28\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 181/900.\n",
      "Iteration: 182/900.\n",
      "Iteration: 183/900.\n",
      "Iteration: 184/900.\n",
      "Iteration: 185/900.\n",
      "Iteration: 186/900.\n",
      "Iteration: 187/900.\n",
      "Iteration: 188/900.\n",
      "Iteration: 189/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.7051477870750826, 1.8761990151570134, 1.7652035265293575]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.348, Val acc: 0.24\n",
      "\n",
      "\n",
      "Iteration: 190/900.\n",
      "Iteration: 191/900.\n",
      "Iteration: 192/900.\n",
      "Iteration: 193/900.\n",
      "Iteration: 194/900.\n",
      "Iteration: 195/900.\n",
      "Iteration: 196/900.\n",
      "Iteration: 197/900.\n",
      "Iteration: 198/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.596278011137305, 1.701009728017043, 1.7749903674027605]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.35, Val acc: 0.24\n",
      "\n",
      "\n",
      "Iteration: 199/900.\n",
      "Iteration: 200/900.\n",
      "Iteration: 201/900.\n",
      "Iteration: 202/900.\n",
      "Iteration: 203/900.\n",
      "Iteration: 204/900.\n",
      "Iteration: 205/900.\n",
      "Iteration: 206/900.\n",
      "Iteration: 207/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.6363141006972006, 1.7082590300892897, 1.6825777474236876]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.382, Val acc: 0.28\n",
      "\n",
      "\n",
      "Iteration: 208/900.\n",
      "Iteration: 209/900.\n",
      "Iteration: 210/900.\n",
      "Iteration: 211/900.\n",
      "Iteration: 212/900.\n",
      "Iteration: 213/900.\n",
      "Iteration: 214/900.\n",
      "Iteration: 215/900.\n",
      "Iteration: 216/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.6130065855888338, 1.6542651755300748, 1.629595698954837]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.368, Val acc: 0.28\n",
      "\n",
      "\n",
      "Iteration: 217/900.\n",
      "Iteration: 218/900.\n",
      "Iteration: 219/900.\n",
      "Iteration: 220/900.\n",
      "Iteration: 221/900.\n",
      "Iteration: 222/900.\n",
      "Iteration: 223/900.\n",
      "Iteration: 224/900.\n",
      "Iteration: 225/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.59953985041899, 1.5972963085508074, 1.7592375016321027]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.346, Val acc: 0.22\n",
      "\n",
      "\n",
      "Iteration: 226/900.\n",
      "Iteration: 227/900.\n",
      "Iteration: 228/900.\n",
      "Iteration: 229/900.\n",
      "Iteration: 230/900.\n",
      "Iteration: 231/900.\n",
      "Iteration: 232/900.\n",
      "Iteration: 233/900.\n",
      "Iteration: 234/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.5387818673117415, 1.5377811395553664, 1.4924018093529097]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.376, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 235/900.\n",
      "Iteration: 236/900.\n",
      "Iteration: 237/900.\n",
      "Iteration: 238/900.\n",
      "Iteration: 239/900.\n",
      "Iteration: 240/900.\n",
      "Iteration: 241/900.\n",
      "Iteration: 242/900.\n",
      "Iteration: 243/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.6458203189281637, 1.5723653734903849, 1.4327402892938803]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.378, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 244/900.\n",
      "Iteration: 245/900.\n",
      "Iteration: 246/900.\n",
      "Iteration: 247/900.\n",
      "Iteration: 248/900.\n",
      "Iteration: 249/900.\n",
      "Iteration: 250/900.\n",
      "Iteration: 251/900.\n",
      "Iteration: 252/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.5715581722703236, 1.716945329625865, 1.530730344499275]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.37, Val acc: 0.26\n",
      "\n",
      "\n",
      "Iteration: 253/900.\n",
      "Iteration: 254/900.\n",
      "Iteration: 255/900.\n",
      "Iteration: 256/900.\n",
      "Iteration: 257/900.\n",
      "Iteration: 258/900.\n",
      "Iteration: 259/900.\n",
      "Iteration: 260/900.\n",
      "Iteration: 261/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.4370048777171323, 1.6357765896945884, 1.4888334151718516]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.428, Val acc: 0.24\n",
      "\n",
      "\n",
      "Iteration: 262/900.\n",
      "Iteration: 263/900.\n",
      "Iteration: 264/900.\n",
      "Iteration: 265/900.\n",
      "Iteration: 266/900.\n",
      "Iteration: 267/900.\n",
      "Iteration: 268/900.\n",
      "Iteration: 269/900.\n",
      "Iteration: 270/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.5752371175243918, 1.4081161110879266, 1.6306340489678628]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.366, Val acc: 0.26\n",
      "\n",
      "\n",
      "Iteration: 271/900.\n",
      "Iteration: 272/900.\n",
      "Iteration: 273/900.\n",
      "Iteration: 274/900.\n",
      "Iteration: 275/900.\n",
      "Iteration: 276/900.\n",
      "Iteration: 277/900.\n",
      "Iteration: 278/900.\n",
      "Iteration: 279/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.48125608770169, 1.537788403344535, 1.604222268017219]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.43, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 280/900.\n",
      "Iteration: 281/900.\n",
      "Iteration: 282/900.\n",
      "Iteration: 283/900.\n",
      "Iteration: 284/900.\n",
      "Iteration: 285/900.\n",
      "Iteration: 286/900.\n",
      "Iteration: 287/900.\n",
      "Iteration: 288/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.4025976811365704, 1.562807778048466, 1.675157013842027]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.398, Val acc: 0.22\n",
      "\n",
      "\n",
      "Iteration: 289/900.\n",
      "Iteration: 290/900.\n",
      "Iteration: 291/900.\n",
      "Iteration: 292/900.\n",
      "Iteration: 293/900.\n",
      "Iteration: 294/900.\n",
      "Iteration: 295/900.\n",
      "Iteration: 296/900.\n",
      "Iteration: 297/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.5197079700173868, 1.6707745930120388, 1.4999466096175567]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.39, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 298/900.\n",
      "Iteration: 299/900.\n",
      "Iteration: 300/900.\n",
      "Iteration: 301/900.\n",
      "Iteration: 302/900.\n",
      "Iteration: 303/900.\n",
      "Iteration: 304/900.\n",
      "Iteration: 305/900.\n",
      "Iteration: 306/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.5410962888254682, 1.4280129243774276, 1.5310363327933088]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.378, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 307/900.\n",
      "Iteration: 308/900.\n",
      "Iteration: 309/900.\n",
      "Iteration: 310/900.\n",
      "Iteration: 311/900.\n",
      "Iteration: 312/900.\n",
      "Iteration: 313/900.\n",
      "Iteration: 314/900.\n",
      "Iteration: 315/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.4925742515799005, 1.3927596283164634, 1.508690009371583]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.388, Val acc: 0.26\n",
      "\n",
      "\n",
      "Iteration: 316/900.\n",
      "Iteration: 317/900.\n",
      "Iteration: 318/900.\n",
      "Iteration: 319/900.\n",
      "Iteration: 320/900.\n",
      "Iteration: 321/900.\n",
      "Iteration: 322/900.\n",
      "Iteration: 323/900.\n",
      "Iteration: 324/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.4936697422359657, 1.5041105693374868, 1.4375296485279312]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.424, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 325/900.\n",
      "Iteration: 326/900.\n",
      "Iteration: 327/900.\n",
      "Iteration: 328/900.\n",
      "Iteration: 329/900.\n",
      "Iteration: 330/900.\n",
      "Iteration: 331/900.\n",
      "Iteration: 332/900.\n",
      "Iteration: 333/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.346128605873183, 1.5986410439225338, 1.587485383650714]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.46, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 334/900.\n",
      "Iteration: 335/900.\n",
      "Iteration: 336/900.\n",
      "Iteration: 337/900.\n",
      "Iteration: 338/900.\n",
      "Iteration: 339/900.\n",
      "Iteration: 340/900.\n",
      "Iteration: 341/900.\n",
      "Iteration: 342/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.4292317884551993, 1.4717863790355916, 1.4991608387736097]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.468, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 343/900.\n",
      "Iteration: 344/900.\n",
      "Iteration: 345/900.\n",
      "Iteration: 346/900.\n",
      "Iteration: 347/900.\n",
      "Iteration: 348/900.\n",
      "Iteration: 349/900.\n",
      "Iteration: 350/900.\n",
      "Iteration: 351/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.5091854794556567, 1.4310362884495524, 1.553210825786967]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.494, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 352/900.\n",
      "Iteration: 353/900.\n",
      "Iteration: 354/900.\n",
      "Iteration: 355/900.\n",
      "Iteration: 356/900.\n",
      "Iteration: 357/900.\n",
      "Iteration: 358/900.\n",
      "Iteration: 359/900.\n",
      "Iteration: 360/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.3335278970399405, 1.2865105608502907, 1.6632731896000585]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.48, Val acc: 0.28\n",
      "\n",
      "\n",
      "Iteration: 361/900.\n",
      "Iteration: 362/900.\n",
      "Iteration: 363/900.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 364/900.\n",
      "Iteration: 365/900.\n",
      "Iteration: 366/900.\n",
      "Iteration: 367/900.\n",
      "Iteration: 368/900.\n",
      "Iteration: 369/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.2925865972131236, 1.3304528074219732, 1.4809099371126546]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.466, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 370/900.\n",
      "Iteration: 371/900.\n",
      "Iteration: 372/900.\n",
      "Iteration: 373/900.\n",
      "Iteration: 374/900.\n",
      "Iteration: 375/900.\n",
      "Iteration: 376/900.\n",
      "Iteration: 377/900.\n",
      "Iteration: 378/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.3534188997492365, 1.3778564376045375, 1.2517561666574692]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.458, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 379/900.\n",
      "Iteration: 380/900.\n",
      "Iteration: 381/900.\n",
      "Iteration: 382/900.\n",
      "Iteration: 383/900.\n",
      "Iteration: 384/900.\n",
      "Iteration: 385/900.\n",
      "Iteration: 386/900.\n",
      "Iteration: 387/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.460351307321895, 1.4886863257050922, 1.3616109895187796]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.472, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 388/900.\n",
      "Iteration: 389/900.\n",
      "Iteration: 390/900.\n",
      "Iteration: 391/900.\n",
      "Iteration: 392/900.\n",
      "Iteration: 393/900.\n",
      "Iteration: 394/900.\n",
      "Iteration: 395/900.\n",
      "Iteration: 396/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.3107806401661048, 1.2852433204823752, 1.1684487075167644]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.484, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 397/900.\n",
      "Iteration: 398/900.\n",
      "Iteration: 399/900.\n",
      "Iteration: 400/900.\n",
      "Iteration: 401/900.\n",
      "Iteration: 402/900.\n",
      "Iteration: 403/900.\n",
      "Iteration: 404/900.\n",
      "Iteration: 405/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.3358005085064693, 1.3150154770581215, 1.3059373351908408]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.498, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 406/900.\n",
      "Iteration: 407/900.\n",
      "Iteration: 408/900.\n",
      "Iteration: 409/900.\n",
      "Iteration: 410/900.\n",
      "Iteration: 411/900.\n",
      "Iteration: 412/900.\n",
      "Iteration: 413/900.\n",
      "Iteration: 414/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.2607881082205026, 1.5479573507075066, 1.477613967029717]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.444, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 415/900.\n",
      "Iteration: 416/900.\n",
      "Iteration: 417/900.\n",
      "Iteration: 418/900.\n",
      "Iteration: 419/900.\n",
      "Iteration: 420/900.\n",
      "Iteration: 421/900.\n",
      "Iteration: 422/900.\n",
      "Iteration: 423/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.2585852836580875, 1.3889777356493311, 1.5260132959910775]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.504, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 424/900.\n",
      "Iteration: 425/900.\n",
      "Iteration: 426/900.\n",
      "Iteration: 427/900.\n",
      "Iteration: 428/900.\n",
      "Iteration: 429/900.\n",
      "Iteration: 430/900.\n",
      "Iteration: 431/900.\n",
      "Iteration: 432/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.2485051332436894, 1.325135883196313, 1.169933059461714]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.48, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 433/900.\n",
      "Iteration: 434/900.\n",
      "Iteration: 435/900.\n",
      "Iteration: 436/900.\n",
      "Iteration: 437/900.\n",
      "Iteration: 438/900.\n",
      "Iteration: 439/900.\n",
      "Iteration: 440/900.\n",
      "Iteration: 441/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.2312335267626484, 1.1953976070428958, 1.3251409158285388]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.542, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 442/900.\n",
      "Iteration: 443/900.\n",
      "Iteration: 444/900.\n",
      "Iteration: 445/900.\n",
      "Iteration: 446/900.\n",
      "Iteration: 447/900.\n",
      "Iteration: 448/900.\n",
      "Iteration: 449/900.\n",
      "Iteration: 450/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.1886317601422907, 1.3230462810941706, 1.368652764854953]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.528, Val acc: 0.26\n",
      "\n",
      "\n",
      "Iteration: 451/900.\n",
      "Iteration: 452/900.\n",
      "Iteration: 453/900.\n",
      "Iteration: 454/900.\n",
      "Iteration: 455/900.\n",
      "Iteration: 456/900.\n",
      "Iteration: 457/900.\n",
      "Iteration: 458/900.\n",
      "Iteration: 459/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.3219365686477142, 1.2325461484564348, 1.1808329212885884]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.522, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 460/900.\n",
      "Iteration: 461/900.\n",
      "Iteration: 462/900.\n",
      "Iteration: 463/900.\n",
      "Iteration: 464/900.\n",
      "Iteration: 465/900.\n",
      "Iteration: 466/900.\n",
      "Iteration: 467/900.\n",
      "Iteration: 468/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.2294635996782328, 1.2116144441379302, 1.2452559222807602]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.588, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 469/900.\n",
      "Iteration: 470/900.\n",
      "Iteration: 471/900.\n",
      "Iteration: 472/900.\n",
      "Iteration: 473/900.\n",
      "Iteration: 474/900.\n",
      "Iteration: 475/900.\n",
      "Iteration: 476/900.\n",
      "Iteration: 477/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.2249875077943895, 1.3122710247645812, 1.2185617971278406]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.552, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 478/900.\n",
      "Iteration: 479/900.\n",
      "Iteration: 480/900.\n",
      "Iteration: 481/900.\n",
      "Iteration: 482/900.\n",
      "Iteration: 483/900.\n",
      "Iteration: 484/900.\n",
      "Iteration: 485/900.\n",
      "Iteration: 486/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.2841363593772641, 1.5033297026334809, 1.1290497862022009]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.558, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 487/900.\n",
      "Iteration: 488/900.\n",
      "Iteration: 489/900.\n",
      "Iteration: 490/900.\n",
      "Iteration: 491/900.\n",
      "Iteration: 492/900.\n",
      "Iteration: 493/900.\n",
      "Iteration: 494/900.\n",
      "Iteration: 495/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.0791857084942285, 1.3333374365305608, 1.207045605625449]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.522, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 496/900.\n",
      "Iteration: 497/900.\n",
      "Iteration: 498/900.\n",
      "Iteration: 499/900.\n",
      "Iteration: 500/900.\n",
      "Iteration: 501/900.\n",
      "Iteration: 502/900.\n",
      "Iteration: 503/900.\n",
      "Iteration: 504/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.1983425257826876, 1.1899476593645477, 1.2353123961440944]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.562, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 505/900.\n",
      "Iteration: 506/900.\n",
      "Iteration: 507/900.\n",
      "Iteration: 508/900.\n",
      "Iteration: 509/900.\n",
      "Iteration: 510/900.\n",
      "Iteration: 511/900.\n",
      "Iteration: 512/900.\n",
      "Iteration: 513/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.2562836371297346, 1.261970719469073, 1.0426567885940177]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.6, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 514/900.\n",
      "Iteration: 515/900.\n",
      "Iteration: 516/900.\n",
      "Iteration: 517/900.\n",
      "Iteration: 518/900.\n",
      "Iteration: 519/900.\n",
      "Iteration: 520/900.\n",
      "Iteration: 521/900.\n",
      "Iteration: 522/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.1550957790785956, 1.1532604405882998, 0.9470604887754249]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.548, Val acc: 0.28\n",
      "\n",
      "\n",
      "Iteration: 523/900.\n",
      "Iteration: 524/900.\n",
      "Iteration: 525/900.\n",
      "Iteration: 526/900.\n",
      "Iteration: 527/900.\n",
      "Iteration: 528/900.\n",
      "Iteration: 529/900.\n",
      "Iteration: 530/900.\n",
      "Iteration: 531/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.213936443616167, 1.1676572599847455, 1.1955580599797218]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.538, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 532/900.\n",
      "Iteration: 533/900.\n",
      "Iteration: 534/900.\n",
      "Iteration: 535/900.\n",
      "Iteration: 536/900.\n",
      "Iteration: 537/900.\n",
      "Iteration: 538/900.\n",
      "Iteration: 539/900.\n",
      "Iteration: 540/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.0971289630908043, 1.103244665432395, 1.2095328931569251]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.54, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 541/900.\n",
      "Iteration: 542/900.\n",
      "Iteration: 543/900.\n",
      "Iteration: 544/900.\n",
      "Iteration: 545/900.\n",
      "Iteration: 546/900.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 547/900.\n",
      "Iteration: 548/900.\n",
      "Iteration: 549/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.1558656507032627, 1.1536955239028732, 1.023166848370361]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.564, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 550/900.\n",
      "Iteration: 551/900.\n",
      "Iteration: 552/900.\n",
      "Iteration: 553/900.\n",
      "Iteration: 554/900.\n",
      "Iteration: 555/900.\n",
      "Iteration: 556/900.\n",
      "Iteration: 557/900.\n",
      "Iteration: 558/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.1194185394009046, 1.1182215689189423, 1.1221497244198948]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.556, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 559/900.\n",
      "Iteration: 560/900.\n",
      "Iteration: 561/900.\n",
      "Iteration: 562/900.\n",
      "Iteration: 563/900.\n",
      "Iteration: 564/900.\n",
      "Iteration: 565/900.\n",
      "Iteration: 566/900.\n",
      "Iteration: 567/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.3421256013293024, 1.1523134074854533, 1.1710175480204306]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.59, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 568/900.\n",
      "Iteration: 569/900.\n",
      "Iteration: 570/900.\n",
      "Iteration: 571/900.\n",
      "Iteration: 572/900.\n",
      "Iteration: 573/900.\n",
      "Iteration: 574/900.\n",
      "Iteration: 575/900.\n",
      "Iteration: 576/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.1034777412386374, 1.1304235691909954, 1.102143568789469]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.608, Val acc: 0.28\n",
      "\n",
      "\n",
      "Iteration: 577/900.\n",
      "Iteration: 578/900.\n",
      "Iteration: 579/900.\n",
      "Iteration: 580/900.\n",
      "Iteration: 581/900.\n",
      "Iteration: 582/900.\n",
      "Iteration: 583/900.\n",
      "Iteration: 584/900.\n",
      "Iteration: 585/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.0831786471428695, 1.1342897760526556, 0.9520715592010303]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.608, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 586/900.\n",
      "Iteration: 587/900.\n",
      "Iteration: 588/900.\n",
      "Iteration: 589/900.\n",
      "Iteration: 590/900.\n",
      "Iteration: 591/900.\n",
      "Iteration: 592/900.\n",
      "Iteration: 593/900.\n",
      "Iteration: 594/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.1414760978498324, 0.9470970744503228, 1.001795877725173]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.694, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 595/900.\n",
      "Iteration: 596/900.\n",
      "Iteration: 597/900.\n",
      "Iteration: 598/900.\n",
      "Iteration: 599/900.\n",
      "Iteration: 600/900.\n",
      "Iteration: 601/900.\n",
      "Iteration: 602/900.\n",
      "Iteration: 603/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.0194707224746506, 1.1830412871439646, 1.0640555692027196]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.59, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 604/900.\n",
      "Iteration: 605/900.\n",
      "Iteration: 606/900.\n",
      "Iteration: 607/900.\n",
      "Iteration: 608/900.\n",
      "Iteration: 609/900.\n",
      "Iteration: 610/900.\n",
      "Iteration: 611/900.\n",
      "Iteration: 612/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [0.9515990392978023, 1.0560757416021036, 1.0735484374997284]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.598, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 613/900.\n",
      "Iteration: 614/900.\n",
      "Iteration: 615/900.\n",
      "Iteration: 616/900.\n",
      "Iteration: 617/900.\n",
      "Iteration: 618/900.\n",
      "Iteration: 619/900.\n",
      "Iteration: 620/900.\n",
      "Iteration: 621/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.064560943716315, 0.9843972546950425, 1.1177131395054034]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.604, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 622/900.\n",
      "Iteration: 623/900.\n",
      "Iteration: 624/900.\n",
      "Iteration: 625/900.\n",
      "Iteration: 626/900.\n",
      "Iteration: 627/900.\n",
      "Iteration: 628/900.\n",
      "Iteration: 629/900.\n",
      "Iteration: 630/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.0267015357679932, 1.048999064291663, 0.9044729606051938]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.654, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 631/900.\n",
      "Iteration: 632/900.\n",
      "Iteration: 633/900.\n",
      "Iteration: 634/900.\n",
      "Iteration: 635/900.\n",
      "Iteration: 636/900.\n",
      "Iteration: 637/900.\n",
      "Iteration: 638/900.\n",
      "Iteration: 639/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [0.9640053579936495, 1.0509275712113415, 0.8189987876948372]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.58, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 640/900.\n",
      "Iteration: 641/900.\n",
      "Iteration: 642/900.\n",
      "Iteration: 643/900.\n",
      "Iteration: 644/900.\n",
      "Iteration: 645/900.\n",
      "Iteration: 646/900.\n",
      "Iteration: 647/900.\n",
      "Iteration: 648/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [0.807787408207989, 1.1276160547951974, 0.9259588595138466]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.628, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 649/900.\n",
      "Iteration: 650/900.\n",
      "Iteration: 651/900.\n",
      "Iteration: 652/900.\n",
      "Iteration: 653/900.\n",
      "Iteration: 654/900.\n",
      "Iteration: 655/900.\n",
      "Iteration: 656/900.\n",
      "Iteration: 657/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [0.925135274199726, 0.9971822320321497, 0.8387184040421629]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.664, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 658/900.\n",
      "Iteration: 659/900.\n",
      "Iteration: 660/900.\n",
      "Iteration: 661/900.\n",
      "Iteration: 662/900.\n",
      "Iteration: 663/900.\n",
      "Iteration: 664/900.\n",
      "Iteration: 665/900.\n",
      "Iteration: 666/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [0.855290517379757, 0.9027624476847014, 0.9527299500155744]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.708, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 667/900.\n",
      "Iteration: 668/900.\n",
      "Iteration: 669/900.\n",
      "Iteration: 670/900.\n",
      "Iteration: 671/900.\n",
      "Iteration: 672/900.\n",
      "Iteration: 673/900.\n",
      "Iteration: 674/900.\n",
      "Iteration: 675/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [0.8585224608170376, 0.6713389765828931, 0.7455809328918216]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.666, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 676/900.\n",
      "Iteration: 677/900.\n",
      "Iteration: 678/900.\n",
      "Iteration: 679/900.\n",
      "Iteration: 680/900.\n",
      "Iteration: 681/900.\n",
      "Iteration: 682/900.\n",
      "Iteration: 683/900.\n",
      "Iteration: 684/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [0.7185685486289068, 0.8520938061169746, 0.77424724678207]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.692, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 685/900.\n",
      "Iteration: 686/900.\n",
      "Iteration: 687/900.\n",
      "Iteration: 688/900.\n",
      "Iteration: 689/900.\n",
      "Iteration: 690/900.\n",
      "Iteration: 691/900.\n",
      "Iteration: 692/900.\n",
      "Iteration: 693/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [0.7000735731850519, 0.8550246694703947, 0.8861274081127611]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.712, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 694/900.\n",
      "Iteration: 695/900.\n",
      "Iteration: 696/900.\n",
      "Iteration: 697/900.\n",
      "Iteration: 698/900.\n",
      "Iteration: 699/900.\n",
      "Iteration: 700/900.\n",
      "Iteration: 701/900.\n",
      "Iteration: 702/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [0.9287024938333899, 0.9085997740573353, 0.9234545047576033]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.656, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 703/900.\n",
      "Iteration: 704/900.\n",
      "Iteration: 705/900.\n",
      "Iteration: 706/900.\n",
      "Iteration: 707/900.\n",
      "Iteration: 708/900.\n",
      "Iteration: 709/900.\n",
      "Iteration: 710/900.\n",
      "Iteration: 711/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [0.9438650034927123, 0.8772397974329802, 1.0238816493901697]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.68, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 712/900.\n",
      "Iteration: 713/900.\n",
      "Iteration: 714/900.\n",
      "Iteration: 715/900.\n",
      "Iteration: 716/900.\n",
      "Iteration: 717/900.\n",
      "Iteration: 718/900.\n",
      "Iteration: 719/900.\n",
      "Iteration: 720/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [1.0180700477398557, 0.8951242802386642, 0.5977023575471199]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.688, Val acc: 0.24\n",
      "\n",
      "\n",
      "Iteration: 721/900.\n",
      "Iteration: 722/900.\n",
      "Iteration: 723/900.\n",
      "Iteration: 724/900.\n",
      "Iteration: 725/900.\n",
      "Iteration: 726/900.\n",
      "Iteration: 727/900.\n",
      "Iteration: 728/900.\n",
      "Iteration: 729/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [0.6745960093630111, 0.7321659286924226, 0.9066032718465743]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.694, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 730/900.\n",
      "Iteration: 731/900.\n",
      "Iteration: 732/900.\n",
      "Iteration: 733/900.\n",
      "Iteration: 734/900.\n",
      "Iteration: 735/900.\n",
      "Iteration: 736/900.\n",
      "Iteration: 737/900.\n",
      "Iteration: 738/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [0.8046187931309772, 0.8446359894922004, 0.6552574557247701]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.714, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 739/900.\n",
      "Iteration: 740/900.\n",
      "Iteration: 741/900.\n",
      "Iteration: 742/900.\n",
      "Iteration: 743/900.\n",
      "Iteration: 744/900.\n",
      "Iteration: 745/900.\n",
      "Iteration: 746/900.\n",
      "Iteration: 747/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [0.7303700834098155, 0.6830172944475912, 0.7082018369173037]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.778, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 748/900.\n",
      "Iteration: 749/900.\n",
      "Iteration: 750/900.\n",
      "Iteration: 751/900.\n",
      "Iteration: 752/900.\n",
      "Iteration: 753/900.\n",
      "Iteration: 754/900.\n",
      "Iteration: 755/900.\n",
      "Iteration: 756/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [0.723317370608651, 0.6463257892504481, 0.6808368055980522]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.776, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 757/900.\n",
      "Iteration: 758/900.\n",
      "Iteration: 759/900.\n",
      "Iteration: 760/900.\n",
      "Iteration: 761/900.\n",
      "Iteration: 762/900.\n",
      "Iteration: 763/900.\n",
      "Iteration: 764/900.\n",
      "Iteration: 765/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [0.5563608820178676, 0.6113339063856478, 0.6420498114545217]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.758, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 766/900.\n",
      "Iteration: 767/900.\n",
      "Iteration: 768/900.\n",
      "Iteration: 769/900.\n",
      "Iteration: 770/900.\n",
      "Iteration: 771/900.\n",
      "Iteration: 772/900.\n",
      "Iteration: 773/900.\n",
      "Iteration: 774/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [0.7167138280446695, 0.51152073063892, 0.6671682616258433]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.77, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 775/900.\n",
      "Iteration: 776/900.\n",
      "Iteration: 777/900.\n",
      "Iteration: 778/900.\n",
      "Iteration: 779/900.\n",
      "Iteration: 780/900.\n",
      "Iteration: 781/900.\n",
      "Iteration: 782/900.\n",
      "Iteration: 783/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [0.5721904791349384, 0.6853913228726112, 0.7682152558471959]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.768, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 784/900.\n",
      "Iteration: 785/900.\n",
      "Iteration: 786/900.\n",
      "Iteration: 787/900.\n",
      "Iteration: 788/900.\n",
      "Iteration: 789/900.\n",
      "Iteration: 790/900.\n",
      "Iteration: 791/900.\n",
      "Iteration: 792/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [0.6923653732049041, 0.5509323765858741, 0.6291828682820163]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.766, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 793/900.\n",
      "Iteration: 794/900.\n",
      "Iteration: 795/900.\n",
      "Iteration: 796/900.\n",
      "Iteration: 797/900.\n",
      "Iteration: 798/900.\n",
      "Iteration: 799/900.\n",
      "Iteration: 800/900.\n",
      "Iteration: 801/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [0.5950063101254639, 0.7172852807373681, 0.7120947734312887]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.752, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 802/900.\n",
      "Iteration: 803/900.\n",
      "Iteration: 804/900.\n",
      "Iteration: 805/900.\n",
      "Iteration: 806/900.\n",
      "Iteration: 807/900.\n",
      "Iteration: 808/900.\n",
      "Iteration: 809/900.\n",
      "Iteration: 810/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [0.5039473487784114, 0.7990931419746855, 0.6615869943995348]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.742, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 811/900.\n",
      "Iteration: 812/900.\n",
      "Iteration: 813/900.\n",
      "Iteration: 814/900.\n",
      "Iteration: 815/900.\n",
      "Iteration: 816/900.\n",
      "Iteration: 817/900.\n",
      "Iteration: 818/900.\n",
      "Iteration: 819/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [0.7231582049425976, 0.6676361323659618, 0.4796753991230732]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.764, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 820/900.\n",
      "Iteration: 821/900.\n",
      "Iteration: 822/900.\n",
      "Iteration: 823/900.\n",
      "Iteration: 824/900.\n",
      "Iteration: 825/900.\n",
      "Iteration: 826/900.\n",
      "Iteration: 827/900.\n",
      "Iteration: 828/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [0.7342906380840902, 0.4574623412189029, 0.5997573835774961]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.794, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 829/900.\n",
      "Iteration: 830/900.\n",
      "Iteration: 831/900.\n",
      "Iteration: 832/900.\n",
      "Iteration: 833/900.\n",
      "Iteration: 834/900.\n",
      "Iteration: 835/900.\n",
      "Iteration: 836/900.\n",
      "Iteration: 837/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [0.47702189334406814, 0.7135704437105029, 0.5445117956184157]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.824, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 838/900.\n",
      "Iteration: 839/900.\n",
      "Iteration: 840/900.\n",
      "Iteration: 841/900.\n",
      "Iteration: 842/900.\n",
      "Iteration: 843/900.\n",
      "Iteration: 844/900.\n",
      "Iteration: 845/900.\n",
      "Iteration: 846/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [0.547613084657688, 0.5460709187427149, 0.6603992518718904]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.778, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 847/900.\n",
      "Iteration: 848/900.\n",
      "Iteration: 849/900.\n",
      "Iteration: 850/900.\n",
      "Iteration: 851/900.\n",
      "Iteration: 852/900.\n",
      "Iteration: 853/900.\n",
      "Iteration: 854/900.\n",
      "Iteration: 855/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [0.6513135105992754, 0.6815834013165987, 0.7701302144317884]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.824, Val acc: 0.26\n",
      "\n",
      "\n",
      "Iteration: 856/900.\n",
      "Iteration: 857/900.\n",
      "Iteration: 858/900.\n",
      "Iteration: 859/900.\n",
      "Iteration: 860/900.\n",
      "Iteration: 861/900.\n",
      "Iteration: 862/900.\n",
      "Iteration: 863/900.\n",
      "Iteration: 864/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [0.5614225833519241, 0.6265751496780961, 0.5259335853201181]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.794, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 865/900.\n",
      "Iteration: 866/900.\n",
      "Iteration: 867/900.\n",
      "Iteration: 868/900.\n",
      "Iteration: 869/900.\n",
      "Iteration: 870/900.\n",
      "Iteration: 871/900.\n",
      "Iteration: 872/900.\n",
      "Iteration: 873/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [0.5556246531190442, 0.7197104852903817, 0.5812147839277836]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.822, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 874/900.\n",
      "Iteration: 875/900.\n",
      "Iteration: 876/900.\n",
      "Iteration: 877/900.\n",
      "Iteration: 878/900.\n",
      "Iteration: 879/900.\n",
      "Iteration: 880/900.\n",
      "Iteration: 881/900.\n",
      "Iteration: 882/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [0.43599390492474327, 0.46908484518429716, 0.42206693894568553]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.826, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 883/900.\n",
      "Iteration: 884/900.\n",
      "Iteration: 885/900.\n",
      "Iteration: 886/900.\n",
      "Iteration: 887/900.\n",
      "Iteration: 888/900.\n",
      "Iteration: 889/900.\n",
      "Iteration: 890/900.\n",
      "Iteration: 891/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [0.5534001522532802, 0.478862259962117, 0.48878735679220187]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.862, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 892/900.\n",
      "Iteration: 893/900.\n",
      "Iteration: 894/900.\n",
      "Iteration: 895/900.\n",
      "Iteration: 896/900.\n",
      "Iteration: 897/900.\n",
      "Iteration: 898/900.\n",
      "Iteration: 899/900.\n",
      "Iteration: 900/900.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.302553937969369\n",
      "Loss latest three: [0.5535848430807999, 0.4434372840274787, 0.4848033508345498]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.82, Val acc: 0.4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------FINAL OUTPUT----------------\n",
      "  Train acc: 0.818, Val acc: 0.4\n",
      "Loss history: [2.302553937969369, 2.3026285334795693, 2.302734228951114, 2.302738562600058, 2.3023811415013378, 2.3025777600716753, 2.3023472197184054, 2.3015962716512393, 2.300246923982207, 2.298291971986813, 2.287288546212191, 2.296003081877347, 2.2615534393648598, 2.2961783870120875, 2.269958975285027, 2.2834916877801397, 2.181072621319521, 2.224486294256639, 2.2045046067141683, 2.3081745656752357, 2.2174386173909504, 2.1811741197291115, 2.1967819561113244, 2.2480686032645356, 2.2700961926678915, 2.165698444113134, 2.2467884203546866, 2.162224445809297, 2.212227028465348, 2.1105435901005674, 2.1022644957117604, 2.1668591158391997, 2.156921653389469, 2.2047044796823663, 2.092912992117795, 2.1518044880726364, 2.0679891301282196, 2.086643633537961, 1.9785863745012513, 2.092319080811854, 2.1926019899167257, 2.086709495342667, 2.1368221835032823, 2.033727887405248, 2.206101089998645, 2.100397928005566, 2.018761810459804, 1.9401200361881108, 2.0375363014361474, 2.1078199356657743, 2.0386321681028736, 2.0404426808205707, 2.06843584438159, 2.03447305582438, 2.028190473687525, 2.120757004156014, 1.9933411076766456, 2.058727330304375, 2.0129397141292955, 1.9415790307003347, 2.009086863948008, 1.9111959531377714, 1.8807404241732095, 2.103413405552367, 2.1080571465854847, 2.0962889240182574, 2.073999947094489, 2.1138098850898217, 1.904458888669554, 1.9936949660723624, 2.065563280592904, 1.9937547963476032, 2.041604224659542, 1.8798613067473033, 2.145228792464493, 2.086291324329638, 1.9440629270662657, 1.9193732149728076, 1.9744192765772055, 1.9470627710401467, 2.0332638280596256, 1.9506775524789972, 1.9761754862965901, 1.8497226278299232, 1.917427797067552, 1.9908985783157767, 1.965533957916398, 2.059381430015757, 2.045533823096536, 1.9890677354590691, 1.9899097544761364, 2.011079473404416, 1.840013397954904, 1.8821753129647585, 1.8953158322814756, 1.914364276155549, 1.9732560627335378, 1.809160540705878, 1.943813617854872, 1.9526354883219836, 1.9851172245951874, 1.9144693381009108, 1.8731560331061956, 1.8288736145991278, 1.9082638442275868, 1.9226663704739166, 1.9475437194684773, 1.8862642675050745, 1.85621486097549, 1.916086850502215, 1.8491611201277938, 2.042572681672811, 1.8411095981850167, 1.9182459826105551, 1.958998760345062, 1.8015100696238517, 1.8513453433722358, 1.8607038462987158, 1.9314683522575353, 1.8437837414603084, 1.8090777261226934, 1.906921773255647, 1.9558745188721203, 1.805372379654562, 1.8557016964002022, 1.9795760593066385, 1.8353925373389857, 1.8610535236437784, 1.9942922609931766, 1.831048818191113, 1.7641903573134903, 1.8244846458902693, 1.8129983176985607, 1.820793861204638, 1.8538514961285861, 1.842555058287523, 1.8868736107386244, 1.8187793033871749, 1.8845745365816515, 1.7731934314329394, 1.7999706915639582, 1.6525537215843111, 1.7073029149298822, 1.841767098471278, 1.9188411458268622, 1.8162812094551495, 1.7597452644168363, 1.7158686512015027, 1.8584942483086304, 1.6481034242050803, 1.7227366822573238, 1.7838534974350757, 1.6778771973789937, 1.6596085231114053, 1.6935038335243011, 1.83928028788828, 1.798184159534983, 1.8373819127357143, 1.8297010193978693, 1.7398923816407998, 1.7944429321990651, 1.77007997579471, 1.6714714470801764, 1.6271950471825707, 1.6910756890553231, 1.7641147796700176, 1.7631974547289446, 1.7478927751067488, 1.6020448058118362, 1.6802248240840525, 1.566286540275584, 1.6067696871083428, 1.6305730238178637, 1.9248358573027522, 1.7711215130644027, 1.641718720218259, 1.6768408347573074, 1.6682152743129277, 1.6686885782248493, 1.7125876688300727, 1.6609864590144547, 1.6456878507080108, 1.7073203020179077, 1.6119115584414032, 1.4999967202354203, 1.5948407692316906, 1.7051477870750826, 1.8761990151570134, 1.7652035265293575, 1.6496104828577687, 1.8597865504108608, 1.6748021917122595, 1.408777343195753, 1.5908213100444386, 1.721251067543998, 1.596278011137305, 1.701009728017043, 1.7749903674027605, 1.7277418311763404, 1.7146599064222436, 1.6778465717357687, 1.777407494634542, 1.724075949958436, 1.6414634018673968, 1.6363141006972006, 1.7082590300892897, 1.6825777474236876, 1.6912447169057208, 1.473622536304976, 1.6295626099709688, 1.6852864205534248, 1.8676044650039716, 1.6145067234082735, 1.6130065855888338, 1.6542651755300748, 1.629595698954837, 1.5383915669120427, 1.7167308111603563, 1.616609118909329, 1.4100205033413875, 1.608096938310934, 1.6402592928048123, 1.59953985041899, 1.5972963085508074, 1.7592375016321027, 1.6895132345970458, 1.6024843865242602, 1.6460231096617282, 1.7414305872666487, 1.6449648688059002, 1.623216085423478, 1.5387818673117415, 1.5377811395553664, 1.4924018093529097, 1.6898317426785199, 1.5426333748183845, 1.549334665991187, 1.5067145713520993, 1.6257613527013044, 1.5168614923853034, 1.6458203189281637, 1.5723653734903849, 1.4327402892938803, 1.6048202475924487, 1.5305306739278044, 1.6316182987154173, 1.5013787981937523, 1.5118716864013444, 1.6212270646163551, 1.5715581722703236, 1.716945329625865, 1.530730344499275, 1.621662495798466, 1.7330491378875823, 1.6900117860837973, 1.5747480233534525, 1.6505939810812666, 1.4258955849527464, 1.4370048777171323, 1.6357765896945884, 1.4888334151718516, 1.5936900272904992, 1.6224685025735262, 1.6446801165926703, 1.581651338409879, 1.6211304433318094, 1.5963161154523082, 1.5752371175243918, 1.4081161110879266, 1.6306340489678628, 1.6028084450419005, 1.5731294989535147, 1.630330354433482, 1.5662210259527798, 1.6113217252585939, 1.3742752006319752, 1.48125608770169, 1.537788403344535, 1.604222268017219, 1.6808001519658096, 1.5283531600529987, 1.4654932314963542, 1.5855803082789146, 1.4947358239850763, 1.5624759678861346, 1.4025976811365704, 1.562807778048466, 1.675157013842027, 1.4853697828209993, 1.5325488340421014, 1.660061714996944, 1.5192054188635993, 1.5695636798548338, 1.5269777196310654, 1.5197079700173868, 1.6707745930120388, 1.4999466096175567, 1.63435880231931, 1.5110536673017878, 1.5453484651403446, 1.5615938737418809, 1.3664210577578917, 1.5462100303992905, 1.5410962888254682, 1.4280129243774276, 1.5310363327933088, 1.486956407312243, 1.5645376715632344, 1.5254408316055612, 1.4806402921532997, 1.448744768960218, 1.5541375010473824, 1.4925742515799005, 1.3927596283164634, 1.508690009371583, 1.4634811535351464, 1.4152360638478831, 1.4867288829260692, 1.6458039655233723, 1.5019950255025076, 1.6017486661661557, 1.4936697422359657, 1.5041105693374868, 1.4375296485279312, 1.5289586993593822, 1.5107508822434434, 1.4145766488913956, 1.2734766629165089, 1.4144296167458044, 1.4412594787443334, 1.346128605873183, 1.5986410439225338, 1.587485383650714, 1.5208156299262607, 1.4346053627969348, 1.4878023386113295, 1.4131382576900848, 1.600096633060214, 1.4045797301884104, 1.4292317884551993, 1.4717863790355916, 1.4991608387736097, 1.5263459777045538, 1.3846110212750338, 1.3415362589613173, 1.5464749154027897, 1.2547853504881743, 1.4922049470889964, 1.5091854794556567, 1.4310362884495524, 1.553210825786967, 1.2245619629985227, 1.4375063142464442, 1.5558853647192725, 1.5128656848005952, 1.2888295434043593, 1.3405940776037697, 1.3335278970399405, 1.2865105608502907, 1.6632731896000585, 1.3850829728452305, 1.4170226447687404, 1.4362371705673513, 1.5568550098929015, 1.38192820612618, 1.4062403500389797, 1.2925865972131236, 1.3304528074219732, 1.4809099371126546, 1.490280106792788, 1.3736918731690724, 1.2285281134970496, 1.428746762772223, 1.3616039236201691, 1.3445828846505297, 1.3534188997492365, 1.3778564376045375, 1.2517561666574692, 1.5776105307986603, 1.3104103102657754, 1.5061418507349675, 1.3450931535828512, 1.354915214410995, 1.2472503766964789, 1.460351307321895, 1.4886863257050922, 1.3616109895187796, 1.202033608398405, 1.2270204989903568, 1.3341823441017489, 1.4664896838573445, 1.3226874526152639, 1.441163807488069, 1.3107806401661048, 1.2852433204823752, 1.1684487075167644, 1.2324223566980137, 1.3952507842175375, 1.33150009161699, 1.290878025775622, 1.311683079649244, 1.4138292251980706, 1.3358005085064693, 1.3150154770581215, 1.3059373351908408, 1.4739637059708468, 1.3741687762143753, 1.2971006607809548, 1.3518928497205898, 1.2973082202684674, 1.4745161365643649, 1.2607881082205026, 1.5479573507075066, 1.477613967029717, 1.396895210064504, 1.3487167114966576, 1.2607238755787804, 1.380233282310156, 1.241508255353371, 1.3920294295318194, 1.2585852836580875, 1.3889777356493311, 1.5260132959910775, 1.5514720998373601, 1.2614996677887569, 1.2658675504897843, 1.1017955029877953, 1.170649588509415, 1.2578575799318605, 1.2485051332436894, 1.325135883196313, 1.169933059461714, 1.328400392854991, 1.3147713780378967, 1.3139644676597475, 1.327219482232464, 1.1716711638361044, 1.3126530255288278, 1.2312335267626484, 1.1953976070428958, 1.3251409158285388, 1.339499608665684, 1.165870829026026, 1.1453789102935839, 1.32082701421889, 1.3143344115182232, 1.1223350164213068, 1.1886317601422907, 1.3230462810941706, 1.368652764854953, 1.2655017892993305, 1.1390035776682121, 1.100190225236815, 1.3659501432628347, 1.2147544983824683, 1.2274821164020682, 1.3219365686477142, 1.2325461484564348, 1.1808329212885884, 1.2999453902293212, 1.4241407688935694, 1.2174138394009564, 1.2594293571396136, 1.3981204818194524, 1.2322126439958925, 1.2294635996782328, 1.2116144441379302, 1.2452559222807602, 1.1617432042352962, 1.4096319673950861, 1.2566863224369207, 1.292311292045906, 1.1695396540480454, 1.0117929501215768, 1.2249875077943895, 1.3122710247645812, 1.2185617971278406, 1.3453896083373107, 1.2560926985199627, 1.1298998870754162, 1.2018680168279487, 1.2756496401576258, 1.1564303050322957, 1.2841363593772641, 1.5033297026334809, 1.1290497862022009, 1.408610668563336, 1.1474407750176507, 1.1357527770995894, 1.066192414748827, 1.138200525331519, 1.2773401680309566, 1.0791857084942285, 1.3333374365305608, 1.207045605625449, 1.0072281903693925, 1.2117591228715505, 1.20247678008099, 1.2550072870953504, 1.2340024120375666, 1.0889848701415124, 1.1983425257826876, 1.1899476593645477, 1.2353123961440944, 1.3578228389193472, 1.2016211810478208, 1.3562065457501538, 1.0981860694378918, 1.04656741528187, 1.1569144038066548, 1.2562836371297346, 1.261970719469073, 1.0426567885940177, 1.025112236866716, 1.165216902051842, 1.18835503033247, 1.3285074108995165, 1.1241165895176157, 1.130360803120929, 1.1550957790785956, 1.1532604405882998, 0.9470604887754249, 1.2679802393730781, 1.244832556543326, 1.3695398615208516, 1.0957903793880701, 1.335228928648618, 1.0829913355735579, 1.213936443616167, 1.1676572599847455, 1.1955580599797218, 1.0725562793617929, 1.1744008867855615, 1.179972084056432, 0.9816097061657953, 0.9431814096660343, 1.190040812564549, 1.0971289630908043, 1.103244665432395, 1.2095328931569251, 1.0941547412579455, 1.3653718175082523, 1.0402248891607362, 1.1031507351999448, 1.049335108116846, 1.1684458921701262, 1.1558656507032627, 1.1536955239028732, 1.023166848370361, 1.299390766412326, 1.1302517823361506, 1.101240013076433, 1.2894761612437526, 1.058086457886324, 1.1624769721585586, 1.1194185394009046, 1.1182215689189423, 1.1221497244198948, 1.0462347894673936, 1.1440577828208434, 1.1634410877282007, 1.1693936292258202, 1.033633385770976, 0.9916814831057191, 1.3421256013293024, 1.1523134074854533, 1.1710175480204306, 1.082906183516835, 1.084212934635987, 0.9620129151393365, 1.1076470397398055, 1.0893472278204026, 1.0482802833798386, 1.1034777412386374, 1.1304235691909954, 1.102143568789469, 1.0199902171012007, 1.0830575813298926, 1.1795471855674078, 0.9503057973668003, 1.053562149055765, 1.2800375699461057, 1.0831786471428695, 1.1342897760526556, 0.9520715592010303, 0.8945062569244607, 1.0469118173815142, 1.1786129582338165, 1.026947284980159, 1.0598335891157415, 1.194938898632365, 1.1414760978498324, 0.9470970744503228, 1.001795877725173, 0.9967077562355169, 0.9388257506396256, 1.0470288421995875, 1.180460024291232, 1.064355324819344, 0.9415002072230156, 1.0194707224746506, 1.1830412871439646, 1.0640555692027196, 1.1974163829866291, 0.9556162849651944, 0.9615280985205891, 1.047560970083714, 1.1091463791464984, 1.0681884276605633, 0.9515990392978023, 1.0560757416021036, 1.0735484374997284, 1.0623262874218797, 0.9140005367226317, 0.9930300582572553, 1.0676881490543528, 0.8990867844114943, 1.0518573274634178, 1.064560943716315, 0.9843972546950425, 1.1177131395054034, 0.9222135984358099, 0.9315179929532622, 0.9265379706054008, 1.0651257573401969, 0.9891865098580624, 0.9869082291807472, 1.0267015357679932, 1.048999064291663, 0.9044729606051938, 0.8871668065900811, 1.034719381824604, 1.0948406221029503, 0.8043011616319592, 0.9460931365427684, 1.1110397927041338, 0.9640053579936495, 1.0509275712113415, 0.8189987876948372, 1.0116261778682554, 0.9976408509813497, 1.0933964396266358, 1.032323234665454, 0.7833392231631472, 1.0739602935632433, 0.807787408207989, 1.1276160547951974, 0.9259588595138466, 0.7798883899016437, 1.0604371991257366, 0.937595406756464, 0.9746235806530845, 0.8522753811821916, 0.8166216569468883, 0.925135274199726, 0.9971822320321497, 0.8387184040421629, 1.1554019864816498, 0.8287085732836169, 1.1435283278317028, 0.9013623069836784, 0.8743957717308289, 0.8858412671237256, 0.855290517379757, 0.9027624476847014, 0.9527299500155744, 0.8551010140320334, 0.9961164878797989, 0.7197473066929867, 0.881015124087498, 0.8954261799945559, 0.7839933260037839, 0.8585224608170376, 0.6713389765828931, 0.7455809328918216, 0.9185218529489687, 0.8306797649057964, 0.8281693024045013, 0.8790716589417487, 0.9081054482912211, 0.8132105607193859, 0.7185685486289068, 0.8520938061169746, 0.77424724678207, 0.9495166114839338, 0.8155312204341408, 0.9044539768725351, 0.9665525338570354, 0.78917118908225, 0.9440386864348096, 0.7000735731850519, 0.8550246694703947, 0.8861274081127611, 0.7427105940982308, 0.6450429147650804, 0.9093683156790949, 0.844122653514578, 0.7660492292721606, 0.7426367938966573, 0.9287024938333899, 0.9085997740573353, 0.9234545047576033, 0.8906416972350109, 0.979818856679409, 0.8774921682733288, 0.7862104032461887, 0.9154089888025021, 0.8458726490960925, 0.9438650034927123, 0.8772397974329802, 1.0238816493901697, 0.9765619669035589, 1.0249123295012368, 0.858218709501909, 1.0859454316124184, 0.8691576585683647, 1.1178458300984335, 1.0180700477398557, 0.8951242802386642, 0.5977023575471199, 0.9203450132421821, 0.8930804478452464, 0.8916120238047754, 0.6921804432836138, 0.6682461045606144, 0.7312464836505927, 0.6745960093630111, 0.7321659286924226, 0.9066032718465743, 0.7520449309948911, 0.7591039018032202, 0.5918494822205358, 0.7393520638125917, 0.8881917703226774, 0.7934629732073063, 0.8046187931309772, 0.8446359894922004, 0.6552574557247701, 0.7506770175101342, 0.7317145508125888, 0.6924027094475335, 0.8093546160383772, 0.8355660093902679, 0.6882347985005615, 0.7303700834098155, 0.6830172944475912, 0.7082018369173037, 0.8311005818693769, 0.6545644334647073, 0.8233636221216193, 0.7086409209103137, 0.8191692034177644, 0.7158431961035241, 0.723317370608651, 0.6463257892504481, 0.6808368055980522, 0.6735787161306293, 0.6423995900468741, 0.7875888388721585, 0.6107970312335561, 0.7034930874116789, 0.8315501995481078, 0.5563608820178676, 0.6113339063856478, 0.6420498114545217, 0.6770310739669074, 0.5988020138988432, 0.6963558825143337, 0.7241660010134888, 0.6205122739647879, 0.7099279307241448, 0.7167138280446695, 0.51152073063892, 0.6671682616258433, 0.6766032794022527, 0.5530638676233227, 0.6193500626549294, 0.6089485773768566, 0.6916026950321831, 0.6532745867111006, 0.5721904791349384, 0.6853913228726112, 0.7682152558471959, 0.8431629326498948, 0.7361191449657618, 0.5596331254417615, 0.5629148071289783, 0.7574288786071134, 0.7488731821084299, 0.6923653732049041, 0.5509323765858741, 0.6291828682820163, 0.6200257683737234, 0.7064927167957891, 0.582766526722047, 0.5798133875531815, 0.6991432565624338, 0.7919961973031733, 0.5950063101254639, 0.7172852807373681, 0.7120947734312887, 0.49015742064187123, 0.6972848066105806, 0.6929807524630753, 0.7095744020428024, 0.7841934458270164, 0.7314658063710009, 0.5039473487784114, 0.7990931419746855, 0.6615869943995348, 0.5415894906185527, 0.5278748732397596, 0.6010824812841543, 0.42581861996634657, 0.7079227128504291, 0.4815730022401561, 0.7231582049425976, 0.6676361323659618, 0.4796753991230732, 0.6032890880507568, 0.6249660097512999, 0.6070211244857121, 0.7298369280508349, 0.6243182686818424, 0.7808954387980078, 0.7342906380840902, 0.4574623412189029, 0.5997573835774961, 0.5204890592801809, 0.5702954364718484, 0.5542981755002591, 0.5913797092838036, 0.5373241389490252, 0.6531812721280824, 0.47702189334406814, 0.7135704437105029, 0.5445117956184157, 0.48221015798143607, 0.5133023277047317, 0.5696855894105018, 0.5471406924861256, 0.504587429335827, 0.4792259329419807, 0.547613084657688, 0.5460709187427149, 0.6603992518718904, 0.5688698266096689, 0.460805549726052, 0.5556618848881085, 0.6229205798926799, 0.725918945588389, 0.4856576981067252, 0.6513135105992754, 0.6815834013165987, 0.7701302144317884, 0.48241896266306555, 0.7289092349220634, 0.5324059936458851, 0.4969954511546273, 0.6071331215684421, 0.6945284970267401, 0.5614225833519241, 0.6265751496780961, 0.5259335853201181, 0.4988978041541817, 0.5417577969097709, 0.5261675268520749, 0.46194582207650087, 0.5355340500787936, 0.6961205398531874, 0.5556246531190442, 0.7197104852903817, 0.5812147839277836, 0.5986808854859057, 0.347708252862205, 0.6086324426317249, 0.48968526677922397, 0.6707340631183939, 0.6107513668565516, 0.43599390492474327, 0.46908484518429716, 0.42206693894568553, 0.5077708513241015, 0.440873227583376, 0.49900189631279745, 0.4564920343737717, 0.39319646728748403, 0.4543022230156869, 0.5534001522532802, 0.478862259962117, 0.48878735679220187, 0.47077004581085413, 0.44791187757561285, 0.4306506531884623, 0.6021862231641162, 0.4019544399651677, 0.5115893478068653, 0.5535848430807999, 0.4434372840274787, 0.4848033508345498]\n",
      "Accuracy history: [0.074, 0.156, 0.168, 0.188, 0.218, 0.244, 0.214, 0.222, 0.274, 0.28, 0.268, 0.222, 0.324, 0.272, 0.288, 0.316, 0.306, 0.298, 0.324, 0.262, 0.348, 0.35, 0.382, 0.368, 0.346, 0.376, 0.378, 0.37, 0.428, 0.366, 0.43, 0.398, 0.39, 0.378, 0.388, 0.424, 0.46, 0.468, 0.494, 0.48, 0.466, 0.458, 0.472, 0.484, 0.498, 0.444, 0.504, 0.48, 0.542, 0.528, 0.522, 0.588, 0.552, 0.558, 0.522, 0.562, 0.6, 0.548, 0.538, 0.54, 0.564, 0.556, 0.59, 0.608, 0.608, 0.694, 0.59, 0.598, 0.604, 0.654, 0.58, 0.628, 0.664, 0.708, 0.666, 0.692, 0.712, 0.656, 0.68, 0.688, 0.694, 0.714, 0.778, 0.776, 0.758, 0.77, 0.768, 0.766, 0.752, 0.742, 0.764, 0.794, 0.824, 0.778, 0.824, 0.794, 0.822, 0.826, 0.862, 0.82, 0.818]\n"
     ]
    }
   ],
   "source": [
    "input_shape=(3,32,32)\n",
    "x_train = x_train.reshape(x_train.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "x_dev = x_dev.reshape(x_dev.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "x_val = x_val.reshape(x_val.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "stl_imgs = stl_imgs.reshape(stl_imgs.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "data = x_train, y_train, x_test, y_test, x_val, y_val, x_dev, y_dev\n",
    "print(x_train.shape)\n",
    "net = ConvNet4Accel(input_shape=(3, 32, 32), dense_interior_units=(200,), n_kers=(64,))\n",
    "net.compile('adam')\n",
    "print(stl_imgs.shape)\n",
    "net.fit(x_train, y_train, x_dev, y_dev, n_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.435\n"
     ]
    }
   ],
   "source": [
    "x_test = x_test.reshape(x_test.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "print(net.accuracy(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.475\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_test = x_test.reshape(x_test.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "print(net.accuracy(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7d) Analysis of STL-10 training quality\n",
    "\n",
    "Use your trained network that achieves 45%+ accuracy on the test set to make \"high quality\" plots showing the following \n",
    "\n",
    "- Plot the accuracy of the training and validation sets as a function of training epoch. You may have to convert iterations to epochs.\n",
    "- Plot the loss as a function of training iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(net.validation_acc_history)\n",
    "plt.plot(net.train_acc_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(net.loss_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7f) Visualize layer weights\n",
    "\n",
    "Run the following code and submit the inline image of the weight visualization of the 1st layer (convolutional layer) of the network.\n",
    "\n",
    "**Note:**\n",
    "- Setting optional parameter to `True` will let you save a .PNG file in your project folder of your weights. I'd suggest setting it to `False` unless look at your weights and they look like they are worth saving. You don't want a training run that produces undesirable weights to overwrite your good looking results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights(wts, saveFig=True, filename='convWts_adam_overfit.png'):\n",
    "    grid_sz = int(np.sqrt(len(wts)))\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for x in range(grid_sz):\n",
    "        for y in range(grid_sz):\n",
    "            lin_ind = np.ravel_multi_index((x, y), dims=(grid_sz, grid_sz))\n",
    "            plt.subplot(grid_sz, grid_sz, lin_ind+1)\n",
    "            currImg = wts[lin_ind]\n",
    "            low, high = np.min(currImg), np.max(currImg)\n",
    "            currImg = 255*(currImg - low) / (high - low)\n",
    "            currImg = currImg.astype('uint8')\n",
    "            plt.imshow(currImg)\n",
    "            plt.gca().axis('off')\n",
    "    if saveFig:\n",
    "        plt.savefig('convWts_adam_overfit.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 64 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Subsitute your trained network below\n",
    "# netT is my network's name\n",
    "# You shouldn't see RGB noise\n",
    "plot_weights(net.layers[0].wts.transpose(0, 2, 3, 1), saveFig=False, filename='convWts_adam_train_20epoch.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:** What do the learned filters look like? Does this make sense to you / is this what you expected? In which area of the brain do these filters resemble cell receptive fields?\n",
    "\n",
    "Note: you should not see RGB \"noise\". If you do, and you pass the \"overfit\" test with the Adam optimizer, you probably need to increase the number of training epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "**General advice:** When making modifications for extensions, make small changes, then check to make sure you pass test code. Also, test out the network runtime on small examples before/after the changes. If you're not careful, the simulation time can become intractable really quickly!\n",
    "\n",
    "**Remember:** One thorough extension usually is worth more than several \"shallow\" extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Pedal to the metal: achieve high accuracy on STL-10\n",
    "\n",
    "You can achieve higher (>50%) classification accuracy on the STL-10 test set. Find the hyperparameters to achieve this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Experiment with different network architectures.\n",
    "\n",
    "The design of the `Network` class is modular. As long as you're careful about shapes, adding/removing network layers (e.g. `Conv2D`, `Dense`, etc.) should be straight forward. Experiment with adding another sequence of `Conv2D` and `MaxPooling2D` layers. Add another `Dense` hidden layer before the output layer. How do the changes affect classification accuracy and loss? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Experiment with different network hyperparameters.\n",
    "\n",
    "Explore the affect one or more change below has on classification. Be careful about how the hyperparameters may affect the shape of network layers. Thorough analysis will get you more points (not try a few ad hoc values).\n",
    "\n",
    "- Experiment with different numbers of hidden units in the Dense layers.\n",
    "- Experiment different max pooling window sizes and strides.\n",
    "- Experiment with kernel sizes (not 7x7). Can you get away with smaller ones? Do they perform just as well? What is the change in runtime like? What is the impact on their visualized appearance?\n",
    "- Experiment with number of kernels in the convolutional layer. Is more/fewer better? What is the impact on their visualized appearance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. Add and test some training bells and whistles\n",
    "\n",
    "Add features like early stopping, learning rate decay (learning rate at the end of an epoch becomes some fraction of its former value), etc and assess how they affect training loss convergence and accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Additional optimizers\n",
    "\n",
    "Research other optimizers used in backpropogation and implement one or more of them within the model structure. Compare its performance to ones you have implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Optimize your algorithms\n",
    "\n",
    "Find the main performance bottlenecks in the network and improve your code to reduce runtime (e.g. reduce explicit for loops, increase vectorization, etc). Research faster algorithms to do operations like convolution and implement them. Given the complexity of the network, I suggest focusing on one area at a time and make sure everything you change passes the test code before proceeding. Quantify and discuss your performance improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Additional loss functions\n",
    "\n",
    "Implement support for sigmoid, or another activation functions and associated losses. Test it out and compare with softmax/cross entropy. Make sure any necessary changes to the layer's gradient are made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Additional datasets\n",
    "\n",
    "Do classification and analyxe the results with an image dataset of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Performance analysis\n",
    "\n",
    "Do a thorough comparative analysis of the non-accelerated network and accelerated networks with respect to runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train...\n",
      "1485 iterations. 45 iter/epoch.\n",
      "Iteration: 1/1485.\n",
      "Time taken for iteration 0: 0.4887669086456299\n",
      "Estimated time to complete: 725.8188593387604\n",
      "Iteration: 2/1485.\n",
      "Iteration: 3/1485.\n",
      "Iteration: 4/1485.\n",
      "Iteration: 5/1485.\n",
      "Iteration: 6/1485.\n",
      "Iteration: 7/1485.\n",
      "Iteration: 8/1485.\n",
      "Iteration: 9/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [2.301873147133011, 2.2500227794087517, 2.2528956186527007]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.182, Val acc: 0.1\n",
      "\n",
      "\n",
      "Iteration: 10/1485.\n",
      "Iteration: 11/1485.\n",
      "Iteration: 12/1485.\n",
      "Iteration: 13/1485.\n",
      "Iteration: 14/1485.\n",
      "Iteration: 15/1485.\n",
      "Iteration: 16/1485.\n",
      "Iteration: 17/1485.\n",
      "Iteration: 18/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [2.230990432879249, 2.1149906439690698, 2.1096766136833516]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.194, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 19/1485.\n",
      "Iteration: 20/1485.\n",
      "Iteration: 21/1485.\n",
      "Iteration: 22/1485.\n",
      "Iteration: 23/1485.\n",
      "Iteration: 24/1485.\n",
      "Iteration: 25/1485.\n",
      "Iteration: 26/1485.\n",
      "Iteration: 27/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [2.040286746403391, 2.091883500155575, 1.8921736129986013]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.23, Val acc: 0.26\n",
      "\n",
      "\n",
      "Iteration: 28/1485.\n",
      "Iteration: 29/1485.\n",
      "Iteration: 30/1485.\n",
      "Iteration: 31/1485.\n",
      "Iteration: 32/1485.\n",
      "Iteration: 33/1485.\n",
      "Iteration: 34/1485.\n",
      "Iteration: 35/1485.\n",
      "Iteration: 36/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [2.051447662682095, 1.9357271153799098, 1.9488415889766082]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.284, Val acc: 0.2\n",
      "\n",
      "\n",
      "Iteration: 37/1485.\n",
      "Iteration: 38/1485.\n",
      "Iteration: 39/1485.\n",
      "Iteration: 40/1485.\n",
      "Iteration: 41/1485.\n",
      "Iteration: 42/1485.\n",
      "Iteration: 43/1485.\n",
      "Iteration: 44/1485.\n",
      "Iteration: 45/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.8437983257686832, 1.9559151087429394, 1.9747659584310215]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.292, Val acc: 0.24\n",
      "\n",
      "\n",
      "Iteration: 46/1485.\n",
      "Iteration: 47/1485.\n",
      "Iteration: 48/1485.\n",
      "Iteration: 49/1485.\n",
      "Iteration: 50/1485.\n",
      "Iteration: 51/1485.\n",
      "Iteration: 52/1485.\n",
      "Iteration: 53/1485.\n",
      "Iteration: 54/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.7713060600649888, 1.7719742315598266, 1.9707207656255987]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.262, Val acc: 0.24\n",
      "\n",
      "\n",
      "Iteration: 55/1485.\n",
      "Iteration: 56/1485.\n",
      "Iteration: 57/1485.\n",
      "Iteration: 58/1485.\n",
      "Iteration: 59/1485.\n",
      "Iteration: 60/1485.\n",
      "Iteration: 61/1485.\n",
      "Iteration: 62/1485.\n",
      "Iteration: 63/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.8416558088355028, 1.9059486219474309, 1.8519092702673123]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.314, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 64/1485.\n",
      "Iteration: 65/1485.\n",
      "Iteration: 66/1485.\n",
      "Iteration: 67/1485.\n",
      "Iteration: 68/1485.\n",
      "Iteration: 69/1485.\n",
      "Iteration: 70/1485.\n",
      "Iteration: 71/1485.\n",
      "Iteration: 72/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.9017265208068406, 1.9300900062945936, 1.954619791558879]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.28, Val acc: 0.28\n",
      "\n",
      "\n",
      "Iteration: 73/1485.\n",
      "Iteration: 74/1485.\n",
      "Iteration: 75/1485.\n",
      "Iteration: 76/1485.\n",
      "Iteration: 77/1485.\n",
      "Iteration: 78/1485.\n",
      "Iteration: 79/1485.\n",
      "Iteration: 80/1485.\n",
      "Iteration: 81/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.9407716429381854, 1.9393610308808562, 1.7272131214182989]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.314, Val acc: 0.24\n",
      "\n",
      "\n",
      "Iteration: 82/1485.\n",
      "Iteration: 83/1485.\n",
      "Iteration: 84/1485.\n",
      "Iteration: 85/1485.\n",
      "Iteration: 86/1485.\n",
      "Iteration: 87/1485.\n",
      "Iteration: 88/1485.\n",
      "Iteration: 89/1485.\n",
      "Iteration: 90/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.8605889751813791, 1.7760573855575508, 1.635153536285366]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.32, Val acc: 0.22\n",
      "\n",
      "\n",
      "Iteration: 91/1485.\n",
      "Iteration: 92/1485.\n",
      "Iteration: 93/1485.\n",
      "Iteration: 94/1485.\n",
      "Iteration: 95/1485.\n",
      "Iteration: 96/1485.\n",
      "Iteration: 97/1485.\n",
      "Iteration: 98/1485.\n",
      "Iteration: 99/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.7852481097813846, 1.8111841786314753, 1.8345266894278303]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.356, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 100/1485.\n",
      "Iteration: 101/1485.\n",
      "Iteration: 102/1485.\n",
      "Iteration: 103/1485.\n",
      "Iteration: 104/1485.\n",
      "Iteration: 105/1485.\n",
      "Iteration: 106/1485.\n",
      "Iteration: 107/1485.\n",
      "Iteration: 108/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.7070787226445638, 1.9089027991910212, 1.8490574864228009]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.3, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 109/1485.\n",
      "Iteration: 110/1485.\n",
      "Iteration: 111/1485.\n",
      "Iteration: 112/1485.\n",
      "Iteration: 113/1485.\n",
      "Iteration: 114/1485.\n",
      "Iteration: 115/1485.\n",
      "Iteration: 116/1485.\n",
      "Iteration: 117/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.8377132045414093, 1.8240843152088355, 1.6623084599427154]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.356, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 118/1485.\n",
      "Iteration: 119/1485.\n",
      "Iteration: 120/1485.\n",
      "Iteration: 121/1485.\n",
      "Iteration: 122/1485.\n",
      "Iteration: 123/1485.\n",
      "Iteration: 124/1485.\n",
      "Iteration: 125/1485.\n",
      "Iteration: 126/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.8306131816766056, 1.630204267570904, 1.628631636415539]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.36, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 127/1485.\n",
      "Iteration: 128/1485.\n",
      "Iteration: 129/1485.\n",
      "Iteration: 130/1485.\n",
      "Iteration: 131/1485.\n",
      "Iteration: 132/1485.\n",
      "Iteration: 133/1485.\n",
      "Iteration: 134/1485.\n",
      "Iteration: 135/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.6528054045393594, 1.7916239755966046, 1.7737620309598228]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.32, Val acc: 0.28\n",
      "\n",
      "\n",
      "Iteration: 136/1485.\n",
      "Iteration: 137/1485.\n",
      "Iteration: 138/1485.\n",
      "Iteration: 139/1485.\n",
      "Iteration: 140/1485.\n",
      "Iteration: 141/1485.\n",
      "Iteration: 142/1485.\n",
      "Iteration: 143/1485.\n",
      "Iteration: 144/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.7726455080453394, 1.7724559634640467, 1.7263200837109844]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.316, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 145/1485.\n",
      "Iteration: 146/1485.\n",
      "Iteration: 147/1485.\n",
      "Iteration: 148/1485.\n",
      "Iteration: 149/1485.\n",
      "Iteration: 150/1485.\n",
      "Iteration: 151/1485.\n",
      "Iteration: 152/1485.\n",
      "Iteration: 153/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.819783170663278, 1.7552269957476978, 1.9381476954610892]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.322, Val acc: 0.28\n",
      "\n",
      "\n",
      "Iteration: 154/1485.\n",
      "Iteration: 155/1485.\n",
      "Iteration: 156/1485.\n",
      "Iteration: 157/1485.\n",
      "Iteration: 158/1485.\n",
      "Iteration: 159/1485.\n",
      "Iteration: 160/1485.\n",
      "Iteration: 161/1485.\n",
      "Iteration: 162/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.8024477671012513, 1.842571926103743, 1.710587023126368]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.326, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 163/1485.\n",
      "Iteration: 164/1485.\n",
      "Iteration: 165/1485.\n",
      "Iteration: 166/1485.\n",
      "Iteration: 167/1485.\n",
      "Iteration: 168/1485.\n",
      "Iteration: 169/1485.\n",
      "Iteration: 170/1485.\n",
      "Iteration: 171/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.6774495336083515, 1.6759392219740574, 1.7836601897564492]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.364, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 172/1485.\n",
      "Iteration: 173/1485.\n",
      "Iteration: 174/1485.\n",
      "Iteration: 175/1485.\n",
      "Iteration: 176/1485.\n",
      "Iteration: 177/1485.\n",
      "Iteration: 178/1485.\n",
      "Iteration: 179/1485.\n",
      "Iteration: 180/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.6436464138137865, 1.6196268751724514, 1.7434020830275763]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.368, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 181/1485.\n",
      "Iteration: 182/1485.\n",
      "Iteration: 183/1485.\n",
      "Iteration: 184/1485.\n",
      "Iteration: 185/1485.\n",
      "Iteration: 186/1485.\n",
      "Iteration: 187/1485.\n",
      "Iteration: 188/1485.\n",
      "Iteration: 189/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.6117549748955529, 1.680272052141252, 1.7998692957003273]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.362, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 190/1485.\n",
      "Iteration: 191/1485.\n",
      "Iteration: 192/1485.\n",
      "Iteration: 193/1485.\n",
      "Iteration: 194/1485.\n",
      "Iteration: 195/1485.\n",
      "Iteration: 196/1485.\n",
      "Iteration: 197/1485.\n",
      "Iteration: 198/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.6398577725723087, 1.663734883061584, 1.7210220349441085]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.322, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 199/1485.\n",
      "Iteration: 200/1485.\n",
      "Iteration: 201/1485.\n",
      "Iteration: 202/1485.\n",
      "Iteration: 203/1485.\n",
      "Iteration: 204/1485.\n",
      "Iteration: 205/1485.\n",
      "Iteration: 206/1485.\n",
      "Iteration: 207/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.5842668709244294, 1.730557998074906, 1.7345373379298452]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.39, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 208/1485.\n",
      "Iteration: 209/1485.\n",
      "Iteration: 210/1485.\n",
      "Iteration: 211/1485.\n",
      "Iteration: 212/1485.\n",
      "Iteration: 213/1485.\n",
      "Iteration: 214/1485.\n",
      "Iteration: 215/1485.\n",
      "Iteration: 216/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.6025201757223773, 1.5145790280285818, 1.6824116660417727]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.342, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 217/1485.\n",
      "Iteration: 218/1485.\n",
      "Iteration: 219/1485.\n",
      "Iteration: 220/1485.\n",
      "Iteration: 221/1485.\n",
      "Iteration: 222/1485.\n",
      "Iteration: 223/1485.\n",
      "Iteration: 224/1485.\n",
      "Iteration: 225/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.6429420273161262, 1.5517453936417567, 1.6501259705793327]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.416, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 226/1485.\n",
      "Iteration: 227/1485.\n",
      "Iteration: 228/1485.\n",
      "Iteration: 229/1485.\n",
      "Iteration: 230/1485.\n",
      "Iteration: 231/1485.\n",
      "Iteration: 232/1485.\n",
      "Iteration: 233/1485.\n",
      "Iteration: 234/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.7054045172410377, 1.5517633865179596, 1.6462956679560907]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.352, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 235/1485.\n",
      "Iteration: 236/1485.\n",
      "Iteration: 237/1485.\n",
      "Iteration: 238/1485.\n",
      "Iteration: 239/1485.\n",
      "Iteration: 240/1485.\n",
      "Iteration: 241/1485.\n",
      "Iteration: 242/1485.\n",
      "Iteration: 243/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.7110436020993123, 1.6715004847989232, 1.6389062885624808]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.38, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 244/1485.\n",
      "Iteration: 245/1485.\n",
      "Iteration: 246/1485.\n",
      "Iteration: 247/1485.\n",
      "Iteration: 248/1485.\n",
      "Iteration: 249/1485.\n",
      "Iteration: 250/1485.\n",
      "Iteration: 251/1485.\n",
      "Iteration: 252/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.59665360232518, 1.6164312798518714, 1.7236751788932951]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.412, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 253/1485.\n",
      "Iteration: 254/1485.\n",
      "Iteration: 255/1485.\n",
      "Iteration: 256/1485.\n",
      "Iteration: 257/1485.\n",
      "Iteration: 258/1485.\n",
      "Iteration: 259/1485.\n",
      "Iteration: 260/1485.\n",
      "Iteration: 261/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.6898399065125689, 1.6542290674917517, 1.452406349971957]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.416, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 262/1485.\n",
      "Iteration: 263/1485.\n",
      "Iteration: 264/1485.\n",
      "Iteration: 265/1485.\n",
      "Iteration: 266/1485.\n",
      "Iteration: 267/1485.\n",
      "Iteration: 268/1485.\n",
      "Iteration: 269/1485.\n",
      "Iteration: 270/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.5415944286757783, 1.5766113509709612, 1.6724267221259155]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.388, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 271/1485.\n",
      "Iteration: 272/1485.\n",
      "Iteration: 273/1485.\n",
      "Iteration: 274/1485.\n",
      "Iteration: 275/1485.\n",
      "Iteration: 276/1485.\n",
      "Iteration: 277/1485.\n",
      "Iteration: 278/1485.\n",
      "Iteration: 279/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.5184650463860525, 1.733310312603863, 1.45273642358619]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.374, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 280/1485.\n",
      "Iteration: 281/1485.\n",
      "Iteration: 282/1485.\n",
      "Iteration: 283/1485.\n",
      "Iteration: 284/1485.\n",
      "Iteration: 285/1485.\n",
      "Iteration: 286/1485.\n",
      "Iteration: 287/1485.\n",
      "Iteration: 288/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.4811937265363957, 1.5120071213520607, 1.5955602769941997]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.402, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 289/1485.\n",
      "Iteration: 290/1485.\n",
      "Iteration: 291/1485.\n",
      "Iteration: 292/1485.\n",
      "Iteration: 293/1485.\n",
      "Iteration: 294/1485.\n",
      "Iteration: 295/1485.\n",
      "Iteration: 296/1485.\n",
      "Iteration: 297/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.4565737599976951, 1.6386497816474177, 1.5852928795538075]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.442, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 298/1485.\n",
      "Iteration: 299/1485.\n",
      "Iteration: 300/1485.\n",
      "Iteration: 301/1485.\n",
      "Iteration: 302/1485.\n",
      "Iteration: 303/1485.\n",
      "Iteration: 304/1485.\n",
      "Iteration: 305/1485.\n",
      "Iteration: 306/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.5050806127491916, 1.5619986023292907, 1.5465385679162225]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.482, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 307/1485.\n",
      "Iteration: 308/1485.\n",
      "Iteration: 309/1485.\n",
      "Iteration: 310/1485.\n",
      "Iteration: 311/1485.\n",
      "Iteration: 312/1485.\n",
      "Iteration: 313/1485.\n",
      "Iteration: 314/1485.\n",
      "Iteration: 315/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.558604076701741, 1.7285232856602195, 1.5530230725944172]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.476, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 316/1485.\n",
      "Iteration: 317/1485.\n",
      "Iteration: 318/1485.\n",
      "Iteration: 319/1485.\n",
      "Iteration: 320/1485.\n",
      "Iteration: 321/1485.\n",
      "Iteration: 322/1485.\n",
      "Iteration: 323/1485.\n",
      "Iteration: 324/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.5623548278721797, 1.2879084458735508, 1.4779218207182356]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.444, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 325/1485.\n",
      "Iteration: 326/1485.\n",
      "Iteration: 327/1485.\n",
      "Iteration: 328/1485.\n",
      "Iteration: 329/1485.\n",
      "Iteration: 330/1485.\n",
      "Iteration: 331/1485.\n",
      "Iteration: 332/1485.\n",
      "Iteration: 333/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.491849018003894, 1.5007408972598908, 1.506357601471122]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.406, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 334/1485.\n",
      "Iteration: 335/1485.\n",
      "Iteration: 336/1485.\n",
      "Iteration: 337/1485.\n",
      "Iteration: 338/1485.\n",
      "Iteration: 339/1485.\n",
      "Iteration: 340/1485.\n",
      "Iteration: 341/1485.\n",
      "Iteration: 342/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.4857677806549128, 1.4389946527278192, 1.6361758659532593]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.454, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 343/1485.\n",
      "Iteration: 344/1485.\n",
      "Iteration: 345/1485.\n",
      "Iteration: 346/1485.\n",
      "Iteration: 347/1485.\n",
      "Iteration: 348/1485.\n",
      "Iteration: 349/1485.\n",
      "Iteration: 350/1485.\n",
      "Iteration: 351/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.4496513512095335, 1.453255475608344, 1.5575582382027526]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.442, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 352/1485.\n",
      "Iteration: 353/1485.\n",
      "Iteration: 354/1485.\n",
      "Iteration: 355/1485.\n",
      "Iteration: 356/1485.\n",
      "Iteration: 357/1485.\n",
      "Iteration: 358/1485.\n",
      "Iteration: 359/1485.\n",
      "Iteration: 360/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.5781035675227875, 1.410641588479197, 1.5279013954613823]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.46, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 361/1485.\n",
      "Iteration: 362/1485.\n",
      "Iteration: 363/1485.\n",
      "Iteration: 364/1485.\n",
      "Iteration: 365/1485.\n",
      "Iteration: 366/1485.\n",
      "Iteration: 367/1485.\n",
      "Iteration: 368/1485.\n",
      "Iteration: 369/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.4999388183856286, 1.5809875596776781, 1.410948735368885]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.468, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 370/1485.\n",
      "Iteration: 371/1485.\n",
      "Iteration: 372/1485.\n",
      "Iteration: 373/1485.\n",
      "Iteration: 374/1485.\n",
      "Iteration: 375/1485.\n",
      "Iteration: 376/1485.\n",
      "Iteration: 377/1485.\n",
      "Iteration: 378/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.4512285850198103, 1.4407832884959033, 1.4338142023473937]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.484, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 379/1485.\n",
      "Iteration: 380/1485.\n",
      "Iteration: 381/1485.\n",
      "Iteration: 382/1485.\n",
      "Iteration: 383/1485.\n",
      "Iteration: 384/1485.\n",
      "Iteration: 385/1485.\n",
      "Iteration: 386/1485.\n",
      "Iteration: 387/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.4340163987540848, 1.5725654314756852, 1.617204862369583]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.444, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 388/1485.\n",
      "Iteration: 389/1485.\n",
      "Iteration: 390/1485.\n",
      "Iteration: 391/1485.\n",
      "Iteration: 392/1485.\n",
      "Iteration: 393/1485.\n",
      "Iteration: 394/1485.\n",
      "Iteration: 395/1485.\n",
      "Iteration: 396/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.414874246995088, 1.508977458097116, 1.5395553848964005]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.484, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 397/1485.\n",
      "Iteration: 398/1485.\n",
      "Iteration: 399/1485.\n",
      "Iteration: 400/1485.\n",
      "Iteration: 401/1485.\n",
      "Iteration: 402/1485.\n",
      "Iteration: 403/1485.\n",
      "Iteration: 404/1485.\n",
      "Iteration: 405/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.3984743052520432, 1.4557030579985313, 1.4685430877907906]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.458, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 406/1485.\n",
      "Iteration: 407/1485.\n",
      "Iteration: 408/1485.\n",
      "Iteration: 409/1485.\n",
      "Iteration: 410/1485.\n",
      "Iteration: 411/1485.\n",
      "Iteration: 412/1485.\n",
      "Iteration: 413/1485.\n",
      "Iteration: 414/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.3033568366940036, 1.4124325625489054, 1.4310771810036262]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.482, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 415/1485.\n",
      "Iteration: 416/1485.\n",
      "Iteration: 417/1485.\n",
      "Iteration: 418/1485.\n",
      "Iteration: 419/1485.\n",
      "Iteration: 420/1485.\n",
      "Iteration: 421/1485.\n",
      "Iteration: 422/1485.\n",
      "Iteration: 423/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.2784898391360862, 1.51403852865372, 1.5014483395682618]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.506, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 424/1485.\n",
      "Iteration: 425/1485.\n",
      "Iteration: 426/1485.\n",
      "Iteration: 427/1485.\n",
      "Iteration: 428/1485.\n",
      "Iteration: 429/1485.\n",
      "Iteration: 430/1485.\n",
      "Iteration: 431/1485.\n",
      "Iteration: 432/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.3192265693340328, 1.3738786281571924, 1.3411369100825834]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.478, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 433/1485.\n",
      "Iteration: 434/1485.\n",
      "Iteration: 435/1485.\n",
      "Iteration: 436/1485.\n",
      "Iteration: 437/1485.\n",
      "Iteration: 438/1485.\n",
      "Iteration: 439/1485.\n",
      "Iteration: 440/1485.\n",
      "Iteration: 441/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.4152601246984973, 1.4642663505613505, 1.3792307534180037]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.444, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 442/1485.\n",
      "Iteration: 443/1485.\n",
      "Iteration: 444/1485.\n",
      "Iteration: 445/1485.\n",
      "Iteration: 446/1485.\n",
      "Iteration: 447/1485.\n",
      "Iteration: 448/1485.\n",
      "Iteration: 449/1485.\n",
      "Iteration: 450/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.3597240858887145, 1.4652927523714754, 1.5544307721960124]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.496, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 451/1485.\n",
      "Iteration: 452/1485.\n",
      "Iteration: 453/1485.\n",
      "Iteration: 454/1485.\n",
      "Iteration: 455/1485.\n",
      "Iteration: 456/1485.\n",
      "Iteration: 457/1485.\n",
      "Iteration: 458/1485.\n",
      "Iteration: 459/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.3501788676720319, 1.4661698097922917, 1.348096895264067]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.5, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 460/1485.\n",
      "Iteration: 461/1485.\n",
      "Iteration: 462/1485.\n",
      "Iteration: 463/1485.\n",
      "Iteration: 464/1485.\n",
      "Iteration: 465/1485.\n",
      "Iteration: 466/1485.\n",
      "Iteration: 467/1485.\n",
      "Iteration: 468/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.3010774246388925, 1.4108092629753999, 1.4913164386430975]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.47, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 469/1485.\n",
      "Iteration: 470/1485.\n",
      "Iteration: 471/1485.\n",
      "Iteration: 472/1485.\n",
      "Iteration: 473/1485.\n",
      "Iteration: 474/1485.\n",
      "Iteration: 475/1485.\n",
      "Iteration: 476/1485.\n",
      "Iteration: 477/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.5163326639369592, 1.22790077323162, 1.4863954794074752]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.428, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 478/1485.\n",
      "Iteration: 479/1485.\n",
      "Iteration: 480/1485.\n",
      "Iteration: 481/1485.\n",
      "Iteration: 482/1485.\n",
      "Iteration: 483/1485.\n",
      "Iteration: 484/1485.\n",
      "Iteration: 485/1485.\n",
      "Iteration: 486/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.485964615623005, 1.4253524197960754, 1.397202864247266]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.486, Val acc: 0.5\n",
      "\n",
      "\n",
      "Iteration: 487/1485.\n",
      "Iteration: 488/1485.\n",
      "Iteration: 489/1485.\n",
      "Iteration: 490/1485.\n",
      "Iteration: 491/1485.\n",
      "Iteration: 492/1485.\n",
      "Iteration: 493/1485.\n",
      "Iteration: 494/1485.\n",
      "Iteration: 495/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.3173035360558638, 1.439649404669488, 1.4400903081724443]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.49, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 496/1485.\n",
      "Iteration: 497/1485.\n",
      "Iteration: 498/1485.\n",
      "Iteration: 499/1485.\n",
      "Iteration: 500/1485.\n",
      "Iteration: 501/1485.\n",
      "Iteration: 502/1485.\n",
      "Iteration: 503/1485.\n",
      "Iteration: 504/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.413550007390241, 1.3071885293699117, 1.6286592619733646]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.494, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 505/1485.\n",
      "Iteration: 506/1485.\n",
      "Iteration: 507/1485.\n",
      "Iteration: 508/1485.\n",
      "Iteration: 509/1485.\n",
      "Iteration: 510/1485.\n",
      "Iteration: 511/1485.\n",
      "Iteration: 512/1485.\n",
      "Iteration: 513/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.3231102291553214, 1.4542878486712139, 1.3633881222611028]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.502, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 514/1485.\n",
      "Iteration: 515/1485.\n",
      "Iteration: 516/1485.\n",
      "Iteration: 517/1485.\n",
      "Iteration: 518/1485.\n",
      "Iteration: 519/1485.\n",
      "Iteration: 520/1485.\n",
      "Iteration: 521/1485.\n",
      "Iteration: 522/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.4539293989375819, 1.343556855175209, 1.3445161905814356]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.498, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 523/1485.\n",
      "Iteration: 524/1485.\n",
      "Iteration: 525/1485.\n",
      "Iteration: 526/1485.\n",
      "Iteration: 527/1485.\n",
      "Iteration: 528/1485.\n",
      "Iteration: 529/1485.\n",
      "Iteration: 530/1485.\n",
      "Iteration: 531/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.3176452603426307, 1.3044741689331147, 1.3133451389849757]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.534, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 532/1485.\n",
      "Iteration: 533/1485.\n",
      "Iteration: 534/1485.\n",
      "Iteration: 535/1485.\n",
      "Iteration: 536/1485.\n",
      "Iteration: 537/1485.\n",
      "Iteration: 538/1485.\n",
      "Iteration: 539/1485.\n",
      "Iteration: 540/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.269992007709889, 1.3005148992396114, 1.2406554236592409]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.532, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 541/1485.\n",
      "Iteration: 542/1485.\n",
      "Iteration: 543/1485.\n",
      "Iteration: 544/1485.\n",
      "Iteration: 545/1485.\n",
      "Iteration: 546/1485.\n",
      "Iteration: 547/1485.\n",
      "Iteration: 548/1485.\n",
      "Iteration: 549/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.4236339142765377, 1.3395100701188287, 1.4078794144346116]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.516, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 550/1485.\n",
      "Iteration: 551/1485.\n",
      "Iteration: 552/1485.\n",
      "Iteration: 553/1485.\n",
      "Iteration: 554/1485.\n",
      "Iteration: 555/1485.\n",
      "Iteration: 556/1485.\n",
      "Iteration: 557/1485.\n",
      "Iteration: 558/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.2561255224883734, 1.3292067876299836, 1.3030672708757445]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.506, Val acc: 0.48\n",
      "\n",
      "\n",
      "Iteration: 559/1485.\n",
      "Iteration: 560/1485.\n",
      "Iteration: 561/1485.\n",
      "Iteration: 562/1485.\n",
      "Iteration: 563/1485.\n",
      "Iteration: 564/1485.\n",
      "Iteration: 565/1485.\n",
      "Iteration: 566/1485.\n",
      "Iteration: 567/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.3008551570464177, 1.3089350051503499, 1.2253757576970103]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.526, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 568/1485.\n",
      "Iteration: 569/1485.\n",
      "Iteration: 570/1485.\n",
      "Iteration: 571/1485.\n",
      "Iteration: 572/1485.\n",
      "Iteration: 573/1485.\n",
      "Iteration: 574/1485.\n",
      "Iteration: 575/1485.\n",
      "Iteration: 576/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.2826464110822406, 1.3042585778096567, 1.3104385805935885]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.53, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 577/1485.\n",
      "Iteration: 578/1485.\n",
      "Iteration: 579/1485.\n",
      "Iteration: 580/1485.\n",
      "Iteration: 581/1485.\n",
      "Iteration: 582/1485.\n",
      "Iteration: 583/1485.\n",
      "Iteration: 584/1485.\n",
      "Iteration: 585/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.1288244505775813, 1.156202616925294, 1.332721394201669]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.58, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 586/1485.\n",
      "Iteration: 587/1485.\n",
      "Iteration: 588/1485.\n",
      "Iteration: 589/1485.\n",
      "Iteration: 590/1485.\n",
      "Iteration: 591/1485.\n",
      "Iteration: 592/1485.\n",
      "Iteration: 593/1485.\n",
      "Iteration: 594/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.1519215459151555, 1.007901903589812, 1.4008299085446583]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.51, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 595/1485.\n",
      "Iteration: 596/1485.\n",
      "Iteration: 597/1485.\n",
      "Iteration: 598/1485.\n",
      "Iteration: 599/1485.\n",
      "Iteration: 600/1485.\n",
      "Iteration: 601/1485.\n",
      "Iteration: 602/1485.\n",
      "Iteration: 603/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.1823119168214407, 1.2177203665488818, 1.2929611630516138]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.548, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 604/1485.\n",
      "Iteration: 605/1485.\n",
      "Iteration: 606/1485.\n",
      "Iteration: 607/1485.\n",
      "Iteration: 608/1485.\n",
      "Iteration: 609/1485.\n",
      "Iteration: 610/1485.\n",
      "Iteration: 611/1485.\n",
      "Iteration: 612/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.3403214247891548, 1.143113432709407, 1.2321746165741008]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.566, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 613/1485.\n",
      "Iteration: 614/1485.\n",
      "Iteration: 615/1485.\n",
      "Iteration: 616/1485.\n",
      "Iteration: 617/1485.\n",
      "Iteration: 618/1485.\n",
      "Iteration: 619/1485.\n",
      "Iteration: 620/1485.\n",
      "Iteration: 621/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.2416591575442226, 1.111237302597651, 1.318291017433696]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.534, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 622/1485.\n",
      "Iteration: 623/1485.\n",
      "Iteration: 624/1485.\n",
      "Iteration: 625/1485.\n",
      "Iteration: 626/1485.\n",
      "Iteration: 627/1485.\n",
      "Iteration: 628/1485.\n",
      "Iteration: 629/1485.\n",
      "Iteration: 630/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.21826925444824, 1.1864083746885494, 1.23573027886704]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.594, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 631/1485.\n",
      "Iteration: 632/1485.\n",
      "Iteration: 633/1485.\n",
      "Iteration: 634/1485.\n",
      "Iteration: 635/1485.\n",
      "Iteration: 636/1485.\n",
      "Iteration: 637/1485.\n",
      "Iteration: 638/1485.\n",
      "Iteration: 639/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.262181080019778, 1.1426459634388895, 1.084226388897256]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.544, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 640/1485.\n",
      "Iteration: 641/1485.\n",
      "Iteration: 642/1485.\n",
      "Iteration: 643/1485.\n",
      "Iteration: 644/1485.\n",
      "Iteration: 645/1485.\n",
      "Iteration: 646/1485.\n",
      "Iteration: 647/1485.\n",
      "Iteration: 648/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.149267647181754, 1.2170912215981786, 1.1899954286517869]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.602, Val acc: 0.52\n",
      "\n",
      "\n",
      "Iteration: 649/1485.\n",
      "Iteration: 650/1485.\n",
      "Iteration: 651/1485.\n",
      "Iteration: 652/1485.\n",
      "Iteration: 653/1485.\n",
      "Iteration: 654/1485.\n",
      "Iteration: 655/1485.\n",
      "Iteration: 656/1485.\n",
      "Iteration: 657/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.2292232921812687, 1.3340153408998798, 1.316506950781434]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.554, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 658/1485.\n",
      "Iteration: 659/1485.\n",
      "Iteration: 660/1485.\n",
      "Iteration: 661/1485.\n",
      "Iteration: 662/1485.\n",
      "Iteration: 663/1485.\n",
      "Iteration: 664/1485.\n",
      "Iteration: 665/1485.\n",
      "Iteration: 666/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.1376108296389291, 1.1380914335919465, 1.167066496344811]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.572, Val acc: 0.48\n",
      "\n",
      "\n",
      "Iteration: 667/1485.\n",
      "Iteration: 668/1485.\n",
      "Iteration: 669/1485.\n",
      "Iteration: 670/1485.\n",
      "Iteration: 671/1485.\n",
      "Iteration: 672/1485.\n",
      "Iteration: 673/1485.\n",
      "Iteration: 674/1485.\n",
      "Iteration: 675/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.9732794901445343, 1.2155158493544194, 1.0412060700783392]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.58, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 676/1485.\n",
      "Iteration: 677/1485.\n",
      "Iteration: 678/1485.\n",
      "Iteration: 679/1485.\n",
      "Iteration: 680/1485.\n",
      "Iteration: 681/1485.\n",
      "Iteration: 682/1485.\n",
      "Iteration: 683/1485.\n",
      "Iteration: 684/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.1179898937377633, 1.1725843336479098, 1.170602317393033]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.594, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 685/1485.\n",
      "Iteration: 686/1485.\n",
      "Iteration: 687/1485.\n",
      "Iteration: 688/1485.\n",
      "Iteration: 689/1485.\n",
      "Iteration: 690/1485.\n",
      "Iteration: 691/1485.\n",
      "Iteration: 692/1485.\n",
      "Iteration: 693/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.0778066405746527, 1.2245078988346767, 1.1453827288447078]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.588, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 694/1485.\n",
      "Iteration: 695/1485.\n",
      "Iteration: 696/1485.\n",
      "Iteration: 697/1485.\n",
      "Iteration: 698/1485.\n",
      "Iteration: 699/1485.\n",
      "Iteration: 700/1485.\n",
      "Iteration: 701/1485.\n",
      "Iteration: 702/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.2801919586678006, 0.97703494555639, 1.0875814610548387]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.592, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 703/1485.\n",
      "Iteration: 704/1485.\n",
      "Iteration: 705/1485.\n",
      "Iteration: 706/1485.\n",
      "Iteration: 707/1485.\n",
      "Iteration: 708/1485.\n",
      "Iteration: 709/1485.\n",
      "Iteration: 710/1485.\n",
      "Iteration: 711/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.0205603131007122, 1.1376361761098464, 1.1070220768561414]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.574, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 712/1485.\n",
      "Iteration: 713/1485.\n",
      "Iteration: 714/1485.\n",
      "Iteration: 715/1485.\n",
      "Iteration: 716/1485.\n",
      "Iteration: 717/1485.\n",
      "Iteration: 718/1485.\n",
      "Iteration: 719/1485.\n",
      "Iteration: 720/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.1478082270115944, 0.8837831491111774, 1.1681464638920034]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.596, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 721/1485.\n",
      "Iteration: 722/1485.\n",
      "Iteration: 723/1485.\n",
      "Iteration: 724/1485.\n",
      "Iteration: 725/1485.\n",
      "Iteration: 726/1485.\n",
      "Iteration: 727/1485.\n",
      "Iteration: 728/1485.\n",
      "Iteration: 729/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.1514198520905663, 1.282891466047233, 1.2270859749783023]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.578, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 730/1485.\n",
      "Iteration: 731/1485.\n",
      "Iteration: 732/1485.\n",
      "Iteration: 733/1485.\n",
      "Iteration: 734/1485.\n",
      "Iteration: 735/1485.\n",
      "Iteration: 736/1485.\n",
      "Iteration: 737/1485.\n",
      "Iteration: 738/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.0930238402413017, 0.9062983219563698, 1.0320299024209445]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.562, Val acc: 0.48\n",
      "\n",
      "\n",
      "Iteration: 739/1485.\n",
      "Iteration: 740/1485.\n",
      "Iteration: 741/1485.\n",
      "Iteration: 742/1485.\n",
      "Iteration: 743/1485.\n",
      "Iteration: 744/1485.\n",
      "Iteration: 745/1485.\n",
      "Iteration: 746/1485.\n",
      "Iteration: 747/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.0583500092304987, 1.1762666326568494, 1.2316055395874028]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.64, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 748/1485.\n",
      "Iteration: 749/1485.\n",
      "Iteration: 750/1485.\n",
      "Iteration: 751/1485.\n",
      "Iteration: 752/1485.\n",
      "Iteration: 753/1485.\n",
      "Iteration: 754/1485.\n",
      "Iteration: 755/1485.\n",
      "Iteration: 756/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.3264721012204108, 1.221411701337184, 0.9065824287396985]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.608, Val acc: 0.48\n",
      "\n",
      "\n",
      "Iteration: 757/1485.\n",
      "Iteration: 758/1485.\n",
      "Iteration: 759/1485.\n",
      "Iteration: 760/1485.\n",
      "Iteration: 761/1485.\n",
      "Iteration: 762/1485.\n",
      "Iteration: 763/1485.\n",
      "Iteration: 764/1485.\n",
      "Iteration: 765/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.0802620472527682, 1.1025919171874465, 1.0120444707935587]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.624, Val acc: 0.48\n",
      "\n",
      "\n",
      "Iteration: 766/1485.\n",
      "Iteration: 767/1485.\n",
      "Iteration: 768/1485.\n",
      "Iteration: 769/1485.\n",
      "Iteration: 770/1485.\n",
      "Iteration: 771/1485.\n",
      "Iteration: 772/1485.\n",
      "Iteration: 773/1485.\n",
      "Iteration: 774/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.1447184625901943, 1.0899537619231097, 0.9529898810283335]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.626, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 775/1485.\n",
      "Iteration: 776/1485.\n",
      "Iteration: 777/1485.\n",
      "Iteration: 778/1485.\n",
      "Iteration: 779/1485.\n",
      "Iteration: 780/1485.\n",
      "Iteration: 781/1485.\n",
      "Iteration: 782/1485.\n",
      "Iteration: 783/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.0990496667238314, 1.0511274319263828, 1.0916851584340321]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.606, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 784/1485.\n",
      "Iteration: 785/1485.\n",
      "Iteration: 786/1485.\n",
      "Iteration: 787/1485.\n",
      "Iteration: 788/1485.\n",
      "Iteration: 789/1485.\n",
      "Iteration: 790/1485.\n",
      "Iteration: 791/1485.\n",
      "Iteration: 792/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.0364291762384132, 1.1909746131119958, 1.0429743639887814]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.59, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 793/1485.\n",
      "Iteration: 794/1485.\n",
      "Iteration: 795/1485.\n",
      "Iteration: 796/1485.\n",
      "Iteration: 797/1485.\n",
      "Iteration: 798/1485.\n",
      "Iteration: 799/1485.\n",
      "Iteration: 800/1485.\n",
      "Iteration: 801/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.1963783449148222, 1.2563248625307917, 1.1093455191618644]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.694, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 802/1485.\n",
      "Iteration: 803/1485.\n",
      "Iteration: 804/1485.\n",
      "Iteration: 805/1485.\n",
      "Iteration: 806/1485.\n",
      "Iteration: 807/1485.\n",
      "Iteration: 808/1485.\n",
      "Iteration: 809/1485.\n",
      "Iteration: 810/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.1138202091600506, 1.1476933928544109, 1.106431851216113]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.632, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 811/1485.\n",
      "Iteration: 812/1485.\n",
      "Iteration: 813/1485.\n",
      "Iteration: 814/1485.\n",
      "Iteration: 815/1485.\n",
      "Iteration: 816/1485.\n",
      "Iteration: 817/1485.\n",
      "Iteration: 818/1485.\n",
      "Iteration: 819/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.8609561814126732, 1.0034614600777065, 1.018394639053404]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.604, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 820/1485.\n",
      "Iteration: 821/1485.\n",
      "Iteration: 822/1485.\n",
      "Iteration: 823/1485.\n",
      "Iteration: 824/1485.\n",
      "Iteration: 825/1485.\n",
      "Iteration: 826/1485.\n",
      "Iteration: 827/1485.\n",
      "Iteration: 828/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.0445380097422632, 0.8883567805140967, 0.9661278246757052]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.564, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 829/1485.\n",
      "Iteration: 830/1485.\n",
      "Iteration: 831/1485.\n",
      "Iteration: 832/1485.\n",
      "Iteration: 833/1485.\n",
      "Iteration: 834/1485.\n",
      "Iteration: 835/1485.\n",
      "Iteration: 836/1485.\n",
      "Iteration: 837/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.111135023748373, 0.9141386447688744, 0.8805842883181758]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.634, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 838/1485.\n",
      "Iteration: 839/1485.\n",
      "Iteration: 840/1485.\n",
      "Iteration: 841/1485.\n",
      "Iteration: 842/1485.\n",
      "Iteration: 843/1485.\n",
      "Iteration: 844/1485.\n",
      "Iteration: 845/1485.\n",
      "Iteration: 846/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [1.0002711688318313, 0.9913188854642503, 1.026928002895162]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.582, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 847/1485.\n",
      "Iteration: 848/1485.\n",
      "Iteration: 849/1485.\n",
      "Iteration: 850/1485.\n",
      "Iteration: 851/1485.\n",
      "Iteration: 852/1485.\n",
      "Iteration: 853/1485.\n",
      "Iteration: 854/1485.\n",
      "Iteration: 855/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.9955307838435138, 0.9468434672907078, 0.909510587194518]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.656, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 856/1485.\n",
      "Iteration: 857/1485.\n",
      "Iteration: 858/1485.\n",
      "Iteration: 859/1485.\n",
      "Iteration: 860/1485.\n",
      "Iteration: 861/1485.\n",
      "Iteration: 862/1485.\n",
      "Iteration: 863/1485.\n",
      "Iteration: 864/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.9811743122051134, 1.0150771635573825, 0.9703557499567347]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.658, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 865/1485.\n",
      "Iteration: 866/1485.\n",
      "Iteration: 867/1485.\n",
      "Iteration: 868/1485.\n",
      "Iteration: 869/1485.\n",
      "Iteration: 870/1485.\n",
      "Iteration: 871/1485.\n",
      "Iteration: 872/1485.\n",
      "Iteration: 873/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.9406615313276104, 0.9109401796251315, 0.8402450801497874]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.676, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 874/1485.\n",
      "Iteration: 875/1485.\n",
      "Iteration: 876/1485.\n",
      "Iteration: 877/1485.\n",
      "Iteration: 878/1485.\n",
      "Iteration: 879/1485.\n",
      "Iteration: 880/1485.\n",
      "Iteration: 881/1485.\n",
      "Iteration: 882/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.834298197414362, 0.887610646386123, 0.986976499480563]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.66, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 883/1485.\n",
      "Iteration: 884/1485.\n",
      "Iteration: 885/1485.\n",
      "Iteration: 886/1485.\n",
      "Iteration: 887/1485.\n",
      "Iteration: 888/1485.\n",
      "Iteration: 889/1485.\n",
      "Iteration: 890/1485.\n",
      "Iteration: 891/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.9276640249169738, 0.927547249218368, 0.9471106566545434]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.712, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 892/1485.\n",
      "Iteration: 893/1485.\n",
      "Iteration: 894/1485.\n",
      "Iteration: 895/1485.\n",
      "Iteration: 896/1485.\n",
      "Iteration: 897/1485.\n",
      "Iteration: 898/1485.\n",
      "Iteration: 899/1485.\n",
      "Iteration: 900/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.9067815395714018, 1.0454335731345998, 1.0607225993335299]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.658, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 901/1485.\n",
      "Iteration: 902/1485.\n",
      "Iteration: 903/1485.\n",
      "Iteration: 904/1485.\n",
      "Iteration: 905/1485.\n",
      "Iteration: 906/1485.\n",
      "Iteration: 907/1485.\n",
      "Iteration: 908/1485.\n",
      "Iteration: 909/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.8427859543136914, 0.942160493663663, 0.9567560222790195]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.642, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 910/1485.\n",
      "Iteration: 911/1485.\n",
      "Iteration: 912/1485.\n",
      "Iteration: 913/1485.\n",
      "Iteration: 914/1485.\n",
      "Iteration: 915/1485.\n",
      "Iteration: 916/1485.\n",
      "Iteration: 917/1485.\n",
      "Iteration: 918/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.9418253904827638, 0.9093623970922358, 0.8483660994376551]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.654, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 919/1485.\n",
      "Iteration: 920/1485.\n",
      "Iteration: 921/1485.\n",
      "Iteration: 922/1485.\n",
      "Iteration: 923/1485.\n",
      "Iteration: 924/1485.\n",
      "Iteration: 925/1485.\n",
      "Iteration: 926/1485.\n",
      "Iteration: 927/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.9651747623553592, 1.159818800455569, 0.9212908881239409]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.696, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 928/1485.\n",
      "Iteration: 929/1485.\n",
      "Iteration: 930/1485.\n",
      "Iteration: 931/1485.\n",
      "Iteration: 932/1485.\n",
      "Iteration: 933/1485.\n",
      "Iteration: 934/1485.\n",
      "Iteration: 935/1485.\n",
      "Iteration: 936/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.8446944992537003, 1.096209980508435, 0.8915794111385088]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.712, Val acc: 0.48\n",
      "\n",
      "\n",
      "Iteration: 937/1485.\n",
      "Iteration: 938/1485.\n",
      "Iteration: 939/1485.\n",
      "Iteration: 940/1485.\n",
      "Iteration: 941/1485.\n",
      "Iteration: 942/1485.\n",
      "Iteration: 943/1485.\n",
      "Iteration: 944/1485.\n",
      "Iteration: 945/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.7688078617990485, 0.7644698957362109, 0.8773279212360339]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.68, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 946/1485.\n",
      "Iteration: 947/1485.\n",
      "Iteration: 948/1485.\n",
      "Iteration: 949/1485.\n",
      "Iteration: 950/1485.\n",
      "Iteration: 951/1485.\n",
      "Iteration: 952/1485.\n",
      "Iteration: 953/1485.\n",
      "Iteration: 954/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.7865605967915533, 0.9845096679606521, 0.9899391147072014]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.668, Val acc: 0.34\n",
      "\n",
      "\n",
      "Iteration: 955/1485.\n",
      "Iteration: 956/1485.\n",
      "Iteration: 957/1485.\n",
      "Iteration: 958/1485.\n",
      "Iteration: 959/1485.\n",
      "Iteration: 960/1485.\n",
      "Iteration: 961/1485.\n",
      "Iteration: 962/1485.\n",
      "Iteration: 963/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.7509827576596279, 0.7351952118999756, 0.7910339870439728]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.668, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 964/1485.\n",
      "Iteration: 965/1485.\n",
      "Iteration: 966/1485.\n",
      "Iteration: 967/1485.\n",
      "Iteration: 968/1485.\n",
      "Iteration: 969/1485.\n",
      "Iteration: 970/1485.\n",
      "Iteration: 971/1485.\n",
      "Iteration: 972/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.7365149117299004, 0.9026675413469291, 0.695172115863497]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.756, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 973/1485.\n",
      "Iteration: 974/1485.\n",
      "Iteration: 975/1485.\n",
      "Iteration: 976/1485.\n",
      "Iteration: 977/1485.\n",
      "Iteration: 978/1485.\n",
      "Iteration: 979/1485.\n",
      "Iteration: 980/1485.\n",
      "Iteration: 981/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.925344185901762, 0.8238442869202974, 0.9877696822090704]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.662, Val acc: 0.5\n",
      "\n",
      "\n",
      "Iteration: 982/1485.\n",
      "Iteration: 983/1485.\n",
      "Iteration: 984/1485.\n",
      "Iteration: 985/1485.\n",
      "Iteration: 986/1485.\n",
      "Iteration: 987/1485.\n",
      "Iteration: 988/1485.\n",
      "Iteration: 989/1485.\n",
      "Iteration: 990/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.8248673100379585, 0.7355965321264111, 0.8985336771716618]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.664, Val acc: 0.3\n",
      "\n",
      "\n",
      "Iteration: 991/1485.\n",
      "Iteration: 992/1485.\n",
      "Iteration: 993/1485.\n",
      "Iteration: 994/1485.\n",
      "Iteration: 995/1485.\n",
      "Iteration: 996/1485.\n",
      "Iteration: 997/1485.\n",
      "Iteration: 998/1485.\n",
      "Iteration: 999/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.8892037729181188, 0.7011140008324215, 0.740212031718093]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.734, Val acc: 0.48\n",
      "\n",
      "\n",
      "Iteration: 1000/1485.\n",
      "Iteration: 1001/1485.\n",
      "Iteration: 1002/1485.\n",
      "Iteration: 1003/1485.\n",
      "Iteration: 1004/1485.\n",
      "Iteration: 1005/1485.\n",
      "Iteration: 1006/1485.\n",
      "Iteration: 1007/1485.\n",
      "Iteration: 1008/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.9184380152952702, 0.9290785442962054, 0.6458994910487427]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.734, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 1009/1485.\n",
      "Iteration: 1010/1485.\n",
      "Iteration: 1011/1485.\n",
      "Iteration: 1012/1485.\n",
      "Iteration: 1013/1485.\n",
      "Iteration: 1014/1485.\n",
      "Iteration: 1015/1485.\n",
      "Iteration: 1016/1485.\n",
      "Iteration: 1017/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.6110802963595195, 0.8353633715815714, 0.9436510668157254]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.708, Val acc: 0.32\n",
      "\n",
      "\n",
      "Iteration: 1018/1485.\n",
      "Iteration: 1019/1485.\n",
      "Iteration: 1020/1485.\n",
      "Iteration: 1021/1485.\n",
      "Iteration: 1022/1485.\n",
      "Iteration: 1023/1485.\n",
      "Iteration: 1024/1485.\n",
      "Iteration: 1025/1485.\n",
      "Iteration: 1026/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.8409701561882207, 0.8740852922054083, 0.7641421266457356]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.688, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1027/1485.\n",
      "Iteration: 1028/1485.\n",
      "Iteration: 1029/1485.\n",
      "Iteration: 1030/1485.\n",
      "Iteration: 1031/1485.\n",
      "Iteration: 1032/1485.\n",
      "Iteration: 1033/1485.\n",
      "Iteration: 1034/1485.\n",
      "Iteration: 1035/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.810738861945341, 0.8330717876841013, 0.7041309004750573]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.708, Val acc: 0.56\n",
      "\n",
      "\n",
      "Iteration: 1036/1485.\n",
      "Iteration: 1037/1485.\n",
      "Iteration: 1038/1485.\n",
      "Iteration: 1039/1485.\n",
      "Iteration: 1040/1485.\n",
      "Iteration: 1041/1485.\n",
      "Iteration: 1042/1485.\n",
      "Iteration: 1043/1485.\n",
      "Iteration: 1044/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.8060342692596143, 0.6729492997373338, 0.8502771092327411]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.748, Val acc: 0.48\n",
      "\n",
      "\n",
      "Iteration: 1045/1485.\n",
      "Iteration: 1046/1485.\n",
      "Iteration: 1047/1485.\n",
      "Iteration: 1048/1485.\n",
      "Iteration: 1049/1485.\n",
      "Iteration: 1050/1485.\n",
      "Iteration: 1051/1485.\n",
      "Iteration: 1052/1485.\n",
      "Iteration: 1053/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.7908500295635194, 0.7012661165339732, 0.650527484159589]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.734, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 1054/1485.\n",
      "Iteration: 1055/1485.\n",
      "Iteration: 1056/1485.\n",
      "Iteration: 1057/1485.\n",
      "Iteration: 1058/1485.\n",
      "Iteration: 1059/1485.\n",
      "Iteration: 1060/1485.\n",
      "Iteration: 1061/1485.\n",
      "Iteration: 1062/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.9388494247142442, 0.9571191397076626, 0.8698137248879403]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.694, Val acc: 0.54\n",
      "\n",
      "\n",
      "Iteration: 1063/1485.\n",
      "Iteration: 1064/1485.\n",
      "Iteration: 1065/1485.\n",
      "Iteration: 1066/1485.\n",
      "Iteration: 1067/1485.\n",
      "Iteration: 1068/1485.\n",
      "Iteration: 1069/1485.\n",
      "Iteration: 1070/1485.\n",
      "Iteration: 1071/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.7380067901991326, 0.7549184449389458, 0.6938185720656685]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.748, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 1072/1485.\n",
      "Iteration: 1073/1485.\n",
      "Iteration: 1074/1485.\n",
      "Iteration: 1075/1485.\n",
      "Iteration: 1076/1485.\n",
      "Iteration: 1077/1485.\n",
      "Iteration: 1078/1485.\n",
      "Iteration: 1079/1485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1080/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.7618266713276903, 0.6544097563019945, 0.6942676964617512]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.758, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 1081/1485.\n",
      "Iteration: 1082/1485.\n",
      "Iteration: 1083/1485.\n",
      "Iteration: 1084/1485.\n",
      "Iteration: 1085/1485.\n",
      "Iteration: 1086/1485.\n",
      "Iteration: 1087/1485.\n",
      "Iteration: 1088/1485.\n",
      "Iteration: 1089/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.8399014777650902, 0.7735057554807135, 0.7409869177520839]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.72, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 1090/1485.\n",
      "Iteration: 1091/1485.\n",
      "Iteration: 1092/1485.\n",
      "Iteration: 1093/1485.\n",
      "Iteration: 1094/1485.\n",
      "Iteration: 1095/1485.\n",
      "Iteration: 1096/1485.\n",
      "Iteration: 1097/1485.\n",
      "Iteration: 1098/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.8304020598200471, 0.9913156545225121, 0.9114896348877648]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.722, Val acc: 0.52\n",
      "\n",
      "\n",
      "Iteration: 1099/1485.\n",
      "Iteration: 1100/1485.\n",
      "Iteration: 1101/1485.\n",
      "Iteration: 1102/1485.\n",
      "Iteration: 1103/1485.\n",
      "Iteration: 1104/1485.\n",
      "Iteration: 1105/1485.\n",
      "Iteration: 1106/1485.\n",
      "Iteration: 1107/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.8801226846927083, 0.8727642513708814, 0.6981917657202653]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.726, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1108/1485.\n",
      "Iteration: 1109/1485.\n",
      "Iteration: 1110/1485.\n",
      "Iteration: 1111/1485.\n",
      "Iteration: 1112/1485.\n",
      "Iteration: 1113/1485.\n",
      "Iteration: 1114/1485.\n",
      "Iteration: 1115/1485.\n",
      "Iteration: 1116/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.6316050864357038, 0.5791042187264205, 0.8872640252761416]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.722, Val acc: 0.54\n",
      "\n",
      "\n",
      "Iteration: 1117/1485.\n",
      "Iteration: 1118/1485.\n",
      "Iteration: 1119/1485.\n",
      "Iteration: 1120/1485.\n",
      "Iteration: 1121/1485.\n",
      "Iteration: 1122/1485.\n",
      "Iteration: 1123/1485.\n",
      "Iteration: 1124/1485.\n",
      "Iteration: 1125/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.63291807424533, 0.6792139173041799, 0.621702055230249]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.754, Val acc: 0.56\n",
      "\n",
      "\n",
      "Iteration: 1126/1485.\n",
      "Iteration: 1127/1485.\n",
      "Iteration: 1128/1485.\n",
      "Iteration: 1129/1485.\n",
      "Iteration: 1130/1485.\n",
      "Iteration: 1131/1485.\n",
      "Iteration: 1132/1485.\n",
      "Iteration: 1133/1485.\n",
      "Iteration: 1134/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.7197501131775544, 0.7694791531468543, 0.7438660433046468]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.756, Val acc: 0.5\n",
      "\n",
      "\n",
      "Iteration: 1135/1485.\n",
      "Iteration: 1136/1485.\n",
      "Iteration: 1137/1485.\n",
      "Iteration: 1138/1485.\n",
      "Iteration: 1139/1485.\n",
      "Iteration: 1140/1485.\n",
      "Iteration: 1141/1485.\n",
      "Iteration: 1142/1485.\n",
      "Iteration: 1143/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.6631156681014011, 0.5730173081895705, 0.6119822667371536]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.734, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 1144/1485.\n",
      "Iteration: 1145/1485.\n",
      "Iteration: 1146/1485.\n",
      "Iteration: 1147/1485.\n",
      "Iteration: 1148/1485.\n",
      "Iteration: 1149/1485.\n",
      "Iteration: 1150/1485.\n",
      "Iteration: 1151/1485.\n",
      "Iteration: 1152/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.6952082168574143, 0.6152646015233146, 0.8531127641946726]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.796, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1153/1485.\n",
      "Iteration: 1154/1485.\n",
      "Iteration: 1155/1485.\n",
      "Iteration: 1156/1485.\n",
      "Iteration: 1157/1485.\n",
      "Iteration: 1158/1485.\n",
      "Iteration: 1159/1485.\n",
      "Iteration: 1160/1485.\n",
      "Iteration: 1161/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.5209582227267812, 0.7639332054499453, 0.7665743395499984]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.73, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 1162/1485.\n",
      "Iteration: 1163/1485.\n",
      "Iteration: 1164/1485.\n",
      "Iteration: 1165/1485.\n",
      "Iteration: 1166/1485.\n",
      "Iteration: 1167/1485.\n",
      "Iteration: 1168/1485.\n",
      "Iteration: 1169/1485.\n",
      "Iteration: 1170/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.5731906816326281, 0.7685427264285799, 0.7956345723199787]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.68, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 1171/1485.\n",
      "Iteration: 1172/1485.\n",
      "Iteration: 1173/1485.\n",
      "Iteration: 1174/1485.\n",
      "Iteration: 1175/1485.\n",
      "Iteration: 1176/1485.\n",
      "Iteration: 1177/1485.\n",
      "Iteration: 1178/1485.\n",
      "Iteration: 1179/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.6268765319014213, 0.480560748121569, 0.7228226575614985]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.696, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1180/1485.\n",
      "Iteration: 1181/1485.\n",
      "Iteration: 1182/1485.\n",
      "Iteration: 1183/1485.\n",
      "Iteration: 1184/1485.\n",
      "Iteration: 1185/1485.\n",
      "Iteration: 1186/1485.\n",
      "Iteration: 1187/1485.\n",
      "Iteration: 1188/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.751318176380917, 0.663475217216961, 0.7686533706525158]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.732, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 1189/1485.\n",
      "Iteration: 1190/1485.\n",
      "Iteration: 1191/1485.\n",
      "Iteration: 1192/1485.\n",
      "Iteration: 1193/1485.\n",
      "Iteration: 1194/1485.\n",
      "Iteration: 1195/1485.\n",
      "Iteration: 1196/1485.\n",
      "Iteration: 1197/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.7042272756782721, 0.8056101972571197, 0.6453387372560261]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.792, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 1198/1485.\n",
      "Iteration: 1199/1485.\n",
      "Iteration: 1200/1485.\n",
      "Iteration: 1201/1485.\n",
      "Iteration: 1202/1485.\n",
      "Iteration: 1203/1485.\n",
      "Iteration: 1204/1485.\n",
      "Iteration: 1205/1485.\n",
      "Iteration: 1206/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.639082832930546, 0.7381953824752867, 0.6782729584242994]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.814, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 1207/1485.\n",
      "Iteration: 1208/1485.\n",
      "Iteration: 1209/1485.\n",
      "Iteration: 1210/1485.\n",
      "Iteration: 1211/1485.\n",
      "Iteration: 1212/1485.\n",
      "Iteration: 1213/1485.\n",
      "Iteration: 1214/1485.\n",
      "Iteration: 1215/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.6068370849682713, 0.5879709992917811, 0.5672012636685473]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.806, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1216/1485.\n",
      "Iteration: 1217/1485.\n",
      "Iteration: 1218/1485.\n",
      "Iteration: 1219/1485.\n",
      "Iteration: 1220/1485.\n",
      "Iteration: 1221/1485.\n",
      "Iteration: 1222/1485.\n",
      "Iteration: 1223/1485.\n",
      "Iteration: 1224/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.5780939199020525, 0.7247082640470212, 0.5653762183912656]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.83, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 1225/1485.\n",
      "Iteration: 1226/1485.\n",
      "Iteration: 1227/1485.\n",
      "Iteration: 1228/1485.\n",
      "Iteration: 1229/1485.\n",
      "Iteration: 1230/1485.\n",
      "Iteration: 1231/1485.\n",
      "Iteration: 1232/1485.\n",
      "Iteration: 1233/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.78460821479328, 0.7481795855584596, 0.5735905983712006]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.786, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1234/1485.\n",
      "Iteration: 1235/1485.\n",
      "Iteration: 1236/1485.\n",
      "Iteration: 1237/1485.\n",
      "Iteration: 1238/1485.\n",
      "Iteration: 1239/1485.\n",
      "Iteration: 1240/1485.\n",
      "Iteration: 1241/1485.\n",
      "Iteration: 1242/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.49782751485361315, 0.518695511448751, 0.4330243458312796]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.788, Val acc: 0.52\n",
      "\n",
      "\n",
      "Iteration: 1243/1485.\n",
      "Iteration: 1244/1485.\n",
      "Iteration: 1245/1485.\n",
      "Iteration: 1246/1485.\n",
      "Iteration: 1247/1485.\n",
      "Iteration: 1248/1485.\n",
      "Iteration: 1249/1485.\n",
      "Iteration: 1250/1485.\n",
      "Iteration: 1251/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.7345405892814383, 0.8045199797421269, 0.7636770092255766]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train acc: 0.708, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 1252/1485.\n",
      "Iteration: 1253/1485.\n",
      "Iteration: 1254/1485.\n",
      "Iteration: 1255/1485.\n",
      "Iteration: 1256/1485.\n",
      "Iteration: 1257/1485.\n",
      "Iteration: 1258/1485.\n",
      "Iteration: 1259/1485.\n",
      "Iteration: 1260/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.5175385932679349, 0.5081483352699074, 0.5447556507061871]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.808, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 1261/1485.\n",
      "Iteration: 1262/1485.\n",
      "Iteration: 1263/1485.\n",
      "Iteration: 1264/1485.\n",
      "Iteration: 1265/1485.\n",
      "Iteration: 1266/1485.\n",
      "Iteration: 1267/1485.\n",
      "Iteration: 1268/1485.\n",
      "Iteration: 1269/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.5089409524188038, 0.6254075863815982, 0.5875134817913333]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.802, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 1270/1485.\n",
      "Iteration: 1271/1485.\n",
      "Iteration: 1272/1485.\n",
      "Iteration: 1273/1485.\n",
      "Iteration: 1274/1485.\n",
      "Iteration: 1275/1485.\n",
      "Iteration: 1276/1485.\n",
      "Iteration: 1277/1485.\n",
      "Iteration: 1278/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.6160500412594098, 0.530167483128414, 0.4309412512116603]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.794, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 1279/1485.\n",
      "Iteration: 1280/1485.\n",
      "Iteration: 1281/1485.\n",
      "Iteration: 1282/1485.\n",
      "Iteration: 1283/1485.\n",
      "Iteration: 1284/1485.\n",
      "Iteration: 1285/1485.\n",
      "Iteration: 1286/1485.\n",
      "Iteration: 1287/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.579840972661745, 0.48281517248800215, 0.5425944702214803]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.842, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 1288/1485.\n",
      "Iteration: 1289/1485.\n",
      "Iteration: 1290/1485.\n",
      "Iteration: 1291/1485.\n",
      "Iteration: 1292/1485.\n",
      "Iteration: 1293/1485.\n",
      "Iteration: 1294/1485.\n",
      "Iteration: 1295/1485.\n",
      "Iteration: 1296/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.7177733470744266, 0.459175577854445, 0.47101352608564384]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.846, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 1297/1485.\n",
      "Iteration: 1298/1485.\n",
      "Iteration: 1299/1485.\n",
      "Iteration: 1300/1485.\n",
      "Iteration: 1301/1485.\n",
      "Iteration: 1302/1485.\n",
      "Iteration: 1303/1485.\n",
      "Iteration: 1304/1485.\n",
      "Iteration: 1305/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.672252163739545, 0.947235740734324, 0.6084319125240255]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.812, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 1306/1485.\n",
      "Iteration: 1307/1485.\n",
      "Iteration: 1308/1485.\n",
      "Iteration: 1309/1485.\n",
      "Iteration: 1310/1485.\n",
      "Iteration: 1311/1485.\n",
      "Iteration: 1312/1485.\n",
      "Iteration: 1313/1485.\n",
      "Iteration: 1314/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.5068521615231688, 0.463545003902302, 0.4666935005219137]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.79, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 1315/1485.\n",
      "Iteration: 1316/1485.\n",
      "Iteration: 1317/1485.\n",
      "Iteration: 1318/1485.\n",
      "Iteration: 1319/1485.\n",
      "Iteration: 1320/1485.\n",
      "Iteration: 1321/1485.\n",
      "Iteration: 1322/1485.\n",
      "Iteration: 1323/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.3813650650741876, 0.4080350613834247, 0.47137253347218006]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.85, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 1324/1485.\n",
      "Iteration: 1325/1485.\n",
      "Iteration: 1326/1485.\n",
      "Iteration: 1327/1485.\n",
      "Iteration: 1328/1485.\n",
      "Iteration: 1329/1485.\n",
      "Iteration: 1330/1485.\n",
      "Iteration: 1331/1485.\n",
      "Iteration: 1332/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.4088379942447534, 0.4672052283475314, 0.504108226367286]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.878, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1333/1485.\n",
      "Iteration: 1334/1485.\n",
      "Iteration: 1335/1485.\n",
      "Iteration: 1336/1485.\n",
      "Iteration: 1337/1485.\n",
      "Iteration: 1338/1485.\n",
      "Iteration: 1339/1485.\n",
      "Iteration: 1340/1485.\n",
      "Iteration: 1341/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.6220278938817991, 0.5962830695900962, 0.34129214069782876]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.874, Val acc: 0.36\n",
      "\n",
      "\n",
      "Iteration: 1342/1485.\n",
      "Iteration: 1343/1485.\n",
      "Iteration: 1344/1485.\n",
      "Iteration: 1345/1485.\n",
      "Iteration: 1346/1485.\n",
      "Iteration: 1347/1485.\n",
      "Iteration: 1348/1485.\n",
      "Iteration: 1349/1485.\n",
      "Iteration: 1350/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.6044496139368165, 0.5840754718879066, 0.5044818365703043]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.854, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1351/1485.\n",
      "Iteration: 1352/1485.\n",
      "Iteration: 1353/1485.\n",
      "Iteration: 1354/1485.\n",
      "Iteration: 1355/1485.\n",
      "Iteration: 1356/1485.\n",
      "Iteration: 1357/1485.\n",
      "Iteration: 1358/1485.\n",
      "Iteration: 1359/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.32899429764581906, 0.4552106207073862, 0.7136078117397855]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.764, Val acc: 0.48\n",
      "\n",
      "\n",
      "Iteration: 1360/1485.\n",
      "Iteration: 1361/1485.\n",
      "Iteration: 1362/1485.\n",
      "Iteration: 1363/1485.\n",
      "Iteration: 1364/1485.\n",
      "Iteration: 1365/1485.\n",
      "Iteration: 1366/1485.\n",
      "Iteration: 1367/1485.\n",
      "Iteration: 1368/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.46024045344941455, 0.3861036337542681, 0.33061367599746955]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.834, Val acc: 0.52\n",
      "\n",
      "\n",
      "Iteration: 1369/1485.\n",
      "Iteration: 1370/1485.\n",
      "Iteration: 1371/1485.\n",
      "Iteration: 1372/1485.\n",
      "Iteration: 1373/1485.\n",
      "Iteration: 1374/1485.\n",
      "Iteration: 1375/1485.\n",
      "Iteration: 1376/1485.\n",
      "Iteration: 1377/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.5709725690818614, 0.7383884668077255, 0.7443166812124444]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.866, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 1378/1485.\n",
      "Iteration: 1379/1485.\n",
      "Iteration: 1380/1485.\n",
      "Iteration: 1381/1485.\n",
      "Iteration: 1382/1485.\n",
      "Iteration: 1383/1485.\n",
      "Iteration: 1384/1485.\n",
      "Iteration: 1385/1485.\n",
      "Iteration: 1386/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.6862983479637055, 0.5373638291230602, 0.36242148872632324]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.8, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 1387/1485.\n",
      "Iteration: 1388/1485.\n",
      "Iteration: 1389/1485.\n",
      "Iteration: 1390/1485.\n",
      "Iteration: 1391/1485.\n",
      "Iteration: 1392/1485.\n",
      "Iteration: 1393/1485.\n",
      "Iteration: 1394/1485.\n",
      "Iteration: 1395/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.6533441609655329, 0.43619782183473177, 0.528587017352147]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.91, Val acc: 0.4\n",
      "\n",
      "\n",
      "Iteration: 1396/1485.\n",
      "Iteration: 1397/1485.\n",
      "Iteration: 1398/1485.\n",
      "Iteration: 1399/1485.\n",
      "Iteration: 1400/1485.\n",
      "Iteration: 1401/1485.\n",
      "Iteration: 1402/1485.\n",
      "Iteration: 1403/1485.\n",
      "Iteration: 1404/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.350461735448233, 0.41170360474014395, 0.42076174464184746]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.882, Val acc: 0.46\n",
      "\n",
      "\n",
      "Iteration: 1405/1485.\n",
      "Iteration: 1406/1485.\n",
      "Iteration: 1407/1485.\n",
      "Iteration: 1408/1485.\n",
      "Iteration: 1409/1485.\n",
      "Iteration: 1410/1485.\n",
      "Iteration: 1411/1485.\n",
      "Iteration: 1412/1485.\n",
      "Iteration: 1413/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.3260826188729237, 0.40531616950327987, 0.3785891485339032]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.906, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 1414/1485.\n",
      "Iteration: 1415/1485.\n",
      "Iteration: 1416/1485.\n",
      "Iteration: 1417/1485.\n",
      "Iteration: 1418/1485.\n",
      "Iteration: 1419/1485.\n",
      "Iteration: 1420/1485.\n",
      "Iteration: 1421/1485.\n",
      "Iteration: 1422/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.3576551865052129, 0.36084371367343004, 0.4148165053537193]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.862, Val acc: 0.54\n",
      "\n",
      "\n",
      "Iteration: 1423/1485.\n",
      "Iteration: 1424/1485.\n",
      "Iteration: 1425/1485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1426/1485.\n",
      "Iteration: 1427/1485.\n",
      "Iteration: 1428/1485.\n",
      "Iteration: 1429/1485.\n",
      "Iteration: 1430/1485.\n",
      "Iteration: 1431/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.48276099649321697, 0.3527491098812574, 0.41736246153234535]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.808, Val acc: 0.38\n",
      "\n",
      "\n",
      "Iteration: 1432/1485.\n",
      "Iteration: 1433/1485.\n",
      "Iteration: 1434/1485.\n",
      "Iteration: 1435/1485.\n",
      "Iteration: 1436/1485.\n",
      "Iteration: 1437/1485.\n",
      "Iteration: 1438/1485.\n",
      "Iteration: 1439/1485.\n",
      "Iteration: 1440/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.3348515160734279, 0.4061479869006761, 0.41179483405976414]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.884, Val acc: 0.5\n",
      "\n",
      "\n",
      "Iteration: 1441/1485.\n",
      "Iteration: 1442/1485.\n",
      "Iteration: 1443/1485.\n",
      "Iteration: 1444/1485.\n",
      "Iteration: 1445/1485.\n",
      "Iteration: 1446/1485.\n",
      "Iteration: 1447/1485.\n",
      "Iteration: 1448/1485.\n",
      "Iteration: 1449/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.5211939153493819, 0.4032457683985728, 0.4898039398492619]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.832, Val acc: 0.48\n",
      "\n",
      "\n",
      "Iteration: 1450/1485.\n",
      "Iteration: 1451/1485.\n",
      "Iteration: 1452/1485.\n",
      "Iteration: 1453/1485.\n",
      "Iteration: 1454/1485.\n",
      "Iteration: 1455/1485.\n",
      "Iteration: 1456/1485.\n",
      "Iteration: 1457/1485.\n",
      "Iteration: 1458/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.41313489222349736, 0.39115101084865694, 0.37118335767564764]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.85, Val acc: 0.44\n",
      "\n",
      "\n",
      "Iteration: 1459/1485.\n",
      "Iteration: 1460/1485.\n",
      "Iteration: 1461/1485.\n",
      "Iteration: 1462/1485.\n",
      "Iteration: 1463/1485.\n",
      "Iteration: 1464/1485.\n",
      "Iteration: 1465/1485.\n",
      "Iteration: 1466/1485.\n",
      "Iteration: 1467/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.36217147059330607, 0.387726352198783, 0.4493382025518831]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.866, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1468/1485.\n",
      "Iteration: 1469/1485.\n",
      "Iteration: 1470/1485.\n",
      "Iteration: 1471/1485.\n",
      "Iteration: 1472/1485.\n",
      "Iteration: 1473/1485.\n",
      "Iteration: 1474/1485.\n",
      "Iteration: 1475/1485.\n",
      "Iteration: 1476/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.47202133505515453, 0.3926162134692519, 0.39086659087194336]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.894, Val acc: 0.42\n",
      "\n",
      "\n",
      "Iteration: 1477/1485.\n",
      "Iteration: 1478/1485.\n",
      "Iteration: 1479/1485.\n",
      "Iteration: 1480/1485.\n",
      "Iteration: 1481/1485.\n",
      "Iteration: 1482/1485.\n",
      "Iteration: 1483/1485.\n",
      "Iteration: 1484/1485.\n",
      "Iteration: 1485/1485.\n",
      "\n",
      "-------------LOSS HISTORIES-------------\n",
      "\n",
      "Loss original: 2.3024789886306367\n",
      "Loss latest three: [0.45236176908635317, 0.4599645705081885, 0.5542665687539174]\n",
      "\n",
      "\n",
      "-----------ACCURACIES-----------\n",
      "\n",
      "  Train acc: 0.87, Val acc: 0.52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "----------------FINAL OUTPUT----------------\n",
      "  Train acc: 0.896, Val acc: 0.52\n",
      "Loss history: [2.3024789886306367, 2.3022782304464005, 2.3027355094703834, 2.302908300934751, 2.301313538266866, 2.2997475820966815, 2.301873147133011, 2.2500227794087517, 2.2528956186527007, 2.2553535514337635, 2.232264837396803, 2.186655129036276, 2.2135830241707137, 2.1866392299219055, 2.2478099557307916, 2.230990432879249, 2.1149906439690698, 2.1096766136833516, 2.029248812438277, 2.157151373389407, 2.0652548287714967, 1.9917966797913114, 1.9839803965226401, 1.895538890169808, 2.040286746403391, 2.091883500155575, 1.8921736129986013, 1.9860102856943298, 1.9657236573277361, 1.8864232168059119, 1.88401019275079, 1.8304871288600362, 1.9866992411445683, 2.051447662682095, 1.9357271153799098, 1.9488415889766082, 1.9676938569918159, 1.9052640121584248, 1.845657303552177, 1.8644559332688722, 2.083433445113628, 1.9581851806104886, 1.8437983257686832, 1.9559151087429394, 1.9747659584310215, 1.880710737469236, 2.103372695915405, 2.0215818989054886, 1.8511676808493474, 1.876521960226766, 1.8443780198003026, 1.7713060600649888, 1.7719742315598266, 1.9707207656255987, 1.980819996234403, 1.8230172016229051, 1.968865610723164, 1.9784552739790644, 1.7572298015207037, 1.8087773128447373, 1.8416558088355028, 1.9059486219474309, 1.8519092702673123, 1.829075111494044, 1.9959876909941856, 1.877187275096096, 1.7312219503561233, 1.6965375050542202, 1.8918808983021638, 1.9017265208068406, 1.9300900062945936, 1.954619791558879, 1.927137852155666, 1.9765861547367756, 1.908756217701468, 1.8136563570562907, 1.8893119226550805, 1.8932207590826073, 1.9407716429381854, 1.9393610308808562, 1.7272131214182989, 1.8181180209943628, 1.7592326395886628, 1.9240010824412395, 1.967125350514675, 1.814538347069329, 1.6945362391861434, 1.8605889751813791, 1.7760573855575508, 1.635153536285366, 1.7907586587644073, 1.892673553855733, 1.8965549896223453, 1.8382375262950266, 1.8211874076590877, 1.8220204098218709, 1.7852481097813846, 1.8111841786314753, 1.8345266894278303, 1.7496350712066366, 1.7097432948594253, 1.8126927087520743, 1.757421543682245, 1.6577876265478326, 1.729156743531727, 1.7070787226445638, 1.9089027991910212, 1.8490574864228009, 1.7942166896911367, 1.7340627422075239, 1.8784903735473173, 1.6595034616928948, 1.7483694853219605, 1.7441447596092672, 1.8377132045414093, 1.8240843152088355, 1.6623084599427154, 1.7260202746994253, 1.7269292873594844, 1.5646280576944944, 1.6368987642460817, 1.7326017700004688, 1.6728696632278037, 1.8306131816766056, 1.630204267570904, 1.628631636415539, 1.7712301632455367, 1.8511590678572938, 1.867558505742272, 1.872682827814583, 1.9081986040852281, 1.6340169330637977, 1.6528054045393594, 1.7916239755966046, 1.7737620309598228, 1.7744319541206588, 1.8999798083945547, 1.717451790682322, 1.8685193291335003, 1.6492220582530914, 1.8163209239236344, 1.7726455080453394, 1.7724559634640467, 1.7263200837109844, 1.8183454854449983, 1.8541802599812036, 1.6022696890898098, 1.620751917302077, 1.6352392330350682, 1.7387731132009376, 1.819783170663278, 1.7552269957476978, 1.9381476954610892, 1.6377730940643245, 1.705546841234543, 1.8418736446110444, 1.7090063159613704, 1.5375891797800127, 1.6127493793248644, 1.8024477671012513, 1.842571926103743, 1.710587023126368, 1.7766120138977615, 1.9016180255236754, 1.7007970992249182, 1.4855760068223265, 1.6024833791076072, 1.763618188592308, 1.6774495336083515, 1.6759392219740574, 1.7836601897564492, 1.6995958908913518, 1.8560434505832775, 1.7504713950118373, 1.7522216039973717, 1.7548998568797023, 1.5267345938604562, 1.6436464138137865, 1.6196268751724514, 1.7434020830275763, 1.718427083721557, 1.7648898429472002, 1.5670047885971832, 1.5910323302543337, 1.7950836347152876, 1.7961012939421341, 1.6117549748955529, 1.680272052141252, 1.7998692957003273, 1.8249867165709237, 1.7013060241029947, 1.615702678025339, 1.71265139743791, 1.6127303155493158, 1.523589570065152, 1.6398577725723087, 1.663734883061584, 1.7210220349441085, 1.5947125536527487, 1.610221156434655, 1.600515273841487, 1.6714435518672741, 1.6484806690487666, 1.5828622027679242, 1.5842668709244294, 1.730557998074906, 1.7345373379298452, 1.5307654456351738, 1.6335461669958289, 1.581449291279709, 1.5695588951965453, 1.760807713324895, 1.707784181237844, 1.6025201757223773, 1.5145790280285818, 1.6824116660417727, 1.7753487905286136, 1.5540650360248751, 1.7074042163518066, 1.4216563851355961, 1.585371277078722, 1.6389162776088193, 1.6429420273161262, 1.5517453936417567, 1.6501259705793327, 1.587545378806359, 1.7878286035593502, 1.7971189458913663, 1.6210305205063713, 1.4468778594962424, 1.4221015731829374, 1.7054045172410377, 1.5517633865179596, 1.6462956679560907, 1.748800477252578, 1.4469512940847817, 1.5832516446281313, 1.5876350459740038, 1.5682953581034387, 1.4665194820467466, 1.7110436020993123, 1.6715004847989232, 1.6389062885624808, 1.5446669983992747, 1.633079519884246, 1.7532552501054108, 1.5449916611677765, 1.5450289655753817, 1.6785256494305676, 1.59665360232518, 1.6164312798518714, 1.7236751788932951, 1.69592592924712, 1.6847417584005735, 1.5299081107457122, 1.604286802517593, 1.5618771599368644, 1.5681737996718206, 1.6898399065125689, 1.6542290674917517, 1.452406349971957, 1.7450849030495854, 1.6208715682706822, 1.4775340085879825, 1.6892607901555103, 1.583968087095703, 1.5699228053935732, 1.5415944286757783, 1.5766113509709612, 1.6724267221259155, 1.5803311099349446, 1.485896052143281, 1.6902540487150128, 1.6499851042535199, 1.5593339078457917, 1.6178088384240414, 1.5184650463860525, 1.733310312603863, 1.45273642358619, 1.552361684273866, 1.5515982068475687, 1.4075779162429174, 1.655521952550729, 1.6776615561899544, 1.610887592713975, 1.4811937265363957, 1.5120071213520607, 1.5955602769941997, 1.6901652127289135, 1.5692269678425705, 1.5001708233239794, 1.5331759926105233, 1.5070216046848621, 1.4119992016111065, 1.4565737599976951, 1.6386497816474177, 1.5852928795538075, 1.4805024042709818, 1.6415484837563126, 1.592904055832215, 1.7552546023094688, 1.7287283288643627, 1.4868372466427346, 1.5050806127491916, 1.5619986023292907, 1.5465385679162225, 1.5779353877237856, 1.5042209026215547, 1.7258594356396666, 1.548572879132218, 1.6412592847939067, 1.4182733408383055, 1.558604076701741, 1.7285232856602195, 1.5530230725944172, 1.3333873666476956, 1.3363538706881795, 1.5992846762920312, 1.396954822933844, 1.4806303737068627, 1.4408376817704487, 1.5623548278721797, 1.2879084458735508, 1.4779218207182356, 1.4576629645676145, 1.7345819571773178, 1.5547928926869319, 1.339639022721926, 1.3945855733246888, 1.4487744921860781, 1.491849018003894, 1.5007408972598908, 1.506357601471122, 1.4789831670042677, 1.3807856323470205, 1.687594016646422, 1.55817035975392, 1.439717620642554, 1.4480737158849937, 1.4857677806549128, 1.4389946527278192, 1.6361758659532593, 1.4496612725310547, 1.538370740756813, 1.5802240252663604, 1.5604490372592423, 1.563682863495996, 1.6393157670476182, 1.4496513512095335, 1.453255475608344, 1.5575582382027526, 1.3598138397945263, 1.7913291998220247, 1.5237770214301924, 1.4964943004410836, 1.354003534292017, 1.3646295363301257, 1.5781035675227875, 1.410641588479197, 1.5279013954613823, 1.3595408638226738, 1.6004276028727937, 1.5503515272674298, 1.4661501111201143, 1.4813897423324205, 1.5704068397013993, 1.4999388183856286, 1.5809875596776781, 1.410948735368885, 1.4465552725577075, 1.5108287631232284, 1.4693140678733325, 1.446067602309479, 1.548483929276145, 1.593814564185946, 1.4512285850198103, 1.4407832884959033, 1.4338142023473937, 1.336939484243902, 1.3190471516038784, 1.4202458067773354, 1.2927698315015497, 1.5028079606891374, 1.6215762094389123, 1.4340163987540848, 1.5725654314756852, 1.617204862369583, 1.4086604673170435, 1.3946645018205746, 1.4519283265085254, 1.411910349062285, 1.4270125522529218, 1.3815929601861288, 1.414874246995088, 1.508977458097116, 1.5395553848964005, 1.3739873831269551, 1.444698567797497, 1.3444111971130606, 1.4737315182255253, 1.6138136224107476, 1.489690290957374, 1.3984743052520432, 1.4557030579985313, 1.4685430877907906, 1.5818131096169257, 1.4862774449736111, 1.4815006494327627, 1.5804979173973601, 1.387572286467221, 1.417782144262315, 1.3033568366940036, 1.4124325625489054, 1.4310771810036262, 1.4573158439789788, 1.4145750657242004, 1.3649030935158835, 1.3637069734134133, 1.4918294310452707, 1.528222741721574, 1.2784898391360862, 1.51403852865372, 1.5014483395682618, 1.4817903702860584, 1.4482629156343831, 1.4274530675436328, 1.3578294580429282, 1.3864317843408562, 1.3788114910237588, 1.3192265693340328, 1.3738786281571924, 1.3411369100825834, 1.3136771930742461, 1.333992790960584, 1.515855965112158, 1.4397117196422897, 1.3834543036671352, 1.4184899956467367, 1.4152601246984973, 1.4642663505613505, 1.3792307534180037, 1.376912467456102, 1.4502874768609204, 1.4379912050709813, 1.4454448100391961, 1.3379765265712233, 1.3760152349503674, 1.3597240858887145, 1.4652927523714754, 1.5544307721960124, 1.4145097052319406, 1.1118228895689752, 1.3189212683226552, 1.4242151205675877, 1.3119210803429926, 1.4226169648590679, 1.3501788676720319, 1.4661698097922917, 1.348096895264067, 1.3544182226887287, 1.303998168196488, 1.469978977792341, 1.3267056943117073, 1.2689559782126136, 1.2568267628155874, 1.3010774246388925, 1.4108092629753999, 1.4913164386430975, 1.3015250436875387, 1.51925823339131, 1.3699487659578062, 1.4390501894328762, 1.4142847509367429, 1.389207908118704, 1.5163326639369592, 1.22790077323162, 1.4863954794074752, 1.367012325789925, 1.3471695315192573, 1.43346470981752, 1.2584181395840621, 1.416300560913611, 1.4322457616983264, 1.485964615623005, 1.4253524197960754, 1.397202864247266, 1.2428351803368087, 1.341907964737492, 1.3109273653817306, 1.3390659522155886, 1.2365156088624683, 1.3185068190737017, 1.3173035360558638, 1.439649404669488, 1.4400903081724443, 1.336997435328038, 1.5283251235215936, 1.4252639034468908, 1.235072654845967, 1.2652268566876452, 1.325821571603751, 1.413550007390241, 1.3071885293699117, 1.6286592619733646, 1.2371467732250618, 1.181415214728442, 1.7984126239946607, 1.3608634713367758, 1.2935979473717156, 1.17634798113773, 1.3231102291553214, 1.4542878486712139, 1.3633881222611028, 1.3104735308253097, 1.2892667077105942, 1.3526902665621778, 1.1937365278714593, 1.1499042734456166, 1.304863216261378, 1.4539293989375819, 1.343556855175209, 1.3445161905814356, 1.3265814779785245, 1.3964155616260812, 1.4053399789643481, 1.3959379015960238, 1.400605092527705, 1.4120373210918549, 1.3176452603426307, 1.3044741689331147, 1.3133451389849757, 1.3427056938129645, 1.3335658937414754, 1.1860365043532652, 1.2130441775681027, 1.4370538631031347, 1.3654203185022222, 1.269992007709889, 1.3005148992396114, 1.2406554236592409, 1.212695155584001, 1.24229194234159, 1.2964991666777834, 1.3215910189698936, 1.3348044434674677, 1.4075164338449975, 1.4236339142765377, 1.3395100701188287, 1.4078794144346116, 1.261263222941026, 1.2263920898655716, 1.388611584522902, 1.2868263004328069, 1.2911967623202363, 1.2997546137110108, 1.2561255224883734, 1.3292067876299836, 1.3030672708757445, 1.3307243103558315, 1.295852841625673, 1.2854331497914302, 1.2369817962707617, 1.3341885192685277, 1.2856754874758431, 1.3008551570464177, 1.3089350051503499, 1.2253757576970103, 1.2760242463183873, 1.5048837735986649, 1.1669123228535447, 1.2403884241834089, 1.2849752377593178, 1.3023958228928947, 1.2826464110822406, 1.3042585778096567, 1.3104385805935885, 1.2494420178192192, 1.2253679028364728, 1.347902015504959, 1.2799540455522584, 1.326934774607634, 1.2215827294575727, 1.1288244505775813, 1.156202616925294, 1.332721394201669, 1.2167058817647884, 1.364783676121181, 1.2851052771136844, 1.4283121290289595, 1.0978891558696964, 1.252139050828615, 1.1519215459151555, 1.007901903589812, 1.4008299085446583, 1.2577591305787192, 1.385468021466591, 1.2141504390919802, 1.2249577691104174, 1.1620581011120654, 1.2992953297133871, 1.1823119168214407, 1.2177203665488818, 1.2929611630516138, 1.1628077455253705, 1.3401580207510093, 1.147606346936437, 1.3385415858338565, 1.2813882307141686, 1.1319856136861959, 1.3403214247891548, 1.143113432709407, 1.2321746165741008, 1.1088452199415328, 1.4931898958507068, 1.2631562617267555, 1.0881425633588193, 1.0389594332575738, 1.27093700775518, 1.2416591575442226, 1.111237302597651, 1.318291017433696, 1.2266509267178047, 1.1738788647460259, 1.2109666575413405, 1.2531881394578308, 1.1858568274746009, 1.3723177827697632, 1.21826925444824, 1.1864083746885494, 1.23573027886704, 1.1773760726812628, 1.2202298461068788, 1.3881191436135059, 1.3269736233143115, 1.4207093150688486, 1.26590872970298, 1.262181080019778, 1.1426459634388895, 1.084226388897256, 1.1810466246674796, 1.1165052302526048, 1.2940857591316919, 1.3386285380282883, 1.1466069606456046, 1.0437722182944962, 1.149267647181754, 1.2170912215981786, 1.1899954286517869, 1.0616540664313758, 1.1576310355227226, 1.205630921947949, 1.1186763825675552, 1.1200450699385436, 1.017587517573052, 1.2292232921812687, 1.3340153408998798, 1.316506950781434, 1.2973793607238993, 1.2209934592060685, 1.1121853735009342, 1.2188198725358816, 1.2188116027513154, 1.1509034092505877, 1.1376108296389291, 1.1380914335919465, 1.167066496344811, 1.268136504852911, 1.1671766492185427, 1.3910140178977122, 1.3091557734015586, 1.3209726322133193, 1.1957506225328662, 0.9732794901445343, 1.2155158493544194, 1.0412060700783392, 1.2716490753962475, 1.2837997303570552, 1.186574681500765, 1.2019298844876287, 1.1951259523690976, 1.134126704918176, 1.1179898937377633, 1.1725843336479098, 1.170602317393033, 1.0999370058667919, 1.1408668824961385, 1.3367955453029443, 1.2273283745434895, 1.0221003208353479, 1.1591620761719958, 1.0778066405746527, 1.2245078988346767, 1.1453827288447078, 1.1354270769270125, 1.210006826963321, 1.1917617825050482, 1.2534797062716805, 1.084275820530938, 1.151766943106378, 1.2801919586678006, 0.97703494555639, 1.0875814610548387, 1.0822231456776095, 1.1188771987166177, 1.0702751421071661, 1.1578156042098233, 1.1119459346725826, 1.4426087012725142, 1.0205603131007122, 1.1376361761098464, 1.1070220768561414, 1.0896316974405797, 1.0749644620632919, 1.1640536804208348, 1.3117783953495044, 1.0176695186567932, 1.0067393070193233, 1.1478082270115944, 0.8837831491111774, 1.1681464638920034, 1.1569562698403733, 1.135923097280172, 1.222503372941241, 1.1001449350453874, 1.0843709409405906, 1.0165445310428847, 1.1514198520905663, 1.282891466047233, 1.2270859749783023, 1.0964721809965947, 1.1171861233571836, 1.1945665713805667, 1.1962442710994712, 1.2985487100288062, 1.1354041997776683, 1.0930238402413017, 0.9062983219563698, 1.0320299024209445, 1.0165762615721274, 1.0386566856800417, 1.1203515511057018, 1.2370288631285422, 1.180190999946732, 0.9077561964654884, 1.0583500092304987, 1.1762666326568494, 1.2316055395874028, 1.2185294839693235, 1.0751417506877257, 0.9553408640998134, 1.1010950223035172, 1.0663694342916978, 1.0046900728575303, 1.3264721012204108, 1.221411701337184, 0.9065824287396985, 1.1476377962681599, 1.0085214048208753, 1.022228173708689, 1.1453149170627477, 0.951562982920119, 1.0991347162271665, 1.0802620472527682, 1.1025919171874465, 1.0120444707935587, 1.0446194000791755, 1.0441149880999312, 1.1094345601290674, 1.1157037247205026, 1.1079201043847722, 1.2114165327622024, 1.1447184625901943, 1.0899537619231097, 0.9529898810283335, 1.0201060883739124, 1.069529860456165, 1.0206485919022328, 1.153845562045142, 1.0081309204250102, 0.9416761861686118, 1.0990496667238314, 1.0511274319263828, 1.0916851584340321, 0.9936533623481058, 0.9794873661643294, 1.0101250582292862, 1.190281614609124, 1.1370317998755446, 1.147768474514566, 1.0364291762384132, 1.1909746131119958, 1.0429743639887814, 1.032781739664956, 0.9341637152722878, 1.1096679956314335, 1.0837256056461586, 1.0679317011897425, 1.2150545233073289, 1.1963783449148222, 1.2563248625307917, 1.1093455191618644, 0.8580839703626486, 1.070759991530997, 0.9775114522740953, 0.7750248876020822, 0.9142105690346732, 1.0473461193757012, 1.1138202091600506, 1.1476933928544109, 1.106431851216113, 0.8544938448092111, 1.004316665615424, 1.087919773213041, 0.8266902610688781, 1.0433832027658672, 1.0800386087084788, 0.8609561814126732, 1.0034614600777065, 1.018394639053404, 1.2136973557366166, 1.0657168555405552, 0.8756760060084717, 1.0089959604107273, 0.963612489909508, 1.0705778234119894, 1.0445380097422632, 0.8883567805140967, 0.9661278246757052, 1.094258074473285, 0.9609447320876681, 0.97952291173798, 0.9418581403477512, 0.9678452666961075, 1.048164006000882, 1.111135023748373, 0.9141386447688744, 0.8805842883181758, 0.9283613867702936, 0.9091176278821075, 0.8840066928811453, 1.0631907257418767, 0.8836351163060457, 0.9550400835653069, 1.0002711688318313, 0.9913188854642503, 1.026928002895162, 0.9848104173520794, 1.2299752390157042, 1.1977686000157595, 0.8984911840772801, 1.1011688125254968, 0.9983453661073594, 0.9955307838435138, 0.9468434672907078, 0.909510587194518, 1.1253139731947766, 1.0716513401653012, 0.8323091301152695, 0.8033881625458149, 0.9340950266829424, 0.9347298361404102, 0.9811743122051134, 1.0150771635573825, 0.9703557499567347, 1.002023115794512, 1.1583927272091, 1.1389225323409897, 0.8287293715994889, 0.9594362433576943, 0.9651763139046743, 0.9406615313276104, 0.9109401796251315, 0.8402450801497874, 0.8993306840381551, 0.9203805594356443, 0.9494986121256136, 1.260157651870213, 1.1364064968618839, 0.9361268577712974, 0.834298197414362, 0.887610646386123, 0.986976499480563, 0.9349269625320845, 0.9154996164390806, 0.8956195062231102, 1.1965210644380495, 1.1491452916643086, 0.9868082239001867, 0.9276640249169738, 0.927547249218368, 0.9471106566545434, 0.9439387099389673, 1.2617032832569093, 1.0918161031293876, 0.9359513259160026, 1.0005629046835183, 0.8431817170683717, 0.9067815395714018, 1.0454335731345998, 1.0607225993335299, 0.9284555103308827, 0.8889902729595506, 0.8453352300834388, 0.9044211899914317, 1.2016508742209506, 1.1392877811558633, 0.8427859543136914, 0.942160493663663, 0.9567560222790195, 0.9001036940163971, 1.0923346240406593, 1.0504026420988841, 1.0504927919060854, 1.0055270694424743, 0.8750630103879873, 0.9418253904827638, 0.9093623970922358, 0.8483660994376551, 0.9440505918873296, 0.8052700481230392, 0.8969721518253788, 0.8441416127403056, 0.9425372456206291, 1.0154920529063884, 0.9651747623553592, 1.159818800455569, 0.9212908881239409, 0.8894325539669015, 0.9490027681514778, 0.7680052289173821, 0.7788368911252508, 0.8772787456289197, 0.9950990375529589, 0.8446944992537003, 1.096209980508435, 0.8915794111385088, 0.8917172677336788, 0.8882111473502241, 0.8940487752767899, 0.9029288055892086, 0.9708241694439553, 0.9758707850778803, 0.7688078617990485, 0.7644698957362109, 0.8773279212360339, 0.9720239179682796, 1.2096057892546412, 1.1386095768240525, 1.0025232948544407, 0.7884835502250092, 0.9677387926643198, 0.7865605967915533, 0.9845096679606521, 0.9899391147072014, 0.998080905719292, 0.9835469952947185, 1.0573192080376035, 0.8186562338025266, 0.781322570119101, 0.8513975728073395, 0.7509827576596279, 0.7351952118999756, 0.7910339870439728, 0.9648914222733302, 0.7602476865992219, 0.9286800759217532, 0.7707811687961049, 0.8681681851075764, 0.8072384675851269, 0.7365149117299004, 0.9026675413469291, 0.695172115863497, 0.9643899828405463, 0.7950411983503386, 0.844852681352459, 0.9858054471866057, 0.7381531479847583, 0.641436840537466, 0.925344185901762, 0.8238442869202974, 0.9877696822090704, 0.8727044839815251, 0.8901940572131204, 0.8691533487776164, 0.9506055688340068, 0.7683277925373674, 0.7372942543530882, 0.8248673100379585, 0.7355965321264111, 0.8985336771716618, 1.0645870990363573, 0.9327845713995587, 0.8600766525070012, 0.9324541762162308, 0.7963965160447584, 0.8796784771038181, 0.8892037729181188, 0.7011140008324215, 0.740212031718093, 0.759857377549701, 0.7737587314824046, 0.6624370165889663, 0.7984174261380129, 0.7687606460111707, 0.907890823605465, 0.9184380152952702, 0.9290785442962054, 0.6458994910487427, 0.8580703032547122, 0.964061057738365, 0.7715671233138239, 0.8208932476062036, 0.8544034913152838, 0.7335781454661704, 0.6110802963595195, 0.8353633715815714, 0.9436510668157254, 0.7767704266335211, 0.6935201053274967, 0.6480774461631179, 0.8068981660244134, 0.7676563986117942, 1.0622644110317279, 0.8409701561882207, 0.8740852922054083, 0.7641421266457356, 0.9414219667917654, 0.679655442798968, 0.8204519190034044, 0.8192129148081577, 0.7484246673288304, 0.8526813579334901, 0.810738861945341, 0.8330717876841013, 0.7041309004750573, 0.6989965373449032, 0.6116929005314095, 0.8095716926020522, 0.7271604073567041, 0.7660435988018219, 0.8403449434939843, 0.8060342692596143, 0.6729492997373338, 0.8502771092327411, 0.8712451946988661, 0.8904277774096941, 0.8229122648881694, 0.6918342802457076, 0.835569997779197, 0.9514971965089359, 0.7908500295635194, 0.7012661165339732, 0.650527484159589, 0.8088783456368549, 0.9062169323385443, 0.7394342775838851, 0.833189567284852, 0.7768171761471243, 0.8085390927209507, 0.9388494247142442, 0.9571191397076626, 0.8698137248879403, 0.7989321409366248, 0.8747904065192467, 0.7745648387326276, 0.6733495948277575, 0.631280186556708, 0.7687715598976288, 0.7380067901991326, 0.7549184449389458, 0.6938185720656685, 0.6358802096428253, 0.5912716866307419, 0.7310814843745533, 0.7133350325091442, 0.8576556868665892, 0.7819184528332089, 0.7618266713276903, 0.6544097563019945, 0.6942676964617512, 0.8130878257280665, 0.7680202620643115, 0.8690925244498852, 0.7865384330052879, 0.674672358698073, 0.7487469494607318, 0.8399014777650902, 0.7735057554807135, 0.7409869177520839, 0.6181373233015278, 0.692683044283754, 0.7594938292444674, 0.7146292380892171, 0.6398397120152421, 0.758941060841323, 0.8304020598200471, 0.9913156545225121, 0.9114896348877648, 0.7664094894592777, 0.8062324988107785, 0.49931421542420795, 0.6695786626816087, 0.8480457705219964, 0.7677102087850315, 0.8801226846927083, 0.8727642513708814, 0.6981917657202653, 0.6508681376858675, 0.7264839147797473, 0.6311385495912453, 0.677771894629054, 0.6566449986220368, 0.6960422703136817, 0.6316050864357038, 0.5791042187264205, 0.8872640252761416, 0.8971670210614566, 0.5998561299765518, 0.8369132606402285, 0.6469296580484108, 0.7921488312133415, 0.6942640180805943, 0.63291807424533, 0.6792139173041799, 0.621702055230249, 0.8740055181131651, 0.6958607255711808, 0.5027110993344732, 0.6002411566818945, 0.6530603140302739, 0.6904739737897623, 0.7197501131775544, 0.7694791531468543, 0.7438660433046468, 0.8057285683229126, 0.700779647854927, 0.6267610345200137, 0.6444585561933669, 0.5535277428231518, 0.6905418697644161, 0.6631156681014011, 0.5730173081895705, 0.6119822667371536, 0.6886644749801795, 0.8052520318576777, 0.8311925935156335, 0.6151195664425734, 0.7154510666847677, 0.6858296497813186, 0.6952082168574143, 0.6152646015233146, 0.8531127641946726, 0.7137150371252708, 0.6965971169081958, 0.6113456681078981, 0.6397573487166488, 0.7159705025317628, 0.5564138378909195, 0.5209582227267812, 0.7639332054499453, 0.7665743395499984, 0.6758724495519818, 0.6340380998888409, 0.61075143120388, 0.6425222515215107, 0.47343066884023743, 0.44218034546287194, 0.5731906816326281, 0.7685427264285799, 0.7956345723199787, 0.8857449102107671, 0.8216368082967754, 0.7416670882531605, 0.6692103692058019, 0.60909688249891, 0.6465878620819169, 0.6268765319014213, 0.480560748121569, 0.7228226575614985, 0.913611719918719, 0.7891526904417043, 0.5282603481994363, 0.5958828313496889, 0.6756242025237839, 0.6284768423591891, 0.751318176380917, 0.663475217216961, 0.7686533706525158, 0.5866543965869458, 0.5661055839418677, 0.5560058793824952, 0.5146573762948892, 0.7314989591911646, 0.7737970409737069, 0.7042272756782721, 0.8056101972571197, 0.6453387372560261, 0.6852459865252974, 0.8719752558180776, 0.6168056695251137, 0.662876156060655, 0.7299489810185381, 0.6981276927442016, 0.639082832930546, 0.7381953824752867, 0.6782729584242994, 0.5332625862229178, 0.614975234620626, 0.5633144470918557, 0.561892528976615, 0.5360021545752186, 0.4992538176864789, 0.6068370849682713, 0.5879709992917811, 0.5672012636685473, 0.7737137479912238, 0.6932053896725066, 0.5317670094961892, 0.7478085719172636, 0.8422421106359231, 0.598725874349902, 0.5780939199020525, 0.7247082640470212, 0.5653762183912656, 0.6973036934636574, 0.7803042924798839, 0.6105666914671202, 0.6922256425909313, 0.7096756203992024, 0.9076812465903783, 0.78460821479328, 0.7481795855584596, 0.5735905983712006, 0.504587512203424, 0.5709531777104719, 0.4819238804723075, 0.5109414931494493, 0.45436904232375896, 0.3714092889008827, 0.49782751485361315, 0.518695511448751, 0.4330243458312796, 0.588371763527748, 0.48765374759145164, 0.5882082725651606, 0.6553169179051384, 0.5584006341954411, 0.6511834793235253, 0.7345405892814383, 0.8045199797421269, 0.7636770092255766, 0.7420715652486994, 0.696458348277435, 0.9228182562203716, 0.7037616387701828, 0.5219914829055362, 0.5274818907063041, 0.5175385932679349, 0.5081483352699074, 0.5447556507061871, 0.4626249404570795, 0.5908699750867835, 0.6268295209337161, 0.6594339419991571, 0.47491934049419016, 0.4936333796109752, 0.5089409524188038, 0.6254075863815982, 0.5875134817913333, 0.5478264287999791, 0.5436804693789193, 0.5134865984672932, 0.6186853313215033, 0.7042901450496776, 0.6744576715321183, 0.6160500412594098, 0.530167483128414, 0.4309412512116603, 0.501945099886992, 0.5785006059522668, 0.6415700262076969, 0.5648524374656854, 0.5958127286335239, 0.460705277295776, 0.579840972661745, 0.48281517248800215, 0.5425944702214803, 0.522684189281604, 0.39258038766740383, 0.5060800815070446, 0.5014562946517742, 0.7314870306368886, 0.5620860013635531, 0.7177733470744266, 0.459175577854445, 0.47101352608564384, 0.6226065048780213, 0.56243686967506, 0.771850703924666, 0.5146299938815309, 0.5605478661819577, 0.398109409297154, 0.672252163739545, 0.947235740734324, 0.6084319125240255, 0.4853055754165206, 0.5678965066462734, 0.5587375468798447, 0.6207936602719447, 0.5037000782875158, 0.6109787574972604, 0.5068521615231688, 0.463545003902302, 0.4666935005219137, 0.6499856000999255, 0.4147224035507224, 0.5423353504796931, 0.5862354995072538, 0.7128155797084136, 0.6941498013610387, 0.3813650650741876, 0.4080350613834247, 0.47137253347218006, 0.42518135778645677, 0.44891842465471427, 0.5929194620950092, 0.5112919788103891, 0.6392953262364872, 0.42478295325102994, 0.4088379942447534, 0.4672052283475314, 0.504108226367286, 0.4792897271523487, 0.36655027819217034, 0.41360715561490347, 0.5528065518215072, 0.5050608469000888, 0.6177563920545692, 0.6220278938817991, 0.5962830695900962, 0.34129214069782876, 0.5156029654591444, 0.5144220140965168, 0.4963398685125689, 0.6112014488952194, 0.5882105052957515, 0.8419702710048818, 0.6044496139368165, 0.5840754718879066, 0.5044818365703043, 0.4972241581792059, 0.4485750575939993, 0.2928552583796957, 0.49958831296049605, 0.40466845265775675, 0.4805947233985103, 0.32899429764581906, 0.4552106207073862, 0.7136078117397855, 0.9364060487429829, 0.6238050841339642, 0.45696375420726937, 0.4319713671236072, 0.35224041283364393, 0.4453652906303504, 0.46024045344941455, 0.3861036337542681, 0.33061367599746955, 0.5554809273377369, 0.48241177964566073, 0.3500311733664785, 0.5648648863556277, 0.7229887843277504, 0.9632935532978728, 0.5709725690818614, 0.7383884668077255, 0.7443166812124444, 0.490611270972076, 0.427754846840307, 0.44348025425557985, 0.3746259672687606, 0.41752977098323124, 0.5433169750769296, 0.6862983479637055, 0.5373638291230602, 0.36242148872632324, 0.4419952091386643, 0.5178328801760721, 0.5022297445915311, 0.41792322119900877, 0.4579871068906066, 0.7201954205416461, 0.6533441609655329, 0.43619782183473177, 0.528587017352147, 0.534583513292372, 0.4072222558289127, 0.5674890375060327, 0.4353328571865059, 0.4580538830202747, 0.38390426073341105, 0.350461735448233, 0.41170360474014395, 0.42076174464184746, 0.417192085694779, 0.39854638228301964, 0.5430507143876976, 0.6680603430207697, 0.5381234330781135, 0.4908744901291518, 0.3260826188729237, 0.40531616950327987, 0.3785891485339032, 0.42927212408993065, 0.7690172235431727, 0.670630652808405, 0.5621966082161002, 0.43090643237629567, 0.4555860204973364, 0.3576551865052129, 0.36084371367343004, 0.4148165053537193, 0.316684944369143, 0.46391377135550604, 0.3848783906872628, 0.48678019486508806, 0.4636760018577544, 0.48582935619551626, 0.48276099649321697, 0.3527491098812574, 0.41736246153234535, 0.44568166061011566, 0.3994081719705263, 0.4414000704811928, 0.41914553116674735, 0.32582137229321656, 0.45242284627851936, 0.3348515160734279, 0.4061479869006761, 0.41179483405976414, 0.4055740613280282, 0.4951316037877911, 0.5742155239689248, 0.6455388083706852, 0.6089756919857433, 0.42406138522414133, 0.5211939153493819, 0.4032457683985728, 0.4898039398492619, 0.47489280702192077, 0.43492562054586414, 0.37980380866723734, 0.5040954070187832, 0.4172479122427002, 0.4136604794231112, 0.41313489222349736, 0.39115101084865694, 0.37118335767564764, 0.32631316764256313, 0.40895186990826554, 0.5258715893527203, 0.4006382465868111, 0.328065931483058, 0.3974230871270131, 0.36217147059330607, 0.387726352198783, 0.4493382025518831, 0.3848259799226251, 0.3286886604835379, 0.518551739677484, 0.748200729267494, 0.718897364677747, 0.6551306999631805, 0.47202133505515453, 0.3926162134692519, 0.39086659087194336, 0.27945944286728663, 0.28623254444228385, 0.31751504150080656, 0.3292453053890853, 0.3143563334030787, 0.3202395206931041, 0.45236176908635317, 0.4599645705081885, 0.5542665687539174]\n",
      "Accuracy history: [0.182, 0.194, 0.23, 0.284, 0.292, 0.262, 0.314, 0.28, 0.314, 0.32, 0.356, 0.3, 0.356, 0.36, 0.32, 0.316, 0.322, 0.326, 0.364, 0.368, 0.362, 0.322, 0.39, 0.342, 0.416, 0.352, 0.38, 0.412, 0.416, 0.388, 0.374, 0.402, 0.442, 0.482, 0.476, 0.444, 0.406, 0.454, 0.442, 0.46, 0.468, 0.484, 0.444, 0.484, 0.458, 0.482, 0.506, 0.478, 0.444, 0.496, 0.5, 0.47, 0.428, 0.486, 0.49, 0.494, 0.502, 0.498, 0.534, 0.532, 0.516, 0.506, 0.526, 0.53, 0.58, 0.51, 0.548, 0.566, 0.534, 0.594, 0.544, 0.602, 0.554, 0.572, 0.58, 0.594, 0.588, 0.592, 0.574, 0.596, 0.578, 0.562, 0.64, 0.608, 0.624, 0.626, 0.606, 0.59, 0.694, 0.632, 0.604, 0.564, 0.634, 0.582, 0.656, 0.658, 0.676, 0.66, 0.712, 0.658, 0.642, 0.654, 0.696, 0.712, 0.68, 0.668, 0.668, 0.756, 0.662, 0.664, 0.734, 0.734, 0.708, 0.688, 0.708, 0.748, 0.734, 0.694, 0.748, 0.758, 0.72, 0.722, 0.726, 0.722, 0.754, 0.756, 0.734, 0.796, 0.73, 0.68, 0.696, 0.732, 0.792, 0.814, 0.806, 0.83, 0.786, 0.788, 0.708, 0.808, 0.802, 0.794, 0.842, 0.846, 0.812, 0.79, 0.85, 0.878, 0.874, 0.854, 0.764, 0.834, 0.866, 0.8, 0.91, 0.882, 0.906, 0.862, 0.808, 0.884, 0.832, 0.85, 0.866, 0.894, 0.87, 0.896]\n"
     ]
    }
   ],
   "source": [
    "# RMSprop\n",
    "rms = ConvNet4Accel(input_shape=input_shape, verbose=False)\n",
    "rms.compile('rmsprop')\n",
    "rms.fit(x_train,y_train, x_dev, y_dev, n_epochs=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.405"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rms.accuracy(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of RMSProp**\n",
    "To preface this analysis, it must be stated we used this article as a starting point for the implementation of RMSProp: https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a.\n",
    "\n",
    "We predicted that RMSProp is comparable to Adam in performance. RMSProp is an extension of RProp, which makes a weight by weight comparison of the weight gradient and if the signs are the same it increases learning rate, and if they are different, it decreases. This is because if the sign of the gradient is the same for two updates in a row, it is going in the right direction, so it can speed up its descent of the loss space. If the signs are different, that means that it went too far in loss space, so it should decrease the adjustment to the weight for the next update.\n",
    "\n",
    "RProp is good for training when the batch size is the entire epoch, but fails when there are mini-batches because it has to divide by different gradients each time. RMSProp extends on the idea of RProp by eliminating the problem of mini-batches. It solves this problem by introducing the root of the square of the gradients (root mean square -> RMS), which is a moving average of the gradient. The moving average solves the problem of a gradient that changes erratically because the gradient that we actually use in the equation no longer changes erratically. Additionally, the beta parameter helps solve this problem by weighting the influence that previous gradients have on the new gradient.\n",
    "\n",
    "We created a ConvNet4Accel network using RMSProp and the full dataset run as we used on Adam in part 7. Using these two runs, we can build a comparison of the two optimizers. \n",
    "\n",
    "Time: From a time-per-iteration perspective, RMSProp takes about .489 seconds and a total of 726 seconds to complete for 1485 iterations. Adam takes .522 seconds per iteration and a total of 775 seconds to run. If these were running for longer, that time difference would be even larger. Therefore, the smaller amount of computation in RMSProp makes it a more efficient algorithm to run. \n",
    "\n",
    "Accuracy: Our Adam optimizer scores a higher accuracy. We can see there is a 40.5% accuracy compared to a 42.5% accuracy when the models are both run for 33 epochs on default parameters. So, if accuracy matters less and there is only so long to train, then RMSProp is the optimizer to use. It would be curious to see if there would be an improvement if they were trained on a time basis rather than an epochs basis. If given the same amount of time, does RMSProp do better rather than Adam?\n",
    "\n",
    "Below are some graphs that plot the loss of RMSProp and Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rms.loss_history, label=\"RMSProp Loss History\")\n",
    "plt.plot(net.loss_history, label=\"Adam Loss History\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rms.train_acc_history, label=\"RMSProp Accuracy History\")\n",
    "plt.plot(net.train_acc_history, label=\"Adam Accuracy History\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the graphs above, Blue is RMSProp, Green is Adam. It is clear the Adam is more performant in the epochs. Both Loss and Accuracy from training were worse in RMSProp than Adam. If we had to choose one, we would take Adam, even though we found RMSProp to be faster.\n",
    "\n",
    "We chose to do this extension because we have seen RMSProp in other classes and documentation. We saw it in class and in Keras documentation as well as Medium articles so we decided we wanted to understand what it was. Now we know! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
