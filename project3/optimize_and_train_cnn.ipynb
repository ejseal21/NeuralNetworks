{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cole Turner and Ethan Seal**\n",
    "\n",
    "Fall 2019\n",
    "\n",
    "CS343: Neural Networks\n",
    "\n",
    "Project 3: Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(['seaborn-colorblind', 'seaborn-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=7)\n",
    "\n",
    "# Automatically reload your external source code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global note: Make sure any debug printouts do not appear if `verbose=False`!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4) Implement weight optimizers for gradient descent\n",
    "\n",
    "To change the weights during training, we need an optimization algorithm to have our loss decrease over epochs as we learn the structure of the input patterns. Until now, we used **Stochastic gradient descent (SGD)**, which is the simplest algorithm. We will implement 3 popular algorithms:\n",
    "\n",
    "- `SGD` (stochastic gradient descent)\n",
    "- `SGD_Momentum` (stochastic gradient descent with momentum)\n",
    "- `Adam` (Adaptive Moment Estimation)\n",
    "\n",
    "Implement each of these according to the update equations (in `optimizer.py::update_weights` in each subclass). Let's use $w_t$ in the math below to represent the weights in a layer at time step $t$, $dw$ to represent the gradient of the weights in a layer, and $\\eta$ represent the learning rate. We use vectorized notation below (update applies to all weights element-wise). Then:\n",
    "\n",
    "**SGD**: \n",
    "\n",
    "$w_{t} = w_{t-1} - \\eta \\times dw$\n",
    "\n",
    "**SGD (momentum)**:\n",
    "\n",
    "$v_{t} = m \\times v_{t-1} - \\eta \\times dw$\n",
    "\n",
    "$w_{t} = w_{t-1} + v_t$\n",
    "\n",
    "where $v_t$ is called the `velocity` at time $t$. At the first time step (0), velocity should be set to all zeros and have the same shape as $w$. $m$ is a constant that determines how much of the gradient obtained on the previous time step should factor into the weight update for the current time step.\n",
    "\n",
    "\n",
    "**Adam**:\n",
    "\n",
    "$m_{t} = \\beta_1 \\times m_{t-1} + (1 - \\beta_1)\\times dw$\n",
    "\n",
    "$v_{t} = \\beta_2 \\times v_{t-1} + (1 - \\beta_2)\\times dw^2$\n",
    "\n",
    "$n = m_{t} / \\left (1-(\\beta_1^t) \\right )$\n",
    "\n",
    "$u = v_{t} / \\left (1-(\\beta_2^t) \\right )$\n",
    "\n",
    "$w_{t} = w_{t-1} - \\left ( \\eta \\times n \\right ) / \\left ( \\sqrt(u) + \\epsilon \\right ) $\n",
    "\n",
    "\n",
    "Like SGD (momentum), Adam records momentum terms $m$ and $v$. At time step 0, you should initialize them to zeros in an array equal in size to the weights. $n$ and $u$ are variables computed on each time step. The remaining quantities are constants. Note that $t$ keeps track of the integer time step, and needs to be incremented on each update. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimizer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Test SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SGD' object has no attribute 'velocity'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-0b6831f7cdb6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_wts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mnew_wts_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\onedrive\\documents\\classes\\NeuralNetworks\\project3\\optimizer.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(self, wts, d_wts)\u001b[0m\n\u001b[0;32m     17\u001b[0m         '''Stores weights and their gradient before an update step is performed.\n\u001b[0;32m     18\u001b[0m         '''\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvelocity\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvelocity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SGD' object has no attribute 'velocity'"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "wts = np.arange(-3, 3, dtype=np.float64)\n",
    "d_wts = np.random.randn(len(wts))\n",
    "\n",
    "optimizer = SGD()\n",
    "optimizer.prepare(wts, d_wts)\n",
    "\n",
    "new_wts_1 = optimizer.update_weights()\n",
    "new_wts_2 = optimizer.update_weights()\n",
    "\n",
    "print(f'SGD: Wts after 1 iter {new_wts_1}')\n",
    "print(f'SGD: Wts after 2 iter {new_wts_2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should be:\n",
    "\n",
    "    SGD: Wts after 1 iter [-3.1764052 -2.0400157 -1.0978738 -0.2240893  0.8132442  2.0977278]\n",
    "    SGD: Wts after 2 iter [-3.3528105 -2.0800314 -1.1957476 -0.4481786  0.6264884  2.1954556]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Test SGD_Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD M: Wts after 1 iter\n",
      "[[ 1.6879486  0.3879897  0.9343517  2.2075258]\n",
      " [ 1.7181501 -0.9567621  0.9187816 -0.0659476]\n",
      " [ 0.1520801  0.3452366  0.0576     1.52849  ]]\n",
      "SGD M: Wts after 2 iter\n",
      "[[ 1.5661825  0.3685217  0.8633335  2.1541379]\n",
      " [ 1.4790974 -0.9239367  0.8686908  0.0707077]\n",
      " [ 0.5605585  0.2406577 -0.0807098  1.6472364]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "wts = np.random.randn(3, 4)\n",
    "d_wts = np.random.randn(3, 4)\n",
    "\n",
    "optimizer = SGD_Momentum(lr=0.1, m=0.6)\n",
    "optimizer.prepare(wts, d_wts)\n",
    "\n",
    "new_wts_1 = optimizer.update_weights()\n",
    "new_wts_2 = optimizer.update_weights()\n",
    "\n",
    "print(f'SGD M: Wts after 1 iter\\n{new_wts_1}')\n",
    "print(f'SGD M: Wts after 2 iter\\n{new_wts_2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should be:\n",
    "\n",
    "    SGD M: Wts after 1 iter\n",
    "    [[ 1.6879486  0.3879897  0.9343517  2.2075258]\n",
    "     [ 1.7181501 -0.9567621  0.9187816 -0.0659476]\n",
    "     [ 0.1520801  0.3452366  0.0576     1.52849  ]]\n",
    "    SGD M: Wts after 2 iter\n",
    "    [[ 1.5661825  0.3685217  0.8633335  2.1541379]\n",
    "     [ 1.4790974 -0.9239367  0.8686908  0.0707077]\n",
    "     [ 0.5605585  0.2406577 -0.0807098  1.6472364]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Test Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam: Wts after 1 iter\n",
      "[[ 1.6640523  0.3001572  0.878738   2.1408932]\n",
      " [ 1.767558  -0.8772779  0.8500884 -0.0513572]\n",
      " [-0.0032189  0.3105985  0.0440436  1.5542735]]\n",
      "Adam: Wts after 2 iter\n",
      "[[ 1.5640523  0.2001572  0.778738   2.0408932]\n",
      " [ 1.667558  -0.7772779  0.7500884  0.0486428]\n",
      " [ 0.0967811  0.2105985 -0.0559564  1.6542735]]\n",
      "Adam: Wts after 3 iter\n",
      "[[ 1.4640523  0.1001572  0.678738   1.9408932]\n",
      " [ 1.567558  -0.6772779  0.6500884  0.1486428]\n",
      " [ 0.1967811  0.1105985 -0.1559564  1.7542735]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "wts = np.random.randn(3, 4)\n",
    "d_wts = np.random.randn(3, 4)\n",
    "\n",
    "optimizer = Adam(lr=0.1)\n",
    "optimizer.prepare(wts, d_wts)\n",
    "\n",
    "new_wts_1 = optimizer.update_weights()\n",
    "new_wts_2 = optimizer.update_weights()\n",
    "new_wts_3 = optimizer.update_weights()\n",
    "\n",
    "print(f'Adam: Wts after 1 iter\\n{new_wts_1}')\n",
    "print(f'Adam: Wts after 2 iter\\n{new_wts_2}')\n",
    "print(f'Adam: Wts after 3 iter\\n{new_wts_3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should be:\n",
    "\n",
    "    Adam: Wts after 1 iter\n",
    "    [[ 1.6640523  0.3001572  0.878738   2.1408932]\n",
    "     [ 1.767558  -0.8772779  0.8500884 -0.0513572]\n",
    "     [-0.0032189  0.3105985  0.0440436  1.5542735]]\n",
    "    Adam: Wts after 2 iter\n",
    "    [[ 1.5640523  0.2001572  0.778738   2.0408932]\n",
    "     [ 1.667558  -0.7772779  0.7500884  0.0486428]\n",
    "     [ 0.0967811  0.2105985 -0.0559564  1.6542735]]\n",
    "    Adam: Wts after 3 iter\n",
    "    [[ 1.4640523  0.1001572  0.678738   1.9408932]\n",
    "     [ 1.567558  -0.6772779  0.6500884  0.1486428]\n",
    "     [ 0.1967811  0.1105985 -0.1559564  1.7542735]]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5) Write network training methods\n",
    "\n",
    "Implement methods in `network.py` to actually train the network, using all the building blocks that you have created. The methods to implement are:\n",
    "\n",
    "- `predict`\n",
    "- `fit`. Add an optional parameter `print_every=1` that controls the frequency (in iterations) with which to wait before printing out the loss and iteration number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6) Overfitting a convolutional neural network\n",
    "\n",
    "Usually we try to prevent overfitting, but we can use it as a valuable debugging tool to test out a complex backprop-style neural network. Assuming everything is working, it is almost always the case that we should be able to overfit a tiny dataset with a huge model with tons of parameters (i.e. your CNN). You will use this strategy to verify that your network is working.\n",
    "\n",
    "Let's use a small amount of real data from STL-10. If everything is working properly, the network should overfit and you should see a significant drop in the loss from its starting value of ~2.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6a) Move your `preprocess_data.py` from the MLP project\n",
    "\n",
    "Make the one following change:\n",
    "\n",
    "- Re-arrange dimensions of `imgs` so that when it is returned, `shape=(Num imgs, RGB color chans, height, width)` (No longer flatten non-batch dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_stl10_dataset\n",
    "import preprocess_data\n",
    "from network import ConvNet4\n",
    "import optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b) Load in STL-10 at 16x16 resolution\n",
    "\n",
    "If you don't want to wait for STL-10 to download from the internet and resize, copy over your data and numpy folders from your MLP project.\n",
    "\n",
    "**Notes:**\n",
    "- You will need to download the new version of `load_stl10_dataset`.\n",
    "- The different train/test split here won't work if you hard coded the proportions in your `create_splits` implementation! *This isn't catastrophic, it just means that it will take longer to compute accuracy on the validation set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images are: (5000, 96, 96, 3)\n",
      "Labels are: (5000,)\n",
      "Resizing 5000 images to 16x16...Done!\n",
      "Saving Numpy arrays the images and labels to ./numpy...Done!\n",
      "imgs.shape (5000, 16, 16, 3)\n",
      "data.shape (5000, 768)\n",
      "Train data shape:  (4548, 768)\n",
      "Train labels shape:  (4548,)\n",
      "Test data shape:  (400, 768)\n",
      "Test labels shape:  (400,)\n",
      "Validation data shape:  (2, 768)\n",
      "Validation labels shape:  (2,)\n",
      "dev data shape:  (50, 768)\n",
      "dev labels shape:  (50,)\n"
     ]
    }
   ],
   "source": [
    "# Download the STL-10 dataset from the internet, convert it to Numpy ndarray, resize to 16x16\n",
    "# cache it locally on your computer for faster loading next time.\n",
    "load_stl10_dataset.purge_cached_dataset()\n",
    "stl_imgs, stl_labels = load_stl10_dataset.load(scale_fact=6)\n",
    "# preprocess\n",
    "stl_imgs, stl_labels = preprocess_data.preprocess_stl(stl_imgs, stl_labels)\n",
    "# create splits\n",
    "x_train, y_train, x_test, y_test, x_val, y_val, x_dev, y_dev = preprocess_data.create_splits(\n",
    "    stl_imgs, stl_labels, n_train_samps=4548, n_test_samps=400, n_valid_samps=2, n_dev_samps=50)\n",
    "\n",
    "print ('Train data shape: ', x_train.shape)\n",
    "print ('Train labels shape: ', y_train.shape)\n",
    "print ('Test data shape: ', x_test.shape)\n",
    "print ('Test labels shape: ', y_test.shape)\n",
    "print ('Validation data shape: ', x_val.shape)\n",
    "print ('Validation labels shape: ', y_val.shape)\n",
    "print ('dev data shape: ', x_dev.shape)\n",
    "print ('dev labels shape: ', y_dev.shape)\n",
    "\n",
    "classes = np.loadtxt(os.path.join('data', 'stl10_binary', 'class_names.txt'), dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6c) Train and overfit the network on a small STL-10 sample with each optimizer\n",
    "\n",
    "**Goal:** If your network works, you should see a drop in loss over epochs to 0.\n",
    "\n",
    "In 3 seperate cells below\n",
    "\n",
    "- Create 3 different `ConvNet4` networks.\n",
    "- Compile each with a different optimizer (each net uses a different optimizer).\n",
    "- Train each on the **dev** set and validate on the tiny validation set (we dont care about out-of-training-set performance here).\n",
    "\n",
    "You will be making plots demonstrating the overfitting for each optimizer below. **You should train the nets with the same number of epochs such that at least 2/3 of them clearly show loss convergence to a small value; one optimizer may not converge yet, and that's ok**. Cut off the simulations based on the 2/3 that do converge.\n",
    "\n",
    "Guidelines:\n",
    "\n",
    "- Weight scales and learning rates of `1e-2` should work well.\n",
    "- Start by testing the Adam optimizer.\n",
    "- Remember that the input shape is (3, 16, 16). You need to specify this to the network constructor.\n",
    "- The hyperparameters are up to you, though I wouldn't recommend a batch size that is too small (close to 1), otherwise it may be tricky to see whether the loss is actually decreasing on average.\n",
    "- Decreasing `acc_freq` will make the `fit` function evaluate the training and validation accuracy more often. This is a computationally intensive process, so small values come with an increase in training time. On the other hand, checking the accuracy too infrequently means you won't know whether the network is trending toward overfitting the training data, which is what you're checking for.\n",
    "- Each training session takes ~30 mins on my laptop.\n",
    "\n",
    "**Caveat emptor:** Training convolutional networks is notoriously computationally intensive. If you experiment with hyperparameters, each training session may take several hours. Use the loss/accuracy print outs to quickly gauge whether your hyperparameter choices are getting your network to decrease in loss. Monitor print outs and interrupt the Jupyter kernel if things are not trending in the right direction. Consider using the Davis 102 iMacs if this is running too slow on your laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_scale = 1e-2\n",
    "lr = 1e-2\n",
    "input_shape = (3, 16, 16)\n",
    "mini_batch_sz = 5\n",
    "\n",
    "#preprocessing flattened it when we actually wanted it not flattened.\n",
    "x_dev = x_dev.reshape(x_dev.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "x_val = x_val.reshape(x_val.shape[0], input_shape[0], input_shape[1], input_shape[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train...\n",
      "250 iterations. 10 iter/epoch.\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 1 of 250. \n",
      "Time taken for iteration 0: 1.249053955078125\n",
      "Estimated time to complete: 312.26348876953125\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 2 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 3 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 4 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 5 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 6 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 7 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 8 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 9 of 250. \n",
      "Loss History: [2.2989123053584617, 2.294599148673949, 2.2873880607777233, 2.3111326756464847, 2.311918360233595, 2.2665860570304703, 2.3113340090274668, 2.283508461198438, 2.307325430813852]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (2, 32, 8, 8)\n",
      "[5 5]\n",
      "[0 3]\n",
      "  Train acc: 0.22, Val acc: 0.0\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 10 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 11 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 12 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 13 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 14 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 15 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 16 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 17 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 18 of 250. \n",
      "Loss History: [2.2989123053584617, 2.294599148673949, 2.2873880607777233, 2.3111326756464847, 2.311918360233595, 2.2665860570304703, 2.3113340090274668, 2.283508461198438, 2.307325430813852, 2.245309583374415, 2.2359952817610265, 2.254083924787579, 2.2569937339722137, 2.2977081933131127, 2.0908347668857243, 1.954958318138915, 2.276603400195442, 1.7043690584220403]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 8 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 8 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 8 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (2, 32, 8, 8)\n",
      "[5 5]\n",
      "[0 3]\n",
      "  Train acc: 0.26, Val acc: 0.0\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 19 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 20 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 21 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 22 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 23 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 24 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 25 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 26 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 27 of 250. \n",
      "Loss History: [2.2989123053584617, 2.294599148673949, 2.2873880607777233, 2.3111326756464847, 2.311918360233595, 2.2665860570304703, 2.3113340090274668, 2.283508461198438, 2.307325430813852, 2.245309583374415, 2.2359952817610265, 2.254083924787579, 2.2569937339722137, 2.2977081933131127, 2.0908347668857243, 1.954958318138915, 2.276603400195442, 1.7043690584220403, 2.1814637958034395, 2.3232108448525546, 1.9884770944220502, 1.5508812213638903, 2.486222951039882, 2.3390461711723725, 2.031379356952344, 1.8973910714199462, 2.3778533837120235]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 8 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 8 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 5 8 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 8]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (2, 32, 8, 8)\n",
      "[5 5]\n",
      "[0 3]\n",
      "  Train acc: 0.28, Val acc: 0.0\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 28 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 29 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 30 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 31 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 32 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 33 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 34 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 35 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 36 of 250. \n",
      "Loss History: [2.2989123053584617, 2.294599148673949, 2.2873880607777233, 2.3111326756464847, 2.311918360233595, 2.2665860570304703, 2.3113340090274668, 2.283508461198438, 2.307325430813852, 2.245309583374415, 2.2359952817610265, 2.254083924787579, 2.2569937339722137, 2.2977081933131127, 2.0908347668857243, 1.954958318138915, 2.276603400195442, 1.7043690584220403, 2.1814637958034395, 2.3232108448525546, 1.9884770944220502, 1.5508812213638903, 2.486222951039882, 2.3390461711723725, 2.031379356952344, 1.8973910714199462, 2.3778533837120235, 1.5804281316255304, 2.543462645847246, 2.3162031547232864, 2.2469160238531583, 2.0835549982797823, 1.8966231603011825, 2.0022379235867906, 1.6507290023589343, 2.2292021021673007]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 8 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 8 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 5 8 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 8]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (2, 32, 8, 8)\n",
      "[5 5]\n",
      "[0 3]\n",
      "  Train acc: 0.28, Val acc: 0.0\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 37 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 38 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 39 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 40 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 41 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 42 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 43 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 44 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 45 of 250. \n",
      "Loss History: [2.2989123053584617, 2.294599148673949, 2.2873880607777233, 2.3111326756464847, 2.311918360233595, 2.2665860570304703, 2.3113340090274668, 2.283508461198438, 2.307325430813852, 2.245309583374415, 2.2359952817610265, 2.254083924787579, 2.2569937339722137, 2.2977081933131127, 2.0908347668857243, 1.954958318138915, 2.276603400195442, 1.7043690584220403, 2.1814637958034395, 2.3232108448525546, 1.9884770944220502, 1.5508812213638903, 2.486222951039882, 2.3390461711723725, 2.031379356952344, 1.8973910714199462, 2.3778533837120235, 1.5804281316255304, 2.543462645847246, 2.3162031547232864, 2.2469160238531583, 2.0835549982797823, 1.8966231603011825, 2.0022379235867906, 1.6507290023589343, 2.2292021021673007, 2.0730075820418636, 1.5633424937079916, 2.0534017119082337, 1.9591511383661975, 2.121440701488261, 2.0572297009457507, 1.8368117460946296, 1.362534435948687, 1.572336577925003]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 8 5 5 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 5 8 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 8 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 8 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 5 8 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 8 8 5 8]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 8 5]\n",
      "Your output shape is (2, 32, 8, 8)\n",
      "[5 5]\n",
      "[0 3]\n",
      "  Train acc: 0.32, Val acc: 0.0\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 46 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 47 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 48 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 49 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 50 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 51 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 52 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 53 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 54 of 250. \n",
      "Loss History: [2.2989123053584617, 2.294599148673949, 2.2873880607777233, 2.3111326756464847, 2.311918360233595, 2.2665860570304703, 2.3113340090274668, 2.283508461198438, 2.307325430813852, 2.245309583374415, 2.2359952817610265, 2.254083924787579, 2.2569937339722137, 2.2977081933131127, 2.0908347668857243, 1.954958318138915, 2.276603400195442, 1.7043690584220403, 2.1814637958034395, 2.3232108448525546, 1.9884770944220502, 1.5508812213638903, 2.486222951039882, 2.3390461711723725, 2.031379356952344, 1.8973910714199462, 2.3778533837120235, 1.5804281316255304, 2.543462645847246, 2.3162031547232864, 2.2469160238531583, 2.0835549982797823, 1.8966231603011825, 2.0022379235867906, 1.6507290023589343, 2.2292021021673007, 2.0730075820418636, 1.5633424937079916, 2.0534017119082337, 1.9591511383661975, 2.121440701488261, 2.0572297009457507, 1.8368117460946296, 1.362534435948687, 1.572336577925003, 1.8978116531542633, 2.053305438350159, 1.7109994424219312, 2.42126028969075, 2.2484030735700005, 2.4080966100655217, 1.9662666642161282, 2.0328108016491413, 2.1711233356636535]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 8 1 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 5 8 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 8 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 8 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 5 5 5 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 5 8 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 8 8 5 8]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 5 5 8 5]\n",
      "Your output shape is (2, 32, 8, 8)\n",
      "[5 5]\n",
      "[0 3]\n",
      "  Train acc: 0.36, Val acc: 0.0\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 55 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 56 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 57 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 58 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 59 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 60 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 61 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 62 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 63 of 250. \n",
      "Loss History: [2.2989123053584617, 2.294599148673949, 2.2873880607777233, 2.3111326756464847, 2.311918360233595, 2.2665860570304703, 2.3113340090274668, 2.283508461198438, 2.307325430813852, 2.245309583374415, 2.2359952817610265, 2.254083924787579, 2.2569937339722137, 2.2977081933131127, 2.0908347668857243, 1.954958318138915, 2.276603400195442, 1.7043690584220403, 2.1814637958034395, 2.3232108448525546, 1.9884770944220502, 1.5508812213638903, 2.486222951039882, 2.3390461711723725, 2.031379356952344, 1.8973910714199462, 2.3778533837120235, 1.5804281316255304, 2.543462645847246, 2.3162031547232864, 2.2469160238531583, 2.0835549982797823, 1.8966231603011825, 2.0022379235867906, 1.6507290023589343, 2.2292021021673007, 2.0730075820418636, 1.5633424937079916, 2.0534017119082337, 1.9591511383661975, 2.121440701488261, 2.0572297009457507, 1.8368117460946296, 1.362534435948687, 1.572336577925003, 1.8978116531542633, 2.053305438350159, 1.7109994424219312, 2.42126028969075, 2.2484030735700005, 2.4080966100655217, 1.9662666642161282, 2.0328108016491413, 2.1711233356636535, 2.069289701099213, 2.0769760155294317, 1.8610724599914246, 1.3889948753522026, 1.468398490781694, 1.5278757004000796, 1.5677309038086544, 1.7855080639801681, 1.3484554846492256]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 8 1 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 5 8 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 8 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 8 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 5 5 5 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 5 8 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 8 8 5 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 5 5 1 5]\n",
      "Your output shape is (2, 32, 8, 8)\n",
      "[5 5]\n",
      "[0 3]\n",
      "  Train acc: 0.38, Val acc: 0.0\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 64 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 65 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 66 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 67 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 68 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 69 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 70 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 71 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 72 of 250. \n",
      "Loss History: [2.2989123053584617, 2.294599148673949, 2.2873880607777233, 2.3111326756464847, 2.311918360233595, 2.2665860570304703, 2.3113340090274668, 2.283508461198438, 2.307325430813852, 2.245309583374415, 2.2359952817610265, 2.254083924787579, 2.2569937339722137, 2.2977081933131127, 2.0908347668857243, 1.954958318138915, 2.276603400195442, 1.7043690584220403, 2.1814637958034395, 2.3232108448525546, 1.9884770944220502, 1.5508812213638903, 2.486222951039882, 2.3390461711723725, 2.031379356952344, 1.8973910714199462, 2.3778533837120235, 1.5804281316255304, 2.543462645847246, 2.3162031547232864, 2.2469160238531583, 2.0835549982797823, 1.8966231603011825, 2.0022379235867906, 1.6507290023589343, 2.2292021021673007, 2.0730075820418636, 1.5633424937079916, 2.0534017119082337, 1.9591511383661975, 2.121440701488261, 2.0572297009457507, 1.8368117460946296, 1.362534435948687, 1.572336577925003, 1.8978116531542633, 2.053305438350159, 1.7109994424219312, 2.42126028969075, 2.2484030735700005, 2.4080966100655217, 1.9662666642161282, 2.0328108016491413, 2.1711233356636535, 2.069289701099213, 2.0769760155294317, 1.8610724599914246, 1.3889948753522026, 1.468398490781694, 1.5278757004000796, 1.5677309038086544, 1.7855080639801681, 1.3484554846492256, 1.8844284532889954, 1.6005258391961232, 1.423357924073259, 1.8726938630865644, 1.6077813662692024, 1.0147674222919354, 1.2812485933499833, 1.6922997054709699, 1.8980794520514352]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 8 1 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 5 8 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 8 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 2 5 8 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 5 5 5 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 5 8 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 8 8 5 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 5 5 1 5]\n",
      "Your output shape is (2, 32, 8, 8)\n",
      "[5 5]\n",
      "[0 3]\n",
      "  Train acc: 0.4, Val acc: 0.0\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 73 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 74 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 75 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 76 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 77 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 78 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 79 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are on iteration: 80 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 81 of 250. \n",
      "Loss History: [2.2989123053584617, 2.294599148673949, 2.2873880607777233, 2.3111326756464847, 2.311918360233595, 2.2665860570304703, 2.3113340090274668, 2.283508461198438, 2.307325430813852, 2.245309583374415, 2.2359952817610265, 2.254083924787579, 2.2569937339722137, 2.2977081933131127, 2.0908347668857243, 1.954958318138915, 2.276603400195442, 1.7043690584220403, 2.1814637958034395, 2.3232108448525546, 1.9884770944220502, 1.5508812213638903, 2.486222951039882, 2.3390461711723725, 2.031379356952344, 1.8973910714199462, 2.3778533837120235, 1.5804281316255304, 2.543462645847246, 2.3162031547232864, 2.2469160238531583, 2.0835549982797823, 1.8966231603011825, 2.0022379235867906, 1.6507290023589343, 2.2292021021673007, 2.0730075820418636, 1.5633424937079916, 2.0534017119082337, 1.9591511383661975, 2.121440701488261, 2.0572297009457507, 1.8368117460946296, 1.362534435948687, 1.572336577925003, 1.8978116531542633, 2.053305438350159, 1.7109994424219312, 2.42126028969075, 2.2484030735700005, 2.4080966100655217, 1.9662666642161282, 2.0328108016491413, 2.1711233356636535, 2.069289701099213, 2.0769760155294317, 1.8610724599914246, 1.3889948753522026, 1.468398490781694, 1.5278757004000796, 1.5677309038086544, 1.7855080639801681, 1.3484554846492256, 1.8844284532889954, 1.6005258391961232, 1.423357924073259, 1.8726938630865644, 1.6077813662692024, 1.0147674222919354, 1.2812485933499833, 1.6922997054709699, 1.8980794520514352, 1.5478596063744368, 2.802548726652087, 2.2105474724615783, 0.9061311042152241, 0.5999448290563282, 1.7691638532038596, 1.3310853210512865, 1.5606902988722107, 1.9787461471959942]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 2]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 8 2 1 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 5 8 2 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[2 1 5 2 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 2 5 8 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[2 5 5 5 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 5 8 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 8 8 5 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 2 5 1 5]\n",
      "Your output shape is (2, 32, 8, 8)\n",
      "[5 1]\n",
      "[0 3]\n",
      "  Train acc: 0.4, Val acc: 0.0\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 82 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 83 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 84 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 85 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 86 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 87 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 88 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 89 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 90 of 250. \n",
      "Loss History: [2.2989123053584617, 2.294599148673949, 2.2873880607777233, 2.3111326756464847, 2.311918360233595, 2.2665860570304703, 2.3113340090274668, 2.283508461198438, 2.307325430813852, 2.245309583374415, 2.2359952817610265, 2.254083924787579, 2.2569937339722137, 2.2977081933131127, 2.0908347668857243, 1.954958318138915, 2.276603400195442, 1.7043690584220403, 2.1814637958034395, 2.3232108448525546, 1.9884770944220502, 1.5508812213638903, 2.486222951039882, 2.3390461711723725, 2.031379356952344, 1.8973910714199462, 2.3778533837120235, 1.5804281316255304, 2.543462645847246, 2.3162031547232864, 2.2469160238531583, 2.0835549982797823, 1.8966231603011825, 2.0022379235867906, 1.6507290023589343, 2.2292021021673007, 2.0730075820418636, 1.5633424937079916, 2.0534017119082337, 1.9591511383661975, 2.121440701488261, 2.0572297009457507, 1.8368117460946296, 1.362534435948687, 1.572336577925003, 1.8978116531542633, 2.053305438350159, 1.7109994424219312, 2.42126028969075, 2.2484030735700005, 2.4080966100655217, 1.9662666642161282, 2.0328108016491413, 2.1711233356636535, 2.069289701099213, 2.0769760155294317, 1.8610724599914246, 1.3889948753522026, 1.468398490781694, 1.5278757004000796, 1.5677309038086544, 1.7855080639801681, 1.3484554846492256, 1.8844284532889954, 1.6005258391961232, 1.423357924073259, 1.8726938630865644, 1.6077813662692024, 1.0147674222919354, 1.2812485933499833, 1.6922997054709699, 1.8980794520514352, 1.5478596063744368, 2.802548726652087, 2.2105474724615783, 0.9061311042152241, 0.5999448290563282, 1.7691638532038596, 1.3310853210512865, 1.5606902988722107, 1.9787461471959942, 2.099847820577331, 2.4773103458154537, 1.7864675788832831, 1.6713901170538517, 1.797950798967486, 2.0230430722947195, 1.9499708575428016, 1.8730376849945012, 1.2445583143680587]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[3 1 5 3 2]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 8 0 7 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 3 8 2 3]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[4 1 3 2 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[3 2 3 8 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 1 1 5 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 5 8 1 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[3 3 3 5 3]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 8 8 3 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 5 5 1 5]\n",
      "Your output shape is (2, 32, 8, 8)\n",
      "[3 1]\n",
      "[0 3]\n",
      "  Train acc: 0.42, Val acc: 0.0\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 91 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 92 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 93 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 94 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 95 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 96 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 97 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 98 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 99 of 250. \n",
      "Loss History: [2.2989123053584617, 2.294599148673949, 2.2873880607777233, 2.3111326756464847, 2.311918360233595, 2.2665860570304703, 2.3113340090274668, 2.283508461198438, 2.307325430813852, 2.245309583374415, 2.2359952817610265, 2.254083924787579, 2.2569937339722137, 2.2977081933131127, 2.0908347668857243, 1.954958318138915, 2.276603400195442, 1.7043690584220403, 2.1814637958034395, 2.3232108448525546, 1.9884770944220502, 1.5508812213638903, 2.486222951039882, 2.3390461711723725, 2.031379356952344, 1.8973910714199462, 2.3778533837120235, 1.5804281316255304, 2.543462645847246, 2.3162031547232864, 2.2469160238531583, 2.0835549982797823, 1.8966231603011825, 2.0022379235867906, 1.6507290023589343, 2.2292021021673007, 2.0730075820418636, 1.5633424937079916, 2.0534017119082337, 1.9591511383661975, 2.121440701488261, 2.0572297009457507, 1.8368117460946296, 1.362534435948687, 1.572336577925003, 1.8978116531542633, 2.053305438350159, 1.7109994424219312, 2.42126028969075, 2.2484030735700005, 2.4080966100655217, 1.9662666642161282, 2.0328108016491413, 2.1711233356636535, 2.069289701099213, 2.0769760155294317, 1.8610724599914246, 1.3889948753522026, 1.468398490781694, 1.5278757004000796, 1.5677309038086544, 1.7855080639801681, 1.3484554846492256, 1.8844284532889954, 1.6005258391961232, 1.423357924073259, 1.8726938630865644, 1.6077813662692024, 1.0147674222919354, 1.2812485933499833, 1.6922997054709699, 1.8980794520514352, 1.5478596063744368, 2.802548726652087, 2.2105474724615783, 0.9061311042152241, 0.5999448290563282, 1.7691638532038596, 1.3310853210512865, 1.5606902988722107, 1.9787461471959942, 2.099847820577331, 2.4773103458154537, 1.7864675788832831, 1.6713901170538517, 1.797950798967486, 2.0230430722947195, 1.9499708575428016, 1.8730376849945012, 1.2445583143680587, 1.3694282008927436, 1.437673764963011, 2.2261079471414726, 1.6635689978808372, 1.4457511988701315, 1.7312742898299522, 1.2615689089098698, 1.753410795072626, 1.8606628444984339]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[3 7 3 3 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[7 8 0 7 3]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[0 3 8 5 3]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[7 0 3 2 3]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[3 2 3 8 7]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[7 7 7 5 7]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 3 8 1 3]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[3 3 3 3 3]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[0 0 8 3 7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your output shape is (5, 32, 8, 8)\n",
      "[0 5 5 7 3]\n",
      "Your output shape is (2, 32, 8, 8)\n",
      "[3 1]\n",
      "[0 3]\n",
      "  Train acc: 0.32, Val acc: 0.0\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 100 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 101 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 102 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 103 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 104 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 105 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 106 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 107 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 108 of 250. \n",
      "Loss History: [2.2989123053584617, 2.294599148673949, 2.2873880607777233, 2.3111326756464847, 2.311918360233595, 2.2665860570304703, 2.3113340090274668, 2.283508461198438, 2.307325430813852, 2.245309583374415, 2.2359952817610265, 2.254083924787579, 2.2569937339722137, 2.2977081933131127, 2.0908347668857243, 1.954958318138915, 2.276603400195442, 1.7043690584220403, 2.1814637958034395, 2.3232108448525546, 1.9884770944220502, 1.5508812213638903, 2.486222951039882, 2.3390461711723725, 2.031379356952344, 1.8973910714199462, 2.3778533837120235, 1.5804281316255304, 2.543462645847246, 2.3162031547232864, 2.2469160238531583, 2.0835549982797823, 1.8966231603011825, 2.0022379235867906, 1.6507290023589343, 2.2292021021673007, 2.0730075820418636, 1.5633424937079916, 2.0534017119082337, 1.9591511383661975, 2.121440701488261, 2.0572297009457507, 1.8368117460946296, 1.362534435948687, 1.572336577925003, 1.8978116531542633, 2.053305438350159, 1.7109994424219312, 2.42126028969075, 2.2484030735700005, 2.4080966100655217, 1.9662666642161282, 2.0328108016491413, 2.1711233356636535, 2.069289701099213, 2.0769760155294317, 1.8610724599914246, 1.3889948753522026, 1.468398490781694, 1.5278757004000796, 1.5677309038086544, 1.7855080639801681, 1.3484554846492256, 1.8844284532889954, 1.6005258391961232, 1.423357924073259, 1.8726938630865644, 1.6077813662692024, 1.0147674222919354, 1.2812485933499833, 1.6922997054709699, 1.8980794520514352, 1.5478596063744368, 2.802548726652087, 2.2105474724615783, 0.9061311042152241, 0.5999448290563282, 1.7691638532038596, 1.3310853210512865, 1.5606902988722107, 1.9787461471959942, 2.099847820577331, 2.4773103458154537, 1.7864675788832831, 1.6713901170538517, 1.797950798967486, 2.0230430722947195, 1.9499708575428016, 1.8730376849945012, 1.2445583143680587, 1.3694282008927436, 1.437673764963011, 2.2261079471414726, 1.6635689978808372, 1.4457511988701315, 1.7312742898299522, 1.2615689089098698, 1.753410795072626, 1.8606628444984339, 1.393913538607567, 1.2826268234124587, 1.5987687749140609, 1.2018561289288103, 1.4365790064861408, 1.1235525327813816, 1.0070004490091409, 0.9719869829426293, 1.4376951774522746]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 1 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 8 0 7 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 1 8 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 0 1 5 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 2 1 8 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 1 1 5 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 5 8 1 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 8 8 5 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 5 5 1 5]\n",
      "Your output shape is (2, 32, 8, 8)\n",
      "[5 1]\n",
      "[0 3]\n",
      "  Train acc: 0.52, Val acc: 0.0\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 109 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 110 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 111 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 112 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 113 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 114 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 115 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 116 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 117 of 250. \n",
      "Loss History: [2.2989123053584617, 2.294599148673949, 2.2873880607777233, 2.3111326756464847, 2.311918360233595, 2.2665860570304703, 2.3113340090274668, 2.283508461198438, 2.307325430813852, 2.245309583374415, 2.2359952817610265, 2.254083924787579, 2.2569937339722137, 2.2977081933131127, 2.0908347668857243, 1.954958318138915, 2.276603400195442, 1.7043690584220403, 2.1814637958034395, 2.3232108448525546, 1.9884770944220502, 1.5508812213638903, 2.486222951039882, 2.3390461711723725, 2.031379356952344, 1.8973910714199462, 2.3778533837120235, 1.5804281316255304, 2.543462645847246, 2.3162031547232864, 2.2469160238531583, 2.0835549982797823, 1.8966231603011825, 2.0022379235867906, 1.6507290023589343, 2.2292021021673007, 2.0730075820418636, 1.5633424937079916, 2.0534017119082337, 1.9591511383661975, 2.121440701488261, 2.0572297009457507, 1.8368117460946296, 1.362534435948687, 1.572336577925003, 1.8978116531542633, 2.053305438350159, 1.7109994424219312, 2.42126028969075, 2.2484030735700005, 2.4080966100655217, 1.9662666642161282, 2.0328108016491413, 2.1711233356636535, 2.069289701099213, 2.0769760155294317, 1.8610724599914246, 1.3889948753522026, 1.468398490781694, 1.5278757004000796, 1.5677309038086544, 1.7855080639801681, 1.3484554846492256, 1.8844284532889954, 1.6005258391961232, 1.423357924073259, 1.8726938630865644, 1.6077813662692024, 1.0147674222919354, 1.2812485933499833, 1.6922997054709699, 1.8980794520514352, 1.5478596063744368, 2.802548726652087, 2.2105474724615783, 0.9061311042152241, 0.5999448290563282, 1.7691638532038596, 1.3310853210512865, 1.5606902988722107, 1.9787461471959942, 2.099847820577331, 2.4773103458154537, 1.7864675788832831, 1.6713901170538517, 1.797950798967486, 2.0230430722947195, 1.9499708575428016, 1.8730376849945012, 1.2445583143680587, 1.3694282008927436, 1.437673764963011, 2.2261079471414726, 1.6635689978808372, 1.4457511988701315, 1.7312742898299522, 1.2615689089098698, 1.753410795072626, 1.8606628444984339, 1.393913538607567, 1.2826268234124587, 1.5987687749140609, 1.2018561289288103, 1.4365790064861408, 1.1235525327813816, 1.0070004490091409, 0.9719869829426293, 1.4376951774522746, 1.054878898791596, 1.8232433821291858, 0.7950305810519158, 1.0630221914793851, 1.4644210387003609, 1.407086607902143, 1.8038661916041498, 1.573332236835533, 1.235648151592282]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 1 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 8 0 7 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 1 8 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 0 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 2 5 8 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 1 1 5 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 5 8 1 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 8 8 5 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 5 5 1 5]\n",
      "Your output shape is (2, 32, 8, 8)\n",
      "[5 1]\n",
      "[0 3]\n",
      "  Train acc: 0.52, Val acc: 0.0\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 118 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 119 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 120 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 121 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 122 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 123 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 124 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 125 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 126 of 250. \n",
      "Loss History: [2.2989123053584617, 2.294599148673949, 2.2873880607777233, 2.3111326756464847, 2.311918360233595, 2.2665860570304703, 2.3113340090274668, 2.283508461198438, 2.307325430813852, 2.245309583374415, 2.2359952817610265, 2.254083924787579, 2.2569937339722137, 2.2977081933131127, 2.0908347668857243, 1.954958318138915, 2.276603400195442, 1.7043690584220403, 2.1814637958034395, 2.3232108448525546, 1.9884770944220502, 1.5508812213638903, 2.486222951039882, 2.3390461711723725, 2.031379356952344, 1.8973910714199462, 2.3778533837120235, 1.5804281316255304, 2.543462645847246, 2.3162031547232864, 2.2469160238531583, 2.0835549982797823, 1.8966231603011825, 2.0022379235867906, 1.6507290023589343, 2.2292021021673007, 2.0730075820418636, 1.5633424937079916, 2.0534017119082337, 1.9591511383661975, 2.121440701488261, 2.0572297009457507, 1.8368117460946296, 1.362534435948687, 1.572336577925003, 1.8978116531542633, 2.053305438350159, 1.7109994424219312, 2.42126028969075, 2.2484030735700005, 2.4080966100655217, 1.9662666642161282, 2.0328108016491413, 2.1711233356636535, 2.069289701099213, 2.0769760155294317, 1.8610724599914246, 1.3889948753522026, 1.468398490781694, 1.5278757004000796, 1.5677309038086544, 1.7855080639801681, 1.3484554846492256, 1.8844284532889954, 1.6005258391961232, 1.423357924073259, 1.8726938630865644, 1.6077813662692024, 1.0147674222919354, 1.2812485933499833, 1.6922997054709699, 1.8980794520514352, 1.5478596063744368, 2.802548726652087, 2.2105474724615783, 0.9061311042152241, 0.5999448290563282, 1.7691638532038596, 1.3310853210512865, 1.5606902988722107, 1.9787461471959942, 2.099847820577331, 2.4773103458154537, 1.7864675788832831, 1.6713901170538517, 1.797950798967486, 2.0230430722947195, 1.9499708575428016, 1.8730376849945012, 1.2445583143680587, 1.3694282008927436, 1.437673764963011, 2.2261079471414726, 1.6635689978808372, 1.4457511988701315, 1.7312742898299522, 1.2615689089098698, 1.753410795072626, 1.8606628444984339, 1.393913538607567, 1.2826268234124587, 1.5987687749140609, 1.2018561289288103, 1.4365790064861408, 1.1235525327813816, 1.0070004490091409, 0.9719869829426293, 1.4376951774522746, 1.054878898791596, 1.8232433821291858, 0.7950305810519158, 1.0630221914793851, 1.4644210387003609, 1.407086607902143, 1.8038661916041498, 1.573332236835533, 1.235648151592282, 1.2332335261319782, 1.5397860313013425, 1.3347607768328231, 2.286872937731574, 0.8326433065708475, 1.1714940119766275, 0.9045213603078605, 0.4013070158366125, 0.9482588322516435]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your output shape is (5, 32, 8, 8)\n",
      "[7 7 5 5 0]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[7 8 0 7 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 1 8 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 0 3 0 7]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[7 2 7 8 7]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[6 7 7 5 7]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 5 8 7 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[0 0 8 5 4]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[0 5 5 7 5]\n",
      "Your output shape is (2, 32, 8, 8)\n",
      "[5 7]\n",
      "[0 3]\n",
      "  Train acc: 0.54, Val acc: 0.0\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 127 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 128 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 129 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 130 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 131 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 132 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 133 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 134 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 135 of 250. \n",
      "Loss History: [2.2989123053584617, 2.294599148673949, 2.2873880607777233, 2.3111326756464847, 2.311918360233595, 2.2665860570304703, 2.3113340090274668, 2.283508461198438, 2.307325430813852, 2.245309583374415, 2.2359952817610265, 2.254083924787579, 2.2569937339722137, 2.2977081933131127, 2.0908347668857243, 1.954958318138915, 2.276603400195442, 1.7043690584220403, 2.1814637958034395, 2.3232108448525546, 1.9884770944220502, 1.5508812213638903, 2.486222951039882, 2.3390461711723725, 2.031379356952344, 1.8973910714199462, 2.3778533837120235, 1.5804281316255304, 2.543462645847246, 2.3162031547232864, 2.2469160238531583, 2.0835549982797823, 1.8966231603011825, 2.0022379235867906, 1.6507290023589343, 2.2292021021673007, 2.0730075820418636, 1.5633424937079916, 2.0534017119082337, 1.9591511383661975, 2.121440701488261, 2.0572297009457507, 1.8368117460946296, 1.362534435948687, 1.572336577925003, 1.8978116531542633, 2.053305438350159, 1.7109994424219312, 2.42126028969075, 2.2484030735700005, 2.4080966100655217, 1.9662666642161282, 2.0328108016491413, 2.1711233356636535, 2.069289701099213, 2.0769760155294317, 1.8610724599914246, 1.3889948753522026, 1.468398490781694, 1.5278757004000796, 1.5677309038086544, 1.7855080639801681, 1.3484554846492256, 1.8844284532889954, 1.6005258391961232, 1.423357924073259, 1.8726938630865644, 1.6077813662692024, 1.0147674222919354, 1.2812485933499833, 1.6922997054709699, 1.8980794520514352, 1.5478596063744368, 2.802548726652087, 2.2105474724615783, 0.9061311042152241, 0.5999448290563282, 1.7691638532038596, 1.3310853210512865, 1.5606902988722107, 1.9787461471959942, 2.099847820577331, 2.4773103458154537, 1.7864675788832831, 1.6713901170538517, 1.797950798967486, 2.0230430722947195, 1.9499708575428016, 1.8730376849945012, 1.2445583143680587, 1.3694282008927436, 1.437673764963011, 2.2261079471414726, 1.6635689978808372, 1.4457511988701315, 1.7312742898299522, 1.2615689089098698, 1.753410795072626, 1.8606628444984339, 1.393913538607567, 1.2826268234124587, 1.5987687749140609, 1.2018561289288103, 1.4365790064861408, 1.1235525327813816, 1.0070004490091409, 0.9719869829426293, 1.4376951774522746, 1.054878898791596, 1.8232433821291858, 0.7950305810519158, 1.0630221914793851, 1.4644210387003609, 1.407086607902143, 1.8038661916041498, 1.573332236835533, 1.235648151592282, 1.2332335261319782, 1.5397860313013425, 1.3347607768328231, 2.286872937731574, 0.8326433065708475, 1.1714940119766275, 0.9045213603078605, 0.4013070158366125, 0.9482588322516435, 0.6302202843769549, 1.460825714235411, 1.188810491702467, 1.307370705538296, 1.0990431110064922, 1.283495302651857, 0.5601567395398315, 1.335545615594331, 0.980383921982787]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 7 5 5 2]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[7 8 0 7 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 1 8 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[2 0 3 0 7]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[7 2 7 8 7]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[6 7 7 5 7]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 5 8 7 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 6 9 3 4]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 5 7 7 5]\n",
      "Your output shape is (2, 32, 8, 8)\n",
      "[5 7]\n",
      "[0 3]\n",
      "  Train acc: 0.7, Val acc: 0.0\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 136 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 137 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 138 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 139 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 140 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 141 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 142 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 143 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 144 of 250. \n",
      "Loss History: [2.2989123053584617, 2.294599148673949, 2.2873880607777233, 2.3111326756464847, 2.311918360233595, 2.2665860570304703, 2.3113340090274668, 2.283508461198438, 2.307325430813852, 2.245309583374415, 2.2359952817610265, 2.254083924787579, 2.2569937339722137, 2.2977081933131127, 2.0908347668857243, 1.954958318138915, 2.276603400195442, 1.7043690584220403, 2.1814637958034395, 2.3232108448525546, 1.9884770944220502, 1.5508812213638903, 2.486222951039882, 2.3390461711723725, 2.031379356952344, 1.8973910714199462, 2.3778533837120235, 1.5804281316255304, 2.543462645847246, 2.3162031547232864, 2.2469160238531583, 2.0835549982797823, 1.8966231603011825, 2.0022379235867906, 1.6507290023589343, 2.2292021021673007, 2.0730075820418636, 1.5633424937079916, 2.0534017119082337, 1.9591511383661975, 2.121440701488261, 2.0572297009457507, 1.8368117460946296, 1.362534435948687, 1.572336577925003, 1.8978116531542633, 2.053305438350159, 1.7109994424219312, 2.42126028969075, 2.2484030735700005, 2.4080966100655217, 1.9662666642161282, 2.0328108016491413, 2.1711233356636535, 2.069289701099213, 2.0769760155294317, 1.8610724599914246, 1.3889948753522026, 1.468398490781694, 1.5278757004000796, 1.5677309038086544, 1.7855080639801681, 1.3484554846492256, 1.8844284532889954, 1.6005258391961232, 1.423357924073259, 1.8726938630865644, 1.6077813662692024, 1.0147674222919354, 1.2812485933499833, 1.6922997054709699, 1.8980794520514352, 1.5478596063744368, 2.802548726652087, 2.2105474724615783, 0.9061311042152241, 0.5999448290563282, 1.7691638532038596, 1.3310853210512865, 1.5606902988722107, 1.9787461471959942, 2.099847820577331, 2.4773103458154537, 1.7864675788832831, 1.6713901170538517, 1.797950798967486, 2.0230430722947195, 1.9499708575428016, 1.8730376849945012, 1.2445583143680587, 1.3694282008927436, 1.437673764963011, 2.2261079471414726, 1.6635689978808372, 1.4457511988701315, 1.7312742898299522, 1.2615689089098698, 1.753410795072626, 1.8606628444984339, 1.393913538607567, 1.2826268234124587, 1.5987687749140609, 1.2018561289288103, 1.4365790064861408, 1.1235525327813816, 1.0070004490091409, 0.9719869829426293, 1.4376951774522746, 1.054878898791596, 1.8232433821291858, 0.7950305810519158, 1.0630221914793851, 1.4644210387003609, 1.407086607902143, 1.8038661916041498, 1.573332236835533, 1.235648151592282, 1.2332335261319782, 1.5397860313013425, 1.3347607768328231, 2.286872937731574, 0.8326433065708475, 1.1714940119766275, 0.9045213603078605, 0.4013070158366125, 0.9482588322516435, 0.6302202843769549, 1.460825714235411, 1.188810491702467, 1.307370705538296, 1.0990431110064922, 1.283495302651857, 0.5601567395398315, 1.335545615594331, 0.980383921982787, 0.9001299369655721, 1.1852560608210279, 1.8639296386487434, 0.7150590763552066, 0.840283398181575, 1.0444114191244374, 0.6950360444614719, 0.6819717289043639, 1.5257178092538957]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 7 5 5 2]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[7 8 2 7 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 1 8 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[2 0 3 2 7]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[7 2 7 8 7]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[7 1 7 5 7]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 5 8 3 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 6 9 3 4]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 5 5 7 5]\n",
      "Your output shape is (2, 32, 8, 8)\n",
      "[5 1]\n",
      "[0 3]\n",
      "  Train acc: 0.66, Val acc: 0.0\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 145 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 146 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 147 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 148 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 149 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 150 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 151 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 152 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 153 of 250. \n",
      "Loss History: [2.2989123053584617, 2.294599148673949, 2.2873880607777233, 2.3111326756464847, 2.311918360233595, 2.2665860570304703, 2.3113340090274668, 2.283508461198438, 2.307325430813852, 2.245309583374415, 2.2359952817610265, 2.254083924787579, 2.2569937339722137, 2.2977081933131127, 2.0908347668857243, 1.954958318138915, 2.276603400195442, 1.7043690584220403, 2.1814637958034395, 2.3232108448525546, 1.9884770944220502, 1.5508812213638903, 2.486222951039882, 2.3390461711723725, 2.031379356952344, 1.8973910714199462, 2.3778533837120235, 1.5804281316255304, 2.543462645847246, 2.3162031547232864, 2.2469160238531583, 2.0835549982797823, 1.8966231603011825, 2.0022379235867906, 1.6507290023589343, 2.2292021021673007, 2.0730075820418636, 1.5633424937079916, 2.0534017119082337, 1.9591511383661975, 2.121440701488261, 2.0572297009457507, 1.8368117460946296, 1.362534435948687, 1.572336577925003, 1.8978116531542633, 2.053305438350159, 1.7109994424219312, 2.42126028969075, 2.2484030735700005, 2.4080966100655217, 1.9662666642161282, 2.0328108016491413, 2.1711233356636535, 2.069289701099213, 2.0769760155294317, 1.8610724599914246, 1.3889948753522026, 1.468398490781694, 1.5278757004000796, 1.5677309038086544, 1.7855080639801681, 1.3484554846492256, 1.8844284532889954, 1.6005258391961232, 1.423357924073259, 1.8726938630865644, 1.6077813662692024, 1.0147674222919354, 1.2812485933499833, 1.6922997054709699, 1.8980794520514352, 1.5478596063744368, 2.802548726652087, 2.2105474724615783, 0.9061311042152241, 0.5999448290563282, 1.7691638532038596, 1.3310853210512865, 1.5606902988722107, 1.9787461471959942, 2.099847820577331, 2.4773103458154537, 1.7864675788832831, 1.6713901170538517, 1.797950798967486, 2.0230430722947195, 1.9499708575428016, 1.8730376849945012, 1.2445583143680587, 1.3694282008927436, 1.437673764963011, 2.2261079471414726, 1.6635689978808372, 1.4457511988701315, 1.7312742898299522, 1.2615689089098698, 1.753410795072626, 1.8606628444984339, 1.393913538607567, 1.2826268234124587, 1.5987687749140609, 1.2018561289288103, 1.4365790064861408, 1.1235525327813816, 1.0070004490091409, 0.9719869829426293, 1.4376951774522746, 1.054878898791596, 1.8232433821291858, 0.7950305810519158, 1.0630221914793851, 1.4644210387003609, 1.407086607902143, 1.8038661916041498, 1.573332236835533, 1.235648151592282, 1.2332335261319782, 1.5397860313013425, 1.3347607768328231, 2.286872937731574, 0.8326433065708475, 1.1714940119766275, 0.9045213603078605, 0.4013070158366125, 0.9482588322516435, 0.6302202843769549, 1.460825714235411, 1.188810491702467, 1.307370705538296, 1.0990431110064922, 1.283495302651857, 0.5601567395398315, 1.335545615594331, 0.980383921982787, 0.9001299369655721, 1.1852560608210279, 1.8639296386487434, 0.7150590763552066, 0.840283398181575, 1.0444114191244374, 0.6950360444614719, 0.6819717289043639, 1.5257178092538957, 1.1821108912076284, 2.1639442887141516, 1.7518674987366438, 1.502991159289361, 0.7298909890525107, 1.1816967822636462, 0.7951725188455439, 0.8239552414213203, 0.6921702996072105]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 1 5 5 2]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[4 8 0 1 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 1 8 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[2 0 1 2 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 2 1 9 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[6 1 1 5 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 5 8 1 4]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 8 9 3 4]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 5 1 1 5]\n",
      "Your output shape is (2, 32, 8, 8)\n",
      "[5 1]\n",
      "[0 3]\n",
      "  Train acc: 0.64, Val acc: 0.0\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 154 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 155 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 156 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 157 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 158 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 159 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 160 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 161 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 162 of 250. \n",
      "Loss History: [2.2989123053584617, 2.294599148673949, 2.2873880607777233, 2.3111326756464847, 2.311918360233595, 2.2665860570304703, 2.3113340090274668, 2.283508461198438, 2.307325430813852, 2.245309583374415, 2.2359952817610265, 2.254083924787579, 2.2569937339722137, 2.2977081933131127, 2.0908347668857243, 1.954958318138915, 2.276603400195442, 1.7043690584220403, 2.1814637958034395, 2.3232108448525546, 1.9884770944220502, 1.5508812213638903, 2.486222951039882, 2.3390461711723725, 2.031379356952344, 1.8973910714199462, 2.3778533837120235, 1.5804281316255304, 2.543462645847246, 2.3162031547232864, 2.2469160238531583, 2.0835549982797823, 1.8966231603011825, 2.0022379235867906, 1.6507290023589343, 2.2292021021673007, 2.0730075820418636, 1.5633424937079916, 2.0534017119082337, 1.9591511383661975, 2.121440701488261, 2.0572297009457507, 1.8368117460946296, 1.362534435948687, 1.572336577925003, 1.8978116531542633, 2.053305438350159, 1.7109994424219312, 2.42126028969075, 2.2484030735700005, 2.4080966100655217, 1.9662666642161282, 2.0328108016491413, 2.1711233356636535, 2.069289701099213, 2.0769760155294317, 1.8610724599914246, 1.3889948753522026, 1.468398490781694, 1.5278757004000796, 1.5677309038086544, 1.7855080639801681, 1.3484554846492256, 1.8844284532889954, 1.6005258391961232, 1.423357924073259, 1.8726938630865644, 1.6077813662692024, 1.0147674222919354, 1.2812485933499833, 1.6922997054709699, 1.8980794520514352, 1.5478596063744368, 2.802548726652087, 2.2105474724615783, 0.9061311042152241, 0.5999448290563282, 1.7691638532038596, 1.3310853210512865, 1.5606902988722107, 1.9787461471959942, 2.099847820577331, 2.4773103458154537, 1.7864675788832831, 1.6713901170538517, 1.797950798967486, 2.0230430722947195, 1.9499708575428016, 1.8730376849945012, 1.2445583143680587, 1.3694282008927436, 1.437673764963011, 2.2261079471414726, 1.6635689978808372, 1.4457511988701315, 1.7312742898299522, 1.2615689089098698, 1.753410795072626, 1.8606628444984339, 1.393913538607567, 1.2826268234124587, 1.5987687749140609, 1.2018561289288103, 1.4365790064861408, 1.1235525327813816, 1.0070004490091409, 0.9719869829426293, 1.4376951774522746, 1.054878898791596, 1.8232433821291858, 0.7950305810519158, 1.0630221914793851, 1.4644210387003609, 1.407086607902143, 1.8038661916041498, 1.573332236835533, 1.235648151592282, 1.2332335261319782, 1.5397860313013425, 1.3347607768328231, 2.286872937731574, 0.8326433065708475, 1.1714940119766275, 0.9045213603078605, 0.4013070158366125, 0.9482588322516435, 0.6302202843769549, 1.460825714235411, 1.188810491702467, 1.307370705538296, 1.0990431110064922, 1.283495302651857, 0.5601567395398315, 1.335545615594331, 0.980383921982787, 0.9001299369655721, 1.1852560608210279, 1.8639296386487434, 0.7150590763552066, 0.840283398181575, 1.0444114191244374, 0.6950360444614719, 0.6819717289043639, 1.5257178092538957, 1.1821108912076284, 2.1639442887141516, 1.7518674987366438, 1.502991159289361, 0.7298909890525107, 1.1816967822636462, 0.7951725188455439, 0.8239552414213203, 0.6921702996072105, 0.7684029964137635, 0.48587519813356916, 1.2163719550553038, 0.9439441960098972, 0.4873643199749303, 1.1976224660232002, 0.9931276458344199, 1.3334810832438366, 0.8774897192172313]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 1 5 5 2]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[4 8 0 7 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 1 8 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[2 0 4 0 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[4 2 7 8 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[7 4 1 5 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 5 8 4 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 8 9 5 4]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 5 1 1 5]\n",
      "Your output shape is (2, 32, 8, 8)\n",
      "[5 1]\n",
      "[0 3]\n",
      "  Train acc: 0.7, Val acc: 0.0\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 163 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 164 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 165 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 166 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 167 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 168 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 169 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 170 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 171 of 250. \n",
      "Loss History: [2.2989123053584617, 2.294599148673949, 2.2873880607777233, 2.3111326756464847, 2.311918360233595, 2.2665860570304703, 2.3113340090274668, 2.283508461198438, 2.307325430813852, 2.245309583374415, 2.2359952817610265, 2.254083924787579, 2.2569937339722137, 2.2977081933131127, 2.0908347668857243, 1.954958318138915, 2.276603400195442, 1.7043690584220403, 2.1814637958034395, 2.3232108448525546, 1.9884770944220502, 1.5508812213638903, 2.486222951039882, 2.3390461711723725, 2.031379356952344, 1.8973910714199462, 2.3778533837120235, 1.5804281316255304, 2.543462645847246, 2.3162031547232864, 2.2469160238531583, 2.0835549982797823, 1.8966231603011825, 2.0022379235867906, 1.6507290023589343, 2.2292021021673007, 2.0730075820418636, 1.5633424937079916, 2.0534017119082337, 1.9591511383661975, 2.121440701488261, 2.0572297009457507, 1.8368117460946296, 1.362534435948687, 1.572336577925003, 1.8978116531542633, 2.053305438350159, 1.7109994424219312, 2.42126028969075, 2.2484030735700005, 2.4080966100655217, 1.9662666642161282, 2.0328108016491413, 2.1711233356636535, 2.069289701099213, 2.0769760155294317, 1.8610724599914246, 1.3889948753522026, 1.468398490781694, 1.5278757004000796, 1.5677309038086544, 1.7855080639801681, 1.3484554846492256, 1.8844284532889954, 1.6005258391961232, 1.423357924073259, 1.8726938630865644, 1.6077813662692024, 1.0147674222919354, 1.2812485933499833, 1.6922997054709699, 1.8980794520514352, 1.5478596063744368, 2.802548726652087, 2.2105474724615783, 0.9061311042152241, 0.5999448290563282, 1.7691638532038596, 1.3310853210512865, 1.5606902988722107, 1.9787461471959942, 2.099847820577331, 2.4773103458154537, 1.7864675788832831, 1.6713901170538517, 1.797950798967486, 2.0230430722947195, 1.9499708575428016, 1.8730376849945012, 1.2445583143680587, 1.3694282008927436, 1.437673764963011, 2.2261079471414726, 1.6635689978808372, 1.4457511988701315, 1.7312742898299522, 1.2615689089098698, 1.753410795072626, 1.8606628444984339, 1.393913538607567, 1.2826268234124587, 1.5987687749140609, 1.2018561289288103, 1.4365790064861408, 1.1235525327813816, 1.0070004490091409, 0.9719869829426293, 1.4376951774522746, 1.054878898791596, 1.8232433821291858, 0.7950305810519158, 1.0630221914793851, 1.4644210387003609, 1.407086607902143, 1.8038661916041498, 1.573332236835533, 1.235648151592282, 1.2332335261319782, 1.5397860313013425, 1.3347607768328231, 2.286872937731574, 0.8326433065708475, 1.1714940119766275, 0.9045213603078605, 0.4013070158366125, 0.9482588322516435, 0.6302202843769549, 1.460825714235411, 1.188810491702467, 1.307370705538296, 1.0990431110064922, 1.283495302651857, 0.5601567395398315, 1.335545615594331, 0.980383921982787, 0.9001299369655721, 1.1852560608210279, 1.8639296386487434, 0.7150590763552066, 0.840283398181575, 1.0444114191244374, 0.6950360444614719, 0.6819717289043639, 1.5257178092538957, 1.1821108912076284, 2.1639442887141516, 1.7518674987366438, 1.502991159289361, 0.7298909890525107, 1.1816967822636462, 0.7951725188455439, 0.8239552414213203, 0.6921702996072105, 0.7684029964137635, 0.48587519813356916, 1.2163719550553038, 0.9439441960098972, 0.4873643199749303, 1.1976224660232002, 0.9931276458344199, 1.3334810832438366, 0.8774897192172313, 1.0412675308077228, 1.2109899264166395, 1.6767709464738207, 1.0052640122334615, 0.9511589226833062, 0.94910871458553, 0.7768107312224625, 0.8116823032509975, 1.1188237321495837]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 1 5 5 2]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[4 8 0 7 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 1 8 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[2 0 1 0 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 2 7 8 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[7 1 1 5 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 5 8 1 7]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 6 9 3 4]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 5 1 1 5]\n",
      "Your output shape is (2, 32, 8, 8)\n",
      "[5 1]\n",
      "[0 3]\n",
      "  Train acc: 0.74, Val acc: 0.0\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 172 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 173 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 174 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 175 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 176 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 177 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 178 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 179 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 180 of 250. \n",
      "Loss History: [2.2989123053584617, 2.294599148673949, 2.2873880607777233, 2.3111326756464847, 2.311918360233595, 2.2665860570304703, 2.3113340090274668, 2.283508461198438, 2.307325430813852, 2.245309583374415, 2.2359952817610265, 2.254083924787579, 2.2569937339722137, 2.2977081933131127, 2.0908347668857243, 1.954958318138915, 2.276603400195442, 1.7043690584220403, 2.1814637958034395, 2.3232108448525546, 1.9884770944220502, 1.5508812213638903, 2.486222951039882, 2.3390461711723725, 2.031379356952344, 1.8973910714199462, 2.3778533837120235, 1.5804281316255304, 2.543462645847246, 2.3162031547232864, 2.2469160238531583, 2.0835549982797823, 1.8966231603011825, 2.0022379235867906, 1.6507290023589343, 2.2292021021673007, 2.0730075820418636, 1.5633424937079916, 2.0534017119082337, 1.9591511383661975, 2.121440701488261, 2.0572297009457507, 1.8368117460946296, 1.362534435948687, 1.572336577925003, 1.8978116531542633, 2.053305438350159, 1.7109994424219312, 2.42126028969075, 2.2484030735700005, 2.4080966100655217, 1.9662666642161282, 2.0328108016491413, 2.1711233356636535, 2.069289701099213, 2.0769760155294317, 1.8610724599914246, 1.3889948753522026, 1.468398490781694, 1.5278757004000796, 1.5677309038086544, 1.7855080639801681, 1.3484554846492256, 1.8844284532889954, 1.6005258391961232, 1.423357924073259, 1.8726938630865644, 1.6077813662692024, 1.0147674222919354, 1.2812485933499833, 1.6922997054709699, 1.8980794520514352, 1.5478596063744368, 2.802548726652087, 2.2105474724615783, 0.9061311042152241, 0.5999448290563282, 1.7691638532038596, 1.3310853210512865, 1.5606902988722107, 1.9787461471959942, 2.099847820577331, 2.4773103458154537, 1.7864675788832831, 1.6713901170538517, 1.797950798967486, 2.0230430722947195, 1.9499708575428016, 1.8730376849945012, 1.2445583143680587, 1.3694282008927436, 1.437673764963011, 2.2261079471414726, 1.6635689978808372, 1.4457511988701315, 1.7312742898299522, 1.2615689089098698, 1.753410795072626, 1.8606628444984339, 1.393913538607567, 1.2826268234124587, 1.5987687749140609, 1.2018561289288103, 1.4365790064861408, 1.1235525327813816, 1.0070004490091409, 0.9719869829426293, 1.4376951774522746, 1.054878898791596, 1.8232433821291858, 0.7950305810519158, 1.0630221914793851, 1.4644210387003609, 1.407086607902143, 1.8038661916041498, 1.573332236835533, 1.235648151592282, 1.2332335261319782, 1.5397860313013425, 1.3347607768328231, 2.286872937731574, 0.8326433065708475, 1.1714940119766275, 0.9045213603078605, 0.4013070158366125, 0.9482588322516435, 0.6302202843769549, 1.460825714235411, 1.188810491702467, 1.307370705538296, 1.0990431110064922, 1.283495302651857, 0.5601567395398315, 1.335545615594331, 0.980383921982787, 0.9001299369655721, 1.1852560608210279, 1.8639296386487434, 0.7150590763552066, 0.840283398181575, 1.0444114191244374, 0.6950360444614719, 0.6819717289043639, 1.5257178092538957, 1.1821108912076284, 2.1639442887141516, 1.7518674987366438, 1.502991159289361, 0.7298909890525107, 1.1816967822636462, 0.7951725188455439, 0.8239552414213203, 0.6921702996072105, 0.7684029964137635, 0.48587519813356916, 1.2163719550553038, 0.9439441960098972, 0.4873643199749303, 1.1976224660232002, 0.9931276458344199, 1.3334810832438366, 0.8774897192172313, 1.0412675308077228, 1.2109899264166395, 1.6767709464738207, 1.0052640122334615, 0.9511589226833062, 0.94910871458553, 0.7768107312224625, 0.8116823032509975, 1.1188237321495837, 1.0765976947077713, 0.8554811112257812, 0.6351039679855597, 0.9359594998128057, 0.8541627075736316, 0.8828488315690095, 0.787285266857696, 0.6649574589726495, 0.6958753773885499]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 1 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[4 8 0 7 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[7 1 8 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[2 0 3 0 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 2 7 8 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[7 1 1 5 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 5 8 1 7]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 6 9 3 4]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 5 1 1 5]\n",
      "Your output shape is (2, 32, 8, 8)\n",
      "[5 1]\n",
      "[0 3]\n",
      "  Train acc: 0.76, Val acc: 0.0\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 181 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 182 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 183 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 184 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 185 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 186 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 187 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 188 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 189 of 250. \n",
      "Loss History: [2.2989123053584617, 2.294599148673949, 2.2873880607777233, 2.3111326756464847, 2.311918360233595, 2.2665860570304703, 2.3113340090274668, 2.283508461198438, 2.307325430813852, 2.245309583374415, 2.2359952817610265, 2.254083924787579, 2.2569937339722137, 2.2977081933131127, 2.0908347668857243, 1.954958318138915, 2.276603400195442, 1.7043690584220403, 2.1814637958034395, 2.3232108448525546, 1.9884770944220502, 1.5508812213638903, 2.486222951039882, 2.3390461711723725, 2.031379356952344, 1.8973910714199462, 2.3778533837120235, 1.5804281316255304, 2.543462645847246, 2.3162031547232864, 2.2469160238531583, 2.0835549982797823, 1.8966231603011825, 2.0022379235867906, 1.6507290023589343, 2.2292021021673007, 2.0730075820418636, 1.5633424937079916, 2.0534017119082337, 1.9591511383661975, 2.121440701488261, 2.0572297009457507, 1.8368117460946296, 1.362534435948687, 1.572336577925003, 1.8978116531542633, 2.053305438350159, 1.7109994424219312, 2.42126028969075, 2.2484030735700005, 2.4080966100655217, 1.9662666642161282, 2.0328108016491413, 2.1711233356636535, 2.069289701099213, 2.0769760155294317, 1.8610724599914246, 1.3889948753522026, 1.468398490781694, 1.5278757004000796, 1.5677309038086544, 1.7855080639801681, 1.3484554846492256, 1.8844284532889954, 1.6005258391961232, 1.423357924073259, 1.8726938630865644, 1.6077813662692024, 1.0147674222919354, 1.2812485933499833, 1.6922997054709699, 1.8980794520514352, 1.5478596063744368, 2.802548726652087, 2.2105474724615783, 0.9061311042152241, 0.5999448290563282, 1.7691638532038596, 1.3310853210512865, 1.5606902988722107, 1.9787461471959942, 2.099847820577331, 2.4773103458154537, 1.7864675788832831, 1.6713901170538517, 1.797950798967486, 2.0230430722947195, 1.9499708575428016, 1.8730376849945012, 1.2445583143680587, 1.3694282008927436, 1.437673764963011, 2.2261079471414726, 1.6635689978808372, 1.4457511988701315, 1.7312742898299522, 1.2615689089098698, 1.753410795072626, 1.8606628444984339, 1.393913538607567, 1.2826268234124587, 1.5987687749140609, 1.2018561289288103, 1.4365790064861408, 1.1235525327813816, 1.0070004490091409, 0.9719869829426293, 1.4376951774522746, 1.054878898791596, 1.8232433821291858, 0.7950305810519158, 1.0630221914793851, 1.4644210387003609, 1.407086607902143, 1.8038661916041498, 1.573332236835533, 1.235648151592282, 1.2332335261319782, 1.5397860313013425, 1.3347607768328231, 2.286872937731574, 0.8326433065708475, 1.1714940119766275, 0.9045213603078605, 0.4013070158366125, 0.9482588322516435, 0.6302202843769549, 1.460825714235411, 1.188810491702467, 1.307370705538296, 1.0990431110064922, 1.283495302651857, 0.5601567395398315, 1.335545615594331, 0.980383921982787, 0.9001299369655721, 1.1852560608210279, 1.8639296386487434, 0.7150590763552066, 0.840283398181575, 1.0444114191244374, 0.6950360444614719, 0.6819717289043639, 1.5257178092538957, 1.1821108912076284, 2.1639442887141516, 1.7518674987366438, 1.502991159289361, 0.7298909890525107, 1.1816967822636462, 0.7951725188455439, 0.8239552414213203, 0.6921702996072105, 0.7684029964137635, 0.48587519813356916, 1.2163719550553038, 0.9439441960098972, 0.4873643199749303, 1.1976224660232002, 0.9931276458344199, 1.3334810832438366, 0.8774897192172313, 1.0412675308077228, 1.2109899264166395, 1.6767709464738207, 1.0052640122334615, 0.9511589226833062, 0.94910871458553, 0.7768107312224625, 0.8116823032509975, 1.1188237321495837, 1.0765976947077713, 0.8554811112257812, 0.6351039679855597, 0.9359594998128057, 0.8541627075736316, 0.8828488315690095, 0.787285266857696, 0.6649574589726495, 0.6958753773885499, 1.0313430210752166, 0.3226402484072925, 0.39693804516974746, 0.6458339252499187, 0.982692864615375, 0.23866433596947695, 1.19217652182535, 0.3507749914967019, 0.2435394737433555]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 1 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[7 8 0 7 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[7 1 8 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[2 0 3 0 7]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[7 2 7 8 7]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[7 1 7 5 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 5 8 1 7]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 6 9 5 4]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 5 5 7 5]\n",
      "Your output shape is (2, 32, 8, 8)\n",
      "[5 1]\n",
      "[0 3]\n",
      "  Train acc: 0.72, Val acc: 0.0\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 190 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 191 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 192 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 193 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 194 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 195 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 196 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 197 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 198 of 250. \n",
      "Loss History: [2.2989123053584617, 2.294599148673949, 2.2873880607777233, 2.3111326756464847, 2.311918360233595, 2.2665860570304703, 2.3113340090274668, 2.283508461198438, 2.307325430813852, 2.245309583374415, 2.2359952817610265, 2.254083924787579, 2.2569937339722137, 2.2977081933131127, 2.0908347668857243, 1.954958318138915, 2.276603400195442, 1.7043690584220403, 2.1814637958034395, 2.3232108448525546, 1.9884770944220502, 1.5508812213638903, 2.486222951039882, 2.3390461711723725, 2.031379356952344, 1.8973910714199462, 2.3778533837120235, 1.5804281316255304, 2.543462645847246, 2.3162031547232864, 2.2469160238531583, 2.0835549982797823, 1.8966231603011825, 2.0022379235867906, 1.6507290023589343, 2.2292021021673007, 2.0730075820418636, 1.5633424937079916, 2.0534017119082337, 1.9591511383661975, 2.121440701488261, 2.0572297009457507, 1.8368117460946296, 1.362534435948687, 1.572336577925003, 1.8978116531542633, 2.053305438350159, 1.7109994424219312, 2.42126028969075, 2.2484030735700005, 2.4080966100655217, 1.9662666642161282, 2.0328108016491413, 2.1711233356636535, 2.069289701099213, 2.0769760155294317, 1.8610724599914246, 1.3889948753522026, 1.468398490781694, 1.5278757004000796, 1.5677309038086544, 1.7855080639801681, 1.3484554846492256, 1.8844284532889954, 1.6005258391961232, 1.423357924073259, 1.8726938630865644, 1.6077813662692024, 1.0147674222919354, 1.2812485933499833, 1.6922997054709699, 1.8980794520514352, 1.5478596063744368, 2.802548726652087, 2.2105474724615783, 0.9061311042152241, 0.5999448290563282, 1.7691638532038596, 1.3310853210512865, 1.5606902988722107, 1.9787461471959942, 2.099847820577331, 2.4773103458154537, 1.7864675788832831, 1.6713901170538517, 1.797950798967486, 2.0230430722947195, 1.9499708575428016, 1.8730376849945012, 1.2445583143680587, 1.3694282008927436, 1.437673764963011, 2.2261079471414726, 1.6635689978808372, 1.4457511988701315, 1.7312742898299522, 1.2615689089098698, 1.753410795072626, 1.8606628444984339, 1.393913538607567, 1.2826268234124587, 1.5987687749140609, 1.2018561289288103, 1.4365790064861408, 1.1235525327813816, 1.0070004490091409, 0.9719869829426293, 1.4376951774522746, 1.054878898791596, 1.8232433821291858, 0.7950305810519158, 1.0630221914793851, 1.4644210387003609, 1.407086607902143, 1.8038661916041498, 1.573332236835533, 1.235648151592282, 1.2332335261319782, 1.5397860313013425, 1.3347607768328231, 2.286872937731574, 0.8326433065708475, 1.1714940119766275, 0.9045213603078605, 0.4013070158366125, 0.9482588322516435, 0.6302202843769549, 1.460825714235411, 1.188810491702467, 1.307370705538296, 1.0990431110064922, 1.283495302651857, 0.5601567395398315, 1.335545615594331, 0.980383921982787, 0.9001299369655721, 1.1852560608210279, 1.8639296386487434, 0.7150590763552066, 0.840283398181575, 1.0444114191244374, 0.6950360444614719, 0.6819717289043639, 1.5257178092538957, 1.1821108912076284, 2.1639442887141516, 1.7518674987366438, 1.502991159289361, 0.7298909890525107, 1.1816967822636462, 0.7951725188455439, 0.8239552414213203, 0.6921702996072105, 0.7684029964137635, 0.48587519813356916, 1.2163719550553038, 0.9439441960098972, 0.4873643199749303, 1.1976224660232002, 0.9931276458344199, 1.3334810832438366, 0.8774897192172313, 1.0412675308077228, 1.2109899264166395, 1.6767709464738207, 1.0052640122334615, 0.9511589226833062, 0.94910871458553, 0.7768107312224625, 0.8116823032509975, 1.1188237321495837, 1.0765976947077713, 0.8554811112257812, 0.6351039679855597, 0.9359594998128057, 0.8541627075736316, 0.8828488315690095, 0.787285266857696, 0.6649574589726495, 0.6958753773885499, 1.0313430210752166, 0.3226402484072925, 0.39693804516974746, 0.6458339252499187, 0.982692864615375, 0.23866433596947695, 1.19217652182535, 0.3507749914967019, 0.2435394737433555, 1.601694492273477, 0.352314482978928, 0.6310095086118546, 0.5221619325160454, 0.9780599432361501, 0.8200086946833887, 0.6514623547319425, 1.3250187095611021, 0.5848582523620575]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 1 5 5 2]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[7 8 0 7 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[7 1 8 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[2 0 3 0 7]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[7 2 7 8 6]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[6 6 7 5 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 5 8 1 7]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 6 9 3 4]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 5 1 7 5]\n",
      "Your output shape is (2, 32, 8, 8)\n",
      "[5 1]\n",
      "[0 3]\n",
      "  Train acc: 0.82, Val acc: 0.0\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 199 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 200 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 201 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 202 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 203 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 204 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 205 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 206 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 207 of 250. \n",
      "Loss History: [2.2989123053584617, 2.294599148673949, 2.2873880607777233, 2.3111326756464847, 2.311918360233595, 2.2665860570304703, 2.3113340090274668, 2.283508461198438, 2.307325430813852, 2.245309583374415, 2.2359952817610265, 2.254083924787579, 2.2569937339722137, 2.2977081933131127, 2.0908347668857243, 1.954958318138915, 2.276603400195442, 1.7043690584220403, 2.1814637958034395, 2.3232108448525546, 1.9884770944220502, 1.5508812213638903, 2.486222951039882, 2.3390461711723725, 2.031379356952344, 1.8973910714199462, 2.3778533837120235, 1.5804281316255304, 2.543462645847246, 2.3162031547232864, 2.2469160238531583, 2.0835549982797823, 1.8966231603011825, 2.0022379235867906, 1.6507290023589343, 2.2292021021673007, 2.0730075820418636, 1.5633424937079916, 2.0534017119082337, 1.9591511383661975, 2.121440701488261, 2.0572297009457507, 1.8368117460946296, 1.362534435948687, 1.572336577925003, 1.8978116531542633, 2.053305438350159, 1.7109994424219312, 2.42126028969075, 2.2484030735700005, 2.4080966100655217, 1.9662666642161282, 2.0328108016491413, 2.1711233356636535, 2.069289701099213, 2.0769760155294317, 1.8610724599914246, 1.3889948753522026, 1.468398490781694, 1.5278757004000796, 1.5677309038086544, 1.7855080639801681, 1.3484554846492256, 1.8844284532889954, 1.6005258391961232, 1.423357924073259, 1.8726938630865644, 1.6077813662692024, 1.0147674222919354, 1.2812485933499833, 1.6922997054709699, 1.8980794520514352, 1.5478596063744368, 2.802548726652087, 2.2105474724615783, 0.9061311042152241, 0.5999448290563282, 1.7691638532038596, 1.3310853210512865, 1.5606902988722107, 1.9787461471959942, 2.099847820577331, 2.4773103458154537, 1.7864675788832831, 1.6713901170538517, 1.797950798967486, 2.0230430722947195, 1.9499708575428016, 1.8730376849945012, 1.2445583143680587, 1.3694282008927436, 1.437673764963011, 2.2261079471414726, 1.6635689978808372, 1.4457511988701315, 1.7312742898299522, 1.2615689089098698, 1.753410795072626, 1.8606628444984339, 1.393913538607567, 1.2826268234124587, 1.5987687749140609, 1.2018561289288103, 1.4365790064861408, 1.1235525327813816, 1.0070004490091409, 0.9719869829426293, 1.4376951774522746, 1.054878898791596, 1.8232433821291858, 0.7950305810519158, 1.0630221914793851, 1.4644210387003609, 1.407086607902143, 1.8038661916041498, 1.573332236835533, 1.235648151592282, 1.2332335261319782, 1.5397860313013425, 1.3347607768328231, 2.286872937731574, 0.8326433065708475, 1.1714940119766275, 0.9045213603078605, 0.4013070158366125, 0.9482588322516435, 0.6302202843769549, 1.460825714235411, 1.188810491702467, 1.307370705538296, 1.0990431110064922, 1.283495302651857, 0.5601567395398315, 1.335545615594331, 0.980383921982787, 0.9001299369655721, 1.1852560608210279, 1.8639296386487434, 0.7150590763552066, 0.840283398181575, 1.0444114191244374, 0.6950360444614719, 0.6819717289043639, 1.5257178092538957, 1.1821108912076284, 2.1639442887141516, 1.7518674987366438, 1.502991159289361, 0.7298909890525107, 1.1816967822636462, 0.7951725188455439, 0.8239552414213203, 0.6921702996072105, 0.7684029964137635, 0.48587519813356916, 1.2163719550553038, 0.9439441960098972, 0.4873643199749303, 1.1976224660232002, 0.9931276458344199, 1.3334810832438366, 0.8774897192172313, 1.0412675308077228, 1.2109899264166395, 1.6767709464738207, 1.0052640122334615, 0.9511589226833062, 0.94910871458553, 0.7768107312224625, 0.8116823032509975, 1.1188237321495837, 1.0765976947077713, 0.8554811112257812, 0.6351039679855597, 0.9359594998128057, 0.8541627075736316, 0.8828488315690095, 0.787285266857696, 0.6649574589726495, 0.6958753773885499, 1.0313430210752166, 0.3226402484072925, 0.39693804516974746, 0.6458339252499187, 0.982692864615375, 0.23866433596947695, 1.19217652182535, 0.3507749914967019, 0.2435394737433555, 1.601694492273477, 0.352314482978928, 0.6310095086118546, 0.5221619325160454, 0.9780599432361501, 0.8200086946833887, 0.6514623547319425, 1.3250187095611021, 0.5848582523620575, 1.0728108249086454, 0.5515687655423317, 1.0610751897957937, 0.19675312619460605, 0.6377518147929895, 0.8083539594950017, 0.8788357304544493, 0.4240811349738154, 0.6407025901394379]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 1 5 5 2]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[4 8 0 7 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[7 1 8 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[2 0 3 0 6]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[4 2 7 8 6]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[6 1 7 5 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 5 9 3 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 5 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 6 9 3 4]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 5 1 7 5]\n",
      "Your output shape is (2, 32, 8, 8)\n",
      "[5 1]\n",
      "[0 3]\n",
      "  Train acc: 0.88, Val acc: 0.0\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 208 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 209 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 210 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 211 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 212 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 213 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 214 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 215 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 216 of 250. \n",
      "Loss History: [2.2989123053584617, 2.294599148673949, 2.2873880607777233, 2.3111326756464847, 2.311918360233595, 2.2665860570304703, 2.3113340090274668, 2.283508461198438, 2.307325430813852, 2.245309583374415, 2.2359952817610265, 2.254083924787579, 2.2569937339722137, 2.2977081933131127, 2.0908347668857243, 1.954958318138915, 2.276603400195442, 1.7043690584220403, 2.1814637958034395, 2.3232108448525546, 1.9884770944220502, 1.5508812213638903, 2.486222951039882, 2.3390461711723725, 2.031379356952344, 1.8973910714199462, 2.3778533837120235, 1.5804281316255304, 2.543462645847246, 2.3162031547232864, 2.2469160238531583, 2.0835549982797823, 1.8966231603011825, 2.0022379235867906, 1.6507290023589343, 2.2292021021673007, 2.0730075820418636, 1.5633424937079916, 2.0534017119082337, 1.9591511383661975, 2.121440701488261, 2.0572297009457507, 1.8368117460946296, 1.362534435948687, 1.572336577925003, 1.8978116531542633, 2.053305438350159, 1.7109994424219312, 2.42126028969075, 2.2484030735700005, 2.4080966100655217, 1.9662666642161282, 2.0328108016491413, 2.1711233356636535, 2.069289701099213, 2.0769760155294317, 1.8610724599914246, 1.3889948753522026, 1.468398490781694, 1.5278757004000796, 1.5677309038086544, 1.7855080639801681, 1.3484554846492256, 1.8844284532889954, 1.6005258391961232, 1.423357924073259, 1.8726938630865644, 1.6077813662692024, 1.0147674222919354, 1.2812485933499833, 1.6922997054709699, 1.8980794520514352, 1.5478596063744368, 2.802548726652087, 2.2105474724615783, 0.9061311042152241, 0.5999448290563282, 1.7691638532038596, 1.3310853210512865, 1.5606902988722107, 1.9787461471959942, 2.099847820577331, 2.4773103458154537, 1.7864675788832831, 1.6713901170538517, 1.797950798967486, 2.0230430722947195, 1.9499708575428016, 1.8730376849945012, 1.2445583143680587, 1.3694282008927436, 1.437673764963011, 2.2261079471414726, 1.6635689978808372, 1.4457511988701315, 1.7312742898299522, 1.2615689089098698, 1.753410795072626, 1.8606628444984339, 1.393913538607567, 1.2826268234124587, 1.5987687749140609, 1.2018561289288103, 1.4365790064861408, 1.1235525327813816, 1.0070004490091409, 0.9719869829426293, 1.4376951774522746, 1.054878898791596, 1.8232433821291858, 0.7950305810519158, 1.0630221914793851, 1.4644210387003609, 1.407086607902143, 1.8038661916041498, 1.573332236835533, 1.235648151592282, 1.2332335261319782, 1.5397860313013425, 1.3347607768328231, 2.286872937731574, 0.8326433065708475, 1.1714940119766275, 0.9045213603078605, 0.4013070158366125, 0.9482588322516435, 0.6302202843769549, 1.460825714235411, 1.188810491702467, 1.307370705538296, 1.0990431110064922, 1.283495302651857, 0.5601567395398315, 1.335545615594331, 0.980383921982787, 0.9001299369655721, 1.1852560608210279, 1.8639296386487434, 0.7150590763552066, 0.840283398181575, 1.0444114191244374, 0.6950360444614719, 0.6819717289043639, 1.5257178092538957, 1.1821108912076284, 2.1639442887141516, 1.7518674987366438, 1.502991159289361, 0.7298909890525107, 1.1816967822636462, 0.7951725188455439, 0.8239552414213203, 0.6921702996072105, 0.7684029964137635, 0.48587519813356916, 1.2163719550553038, 0.9439441960098972, 0.4873643199749303, 1.1976224660232002, 0.9931276458344199, 1.3334810832438366, 0.8774897192172313, 1.0412675308077228, 1.2109899264166395, 1.6767709464738207, 1.0052640122334615, 0.9511589226833062, 0.94910871458553, 0.7768107312224625, 0.8116823032509975, 1.1188237321495837, 1.0765976947077713, 0.8554811112257812, 0.6351039679855597, 0.9359594998128057, 0.8541627075736316, 0.8828488315690095, 0.787285266857696, 0.6649574589726495, 0.6958753773885499, 1.0313430210752166, 0.3226402484072925, 0.39693804516974746, 0.6458339252499187, 0.982692864615375, 0.23866433596947695, 1.19217652182535, 0.3507749914967019, 0.2435394737433555, 1.601694492273477, 0.352314482978928, 0.6310095086118546, 0.5221619325160454, 0.9780599432361501, 0.8200086946833887, 0.6514623547319425, 1.3250187095611021, 0.5848582523620575, 1.0728108249086454, 0.5515687655423317, 1.0610751897957937, 0.19675312619460605, 0.6377518147929895, 0.8083539594950017, 0.8788357304544493, 0.4240811349738154, 0.6407025901394379, 0.2803929495722308, 0.5098022441470887, 0.8972068666956695, 0.19698847853418214, 0.48225235166517194, 0.5064026047321127, 0.29386144055917207, 0.7558559176089621, 0.788276396675257]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 1 5 5 2]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[4 8 0 7 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[7 1 8 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[2 0 3 0 7]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[4 2 7 8 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[7 1 7 5 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 5 9 4 4]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 3 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 6 9 3 4]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 5 1 7 5]\n",
      "Your output shape is (2, 32, 8, 8)\n",
      "[5 1]\n",
      "[0 3]\n",
      "  Train acc: 0.86, Val acc: 0.0\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 217 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 218 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 219 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 220 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 221 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 222 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 223 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 224 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 225 of 250. \n",
      "Loss History: [2.2989123053584617, 2.294599148673949, 2.2873880607777233, 2.3111326756464847, 2.311918360233595, 2.2665860570304703, 2.3113340090274668, 2.283508461198438, 2.307325430813852, 2.245309583374415, 2.2359952817610265, 2.254083924787579, 2.2569937339722137, 2.2977081933131127, 2.0908347668857243, 1.954958318138915, 2.276603400195442, 1.7043690584220403, 2.1814637958034395, 2.3232108448525546, 1.9884770944220502, 1.5508812213638903, 2.486222951039882, 2.3390461711723725, 2.031379356952344, 1.8973910714199462, 2.3778533837120235, 1.5804281316255304, 2.543462645847246, 2.3162031547232864, 2.2469160238531583, 2.0835549982797823, 1.8966231603011825, 2.0022379235867906, 1.6507290023589343, 2.2292021021673007, 2.0730075820418636, 1.5633424937079916, 2.0534017119082337, 1.9591511383661975, 2.121440701488261, 2.0572297009457507, 1.8368117460946296, 1.362534435948687, 1.572336577925003, 1.8978116531542633, 2.053305438350159, 1.7109994424219312, 2.42126028969075, 2.2484030735700005, 2.4080966100655217, 1.9662666642161282, 2.0328108016491413, 2.1711233356636535, 2.069289701099213, 2.0769760155294317, 1.8610724599914246, 1.3889948753522026, 1.468398490781694, 1.5278757004000796, 1.5677309038086544, 1.7855080639801681, 1.3484554846492256, 1.8844284532889954, 1.6005258391961232, 1.423357924073259, 1.8726938630865644, 1.6077813662692024, 1.0147674222919354, 1.2812485933499833, 1.6922997054709699, 1.8980794520514352, 1.5478596063744368, 2.802548726652087, 2.2105474724615783, 0.9061311042152241, 0.5999448290563282, 1.7691638532038596, 1.3310853210512865, 1.5606902988722107, 1.9787461471959942, 2.099847820577331, 2.4773103458154537, 1.7864675788832831, 1.6713901170538517, 1.797950798967486, 2.0230430722947195, 1.9499708575428016, 1.8730376849945012, 1.2445583143680587, 1.3694282008927436, 1.437673764963011, 2.2261079471414726, 1.6635689978808372, 1.4457511988701315, 1.7312742898299522, 1.2615689089098698, 1.753410795072626, 1.8606628444984339, 1.393913538607567, 1.2826268234124587, 1.5987687749140609, 1.2018561289288103, 1.4365790064861408, 1.1235525327813816, 1.0070004490091409, 0.9719869829426293, 1.4376951774522746, 1.054878898791596, 1.8232433821291858, 0.7950305810519158, 1.0630221914793851, 1.4644210387003609, 1.407086607902143, 1.8038661916041498, 1.573332236835533, 1.235648151592282, 1.2332335261319782, 1.5397860313013425, 1.3347607768328231, 2.286872937731574, 0.8326433065708475, 1.1714940119766275, 0.9045213603078605, 0.4013070158366125, 0.9482588322516435, 0.6302202843769549, 1.460825714235411, 1.188810491702467, 1.307370705538296, 1.0990431110064922, 1.283495302651857, 0.5601567395398315, 1.335545615594331, 0.980383921982787, 0.9001299369655721, 1.1852560608210279, 1.8639296386487434, 0.7150590763552066, 0.840283398181575, 1.0444114191244374, 0.6950360444614719, 0.6819717289043639, 1.5257178092538957, 1.1821108912076284, 2.1639442887141516, 1.7518674987366438, 1.502991159289361, 0.7298909890525107, 1.1816967822636462, 0.7951725188455439, 0.8239552414213203, 0.6921702996072105, 0.7684029964137635, 0.48587519813356916, 1.2163719550553038, 0.9439441960098972, 0.4873643199749303, 1.1976224660232002, 0.9931276458344199, 1.3334810832438366, 0.8774897192172313, 1.0412675308077228, 1.2109899264166395, 1.6767709464738207, 1.0052640122334615, 0.9511589226833062, 0.94910871458553, 0.7768107312224625, 0.8116823032509975, 1.1188237321495837, 1.0765976947077713, 0.8554811112257812, 0.6351039679855597, 0.9359594998128057, 0.8541627075736316, 0.8828488315690095, 0.787285266857696, 0.6649574589726495, 0.6958753773885499, 1.0313430210752166, 0.3226402484072925, 0.39693804516974746, 0.6458339252499187, 0.982692864615375, 0.23866433596947695, 1.19217652182535, 0.3507749914967019, 0.2435394737433555, 1.601694492273477, 0.352314482978928, 0.6310095086118546, 0.5221619325160454, 0.9780599432361501, 0.8200086946833887, 0.6514623547319425, 1.3250187095611021, 0.5848582523620575, 1.0728108249086454, 0.5515687655423317, 1.0610751897957937, 0.19675312619460605, 0.6377518147929895, 0.8083539594950017, 0.8788357304544493, 0.4240811349738154, 0.6407025901394379, 0.2803929495722308, 0.5098022441470887, 0.8972068666956695, 0.19698847853418214, 0.48225235166517194, 0.5064026047321127, 0.29386144055917207, 0.7558559176089621, 0.788276396675257, 0.8302632814804041, 0.5248801249418672, 0.2920927349178197, 0.41573290343738684, 0.8262511700705212, 0.6184148347672361, 0.767906496456532, 0.7780593273326031, 0.5020062136427076]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 1 5 5 2]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[4 8 0 7 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[7 1 8 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[2 0 3 0 7]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[4 2 7 8 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[7 1 7 5 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 5 9 1 7]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 3 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 6 9 3 4]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 5 1 7 5]\n",
      "Your output shape is (2, 32, 8, 8)\n",
      "[5 1]\n",
      "[0 3]\n",
      "  Train acc: 0.88, Val acc: 0.0\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 226 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 227 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 228 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 229 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 230 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 231 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 232 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 233 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 234 of 250. \n",
      "Loss History: [2.2989123053584617, 2.294599148673949, 2.2873880607777233, 2.3111326756464847, 2.311918360233595, 2.2665860570304703, 2.3113340090274668, 2.283508461198438, 2.307325430813852, 2.245309583374415, 2.2359952817610265, 2.254083924787579, 2.2569937339722137, 2.2977081933131127, 2.0908347668857243, 1.954958318138915, 2.276603400195442, 1.7043690584220403, 2.1814637958034395, 2.3232108448525546, 1.9884770944220502, 1.5508812213638903, 2.486222951039882, 2.3390461711723725, 2.031379356952344, 1.8973910714199462, 2.3778533837120235, 1.5804281316255304, 2.543462645847246, 2.3162031547232864, 2.2469160238531583, 2.0835549982797823, 1.8966231603011825, 2.0022379235867906, 1.6507290023589343, 2.2292021021673007, 2.0730075820418636, 1.5633424937079916, 2.0534017119082337, 1.9591511383661975, 2.121440701488261, 2.0572297009457507, 1.8368117460946296, 1.362534435948687, 1.572336577925003, 1.8978116531542633, 2.053305438350159, 1.7109994424219312, 2.42126028969075, 2.2484030735700005, 2.4080966100655217, 1.9662666642161282, 2.0328108016491413, 2.1711233356636535, 2.069289701099213, 2.0769760155294317, 1.8610724599914246, 1.3889948753522026, 1.468398490781694, 1.5278757004000796, 1.5677309038086544, 1.7855080639801681, 1.3484554846492256, 1.8844284532889954, 1.6005258391961232, 1.423357924073259, 1.8726938630865644, 1.6077813662692024, 1.0147674222919354, 1.2812485933499833, 1.6922997054709699, 1.8980794520514352, 1.5478596063744368, 2.802548726652087, 2.2105474724615783, 0.9061311042152241, 0.5999448290563282, 1.7691638532038596, 1.3310853210512865, 1.5606902988722107, 1.9787461471959942, 2.099847820577331, 2.4773103458154537, 1.7864675788832831, 1.6713901170538517, 1.797950798967486, 2.0230430722947195, 1.9499708575428016, 1.8730376849945012, 1.2445583143680587, 1.3694282008927436, 1.437673764963011, 2.2261079471414726, 1.6635689978808372, 1.4457511988701315, 1.7312742898299522, 1.2615689089098698, 1.753410795072626, 1.8606628444984339, 1.393913538607567, 1.2826268234124587, 1.5987687749140609, 1.2018561289288103, 1.4365790064861408, 1.1235525327813816, 1.0070004490091409, 0.9719869829426293, 1.4376951774522746, 1.054878898791596, 1.8232433821291858, 0.7950305810519158, 1.0630221914793851, 1.4644210387003609, 1.407086607902143, 1.8038661916041498, 1.573332236835533, 1.235648151592282, 1.2332335261319782, 1.5397860313013425, 1.3347607768328231, 2.286872937731574, 0.8326433065708475, 1.1714940119766275, 0.9045213603078605, 0.4013070158366125, 0.9482588322516435, 0.6302202843769549, 1.460825714235411, 1.188810491702467, 1.307370705538296, 1.0990431110064922, 1.283495302651857, 0.5601567395398315, 1.335545615594331, 0.980383921982787, 0.9001299369655721, 1.1852560608210279, 1.8639296386487434, 0.7150590763552066, 0.840283398181575, 1.0444114191244374, 0.6950360444614719, 0.6819717289043639, 1.5257178092538957, 1.1821108912076284, 2.1639442887141516, 1.7518674987366438, 1.502991159289361, 0.7298909890525107, 1.1816967822636462, 0.7951725188455439, 0.8239552414213203, 0.6921702996072105, 0.7684029964137635, 0.48587519813356916, 1.2163719550553038, 0.9439441960098972, 0.4873643199749303, 1.1976224660232002, 0.9931276458344199, 1.3334810832438366, 0.8774897192172313, 1.0412675308077228, 1.2109899264166395, 1.6767709464738207, 1.0052640122334615, 0.9511589226833062, 0.94910871458553, 0.7768107312224625, 0.8116823032509975, 1.1188237321495837, 1.0765976947077713, 0.8554811112257812, 0.6351039679855597, 0.9359594998128057, 0.8541627075736316, 0.8828488315690095, 0.787285266857696, 0.6649574589726495, 0.6958753773885499, 1.0313430210752166, 0.3226402484072925, 0.39693804516974746, 0.6458339252499187, 0.982692864615375, 0.23866433596947695, 1.19217652182535, 0.3507749914967019, 0.2435394737433555, 1.601694492273477, 0.352314482978928, 0.6310095086118546, 0.5221619325160454, 0.9780599432361501, 0.8200086946833887, 0.6514623547319425, 1.3250187095611021, 0.5848582523620575, 1.0728108249086454, 0.5515687655423317, 1.0610751897957937, 0.19675312619460605, 0.6377518147929895, 0.8083539594950017, 0.8788357304544493, 0.4240811349738154, 0.6407025901394379, 0.2803929495722308, 0.5098022441470887, 0.8972068666956695, 0.19698847853418214, 0.48225235166517194, 0.5064026047321127, 0.29386144055917207, 0.7558559176089621, 0.788276396675257, 0.8302632814804041, 0.5248801249418672, 0.2920927349178197, 0.41573290343738684, 0.8262511700705212, 0.6184148347672361, 0.767906496456532, 0.7780593273326031, 0.5020062136427076, 0.595547446190224, 0.7740625982767855, 0.19034796130895956, 0.3766576183156776, 0.329804656385708, 0.20615706123784203, 0.10758909638201895, 0.3304325161654673, 0.22491769004114423]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 1 5 5 2]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[4 8 0 7 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[7 1 8 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[2 0 3 0 7]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[4 2 7 8 7]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[6 1 7 5 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 5 8 3 7]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 3 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 6 9 3 4]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 5 1 7 5]\n",
      "Your output shape is (2, 32, 8, 8)\n",
      "[5 1]\n",
      "[0 3]\n",
      "  Train acc: 0.88, Val acc: 0.0\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 235 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 236 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 237 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 238 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 239 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 240 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 241 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 242 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 243 of 250. \n",
      "Loss History: [2.2989123053584617, 2.294599148673949, 2.2873880607777233, 2.3111326756464847, 2.311918360233595, 2.2665860570304703, 2.3113340090274668, 2.283508461198438, 2.307325430813852, 2.245309583374415, 2.2359952817610265, 2.254083924787579, 2.2569937339722137, 2.2977081933131127, 2.0908347668857243, 1.954958318138915, 2.276603400195442, 1.7043690584220403, 2.1814637958034395, 2.3232108448525546, 1.9884770944220502, 1.5508812213638903, 2.486222951039882, 2.3390461711723725, 2.031379356952344, 1.8973910714199462, 2.3778533837120235, 1.5804281316255304, 2.543462645847246, 2.3162031547232864, 2.2469160238531583, 2.0835549982797823, 1.8966231603011825, 2.0022379235867906, 1.6507290023589343, 2.2292021021673007, 2.0730075820418636, 1.5633424937079916, 2.0534017119082337, 1.9591511383661975, 2.121440701488261, 2.0572297009457507, 1.8368117460946296, 1.362534435948687, 1.572336577925003, 1.8978116531542633, 2.053305438350159, 1.7109994424219312, 2.42126028969075, 2.2484030735700005, 2.4080966100655217, 1.9662666642161282, 2.0328108016491413, 2.1711233356636535, 2.069289701099213, 2.0769760155294317, 1.8610724599914246, 1.3889948753522026, 1.468398490781694, 1.5278757004000796, 1.5677309038086544, 1.7855080639801681, 1.3484554846492256, 1.8844284532889954, 1.6005258391961232, 1.423357924073259, 1.8726938630865644, 1.6077813662692024, 1.0147674222919354, 1.2812485933499833, 1.6922997054709699, 1.8980794520514352, 1.5478596063744368, 2.802548726652087, 2.2105474724615783, 0.9061311042152241, 0.5999448290563282, 1.7691638532038596, 1.3310853210512865, 1.5606902988722107, 1.9787461471959942, 2.099847820577331, 2.4773103458154537, 1.7864675788832831, 1.6713901170538517, 1.797950798967486, 2.0230430722947195, 1.9499708575428016, 1.8730376849945012, 1.2445583143680587, 1.3694282008927436, 1.437673764963011, 2.2261079471414726, 1.6635689978808372, 1.4457511988701315, 1.7312742898299522, 1.2615689089098698, 1.753410795072626, 1.8606628444984339, 1.393913538607567, 1.2826268234124587, 1.5987687749140609, 1.2018561289288103, 1.4365790064861408, 1.1235525327813816, 1.0070004490091409, 0.9719869829426293, 1.4376951774522746, 1.054878898791596, 1.8232433821291858, 0.7950305810519158, 1.0630221914793851, 1.4644210387003609, 1.407086607902143, 1.8038661916041498, 1.573332236835533, 1.235648151592282, 1.2332335261319782, 1.5397860313013425, 1.3347607768328231, 2.286872937731574, 0.8326433065708475, 1.1714940119766275, 0.9045213603078605, 0.4013070158366125, 0.9482588322516435, 0.6302202843769549, 1.460825714235411, 1.188810491702467, 1.307370705538296, 1.0990431110064922, 1.283495302651857, 0.5601567395398315, 1.335545615594331, 0.980383921982787, 0.9001299369655721, 1.1852560608210279, 1.8639296386487434, 0.7150590763552066, 0.840283398181575, 1.0444114191244374, 0.6950360444614719, 0.6819717289043639, 1.5257178092538957, 1.1821108912076284, 2.1639442887141516, 1.7518674987366438, 1.502991159289361, 0.7298909890525107, 1.1816967822636462, 0.7951725188455439, 0.8239552414213203, 0.6921702996072105, 0.7684029964137635, 0.48587519813356916, 1.2163719550553038, 0.9439441960098972, 0.4873643199749303, 1.1976224660232002, 0.9931276458344199, 1.3334810832438366, 0.8774897192172313, 1.0412675308077228, 1.2109899264166395, 1.6767709464738207, 1.0052640122334615, 0.9511589226833062, 0.94910871458553, 0.7768107312224625, 0.8116823032509975, 1.1188237321495837, 1.0765976947077713, 0.8554811112257812, 0.6351039679855597, 0.9359594998128057, 0.8541627075736316, 0.8828488315690095, 0.787285266857696, 0.6649574589726495, 0.6958753773885499, 1.0313430210752166, 0.3226402484072925, 0.39693804516974746, 0.6458339252499187, 0.982692864615375, 0.23866433596947695, 1.19217652182535, 0.3507749914967019, 0.2435394737433555, 1.601694492273477, 0.352314482978928, 0.6310095086118546, 0.5221619325160454, 0.9780599432361501, 0.8200086946833887, 0.6514623547319425, 1.3250187095611021, 0.5848582523620575, 1.0728108249086454, 0.5515687655423317, 1.0610751897957937, 0.19675312619460605, 0.6377518147929895, 0.8083539594950017, 0.8788357304544493, 0.4240811349738154, 0.6407025901394379, 0.2803929495722308, 0.5098022441470887, 0.8972068666956695, 0.19698847853418214, 0.48225235166517194, 0.5064026047321127, 0.29386144055917207, 0.7558559176089621, 0.788276396675257, 0.8302632814804041, 0.5248801249418672, 0.2920927349178197, 0.41573290343738684, 0.8262511700705212, 0.6184148347672361, 0.767906496456532, 0.7780593273326031, 0.5020062136427076, 0.595547446190224, 0.7740625982767855, 0.19034796130895956, 0.3766576183156776, 0.329804656385708, 0.20615706123784203, 0.10758909638201895, 0.3304325161654673, 0.22491769004114423, 0.31992079909566357, 0.31382370692464967, 0.14419608028021333, 0.30236840093506606, 0.43139139009990357, 0.6576413217616848, 0.3751279804846685, 0.19848154758593117, 0.9263864032774318]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 1 5 5 2]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[4 8 0 7 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[7 1 8 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[2 0 3 0 6]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[4 2 7 8 6]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[6 1 7 5 1]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[8 4 8 1 7]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[5 5 3 5 5]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 6 9 3 4]\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "[1 5 1 7 5]\n",
      "Your output shape is (2, 32, 8, 8)\n",
      "[5 1]\n",
      "[0 3]\n",
      "  Train acc: 0.9, Val acc: 0.0\n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 244 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 245 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 246 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 247 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 248 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 249 of 250. \n",
      "Your output shape is (5, 32, 8, 8)\n",
      "We are on iteration: 250 of 250. \n"
     ]
    }
   ],
   "source": [
    "# Adam\n",
    "\n",
    "adam = ConvNet4(input_shape=input_shape, wt_scale=wt_scale, verbose=False)\n",
    "adam.compile('adam')\n",
    "adam.fit(x_dev, y_dev, x_val, y_val, mini_batch_sz=mini_batch_sz, n_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD-M\n",
    "sgd_m = ConvNet4(input_shape=input_shape, wt_scale=wt_scale)\n",
    "sgd_m.compile('sgd_momentum')\n",
    "sgd_m.fit(x_dev, y_dev, x_val, y_val, mini_batch_sz=mini_batch_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD\n",
    "sgd = ConvNet4(input_shape=input_shape, wt_scale=wt_scale)\n",
    "sgd.compile('sgd')\n",
    "sgd.fit(x_dev, y_dev, x_val, y_val, mini_batch_sz=mini_batch_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**: Why does decreasing the mini-batch size make the loss print-outs more erratic?\n",
    "\n",
    "It makes it more erratic because if you "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6d) Evaluate the different optimizers\n",
    "\n",
    "Make 2 \"high quality\" plots showing the following\n",
    "\n",
    "- Plot the accuracy (y axis) for the three optimizers as a function of training epoch (x axis).\n",
    "- Plot the loss (y axis) for the three optimizers as a function of training iteration (x axis).\n",
    "\n",
    "A high quality plot consists of:\n",
    "- A useful title\n",
    "- X and Y axis labels\n",
    "- A legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**: Which optimizer works best and why do think it is best?\n",
    "\n",
    "**Question 5**: What is happening with the training set accuracy and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Training convolutional neural network on STL-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7a) Load in STL-10 at 32x32 resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the STL-10 dataset from the internet, convert it to Numpy ndarray, resize to 32x32\n",
    "# cache it locally on your computer for faster loading next time.\n",
    "load_stl10_dataset.purge_cached_dataset()\n",
    "stl_imgs, stl_labels = load_stl10_dataset.load(scale_fact=3)\n",
    "# preprocess\n",
    "stl_imgs, stl_labels = preprocess_data.preprocess_stl(stl_imgs, stl_labels)\n",
    "# create splits\n",
    "x_train, y_train, x_test, y_test, x_val, y_val, x_dev, y_dev = preprocess_data.create_splits(\n",
    "    stl_imgs, stl_labels, n_train_samps=4548, n_test_samps=400, n_valid_samps=2, n_dev_samps=50)\n",
    "\n",
    "print ('Train data shape: ', x_train.shape)\n",
    "print ('Train labels shape: ', y_train.shape)\n",
    "print ('Test data shape: ', x_test.shape)\n",
    "print ('Test labels shape: ', y_test.shape)\n",
    "print ('Validation data shape: ', x_val.shape)\n",
    "print ('Validation labels shape: ', y_val.shape)\n",
    "print ('dev data shape: ', x_dev.shape)\n",
    "print ('dev labels shape: ', y_dev.shape)\n",
    "\n",
    "classes = np.loadtxt(os.path.join('data', 'stl10_binary', 'class_names.txt'), dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7b) Set up accelerated convolution and max pooling layers\n",
    "\n",
    "As you may have noticed, we had to downsize STL-10 to 16x16 resolution to train the network on the dev set (N=50) in a reasonable amount of time. The training set is N=4000, how will we ever manage to process that amount of data!?\n",
    "\n",
    "On one hand, this is an unfortunate inevitable reality of working with large (\"big\") datasets: you can easily find a dataset that is too time consuming to process for any computer, despite how fast/many CPU/GPUs it has.\n",
    "\n",
    "On the other hand, we can do better for this project and STL-10 :) If you were to time (profile) different parts of the training process, you'd notice that largest bottleneck is convolution and max pooling operations (both forward/backward). You implemented those operations intuitively, which does not always yield the best performance. **By swapping out forward/backward convolution and maxpooling for implementations that use different algorithms (im2col, reshaping) that are compiled to C code, we will speed up training up by several orders of magnitude**.\n",
    "\n",
    "Follow these steps to subsitute in the \"accelerated\" convolution and max pooling layers.\n",
    "\n",
    "- Install the `cython` python package: `pip3 install cython` (or `pip3 install cython --user` if working in Davis 102)\n",
    "- Dowload files `im2col_cython.pyx`, `accelerated_layer.py`, `setup.py` from the project website. Put them in your base project folder.\n",
    "- Open terminal, `cd` to Project directory.\n",
    "- Compile the im2col functions: `python3 setup.py build_ext --inplace`. A `.c` and `.so` file should have appeared in your project folder.\n",
    "- Restart Jupyter Notebook kernel\n",
    "- Create a class called `Conv4NetAccel` in `network.py` by copy-pasting the contents of `Conv4Net`. Import `accelerated_layer` at the top and replace the `Conv2D` and `MaxPool2D` layers with `Conv2DAccel` and `MaxPool2DAccel`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7c) Training convolutional neural network on STL-10\n",
    "\n",
    "You are now ready to train on the entire training set.\n",
    "\n",
    "- Create a `Conv4NetAccel` object with hyperparameters of your choice.\n",
    "- Your goal is to achieve 45% accuracy on the test and/or validation set.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- I suggest using your intuition about hyperparameters and over/underfitting to guide your choice, rather than a grid search. This should not be overly challenging.\n",
    "- Use the best / most efficient optimizer based on your prior analysis.\n",
    "- It should take on the order of 1 sec per training iteration. If that's way off, seek help as something could be wrong with running the acclerated code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network import ConvNet4Accel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7d) Analysis of STL-10 training quality\n",
    "\n",
    "Use your trained network that achieves 45%+ accuracy on the test set to make \"high quality\" plots showing the following \n",
    "\n",
    "- Plot the accuracy of the training and validation sets as a function of training epoch. You may have to convert iterations to epochs.\n",
    "- Plot the loss as a function of training iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(netT.validation_acc_history)\n",
    "plt.plot(netT.train_acc_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(netT.loss_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7f) Visualize layer weights\n",
    "\n",
    "Run the following code and submit the inline image of the weight visualization of the 1st layer (convolutional layer) of the network.\n",
    "\n",
    "**Note:**\n",
    "- Setting optional parameter to `True` will let you save a .PNG file in your project folder of your weights. I'd suggest setting it to `False` unless look at your weights and they look like they are worth saving. You don't want a training run that produces undesirable weights to overwrite your good looking results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights(wts, saveFig=True, filename='convWts_adam_overfit.png'):\n",
    "    grid_sz = int(np.sqrt(len(wts)))\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for x in range(grid_sz):\n",
    "        for y in range(grid_sz):\n",
    "            lin_ind = np.ravel_multi_index((x, y), dims=(grid_sz, grid_sz))\n",
    "            plt.subplot(grid_sz, grid_sz, lin_ind+1)\n",
    "            currImg = wts[lin_ind]\n",
    "            low, high = np.min(currImg), np.max(currImg)\n",
    "            currImg = 255*(currImg - low) / (high - low)\n",
    "            currImg = currImg.astype('uint8')\n",
    "            plt.imshow(currImg)\n",
    "            plt.gca().axis('off')\n",
    "    if saveFig:\n",
    "        plt.savefig('convWts_adam_overfit.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsitute your trained network below\n",
    "# netT is my network's name\n",
    "# You shouldn't see RGB noise\n",
    "plot_weights(netT.layers[0].wts.transpose(0, 2, 3, 1), saveFig=False, filename='convWts_adam_train_20epoch.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:** What do the learned filters look like? Does this make sense to you / is this what you expected? In which area of the brain do these filters resemble cell receptive fields?\n",
    "\n",
    "Note: you should not see RGB \"noise\". If you do, and you pass the \"overfit\" test with the Adam optimizer, you probably need to increase the number of training epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "**General advice:** When making modifications for extensions, make small changes, then check to make sure you pass test code. Also, test out the network runtime on small examples before/after the changes. If you're not careful, the simulation time can become intractable really quickly!\n",
    "\n",
    "**Remember:** One thorough extension usually is worth more than several \"shallow\" extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Pedal to the metal: achieve high accuracy on STL-10\n",
    "\n",
    "You can achieve higher (>50%) classification accuracy on the STL-10 test set. Find the hyperparameters to achieve this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Experiment with different network architectures.\n",
    "\n",
    "The design of the `Network` class is modular. As long as you're careful about shapes, adding/removing network layers (e.g. `Conv2D`, `Dense`, etc.) should be straight forward. Experiment with adding another sequence of `Conv2D` and `MaxPooling2D` layers. Add another `Dense` hidden layer before the output layer. How do the changes affect classification accuracy and loss? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Experiment with different network hyperparameters.\n",
    "\n",
    "Explore the affect one or more change below has on classification. Be careful about how the hyperparameters may affect the shape of network layers. Thorough analysis will get you more points (not try a few ad hoc values).\n",
    "\n",
    "- Experiment with different numbers of hidden units in the Dense layers.\n",
    "- Experiment different max pooling window sizes and strides.\n",
    "- Experiment with kernel sizes (not 7x7). Can you get away with smaller ones? Do they perform just as well? What is the change in runtime like? What is the impact on their visualized appearance?\n",
    "- Experiment with number of kernels in the convolutional layer. Is more/fewer better? What is the impact on their visualized appearance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. Add and test some training bells and whistles\n",
    "\n",
    "Add features like early stopping, learning rate decay (learning rate at the end of an epoch becomes some fraction of its former value), etc and assess how they affect training loss convergence and accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Additional optimizers\n",
    "\n",
    "Research other optimizers used in backpropogation and implement one or more of them within the model structure. Compare its performance to ones you have implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Optimize your algorithms\n",
    "\n",
    "Find the main performance bottlenecks in the network and improve your code to reduce runtime (e.g. reduce explicit for loops, increase vectorization, etc). Research faster algorithms to do operations like convolution and implement them. Given the complexity of the network, I suggest focusing on one area at a time and make sure everything you change passes the test code before proceeding. Quantify and discuss your performance improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Additional loss functions\n",
    "\n",
    "Implement support for sigmoid, or another activation functions and associated losses. Test it out and compare with softmax/cross entropy. Make sure any necessary changes to the layer's gradient are made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Additional datasets\n",
    "\n",
    "Do classification and analyxe the results with an image dataset of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Performance analysis\n",
    "\n",
    "Do a thorough comparative analysis of the non-accelerated network and accelerated networks with respect to runtime."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
