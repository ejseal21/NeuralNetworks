{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cole Turner and Ethan Seal**\n",
    "\n",
    "Fall 2019\n",
    "\n",
    "CS343: Neural Networks\n",
    "\n",
    "Project 1: Single layer networks\n",
    "\n",
    "**FINAL DUE 11:59pm Thurs Sept 19**\n",
    "- `binary_classification.ipynb` (this notebook)\n",
    "- `adaline.py`\n",
    "- `adaline_logistic.py`\n",
    "- `binary_regression.ipynb`\n",
    "\n",
    "**REMINDER**: Submit rubric on Google Classroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from adaline import Adaline\n",
    "\n",
    "# Set the color style so that Professor Layton can see your plots\n",
    "plt.style.use(['seaborn-colorblind', 'seaborn-darkgrid'])\n",
    "# Make the font size larger\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "# Turn off scientific notation when printing\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "# Automatically reload your external source code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminders\n",
    "\n",
    "- In this class, use `numpy ndarray` (`np.array()`), not Numpy Matrix.\n",
    "- To safeguard against data loss when working in a jupyter notebook, make sure the notebook is `Trusted` (Top right corner of notebook). This will ensure your work autosaves. **I still recommend manually saving at least every few minutes with (Control+S / Cmd+S)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "In this project, you will implement a single-layer neural network that includes the same fundamental components as larger multi-level networks.\n",
    "\n",
    "**The goals are to:**\n",
    "\n",
    "- Get familar with Jupyter notebooks, Numpy arrays, Pandas.\n",
    "- Get familar with the workflow for preprocessing data, training a neural network, evaluating test data, and examining performance metrics.\n",
    "- Analyze and visualize the decision boundaries formed by single-layer networks.\n",
    "- Use a similar neural network architecture for classification and for regression.\n",
    "- Work with multiple datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1) Implement the ADAptive LInear NEuron (ADALINE) network for binary classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Task 1, complete the methods of the `Adaline` class in `adaline.py`. This includes:\n",
    "\n",
    "- `net_input(self, features)`\n",
    "- `activation(self, net_in)`\n",
    "- `compute_loss(self, errors)`\n",
    "- `compute_accuracy(self, y, y_pred)`\n",
    "- `gradient(self, errors, features)`\n",
    "\n",
    "- `predict(self, features)`\n",
    "- `fit(self, features, y)`\n",
    "\n",
    "**Important:** Before starting, read through the method descriptions and expected inputs/outputs. It probabily woud be a good idea to tackle simpler/smaller methods first, then use them in more complex ones. For example it may be a good idea to work on `net_input` first because it is required to complete `fit`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a summary of the ADALINE network equations\n",
    "\n",
    "##### Net input\n",
    "\n",
    "$$\\text{netIn} = \\sum_{i=1}^M x_i w_i + b$$\n",
    "\n",
    "##### Net activation\n",
    "\n",
    "$$f(x) = x$$\n",
    "\n",
    "##### Loss: Sum of squared error\n",
    "\n",
    "$$L(\\vec{w}) = \\frac{1}{2} \\sum_{i=1}^N \\left ( y_i - \\text{netAct}_i \\right )^2 $$\n",
    "\n",
    "##### Gradient (bias)\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = \\sum_{i=1}^N \\left ( y_i - \\text{netAct}_i \\right )$$\n",
    "\n",
    "##### Gradient (wts)\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_j} = \\sum_{i=1}^N \\left ( y_i - \\text{netAct}_i \\right ) x_j$$\n",
    "\n",
    "##### Gradient descent (delta rule)\n",
    "\n",
    "$$b(t+1) = b(t) + \\eta \\frac{\\partial L}{\\partial b}$$\n",
    "$$w_j(t+1) = w_j(t) + \\eta \\frac{\\partial L}{\\partial w_j}$$\n",
    "\n",
    "above $\\eta$ is the learning rate, and $N$ is the training set (number of data samples in training epoch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Test your ADALINE implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Adaline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `loss` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your loss is 3.6609344768925496 and it should be 3.6609344768925496\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "randErrors = np.array([-0.835,  0.322, -0.381,  0.496, -0.89 , -0.953])\n",
    "net_act = np.random.rand(len(randErrors))\n",
    "debugLoss = net.compute_loss(randErrors, net_act)\n",
    "print(f'Your loss is {debugLoss} and it should be 3.6609344768925496')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `accuracy` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: Your accuracy is 1.0 and it should be 1.0\n",
      "Test 2: Your accuracy is 0.33333333333333337 and it should be 0.33333333333333337\n"
     ]
    }
   ],
   "source": [
    "randClasses1 = np.where(randErrors >= 0, 1, -1)\n",
    "randClasses2 = np.roll(randClasses1, 1)\n",
    "acc1 = net.compute_accuracy(randClasses1, randClasses1)\n",
    "acc2 = net.compute_accuracy(randClasses1, randClasses2)\n",
    "print(f'Test 1: Your accuracy is {acc1} and it should be 1.0')\n",
    "print(f'Test 2: Your accuracy is {acc2} and it should be 0.33333333333333337')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `gradient` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURES in GRADIENT\n",
      "[[ 1.764  0.4    0.979]\n",
      " [ 2.241  1.868 -0.977]\n",
      " [ 0.95  -0.151 -0.103]\n",
      " [ 0.411  0.144  1.454]\n",
      " [ 0.761  0.122  0.444]\n",
      " [ 0.334  1.494 -0.205]\n",
      " [ 0.313 -0.854 -2.553]\n",
      " [ 0.654  0.864 -0.742]\n",
      " [ 2.27  -1.454  0.046]\n",
      " [-0.187  1.533  1.469]]\n",
      "Test 1: Your bias gradient is -0.7839944892482784 and it should be -0.7839944892482784\n",
      "Test 2: Your wt gradient is [-0.4    0.897 -7.689] and it should be [-0.4    0.897 -7.689]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "randFeatures = np.random.normal(loc=0, scale=1, size=(10,3))\n",
    "randErrors1 = np.random.normal(loc=0, scale=1, size=(10,))\n",
    "randBiasGrad, randWtGrad = net.gradient(randErrors1, randFeatures)\n",
    "print(f'Test 1: Your bias gradient is {randBiasGrad} and it should be -0.7839944892482784')\n",
    "print(f'Test 2: Your wt gradient is {randWtGrad} and it should be [-0.4    0.897 -7.689]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `predict` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEIGHTS IN NET INPUT\n",
      "[-0.895  0.387 -0.511 -1.181 -0.028  0.428]\n",
      "Net Input: [-0.836 -1.401 -2.294 -1.662 -2.245 -0.674 -0.074 -3.002  1.165 -1.389]\n",
      "Net Input shape: (10,)\n",
      "Your predicted classes are [-1 -1 -1 -1 -1 -1 -1 -1  1 -1].\n",
      "            They should be [-1 -1 -1 -1 -1 -1 -1 -1  1 -1]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "inputs = np.random.randn(10, 5)\n",
    "net.wts = np.random.randn(6)\n",
    "y_pred = net.predict(inputs)\n",
    "print(f'Your predicted classes are {y_pred}.\\n            They should be [-1 -1 -1 -1 -1 -1 -1 -1  1 -1]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `fit` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.764  0.4    0.979  2.241  1.868]\n",
      " [-0.977  0.95  -0.151 -0.103  0.411]\n",
      " [ 0.144  1.454  0.761  0.122  0.444]\n",
      " [ 0.334  1.494 -0.205  0.313 -0.854]\n",
      " [-2.553  0.654  0.864 -0.742  2.27 ]\n",
      " [-1.454  0.046 -0.187  1.533  1.469]\n",
      " [ 0.155  0.378 -0.888 -1.981 -0.348]\n",
      " [ 0.156  1.23   1.202 -0.387 -0.302]\n",
      " [-1.049 -1.42  -1.706  1.951 -0.51 ]\n",
      " [-0.438 -1.253  0.777 -1.614 -0.213]]\n",
      "[-1.  1. -1. -1. -1.  1.  1.  1. -1. -1.]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,5) (10,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-e61e4f193769>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Your end-of-training loss / accuracy are\\n{loss[-1]} / {acc[-1]}.\\nThey should be\\n3.944874887550384 / 0.6'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Your wts after training are:\\n{net.get_wts()}\\nand should be\\n[-0.298 -0.033  0.33  -0.382 -0.192  0.087]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\onedrive\\documents\\classes\\NeuralNetworks\\project1\\adaline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, features, y)\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m             \u001b[0mnet_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\onedrive\\documents\\classes\\NeuralNetworks\\project1\\adaline.py\u001b[0m in \u001b[0;36mnet_input\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mnet_input\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mShape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mNum\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         '''    \n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mnet_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[0mnet_input\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,5) (10,) "
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "net = Adaline()\n",
    "inputs = np.random.randn(10, 5)\n",
    "y = np.sign(np.random.randn(10))\n",
    "loss, acc = net.fit(inputs, y)\n",
    "print(f'Your end-of-training loss / accuracy are\\n{loss[-1]} / {acc[-1]}.\\nThey should be\\n3.944874887550384 / 0.6')\n",
    "print(f'Your wts after training are:\\n{net.get_wts()}\\nand should be\\n[-0.298 -0.033  0.33  -0.382 -0.192  0.087]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Load in and preprocess old faithful data\n",
    "\n",
    "Write code to do the following in the below cell.\n",
    "\n",
    "- Load in `old_faithful.csv`, represent data using a ndarray. Shape = [Num samps, Num features] = [272, 2].\n",
    "- Assign the output classes (**severe**) to a separate 1D ndarray vector. Shape=(272,)\n",
    "- Preprocess the data by normalizing by the global maximum. I.e. the max value over all features in the transformed dataset should be 1.0. Stated a different way, only one feature component of one data sample vector equals 1.0, everything else is smaller.\n",
    "- Use matplotlib to create a scatter plot of the normalized data, color-coding data points according to their class\n",
    "- I suggest using pandas, but you're welcome to do this however you like.\n",
    "- **Make sure that executing the below cell results in an inline scatter plot, color-coded by class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(272, 4)\n"
     ]
    }
   ],
   "source": [
    "old_faithful = pd.read_csv(\"old_faithful.csv\")\n",
    "print(old_faithful.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Train ADALINE on normalized Old Faithful data using default hyperparameters (i.e. learning rate, epochs)\n",
    "\n",
    "By the final epoch, training loss should reach ~23.74 and accuracy ~97%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Adaline()\n",
    "loss, acc = net.fit(x, y)\n",
    "print(f'At the end of training, your loss is {loss[-1]:.2f} and accuracy is {acc[-1]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the provided function to plot your training results inline in the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_adaline_train(net, loss_list, acc_list, plotMarkers=False):\n",
    "    n_epochs = net.get_num_epochs()\n",
    "    lr = net.get_learning_rate()\n",
    "    \n",
    "    x = np.arange(1, n_epochs+1)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n",
    "    fig.suptitle(f'ADALINE (lr={lr}, {n_epochs} epochs)')\n",
    "    \n",
    "    curveStr = '-r'\n",
    "    if plotMarkers:\n",
    "        curveStr += 'o'\n",
    "    \n",
    "    \n",
    "    ax1.plot(x, loss_list, curveStr)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss (Sum squared error)')\n",
    "    ax2.plot(x, acc_list, curveStr)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    \n",
    "    plt.show()\n",
    "plot_adaline_train(net, loss, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "Create a cell immediately below this one to answer questions in project notebooks.\n",
    "\n",
    "1. Based on your loss and accuracy curves, does it look like your network learned to classify the old faithful data? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers\n",
    "\n",
    "1. Your answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Early stopping\n",
    "\n",
    "A common practice to speed up training is to stop training early (before the prescribed number of epochs is completed) if the loss stops changing (within some criterion level). You will modify the `fit` training function to implement this feature.\n",
    "\n",
    "Add two optional parameters to `fit`:\n",
    "- `early_stopping=False`\n",
    "- `loss_tol=0.1`\n",
    "\n",
    "In this subtask, implement the following:\n",
    "- if `early_stopping` is True, terminate the training process if the difference in loss between the previous and current epoch is `< loss_tol`. This means that the loss has converged before the pre-specified number of training epochs.\n",
    "- Set the number of training epochs to be large (2000). Determine the number of epochs required for the loss to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question\n",
    "\n",
    "2. How many epochs did it take to train the network with the early stopping tolerance of 0.1?\n",
    "3. At what approximate loss value does the network converge to when stopping early?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "2. \n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2) Feature scaling and convergence\n",
    "\n",
    "**Important:** For this task, disable early stopping  in `fit`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Feature scaling\n",
    "\n",
    "Copy your code from Task 1 to import the old faithful data, but this time don't normalize before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "4. What happens to the loss when we don't normalize the features before training? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Test how individually standardizing your features affects the rate at which loss decreases over epochs\n",
    "\n",
    "- Write code in the cell below to train the network on standardized features. Recall that standardizing a variable means applying the transformation $\\frac{x - \\mu}{\\sigma}$\n",
    "- Plot the loss and accuracy and **explain how they compare with findings from Task 1**\n",
    "\n",
    "**The cell should generate an inline pair of plots when executed**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "5. Explain the similarities/differences in loss and accuracy curves between these plots and those that you made in Task 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Visualize class boundaries\n",
    "\n",
    "For this subtask, you will plot the boundary between points (`eruptions`, `waiting` feature pairs) that get classified as severe (+1) or not (-1). To get there, fill in the blanks and answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the learned weights.\n",
    "wts = net.get_wts()\n",
    "wts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "6. What do each of the above learned weights mean in the context of our dataset? Hint: Look at your `net_in` equation, look at the features that you feed into the model, look at the scatterplot you made in 1b, think about what features are present in a single training sample.\n",
    "7. Which feature / weight index corresponds to the \"y axis value\" in your scatterplot from 1b?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "6. \n",
    "7. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform weights for plotting\n",
    "\n",
    "The class boundary equation is $0 = w_0 + w_1 \\times x_i + w_2 \\times y_i$ for sample $i$ in our data ($i$ goes to 272).\n",
    "\n",
    "But to plot it, we want an equation that looks like $y_i = m \\times x_i + b$ where $m$ and $b$ are some combinations of our weights.\n",
    "\n",
    "- Scale the weights so that the one corresponding to the \"y value\" is set to 1, then (by hand) solve for $y$. Once you do, adjust the sign/scale of your weights in code so they match up with the equation you wrote out by hand ( of form $y_i = m \\times x_i + b$).\n",
    "- Once you're done, have the cell below print your transformed weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = net.get_wts()\n",
    "\n",
    "# TODO: Adjust weights for plotting, then print them here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The below code samples 50 equally spaced x values from -1.5 to 1.5 for plotting the class boundary. Given the `x_i` values, generate `y_i` values using the equation $y_i = m \\times x_i + b$ (using your transformed weights from above). \n",
    "\n",
    "**Executing the code below should produce a graph that clearly shows this class boundary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower and upper bounds for x values of decision curve when plotting in normalized coordinates\n",
    "low, high = -1.5, 1.5\n",
    "\n",
    "# Generate 50 equally spaced x-values that we will plug into the equation for the decision boundary curve\n",
    "# to get the corresponding y-value on the curve\n",
    "x_i = np.linspace(low, high)\n",
    "# TODO: Compute y_i here\n",
    "\n",
    "plt.scatter(x[:, 0], x[:, 1], c=y)\n",
    "plt.plot(x_i, y_i)\n",
    "plt.xlabel('eruptions (standardized)')\n",
    "plt.ylabel('waiting (standardized)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3) Hyperparameters and grid search\n",
    "\n",
    "This task focuses on the influence of learning rate (a model **hyperparameter**) on the quality of neural network training.\n",
    "\n",
    "For this task, use the standardized Old Faithful features for input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Influence of learning rate on learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Adaline(lr=0.0001, n_epochs=10)\n",
    "loss, acc = net.fit(x, y)\n",
    "plot_adaline_train(net, loss, acc, plotMarkers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "8. Make small changes to the learning rate hyperparameter above. How does it affect the loss?\n",
    "9. What happens if the learning rate is increased by several orders of magnitude? How does it affect the loss? Can you explain why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "8. \n",
    "9. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Grid search\n",
    "\n",
    "How can we determine a \"good\" value of the learning rate that doesn't result in pathological training behavior? Before, we were just guessing / eye-balling it. \n",
    "\n",
    "One common, simple approach is to perform a **grid search** through the parameter space. We select with a lower bound value for the parameter and walk through the parameter space in equal steps until we hit a upper bound value. After the search concludes, we set the parameter value for \"real simulations\" to the value that \"worked best\" according to some criteria we're interested in optimizing during the grid search.\n",
    "\n",
    "**TODO:** Use grid search to find the highest learning rate that still lowers loss as a function of epoch (i.e. below this value, the loss decreases as a function of epoch, above this critical value, loss increases as a function of epoch). Find the value with precision of at least 1e-3 (10^-3). **Write all your code below and the learning rate that you find in the cell below** â€” do NOT modify the `Adaline` class.\n",
    "\n",
    "**Tips:**\n",
    "- Define lower and upper bound values for the parameter search, informed by your explorations in 3a.\n",
    "- Define a step size at least as small as your starting value. Your simulation shouldn't last more a few seconds.\n",
    "- There is a `set_learning_rate` method\n",
    "- Keep the number of epochs small (e.g. 10) to keep simulation time reasonable\n",
    "- Remember that you are outputting the entire loss history when you execute `fit`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "10. What is the critical learning rate value above which loss starts to increase?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
