{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cole Turner and Ethan Seal**\n",
    "\n",
    "Fall 2019\n",
    "\n",
    "CS343: Neural Networks\n",
    "\n",
    "Project 5: Word embeddings and SOMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import word2vec\n",
    "\n",
    "plt.style.use(['seaborn-colorblind', 'seaborn-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=7)\n",
    "\n",
    "# Automatically reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 5: Word Embeddings and SOMs overview\n",
    "\n",
    "In this project, we will train a Skip-gram (word2vec) neural network commonly used in the field of natural language processing (NLP) on text from IMDb user movie reviews. The goal of the network is to predict context words (words surrounding each word in a sentence). After implementing and training the network, you will extract the weights to obtain $H$ dimensional **word embedding** vectors for English words that appeared in the movie reviews. Because the Skip-gram network is just a slight variant of a softmax classifier with a dense hidden layer, you will leverage the network infrastructure that you developed in the convolutional neural network project.\n",
    "\n",
    "In the second part of the project (`som.ipynb`), you will implement a self-organizing map (SOM), a cooperative biologically-inspired neural network. This network will learn the nonlinear structure of the IMDb word embeddings (in $H$ dimensions) (unsupervised learning) and allow us to visualize the words in 2D. Remarkably, words with similar meanings should appear nearby each other, even though the network knows nothing about the definitions of the words!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1) Preprocess IMDb review dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Load in IMDb review data\n",
    "\n",
    "Load in the movie review text data in `imdb_train.csv`. The goal is to get a Python list of length 25,000 (there are 25,000 reviews in the training set), where element $i$ is a single string representing the $i^{th}$ review. You're welcome to do this however you like. I suggest using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract a subset of reviews\n",
    "\n",
    "- Make a variable that represents the number of reviews that we will take out of the full training set (I'm calling $R$ in instructions, but call it what you want). \n",
    "\n",
    "Larger numbers will increase simulation time and memory usage, so pick a value on the small side for testing and scale it up based on your machine's performance. To give you a baseline, I started with 3 reviews during testing and increased it to 50 reviews for \"real\" simulations. **The test code below assumes $R=10$**, the 1st 10 non-shuffled reviews (you can change this later before doing neural network simulations).\n",
    "- Use the variable to select the first $R$ reviews from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Make corpus\n",
    "\n",
    "In NLP, we usually define a **corpus**, which is the set of documents from which we're interested in learning. For the IMDb dataset and Skip-gram network, this represents a list of sentences across all the reviews (from the subset that we selected). The problem is that our reviews are chunked by review, not sentence.\n",
    "\n",
    "- In `word2vec.py` implement `make_corpus()`, which will build a list where elements are sentences, not reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = word2vec.make_corpus(reviews)\n",
    "print(f'There are {len(corpus)} sentences in the corpus. There should be 99.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify unique words and vocabulary size\n",
    "\n",
    "Now that we have a corpus of sentences, let's figure out the **vocabulary size**, defined as the number of unique words in the corpus (across all the sentences).\n",
    "- In `word2vec.py` implement `find_unique_words()` that returns a list of unique words in the corpus.\n",
    "- Define `vocab_sz` as the vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = word2vec.find_unique_words(corpus)\n",
    "vocab_sz = len(unique_words)\n",
    "print(f'There are {vocab_sz} words in the vocabulary. There should be 1016.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make word <-> unique word index lookup tables\n",
    "\n",
    "It will be useful to assign an int code to each unique word in the range `[0, vocab_sz-1]`.\n",
    "- Write `word2vec.make_word2ind_mapping`. This makes a Python dictionary `word2ind` that allows you to use a word string to look up its int code.\n",
    "- Write `word2vec.make_ind2word_mapping`. This makes a Python dictionary `ind2word` that allows you to use a word int code to look up its word string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make word -> index lookup table\n",
    "word2ind = word2vec.make_word2ind_mapping(unique_words)\n",
    "ind2word = word2vec.make_ind2word_mapping(unique_words)\n",
    "print(f'If you preserved the order of words, the int code for \"robot\" is {word2ind[\"robot\"]} and should be 159.')\n",
    "print(f'If you preserved the order of words, the int code for \"fans\" is {word2ind[\"fans\"]} and should be 108.')\n",
    "      \n",
    "print(f'If you preserved the order of words, the string associated with the int code 100 is {ind2word[100]} and should be call.')\n",
    "print(f'If you preserved the order of words, the string associated with the int code 200 is {ind2word[200]} and should be buddy.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Make training target word and context word sets\n",
    "\n",
    "To train the Skip-gram network, we need to create specially-formatted training data and classes.\n",
    "\n",
    "Each training sample will be a single word in each sentence in the corpus (**target word**). Because the first (non-input) layer of the network is a dense layer with $M$ units, we one-hot code each target word in the vocabulary: the word becomes a vector of zeros with length `vocab_sz` with a 1 at the position represented by the word's int code. For example, if `tourists` has an int code of 2, then its one-hot vector looks like `[0,0,1,0,0,0,0,....]`.\n",
    "\n",
    "Recall that the goal of Skip-gram is to learn to predict **context words**, words that surround the target word (within a window of $W$ words) in a sentence. For example, if $W=2$, the sentence is `I want to see the new Star Wars movie, how about you?`, and the target word is `the`, then the context words are `[to, see, new, Star]`. Context words play the role of `y` of our classes. In our implementation, we will represent a target word's context words in terms of their int codes.\n",
    "\n",
    "- In `word2vec.py`, implement `make_target_context_word_lists()` that returns 1) a Python list of one-hot coded target vectors (ndarrays) and 2) the associated context words, also a Python list of ndarrays, each containing the int codes for context words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_onehot, contexts_int = word2vec.make_target_context_word_lists(corpus, word2ind, vocab_sz)\n",
    "print(f'Training size: {len(targets_onehot)}. It should be 2255.')\n",
    "print(f'Second target vector is {targets_onehot[1]} and should be [[0. 1. 0. ... 0. 0. 0.]]')\n",
    "print(f'Second target vector sums to {np.sum(targets_onehot[1])} and it should be 1.0')\n",
    "print(f'Second context list is {contexts_int[1]} and should be [0 2 3]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2) Implement Skip-gram network\n",
    "\n",
    "The network has the architecture: `Input -> Dense (linear act) -> Dense (softmax act)`.\n",
    "\n",
    "You only need to make a small number of modifications to get Skip-gram to work (*the following list may seem long, but most changes are ~1 line tweaks*).\n",
    "\n",
    "- Copy over your `layer.py`, `network.py`, and `optimizer.py` files from the CNN project. \n",
    "- In `word2vec.py`, implement the Skipgram class constructor. This involves building the network that implements the above layer architecture.\n",
    "- In `word2vec.py`, override in the Skipgram class the `fit` training method.\n",
    "- Add support for a `softmax_embedding` activation function type in `layer.py`. This is needed because the Skip-gram loss function is not *vanilla* cross-entropy (it's the multi-correct-class generalization).\n",
    "    - Add a function `layer::skipgram()`, which is the skipgram loss function. Signature should be `def skipgram(self, y):`, where `y` is the int-coded context word indicies (`shape=(context_sz,)`). Equation for the loss is below.\n",
    "    - Update `layer::loss`.  Make sure the skipgram loss function is called from `loss()` if the net activation function is 'softmax_embedding'.\n",
    "    - Update `layer::compute_net_act`: Make sure the regular softmax is computed in the forward pass even if the activation string is 'softmax_embedding'.\n",
    "    - Update `layer::compute_dlast_net_act`: if activation string is 'softmax_embedding', return a COPY of the net_act in the current layer (this method is only entered for the output layer, so `net_act` = `z_net_act`).\n",
    "    - Update `layer::backward_netAct_to_netIn`: This is the gradient of Skip-gram loss function. When you enter this function for the output `Dense` layer, `d_upstream` is equal to `z_net_act` (due to what you did on the last step). Use the short-cut approach from class to implement.\n",
    "- In `word2vec.py`, implement the `get_word_vector` method that allows us to get a word embedding vector from a trained network.\n",
    "- In `word2vec.py`, implement the `get_all_word_vectors` method that allows us to get all word embedding vector from a list passed in from a trained network.\n",
    "- Test your implementation using the simple toy data below.\n",
    "\n",
    "Equation for skip-gram loss: $$L = C \\times Log\\left ( \\sum_{j=1}^{vocabSz} exp \\left ( \\text{z_net_in}_j \\right ) \\right ) - \\sum_{i=1}^{C} \\text{z_net_in}_{\\vec{y_i}}$$\n",
    "\n",
    "**Left term:** $C$ is the number of words in the current context, $\\text{z_net_in}_j$ are the `net_in` values in the Output layer, and the sum over $j$ is a sum over ALL `net_in` values (before softmax) (summing over units coding each word in the vocab)\n",
    "\n",
    "**Right term:** This is the sum of the Output layer `net_in` values at the context word indices $\\vec{y_i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Simple corpus test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"I love neural networks and studying computer science at colby college\".split()]\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = word2vec.find_unique_words(corpus)\n",
    "# Make word -> index lookup table\n",
    "word2ind = word2vec.make_word2ind_mapping(unique_words)\n",
    "ind2word = word2vec.make_ind2word_mapping(unique_words)\n",
    "word2ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`word2ind` should look like:\n",
    "\n",
    "     {'I': 0,\n",
    "     'love': 1,\n",
    "     'neural': 2,\n",
    "     'networks': 3,\n",
    "     'and': 4,\n",
    "     'studying': 5,\n",
    "     'computer': 6,\n",
    "     'science': 7,\n",
    "     'at': 8,\n",
    "     'colby': 9,\n",
    "     'college': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_onehot, contexts_int = word2vec.make_target_context_word_lists(corpus, word2ind, vocab_sz)\n",
    "print(f'Training size: {len(targets_onehot)}. It should be 11.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** How do you control the dimension of the word vector embedding?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Skipgram loss, forward pass, backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "embedding_sz = 10\n",
    "\n",
    "# Create Skipgram object here\n",
    "net = word2vec.Skipgram(input_shape=(1, vocab_sz),\n",
    "                        dense_interior_units=(embedding_sz,),\n",
    "                        n_classes=vocab_sz,\n",
    "                        wt_scale=1e-1)\n",
    "\n",
    "y = np.array([0, 1, 10])\n",
    "net.layers[-1].net_in = np.random.randn(1, vocab_sz)\n",
    "test_loss = net.layers[-1].loss(y)\n",
    "net.layers[-1].softmax()\n",
    "skipgram_grad = net.layers[-1].backward_netAct_to_netIn(net.layers[-1].net_act, y)\n",
    "\n",
    "print(f'Your loss is {test_loss} and it should be 19.772769210415863.')\n",
    "print(f'Your forward softmax is\\n{net.layers[-1].net_act} and it should be\\n[[0.0007281 0.0015607 0.0006291 ... 0.0001252 0.000447  0.0001193]]')\n",
    "print(f'Your skip-gram gradient is\\n{skipgram_grad} and it should be\\n[[-0.9978156 -0.9953178  0.0018872 ...  0.0003757  0.0013411  0.0003578]]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test entire training workflow\n",
    "\n",
    "- Create a `Skipgram` network with an embedding size of 10 and wt scale of 1e-1.\n",
    "- Compile the network with an optimizer of your choice and learning rate of 0.01.\n",
    "- Fit the model to `targets_onehot`, `contexts_int` with 500 epochs of training.\n",
    "    \n",
    "If your network works, you should see the loss drop to ~49 after about 100 epochs and remain stable for the rest of the simulation (no large jumps). This should take only seconds to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "embedding_sz = 10\n",
    "\n",
    "# Create Skipgram object here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Train model on IMDb data\n",
    "\n",
    "- Load the IMDb data back into memory.\n",
    "- Train the Skip-gram model on the IMDb data. As a starting point, use an embedding size/dimension of 10, 0.01 learning rate, and 20 epochs. If all goes well, the loss should converge and remain stable around 16-17 (assuming 10 reviews are in the corpus).\n",
    "- When you get your self-organizing map (SOM) working to visualize the word vectors, you may want to increase the # reviews in the corpus, depending on the quality of the word vectors (determined by looking at your SOM), and how fast training is going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "embedding_sz = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** Plot the loss history below when there are 10 reviews in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Get and export word embedding vectors\n",
    "\n",
    "- Implement in `word2vec.py` `get_word_vector()`\n",
    "- Implement in `word2vec.py` `get_all_word_vectors()`\n",
    "- Run the following code to save off the IMDb word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `get_word_vector`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The shape of one of your word vectors are {net.get_word_vector(word2ind, \"james\").shape} and should be (10,)')\n",
    "print(f'The word vector for \"james\" looks like:\\n{net.get_word_vector(word2ind, \"james\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `get_all_word_vectors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs = net.get_all_word_vectors(word2ind, unique_words)\n",
    "print(f'Shape of word vectors: {word_vecs.shape} and should be (1016, 10)\\n(for 10 reviews and embedding sz of 10)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('results/imdb_word_vectors', word_vecs)\n",
    "np.save('results/imdb_word_strings', unique_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
