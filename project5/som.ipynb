{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ethan Seal and Cole Turner**\n",
    "\n",
    "Fall 2019\n",
    "\n",
    "CS343: Neural Networks\n",
    "\n",
    "Project 5: Word embeddings and SOMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use(['seaborn-colorblind', 'seaborn-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "# Automatically reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3) Implement a Self-Organizing Map (SOM) neural network\n",
    "\n",
    "You will implement a SOM to visualize the word embeddings in 2D.\n",
    "\n",
    "Implement the method stubs in `som.py`. I suggest working in the following order:\n",
    "- constructor\n",
    "- `compute_decayed_param`. Equation is $$p(t) = p\\times exp \\left (-t / \\left ( \\frac{T}{2} \\right ) \\right ) $$ Above, $p$ is the parameter (e.g. learning rate), $t$ is the current training iteration, $T$ is the total number of training iterations.\n",
    "- `gaussian`.  Equation is: $$g(r, c) = \\alpha(t)\\times exp \\left (- \\left (|| M_{x,y} - (c,r) || \\right ) / \\left ( 2 \\sigma^2 \\right ) \\right ) $$Above, $\\alpha(t)$ is the current (decayed) learning rate, $M_{x,y}$ is the BMU neighborhood grid of (x,y) ordered pairs of SOM unit indices/positions, $(c,r)$ is the center of the Gaussian (BMU column, row position), and $\\sigma$ is the current (decayed) standard deviation, $||\\cdot||$ is the $L^2$ norm (over x,y values).\n",
    "- `fit`\n",
    "- `get_bmu`\n",
    "- `get_nearest_wts`\n",
    "- `update_wts`. SOM update rule is $$\\vec{w_{rc}}(t) = \\vec{w_{rc}}(t-1) + g(r_{bmu}, c_{bmu})\\left ( \\vec{\\text{input_i}} - \\vec{w_{rc}}(t-1)\\right )$$Above, $w_{rc}$ is the SOM weight vector belonging to the unit positioned at row $r$ and column $c$, $t$ is iteration number, $g(r_{bmu}, c_{bmu})$ is the Gaussian neighborhood matrix centered on the BMU evaluated at the SOM $(row, col) = (r, c)$, $\\text{input_i}$ is the data vector. NOTE: This weight update occurs to ALL SOM unit rows and columns $(r, c)$ at time t (due to the Gaussian neighborhood function).\n",
    "- `error`\n",
    "- `u_matrix`\n",
    "\n",
    "As you implement, use the below code to test your implementation. This starts with simple toy data, then on the Iris dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Test implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import som"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your SOM initial weights (shape: (3, 3, 2)) are\n",
      "[[[ 0.975  0.221]\n",
      "  [ 0.4    0.916]\n",
      "  [ 0.886 -0.464]]\n",
      "\n",
      " [[ 0.988 -0.157]\n",
      "  [-0.244  0.97 ]\n",
      "  [ 0.099  0.995]]\n",
      "\n",
      " [[ 0.987  0.158]\n",
      "  [ 0.799  0.601]\n",
      "  [ 0.991 -0.136]]]\n"
     ]
    }
   ],
   "source": [
    "som_sz = 3\n",
    "n_features = 2\n",
    "max_iter = 10\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "test_som = som.SOM(som_sz, n_features, max_iter)\n",
    "print(f'Your SOM initial weights (shape: {test_som.get_wts().shape}) are\\n{test_som.get_wts()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see:\n",
    "\n",
    "    Your SOM initial weights (shape: (3, 3, 2)) are\n",
    "    [[[ 0.975  0.221]\n",
    "      [ 0.4    0.916]\n",
    "      [ 0.886 -0.464]]\n",
    "\n",
    "     [[ 0.988 -0.157]\n",
    "      [-0.244  0.97 ]\n",
    "      [ 0.099  0.995]]\n",
    "\n",
    "     [[ 0.987  0.158]\n",
    "      [ 0.799  0.601]\n",
    "      [ 0.991 -0.136]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of your BMU neighborhood x (3, 3) and y (3, 3).\n",
      "They should be (3, 3), (3, 3), and (3, 3)\n",
      "min/max of your x, y neighborhood grids are 0/2, 0/2\n",
      "They should be 0/2, 0/2\n"
     ]
    }
   ],
   "source": [
    "print(f'Shapes of your BMU neighborhood x {test_som.bmu_neighborhood_x.shape} and y {test_som.bmu_neighborhood_y.shape}.')\n",
    "print('They should be (3, 3), (3, 3), and (3, 3)')\n",
    "min_x, max_x = test_som.bmu_neighborhood_x.min(), test_som.bmu_neighborhood_x.max()\n",
    "min_y, max_y = test_som.bmu_neighborhood_y.min(), test_som.bmu_neighborhood_y.max()\n",
    "print(f'min/max of your x, y neighborhood grids are {min_x}/{max_x}, {min_y}/{max_y}')\n",
    "print('They should be 0/2, 0/2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test learning rate decay function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.    0.819 0.67  0.549 0.449 0.368 0.301 0.247 0.202 0.165]\n",
      "[0.165 0.147 0.128 0.11  0.092 0.073 0.055 0.037 0.018 0.   ]\n"
     ]
    }
   ],
   "source": [
    "vec = np.arange(10)\n",
    "print(test_som.compute_decayed_param(vec, 1))\n",
    "print(test_som.compute_decayed_param(20, vec[::-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see:\n",
    "\n",
    "    [1.    0.819 0.67  0.549 0.449 0.368 0.301 0.247 0.202 0.165]\n",
    "    [0.165 0.147 0.128 0.11  0.092 0.073 0.055 0.037 0.018 0.   ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Gaussian neighborhood function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.368 0.607 0.368]\n",
      " [0.607 1.    0.607]\n",
      " [0.368 0.607 0.368]]\n"
     ]
    }
   ],
   "source": [
    "cent_xy = (1, 1)\n",
    "sigma = 1.0\n",
    "lr = 1.0\n",
    "print(test_som.gaussian(cent_xy, sigma, lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Test 1 Gaussian neighborhood should look like:\n",
    "\n",
    "    [[0.368 0.607 0.368]\n",
    "     [0.607 1.    0.607]\n",
    "     [0.368 0.607 0.368]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.607 1.    0.607]\n",
      " [0.368 0.607 0.368]\n",
      " [0.082 0.135 0.082]]\n"
     ]
    }
   ],
   "source": [
    "cent_xy = (0, 1)\n",
    "print(test_som.gaussian(cent_xy, sigma, lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Test 2 Gaussian neighborhood should look like:\n",
    "\n",
    "    [[0.607 1.    0.607]\n",
    "     [0.368 0.607 0.368]\n",
    "     [0.082 0.135 0.082]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.135 1.    0.135]\n",
      " [0.018 0.135 0.018]\n",
      " [0.    0.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "cent_xy = (0, 1)\n",
    "sigma = 0.5\n",
    "print(test_som.gaussian(cent_xy, sigma, lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Test 3 Gaussian neighborhood should look like:\n",
    "\n",
    "    [[0.135 1.    0.135]\n",
    "     [0.018 0.135 0.018]\n",
    "     [0.    0.    0.   ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.068 0.5   0.068]\n",
      " [0.009 0.068 0.009]\n",
      " [0.    0.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "cent_xy = (0, 1)\n",
    "sigma = 0.5\n",
    "lr = 0.5\n",
    "print(test_som.gaussian(cent_xy, sigma, lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Test 4 Gaussian neighborhood should look like:\n",
    "\n",
    "    [[0.068 0.5   0.068]\n",
    "     [0.009 0.068 0.009]\n",
    "     [0.    0.    0.   ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `get_nearest_wts` and  `get_bmu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nearest wt vector to [0.916, 0.4] is [0.979 0.347] and should be [0.975 0.221]\n",
      "The nearest wt vector to [-1, -1] is [ 0.904 -0.228] and should be [ 0.886 -0.464]\n"
     ]
    }
   ],
   "source": [
    "test_vec1 = np.array([0.916, 0.4])\n",
    "test_vec1 = test_vec1[np.newaxis, ...]\n",
    "test_som.get_nearest_wts(test_vec1)\n",
    "print(f'The nearest wt vector to [0.916, 0.4] is {test_som.get_nearest_wts(test_vec1)[0]} and should be [0.975 0.221]')\n",
    "test_vec2 = np.array([-1, -1])\n",
    "test_vec2 = test_vec2[np.newaxis, ...]\n",
    "print(f'The nearest wt vector to [-1, -1] is {test_som.get_nearest_wts(test_vec2)[0]} and should be [ 0.886 -0.464]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `update_wts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after update are:\n",
      "[[[ 0.979  0.347]\n",
      "  [ 0.498  0.93 ]\n",
      "  [ 0.904 -0.228]]\n",
      "\n",
      " [[ 0.99   0.031]\n",
      "  [-0.04   0.975]\n",
      "  [ 0.245  0.996]]\n",
      "\n",
      " [[ 0.989  0.294]\n",
      "  [ 0.832  0.666]\n",
      "  [ 0.992  0.047]]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "som_sz = 3 # 5, 5\n",
    "n_features = 2\n",
    "max_iter = 10\n",
    "t = 1\n",
    "\n",
    "test_som = som.SOM(som_sz, n_features, max_iter)\n",
    "\n",
    "input_vector = np.array([1, 1])\n",
    "bmu_xy = (1, 1)\n",
    "\n",
    "test_som.update_wts(t, input_vector, bmu_xy)\n",
    "print(f'Weights after update are:\\n{test_som.get_wts()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Weights after update are:\n",
    "    [[[ 0.979  0.347]\n",
    "      [ 0.498  0.93 ]\n",
    "      [ 0.904 -0.228]]\n",
    "\n",
    "     [[ 0.99   0.031]\n",
    "      [-0.04   0.975]\n",
    "      [ 0.245  0.996]]\n",
    "\n",
    "     [[ 0.989  0.294]\n",
    "      [ 0.832  0.666]\n",
    "      [ 0.992  0.047]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test U-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "som_sz = 3 # 3, 3\n",
    "n_features = 2\n",
    "max_iter = 10\n",
    "\n",
    "test_som = som.SOM(som_sz, n_features, max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0 0 : 3\n",
      "\n",
      " 0 1 : 5\n",
      "\n",
      " 0 2 : 3\n",
      "\n",
      " 1 0 : 5\n",
      "\n",
      " 1 1 : 8\n",
      "\n",
      " 1 2 : 5\n",
      "\n",
      " 2 0 : 3\n",
      "\n",
      " 2 1 : 5\n",
      "\n",
      " 2 2 : 3\n",
      "Your U-matrix is:\n",
      "[[0.267 0.448 0.487]\n",
      " [0.43  1.    0.449]\n",
      " [0.224 0.387 0.38 ]]\n"
     ]
    }
   ],
   "source": [
    "print(f'Your U-matrix is:\\n{test_som.u_matrix()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should be:\n",
    "    \n",
    "    Your U-matrix is:\n",
    "    [[0.267 0.448 0.487]\n",
    "     [0.43  1.    0.449]\n",
    "     [0.224 0.387 0.38 ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Test `fit` with Iris dataset\n",
    "\n",
    "You will use the Iris dataset to test your `fit` function. Your goal is to qualitatively (roughly) reproduce the example image on Wikipedia (below).\n",
    "\n",
    "- Preprocess the data in the pandas Dataframe below to produce `iris_x` (training data) and `iris_y` (classes).\n",
    "    - For the training data, pull out all columns except for \"species\", then normalize each row by its Euclidean distance so that each vector sums to 1. Convert it from Dataframe to ndarray.\n",
    "    - For the classes, convert to \"species\" column to an int-code (e.g. values take on 0, 1, 2). Convert from Dataframe to ndarray. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make iris_x\n",
    "iris_x = iris.copy()\n",
    "iris_x = iris_x.drop(columns='species')\n",
    "iris_x = iris_x.to_numpy()\n",
    "for i in range(iris_x.shape[0]):\n",
    "    iris_x[i] = iris_x[i] / np.linalg.norm(iris_x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "# make iris_y\n",
    "iris_y = iris['species']\n",
    "# print(iris_y)\n",
    "int_coded = pd.factorize(iris_y)\n",
    "# print(int_coded)\n",
    "iris_y = int_coded[0]\n",
    "print(iris_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Iris preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st 4 rows of preprocessed iris:\n",
      "[[0.804 0.552 0.221 0.032]\n",
      " [0.828 0.507 0.237 0.034]\n",
      " [0.805 0.548 0.223 0.034]\n",
      " [0.8   0.539 0.261 0.035]]\n",
      "\n",
      "First 4 classes of iris:\n",
      "[0 0 0 0]\n",
      "\n",
      "Last 4 classes of iris:\n",
      "[2 2 2 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'1st 4 rows of preprocessed iris:\\n{iris_x[:4]}\\n')\n",
    "print(f'First 4 classes of iris:\\n{iris_y[:4]}\\n')\n",
    "print(f'Last 4 classes of iris:\\n{iris_y[-4:]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should look like:\n",
    "\n",
    "    1st 4 rows of preprocessed iris:\n",
    "    [[0.804 0.552 0.221 0.032]\n",
    "     [0.828 0.507 0.237 0.034]\n",
    "     [0.805 0.548 0.223 0.034]\n",
    "     [0.8   0.539 0.261 0.035]]\n",
    "\n",
    "    First 4 classes of iris:\n",
    "    [0 0 0 0]\n",
    "\n",
    "    Last 4 classes of iris:\n",
    "    [2 2 2 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test `fit`, `u_matrix` after different number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization error after 1 iterations: 22.73\n",
      "Your initial u-matrix is (before any training):\n",
      "[[0.268 0.495 0.516 0.496 0.635 0.582 0.39 ]\n",
      " [0.538 0.956 0.931 0.794 0.776 0.771 0.464]\n",
      " [0.506 0.911 0.831 0.988 1.    0.751 0.502]\n",
      " [0.524 0.771 0.748 0.85  0.879 0.886 0.618]\n",
      " [0.405 0.745 0.755 0.714 0.782 0.907 0.586]\n",
      " [0.586 0.798 0.733 0.761 0.801 0.959 0.494]\n",
      " [0.311 0.508 0.44  0.409 0.48  0.548 0.417]]\n",
      "Your u-matrix is (after 1 step):\n",
      "[[0.267 0.494 0.516 0.498 0.643 0.595 0.4  ]\n",
      " [0.532 0.946 0.924 0.794 0.782 0.783 0.473]\n",
      " [0.497 0.894 0.819 0.98  1.    0.755 0.51 ]\n",
      " [0.511 0.753 0.732 0.837 0.872 0.885 0.621]\n",
      " [0.392 0.721 0.734 0.698 0.771 0.902 0.587]\n",
      " [0.564 0.771 0.71  0.741 0.787 0.951 0.495]\n",
      " [0.3   0.49  0.425 0.398 0.471 0.543 0.416]]\n",
      "\n",
      "Quantization error after 100 iterations: 10.43\n",
      "Your u-matrix is (after 100 steps):\n",
      "[[0.131 0.271 0.289 0.261 0.217 0.147 0.066]\n",
      " [0.348 0.549 0.529 0.479 0.386 0.285 0.137]\n",
      " [0.518 0.774 0.719 0.63  0.518 0.398 0.218]\n",
      " [0.634 0.971 0.913 0.797 0.647 0.515 0.289]\n",
      " [0.618 0.965 1.    0.905 0.734 0.572 0.316]\n",
      " [0.448 0.839 0.926 0.923 0.781 0.637 0.343]\n",
      " [0.266 0.445 0.576 0.593 0.501 0.412 0.169]]\n",
      "\n",
      "Quantization error after 1000 iterations: 4.68\n"
     ]
    }
   ],
   "source": [
    "som_sz = 7\n",
    "n_features = 4\n",
    "max_iter = 1\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "iris_som = som.SOM(som_sz, n_features, max_iter, init_lr=0.2, init_sigma=10.0, verbose=False)\n",
    "iris_u_map0 = iris_som.u_matrix()\n",
    "iris_som.fit(iris_x.copy())\n",
    "iris_u_map = iris_som.u_matrix()\n",
    "print(f'Quantization error after {max_iter} iterations: {iris_som.error(iris_x):.2f}')\n",
    "print(f'Your initial u-matrix is (before any training):\\n{iris_u_map0}')\n",
    "print(f'Your u-matrix is (after 1 step):\\n{iris_u_map}\\n')\n",
    "\n",
    "max_iter = 100\n",
    "iris_som = som.SOM(som_sz, n_features, max_iter, init_lr=0.2, init_sigma=10.0, verbose=False)\n",
    "iris_som.fit(iris_x.copy())\n",
    "iris_u_map2 = iris_som.u_matrix()\n",
    "print(f'Quantization error after {max_iter} iterations: {iris_som.error(iris_x):.2f}')\n",
    "print(f'Your u-matrix is (after 100 steps):\\n{iris_u_map2}')\n",
    "\n",
    "max_iter = 1000\n",
    "iris_som = som.SOM(som_sz, n_features, max_iter, init_lr=0.2, init_sigma=10.0, verbose=False)\n",
    "iris_som.fit(iris_x.copy())\n",
    "print(f'\\nQuantization error after {max_iter} iterations: {iris_som.error(iris_x):.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see:\n",
    "\n",
    "    Quantization error after 1 iterations: 0.14\n",
    "    Your initial u-matrix is (before any training):\n",
    "    [[0.268 0.495 0.516 0.496 0.635 0.582 0.39 ]\n",
    "     [0.538 0.956 0.931 0.794 0.776 0.771 0.464]\n",
    "     [0.506 0.911 0.831 0.988 1.    0.751 0.502]\n",
    "     [0.524 0.771 0.748 0.85  0.879 0.886 0.618]\n",
    "     [0.405 0.745 0.755 0.714 0.782 0.907 0.586]\n",
    "     [0.586 0.798 0.733 0.761 0.801 0.959 0.494]\n",
    "     [0.311 0.508 0.44  0.409 0.48  0.548 0.417]]\n",
    "    Your u-matrix is (after 1 step):\n",
    "    [[0.263 0.485 0.508 0.492 0.635 0.588 0.395]\n",
    "     [0.528 0.937 0.917 0.787 0.776 0.778 0.472]\n",
    "     [0.496 0.894 0.819 0.979 1.    0.757 0.51 ]\n",
    "     [0.516 0.759 0.74  0.846 0.882 0.896 0.628]\n",
    "     [0.402 0.739 0.752 0.715 0.79  0.923 0.6  ]\n",
    "     [0.585 0.798 0.737 0.766 0.814 0.983 0.511]\n",
    "     [0.312 0.511 0.445 0.415 0.491 0.565 0.433]]\n",
    "\n",
    "    Quantization error after 100 iterations: 0.08\n",
    "    Your u-matrix is (after 100 steps):\n",
    "    [[0.049 0.21  0.419 0.606 0.648 0.583 0.298]\n",
    "     [0.152 0.333 0.649 0.92  0.979 0.874 0.392]\n",
    "     [0.215 0.488 0.71  0.96  1.    0.856 0.363]\n",
    "     [0.435 0.719 0.9   0.98  0.966 0.798 0.38 ]\n",
    "     [0.611 0.957 0.977 0.967 0.849 0.685 0.333]\n",
    "     [0.668 0.999 0.957 0.853 0.722 0.552 0.241]\n",
    "     [0.323 0.475 0.478 0.446 0.382 0.296 0.135]]\n",
    "\n",
    "    Quantization error after 1000 iterations: 0.03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Have SOM learn structure of Iris data\n",
    "\n",
    "If all goes well, your u-map superimposed with the most active units for each training sample should (qualitatively) look like the lower-right image on Wikipedia: https://en.wikipedia.org/wiki/Self-organizing_map#/media/File:SOM_of_Fishers_Iris_flower_data_set.JPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som_sz = 70\n",
    "n_features = 4\n",
    "max_iter = 10000\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "iris_som = som.SOM(som_sz, n_features, max_iter, init_lr=3, init_sigma=51.0, verbose=False)\n",
    "iris_som.fit(iris_x.copy())\n",
    "iris_u_map = iris_som.u_matrix()\n",
    "\n",
    "plt.pcolor(iris_u_map.T)\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "markers = ['o','s','D']\n",
    "colors = ['r','g','b']\n",
    "\n",
    "for i in range(len(iris_x)):\n",
    "    bmu_pos = iris_som.get_bmu(iris_x[i])\n",
    "    plt.plot(bmu_pos[0] + 0.5,\n",
    "             bmu_pos[1] + 0.5,\n",
    "             markers[iris_y[i]],\n",
    "             markeredgecolor=colors[iris_y[i]],\n",
    "             markersize=12,\n",
    "             markerfacecolor='None')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Question 3**: What do the different shaped/colored plot markers represent in the visualization?\n",
    "- **Question 4**: Explain what the above visualizaion is showing us. Why does the plot look different than Iris scatterplots like you're used to seeing? \n",
    "- **Question 5**: How do we make use of the Iris classes (species) in the above visualization? Does training depend on them?\n",
    "- **Question 6**: Copy the plot with default hyperparameters into a separate cell (or save it in your project folder). Then play around with the hyperparameters. Report on what how each affects the map. Does it matter that the clusters may jump around? Explain why.\n",
    "- **Question 7**: How stable are the hyperparameters in generating distinct clusters? Back up your observations with some numbers (i.e. parameter ranges ranges, things you experimented with).\n",
    "- **Question 8**: See what happens when you shrink the map size. What's the consequence for classifying new data with the smaller map size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Visualize word embedding vectors\n",
    "\n",
    "The below parameters should work out-of-the-box for creating a \"word cloud\" of the IMDb word embedding vectors. Tweak as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = np.load('results/imdb_word_vectors.npy')\n",
    "word_strings = np.load('results/imdb_word_strings.npy')\n",
    "print(f'Loaded {len(word_vectors)} word embedding vectors and {len(word_strings)} words.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the SOM on word vectors and look at the U-matrix\n",
    "\n",
    "- Create a SOM object, fit it to the word vectors, get the U-matrix (`word_u_map`), use the below code to plot it.\n",
    "\n",
    "Training should take around 1 minutes or less with the following default parameters:\n",
    "\n",
    "- size of SOM (in either x or y): 100\n",
    "- max iterations: 100,000\n",
    "- learning rate: 2\n",
    "- initial Gaussian neighborhood standard deviation: 40\n",
    "\n",
    "The U-matrix should have structure to it (not look like salt-and-pepper pixel noise or be completely black/white/gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make SOM here, train it, etc\n",
    "\n",
    "plt.pcolor(word_u_map.T)\n",
    "plt.colorbar()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words_to_visualize = 400\n",
    "jitter = 2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30, 30))\n",
    "for i in range(len(word_vectors[:num_words_to_visualize])):\n",
    "    bmu_pos = word_som.get_bmu(word_vectors[i])\n",
    "    # jitter the placement of words so that they are less likely to plot ontop of each other\n",
    "    x = bmu_pos[0] + 0.5 + jitter*(2*np.random.random()-1)\n",
    "    y = bmu_pos[1] + 0.5 + jitter*(2*np.random.random()-1)\n",
    "    ax.scatter(x, y)\n",
    "    ax.annotate(str(word_strings[i]), (x, y), fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9:** What are your favorite word clusters? Are there any word groupings that surprised you (but make sense post-hoc)?\n",
    "\n",
    "**Question 10:** How does the number of iterations affect the apparent quality of the word vector mapping? For example, does the visualization make sense when you decrease/increase the iteration count? How stable does it seem? Does it break down at any point?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Other SOM initialization schemes\n",
    "\n",
    "Other initialization schemes initialize the SOM weights to random values in the training set or according to the 1st two principle components of the data. Investigate how these affect the stablity and convergence of the network weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SOM quantization error\n",
    "\n",
    "Do an analysis (make plots, explain findings) where you investigate how various SOM parameters affect the quantization error. Parameters you might experiment with are:\n",
    "- iteration count\n",
    "- SOM size\n",
    "\n",
    "**NOTE:** Some parameters may need to adjusted together in some fashion. For example, decreasing the SOM size decreases the grid size. Therefore, the Gaussian neighborhood size $\\sigma$ probably needs to be rescaled, as does the learning rate (how much vectors move around in the space duing each update)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. More sophisticated text preprocessing\n",
    "\n",
    "To preprocess text for Skipgram, we defined words as strings with at least one letter. Research and examine whether more sophisticated approaches (e.g. removing stop words, destemming, lemmatization, etc) to preprocessing text yield better word embedding results (better quality word context predictions, better word similiarity as represented by the SOM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Skip-gram word context\n",
    "\n",
    "We only used the Skip-gram network to extract the word embedding vectors. Load in the IMDb test set and explore how a trained Skip-gram network can predict words surrounding each target work. This may require overriding the `predict` function and looking at the softmax values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Continuous Bag of Words (CBOW)\n",
    "\n",
    "In addition to Skip-gram, Mikolov et al. also proposed the CBOW model. Duplicate your Skipgram code and make the necessary modifications to implement CBOW. Note that this requires similar modifications that we made to the output layer, but to the hidden layer. The main change is that we present multiple context word one-hot vectors \"at once\": compute `y_net_in` for each context word, then average them across the layer (to get a $1\\times H$ vector) before computing `y_net_act`. Then proceed like normal with the rest of the forward pass. Because there's one correct class, use standard cross-entropy loss. The backprop process is the same until you get to computing the hidden layer (`y`) `d_wts`. Because we averaged $C$ inputs when computing `y_net_in`, we have the divide `d_wts` by $C$.\n",
    "\n",
    "- How do the word vectors compare to Skipgram?\n",
    "- Analyze the target word predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Removing one-hot vectors from Skip-gram and add mini-batch support\n",
    "\n",
    "- In our Skip-gram implementation, we one-hot coded all target words upfront. This is wasteful with respect to memory consumption because one-hot input vectors simply select the weights of $i^{th}$ hidden layer unit. Override the `Dense` net_in computation method and modify it to handle int indices.\n",
    "\n",
    "- Another improvement would be to add mini-batch support to training. This may be easier to implement if you bypass all usage of one-hot vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Quality of SOM word clusters\n",
    "\n",
    "Experiment with how SOM learning parameters and Skip-gram training time (and other parameters like # embedding dimensions) affect the quality of similar word cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Other text datasets\n",
    "\n",
    "Obtain, load, and preprocess other text datasets. Train Skip-gram, visualize the word vectors using SOM, and interpret what you find."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
