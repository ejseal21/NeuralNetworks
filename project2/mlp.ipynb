{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR NAMES HERE**\n",
    "\n",
    "Fall 2019\n",
    "\n",
    "CS 343: Neural Networks\n",
    "\n",
    "Project 2: Multi-layer Perceptrons\n",
    "\n",
    "**Draft due 11:59pm Thurs Sept 26**\n",
    "\n",
    "\n",
    "Summary of files in this project:\n",
    "- `softmax_layer.ipynb`\n",
    "- `single_layer_net.py`\n",
    "- `preprocess_data.py`\n",
    "- `mlp.ipynb`\n",
    "- `mlp.py`\n",
    "\n",
    "**REMINDER**: Submit rubric on Google Classroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for obtaining the STL-dataset\n",
    "import load_stl10_dataset\n",
    "\n",
    "# for preprocessing dataset\n",
    "import preprocess_data\n",
    "\n",
    "# Set the color style so that Professor Layton can see your plots\n",
    "plt.style.use(['seaborn-colorblind', 'seaborn-darkgrid'])\n",
    "# Make the font size larger\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "# Turn off scientific notation when printing\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "# Automatically reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "### a. STL-10\n",
    "\n",
    "**TODO**: Run the cell below, to preprocess STL-10 dataset like you did in the other notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset from the internet, convert it to Numpy ndarray, resize to 32x32\n",
    "# cache it locally on your computer for faster loading next time.\n",
    "stl_imgs, stl_labels = load_stl10_dataset.load()\n",
    "\n",
    "# Load in the string names for each class\n",
    "classes = np.loadtxt(os.path.join('data', 'stl10_binary', 'class_names.txt'), dtype=str)\n",
    "\n",
    "# Preprocess image pixel values for the MLP net\n",
    "stl_imgs, stl_labels = preprocess_data.preprocess_stl(stl_imgs, stl_labels)\n",
    "print(f'stl_imgs dtype is {stl_imgs.dtype} and it should be float64')\n",
    "print(f'stl_imgs max is {np.max(stl_imgs[:, 1:]):.3f} and it should be 0.668')\n",
    "print(f'stl_imgs shape is {stl_imgs.shape} and it should be (5000, 3072)')\n",
    "print(f'stl_labels span {stl_labels.min()}->{stl_labels.max()} and it should be 0->9')\n",
    "    \n",
    "x_train, y_train, x_test, y_test, x_val, y_val, x_dev, y_dev = preprocess_data.create_splits(stl_imgs, stl_labels)  \n",
    "print ('Train data shape: ', x_train.shape)\n",
    "print ('Train labels shape: ', y_train.shape)\n",
    "print ('Test data shape: ', x_test.shape)\n",
    "print ('Test labels shape: ', y_test.shape)\n",
    "print ('Validation data shape: ', x_val.shape)\n",
    "print ('Validation labels shape: ', y_val.shape)\n",
    "print ('dev data shape: ', x_dev.shape)\n",
    "print ('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circle in a square\n",
    "\n",
    "**TODO** Run the code below from the other notebook to load in the CIS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 20\n",
    "\n",
    "cis_train_path = os.path.join('data', 'cis', 'cis_train.dat')\n",
    "cis_test_path = os.path.join('data', 'cis', 'cis_test.dat')\n",
    "\n",
    "cis_train_all = np.loadtxt(cis_train_path, delimiter='\\t')\n",
    "\n",
    "# shuffle the data\n",
    "s_inds = np.arange(len(cis_train_all))\n",
    "cis_train_all = cis_train_all[s_inds]\n",
    "\n",
    "cis_train_x = cis_train_all[:, :2]\n",
    "cis_train_y = cis_train_all[:, 2].astype(int)\n",
    "\n",
    "cis_val_x = cis_train_x[:val_size]\n",
    "cis_train_x = cis_train_x[val_size:]\n",
    "cis_val_y = cis_train_y[:val_size]\n",
    "cis_train_y = cis_train_y[val_size:]\n",
    "\n",
    "cis_test_all = np.loadtxt(cis_test_path, delimiter='\\t')\n",
    "cis_test_x = cis_test_all[:, :2]\n",
    "cis_test_y = cis_test_all[:, 2].astype(int)\n",
    "\n",
    "print ('CIS Train data shape: ', cis_train_x.shape)\n",
    "print ('CIS Train labels shape: ', cis_train_y.shape)\n",
    "print ('CIS Validation data shape: ', cis_val_x.shape)\n",
    "print ('CIS Validation labels shape: ', cis_val_y.shape)\n",
    "print ('CIS Test data shape: ', cis_test_x.shape)\n",
    "print ('CIS Test labels shape: ', cis_test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Circle in a square dataset\n",
    "\n",
    "**Properties:**\n",
    "- 2 classes of features\n",
    "\n",
    "**TODO**:\n",
    "1. Download the circle in a square dataset. Folder structure is `<project folder>/data/cis/<cis dat files>`\n",
    "2. Create numpy arrays for the train/test data and separate variables for the associated labels by running the below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 20\n",
    "\n",
    "cis_train_path = os.path.join('data', 'cis', 'cis_train.dat')\n",
    "cis_test_path = os.path.join('data', 'cis', 'cis_test.dat')\n",
    "\n",
    "cis_train_all = np.loadtxt(\n",
    "cis_train_path, delimiter='\\t')\n",
    "\n",
    "cis_train_x = cis_train_all[:, :2]\n",
    "cis_train_y = cis_train_all[:, 2].astype(int)\n",
    "\n",
    "cis_val_x = cis_train_x[:val_size]\n",
    "cis_train_x = cis_train_x[val_size:]\n",
    "cis_val_y = cis_train_y[:val_size]\n",
    "cis_train_y = cis_train_y[val_size:]\n",
    "\n",
    "cis_test_all = np.loadtxt(cis_test_path, delimiter='\\t')\n",
    "cis_test_x = cis_test_all[:, :2]\n",
    "cis_test_y = cis_test_all[:, 2].astype(int)\n",
    "\n",
    "print ('CIS Train data shape: ', cis_train_x.shape)\n",
    "print ('CIS Train labels shape: ', cis_train_y.shape)\n",
    "print ('CIS Validation data shape: ', cis_val_x.shape)\n",
    "print ('CIS Validation labels shape: ', cis_val_y.shape)\n",
    "print ('CIS Test data shape: ', cis_test_x.shape)\n",
    "print ('CIS Test labels shape: ', cis_test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you should see a...black circle in a white unit square :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(cis_test_x[:,0], cis_test_x[:,1], c=cis_test_y)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3) Implement Multilayer Perceptron (MLP) with softmax activation and cross-entropy loss\n",
    "\n",
    "Now that we've tested the softmax activation function and cross-entropy loss functions in a single-layer net, let's implement the MLP version.\n",
    "\n",
    "Much of your work on the single layer net will carry over, so go ahead and copy-paste and modify as needed!\n",
    "\n",
    "The structure of our MLP will be:\n",
    "\n",
    "Input layer (X units) -> Hidden layer (Y units) with Rectified Linear activation (ReLu) -> Output layer (Z units) with softmax activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Implement the following functions in `mlp.py`\n",
    "\n",
    "- `initialize_wts`\n",
    "- `accuracy`\n",
    "- `one_hot`\n",
    "- `predict`\n",
    "- `forward`\n",
    "- `backward`\n",
    "- `fit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Test key functions with randomly generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy net for debugging\n",
    "num_inputs = 3\n",
    "num_features = 6\n",
    "num_hidden_units = 7\n",
    "num_classes = 5\n",
    "\n",
    "net = MLP(num_features, num_hidden_units, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random data and classes\n",
    "np.random.seed(0)\n",
    "test_x = np.random.normal(loc=0, scale=100, size=(num_inputs, num_features))\n",
    "test_y = np.random.uniform(low=0, high=num_classes-1, size=(num_inputs,))\n",
    "test_y = test_y.astype(int)\n",
    "print(f'Test input shape: {test_x.shape}')\n",
    "print(f'Test class vector shape: {test_y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test `initialize_wts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.initialize_wts(M=num_features, H=num_hidden_units, C=num_classes, std=0.01)\n",
    "print(f'y wt shape is {net.y_wts.shape} and should be (6, 7)')\n",
    "print(f'y bias shape is {net.y_b.shape} and should be (7,)')\n",
    "print(f'z wt shape is {net.z_wts.shape} and should be (7, 5)')\n",
    "print(f'z bias shape is {net.z_b.shape} and should be (5,)')\n",
    "\n",
    "print(f'1st few y wts are\\n{net.y_wts[:,0]}\\nand should be\\n[ 0.018 -0.002  0.004  0.007  0.015  0.002]')\n",
    "print(f'y bias is\\n{net.y_b}\\nand should be\\n[-0.017  0.02  -0.005 -0.004 -0.013  0.008 -0.016]')\n",
    "print(f'1st few z wts are\\n{net.z_wts[:,0]}\\nand should be\\n[-0.002 -0.    -0.004  0.002  0.001  0.004  0.001]')\n",
    "print(f'z bias is\\n{net.z_b}\\nand should be\\n[ 0.015  0.019  0.012 -0.002 -0.011]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the `predict` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_pred = net.predict(test_x)\n",
    "print(f'Predicted classes are {test_y_pred} and should be [3 0 0]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the `forward` method focusing on`ReLU`(net act of hidden layer `y`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,y_net_act_test,_,_,_ = net.forward(test_x, test_y)\n",
    "\n",
    "correct_y_act = np.array([[7.66 , 4.47 , 0.804, 9.981, 0.   , 0.   , 0.   ],\n",
    "       [2.37 , 2.717, 2.18 , 2.552, 0.357, 0.   , 0.   ],\n",
    "       [3.997, 2.671, 1.195, 3.034, 0.   , 0.   , 0.   ]])\n",
    "\n",
    "print(f'Your y activation is\\n{y_net_act_test}')\n",
    "print(f'The correct y activation (ReLU) is\\n{y_net_act_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the `forward` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,_,probs,_ = net.forward(test_x, test_y)\n",
    "\n",
    "correct_probs = np.array([[0.219, 0.2  , 0.191, 0.219, 0.171],\n",
    "       [0.208, 0.204, 0.201, 0.205, 0.183],\n",
    "       [0.208, 0.202, 0.202, 0.205, 0.183]])\n",
    "\n",
    "print(f'Your z activation (class probabilities) is\\n{probs}')\n",
    "print(f'The correct z activation (class probabilities) is\\n{correct_probs}')\n",
    "print(f'The sums across rows (for each data sample) are {np.sum(probs, axis=1)}.')\n",
    "print(f'  You should know what should be :)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the `forward` method, focusing on loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_in, y_act ,z_in, z_act, loss = net.forward(test_x, test_y)\n",
    "correct_loss = 1.564402690536365\n",
    "\n",
    "print(f'Your average loss is\\n{loss}')\n",
    "print(f'The correct average loss is approx\\n{correct_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the `forward` method, focusing on regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_in, y_act ,z_in, z_act, loss = net.forward(test_x, test_y, reg=1000)\n",
    "correct_loss = 5.257207314928798\n",
    "\n",
    "print(f'Your regularized average loss is\\n{loss}')\n",
    "print(f'The correct regularized average loss is approx\\n{correct_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the `backward` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_in, y_act ,z_in, z_act, loss = net.forward(test_x, test_y, reg=0.5)\n",
    "grads = net.backward(test_x, test_y, y_in, y_act ,z_in, z_act, reg=0.5)\n",
    "\n",
    "print('Your gradient for y_wts is\\n', grads[0])\n",
    "print('Your gradient for y_b is\\n', grads[1])\n",
    "print('Your gradient for z_wts is\\n', grads[2])\n",
    "print('Your gradient for z_b is\\n', grads[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct gradients are:\n",
    "\n",
    "`\n",
    "Your gradient for y_wts is\n",
    " [[-0.476  0.057 -0.458 -0.115  0.03  -0.005  0.005]\n",
    " [-0.002  0.014 -0.046 -0.162  0.004  0.004  0.001]\n",
    " [-0.088  0.038 -0.166 -0.325 -0.001 -0.004 -0.013]\n",
    " [-0.331  0.067 -0.398 -0.332  0.001  0.    -0.001]\n",
    " [-0.318  0.089 -0.465 -0.615 -0.001 -0.01  -0.002]\n",
    " [-0.315 -0.036 -0.036  0.806  0.029 -0.005 -0.007]]\n",
    "Your gradient for y_b is\n",
    " [-0.005  0.    -0.004 -0.     0.     0.     0.   ]\n",
    "Your gradient for z_wts is\n",
    " [[-2.879  0.933  0.131  0.987  0.816]\n",
    " [-1.69   0.669 -0.261  0.699  0.584]\n",
    " [-0.374  0.278 -0.45   0.284  0.242]\n",
    " [-3.221  1.041  0.154  1.111  0.904]\n",
    " [ 0.024  0.027 -0.091  0.029  0.015]\n",
    " [ 0.002 -0.003 -0.004 -0.003 -0.002]\n",
    " [ 0.    -0.006  0.005  0.002 -0.008]]\n",
    "Your gradient for z_b is\n",
    " [-0.455  0.202 -0.135  0.209  0.179]\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test loss over epoch. \n",
    "\n",
    "The below code should generate a curve that rapidly drops to 0 (there might be fluctuations and it might not be monotonic and that's ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLP(num_features, num_hidden_units, num_classes)\n",
    "loss, acc_t, acc_v = net.fit(test_x, test_y, test_x, test_y, reg=0, lr=0.001, mini_batch_sz=3, n_epochs=30)\n",
    "\n",
    "plt.plot(loss)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Avg cross-entropy Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Test MLP with Circle in Square dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First test case where training data = test data.\n",
    "\n",
    "You should see a nice drop and plateau in loss (after a bunch of print outs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = test\n",
    "hidden_size = 50\n",
    "net = MLP(cis_train_x.shape[1], hidden_size, 2)\n",
    "loss, acc_t, acc_v = net.fit(cis_test_x, cis_test_y, cis_val_x, cis_val_y, reg=0, lr=0.01, mini_batch_sz=100, n_epochs=700)\n",
    "\n",
    "plt.plot(loss)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Avg cross-entropy Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Run the below cell after training and generate the CIS plot. You should see a well-defined circle inside a square region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cis_test_y_pred = net.predict(cis_test_x)\n",
    "plt.scatter(cis_test_x[:,0], cis_test_x[:,1], c=cis_test_y_pred)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second test case where training data != test data:** You should see a jagged polygon approximation to a circle inside a square region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train != test\n",
    "net = MLP(cis_train_x.shape[1], hidden_size, 2)\n",
    "loss, acc_t, acc_v = net.fit(cis_train_x, cis_train_y, cis_val_x, cis_val_y, reg=0, lr=0.1, mini_batch_sz=len(cis_train_x), n_epochs=10000)\n",
    "\n",
    "plt.plot(loss)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Avg cross-entropy Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Run the below cell after training and generate the CIS plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cis_test_y_pred = net.predict(cis_test_x)\n",
    "plt.scatter(cis_test_x[:,0], cis_test_x[:,1], c=cis_test_y_pred)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5**: How do you interpret the circle-in-square results? Do you think the single-layer net (with softmax) can handle the CIS dataset? Why or why not? (You're invited to try it, maybe as an extension :)\n",
    "\n",
    "**Question 6**: Play with # hidden units, epochs, regularization strength on CIS training...how does each parameter affect the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Train on STL-10 dataset, plot performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try training the MLP on the below set of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLP(x_train.shape[1], 100, 10)\n",
    "loss, acc_t, acc_v = net.fit(x_train, y_train, x_val, y_val,\n",
    "                             lr=0.05, reg=0.01, mini_batch_sz=500, n_epochs=50, verbose=0)\n",
    "plt.plot(loss)\n",
    "plt.xlabel('Training iteration')\n",
    "plt.ylabel('Avg loss (cross entropy)')\n",
    "plt.title('STL-10 loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_t, label='Training accuracy')\n",
    "plt.plot(acc_v, label='Validation accuracy')\n",
    "plt.xlabel('Training iteration')\n",
    "plt.ylabel('Avg loss (cross entropy)')\n",
    "plt.title('STL-10 accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7**: What do the above loss and training and validation accuracy curves suggest about the quality of the hyperparameters used during training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Optimize on STL-10 dataset, plot performance\n",
    "\n",
    "**TODO** Write code in the cell below to perform a grid search to find the combinations of\n",
    "\n",
    "- learning rate\n",
    "- regularization strength\n",
    "- number hidden units\n",
    "- mini-batch size\n",
    "\n",
    "that yields the highest STL-10 validation set accuracy.\n",
    "\n",
    "You should be able to achieve higher accuracy than in the single layer version of the softmax network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_acc = 0\n",
    "best_lr = None\n",
    "best_reg = None\n",
    "best_hidden = None\n",
    "best_mini_batch_sz = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Set the parameters below to the best ones found by your grid search.\n",
    "- Generate and include the Test STL-10 loss curve\n",
    "- Generate and include the training and validation accuracy curves in the second cell down.\n",
    "\n",
    "Adjust the number of training epochs as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the net with the bestparam settings\n",
    "best_lr = None\n",
    "best_reg = None\n",
    "best_hidden = None\n",
    "\n",
    "bestNet = MLP(x_train.shape[1], best_hidden, 10)\n",
    "loss, acc_t, acc_v = bestNet.fit(x_test, y_test, x_val, y_val,\n",
    "                                 lr=best_lr, reg=best_reg, mini_batch_sz=best_mini_batch_sz, n_epochs=5000, verbose=1)\n",
    "plt.plot(loss)\n",
    "plt.xlabel('Training iteration')\n",
    "plt.ylabel('Avg loss (cross entropy)')\n",
    "plt.title('Test STL-10 data loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_t, label='Training accuracy')\n",
    "plt.plot(acc_v, label='Validation accuracy')\n",
    "plt.xlabel('Training iteration')\n",
    "plt.ylabel('Avg loss (cross entropy)')\n",
    "plt.title('Test STL-10 data accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Visualize learned weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: \n",
    "\n",
    "- In the cell below, get the weights of your best net's hidden layer (Y), reshape/transpose them so that they are `(N, 32, 32, 3)`.\n",
    "- Run the `plot_weights` function to generate a grid visualization of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_y_wts = bestNet.get_y_wts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights(wts):\n",
    "    grid_sz = int(np.sqrt(len(wts)))\n",
    "    plt.figure(figsize=(20,20))\n",
    "    for x in range(grid_sz):\n",
    "        for y in range(grid_sz):\n",
    "            lin_ind = np.ravel_multi_index((x, y), dims=(grid_sz, grid_sz))\n",
    "            plt.subplot(grid_sz, grid_sz, lin_ind+1)\n",
    "            currImg = wts[lin_ind]\n",
    "            low, high = np.min(currImg), np.max(currImg)\n",
    "            currImg = 255*(currImg - low) / (high - low)\n",
    "            currImg = currImg.astype('uint8')\n",
    "            plt.imshow(currImg)\n",
    "            plt.gca().axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weights(best_y_wts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "**Reminder**: Please do not integrate extensions into your base project so that it changes the expected behavior of core functions. It is better to duplicate the base project and add features from there.\n",
    "\n",
    "1) Instead of computing the loss over a mini-batch, compute it over epochs instead. Compare and contrast the approaches. Be sure to include analysis figures.\n",
    "\n",
    "2) Investigate how the single layer softmax network does with the CIS dataset. Explain and provide plots showing your results.\n",
    "\n",
    "3) If you have time to spare (or want to throw more computing power at the STL-10 dataset), process through the SLP and MLP and tune hyperparameters with the dataset at its original resolution (96x96 images). Show images of your learned weights. Can you find a training sweet spot where the learned weight visualizations look particularly cool?\n",
    "\n",
    "4) Implement the sigmoid classifer (same network structure, except use sigmoid for netact) with the cross-entropy loss by creating another subclass of `SingleLayerNet` and/or `MLP`. Compare and contrast results achieved by the softmax/cross-entropy network.\n",
    "\n",
    "5) Explore the effects of batch gradient descent, stochastic gradient descent, and mini-batch gradient descent.\n",
    "\n",
    "6) Obtain, preprocess, train, and evaluate the performance of `SingleLayerNet` and/or `MLP` on another dataset with comparable types of image features. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
