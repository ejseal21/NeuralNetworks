{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR NAMES HERE**\n",
    "\n",
    "Fall 2019\n",
    "\n",
    "CS 343: Neural Networks\n",
    "\n",
    "Project 2: Multi-layer Perceptrons\n",
    "\n",
    "**Draft due 11:59pm Thurs Sept 26**\n",
    "\n",
    "\n",
    "Summary of files in this project:\n",
    "- `softmax_layer.ipynb`\n",
    "- `single_layer_net.py`\n",
    "- `preprocess_data.py`\n",
    "- `mlp.ipynb`\n",
    "- `mlp.py`\n",
    "\n",
    "**REMINDER**: Submit rubric on Google Classroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for obtaining the STL-dataset\n",
    "import load_stl10_dataset\n",
    "\n",
    "# for preprocessing dataset\n",
    "import preprocess_data\n",
    "\n",
    "# Set the color style so that Professor Layton can see your plots\n",
    "plt.style.use(['seaborn-colorblind', 'seaborn-darkgrid'])\n",
    "# Make the font size larger\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "# Turn off scientific notation when printing\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "# Automatically reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this project, you will implement Multilayer Perception (MLP) networks that classify image data belonging to one of 10 classes from the STL-10 dataset. \n",
    "\n",
    "You will create two networks that use the softmax activation function and cross-entropy loss: \n",
    "\n",
    "- One without a hidden layer (**Task 2**; this notebook) and \n",
    "- One with a hidden layer (**Task 3**; ``mlp.ipynb``)\n",
    "\n",
    "Before implementing the MLP with hidden layer, we will create a simpler single layer network (similar architecture to ADALINE) to test your loss and activation function code.\n",
    "\n",
    "Focus areas in this project are:\n",
    "- Working with a large dataset\n",
    "- Batch vs stochastic training\n",
    "- Regularization\n",
    "- Multi-class data classification\n",
    "- Network weight visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1) Download and preprocess datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. STL-10\n",
    "\n",
    "More info: https://cs.stanford.edu/~acoates/stl10/\n",
    "\n",
    "**Properties:**\n",
    "- 10 classes of images: airplane, bird, car, cat, deer, dog, horse, monkey, ship, truck\n",
    "- 5000 images.\n",
    "- 96x96 resolution (we will work with them converted to 32x32 for rapid development and debugging purposes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) Download the STL-10 dataset, convert to 32x32 images\n",
    "\n",
    "This data acquistion script should \"just work\". You're welcome to take a peek at the script to see where the data is going, but you shouldn't need to tinker with it. It needs to download the dataset from the internet, which might take a few minutes. I set up the script so that it caches the dataset locally in your project folder, so you should only need to download it once. There is a progress percentage indicator.\n",
    "\n",
    "You should see the following output:\n",
    "\n",
    "`Images are: (5000, 96, 96, 3)\n",
    "Labels are: (5000,)\n",
    "Resizing 5000 images to 32x32...Done!\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset from the internet, convert it to Numpy ndarray, resize to 32x32\n",
    "# cache it locally on your computer for faster loading next time.\n",
    "stl_imgs, stl_labels = load_stl10_dataset.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Take a look at a sample of the images in the dataset\n",
    "\n",
    "Let's take a look at the image categories that we want the neural network to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the string names for each class\n",
    "classes = np.loadtxt(os.path.join('data', 'stl10_binary', 'class_names.txt'), dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 12))\n",
    "plt.subplot\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    randInd = random.randint(0, len(stl_imgs)-1)\n",
    "    plt.imshow(stl_imgs[randInd])\n",
    "    plt.xlabel(classes[stl_labels[randInd]-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iii) Preprocess the data\n",
    "\n",
    "Preprocessing data is an important part of working with neural networks. \n",
    "\n",
    "**TODO**: Fill in the function `preprocess_stl` in `preprocess_data.py`, implementing the 4 listed goals, to get the desired print out below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess image pixel values for the MLP net\n",
    "stl_imgs, stl_labels = preprocess_data.preprocess_stl(stl_imgs, stl_labels)\n",
    "\n",
    "print(f'stl_imgs dtype is {stl_imgs.dtype} and it should be float64')\n",
    "print(f'stl_imgs max is {np.max(stl_imgs[:, 1:]):.3f} and it should be 0.668')\n",
    "print(f'stl_imgs shape is {stl_imgs.shape} and it should be (5000, 3072)')\n",
    "print(f'stl_labels span {stl_labels.min()}->{stl_labels.max()} and it should be 0->9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (iii) Create train, validation, test, and dev splits\n",
    "\n",
    "**TODO**: Complete the `create_splits` function in `preprocess_data.py` to divvy up the dataset into train/test/validation/development subsets, following the two main goals lists. You should obtain a printout as follows:\n",
    "\n",
    "`Train data shape:  (3500, 3072)\n",
    "Train labels shape:  (3500,)\n",
    "Test data shape:  (500, 3072)\n",
    "Test labels shape:  (500,)\n",
    "Validation data shape:  (500, 3072)\n",
    "Validation labels shape:  (500,)\n",
    "dev data shape:  (500, 3072)\n",
    "dev labels shape:  (500,)\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test, x_val, y_val, x_dev, y_dev = preprocess_data.create_splits(stl_imgs, stl_labels)  \n",
    "\n",
    "print ('Train data shape: ', x_train.shape)\n",
    "print ('Train labels shape: ', y_train.shape)\n",
    "print ('Test data shape: ', x_test.shape)\n",
    "print ('Test labels shape: ', y_test.shape)\n",
    "print ('Validation data shape: ', x_val.shape)\n",
    "print ('Validation labels shape: ', y_val.shape)\n",
    "print ('dev data shape: ', x_dev.shape)\n",
    "print ('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2) Implement single layer network to test softmax activation and cross-entropy loss\n",
    "\n",
    "You will first implement and test out the softmax activation and cross-entropy loss in a single layer net before embedding it in a more complex multiple layer network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Implement the following functions\n",
    "\n",
    "In `single_layer_net.py`, implement the following methods in the base class `SingleLayerNet`:\n",
    "\n",
    "- `fit`\n",
    "- `net_in`\n",
    "- `predict`\n",
    "- `one_hot`\n",
    "- `accuracy`\n",
    "\n",
    "\n",
    "In the child class `SingleLayerNetSoftmax` implement the following methods:\n",
    "\n",
    "- `activation` (softmax) $f(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^C e^{x_j}}$ where $x_i$ are the \"net in\" values and there are $C$ output neurons (one per input class). $f(x_i)$ is the activation values of each output neuron $i$. Since this is softmax, it is the probability that a given input belongs to the class $i$ coded by the output neuron.\n",
    "- `loss` (cross-entropy) $L(x_m) = -\\frac{1}{B}\\sum_{b=1}^B{Log \\left (\\frac{e^{x_m}}{\\sum_{n=1}^C e^{x_n}}\\right )}$. $m$ is the correct class for the $b^{th}$ input. $x_m$ is the output neuron activation for the correct class, $x_n$ is the output neuron activation for all of the classes (in the sum). The batch size is $B$, so the loss is averaged over each mini-batch of inputs. The expression in the $Log$ is just the softmax.\n",
    "- `gradient` (for softmax/cross-entropy)\n",
    "\n",
    "You're welcome to work in any order, but I recommend starting with `fit` because as you work though it, you should recognize why we need most of the other methods. You can finish `fit` or branch off as you need the other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Test key functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from single_layer_net import SingleLayerNetSoftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate some small Gaussian weights equal to the length of an image feature vector\n",
    "np.random.seed(0)\n",
    "randWts = np.random.normal(loc=0, scale=0.01, size=(x_dev.shape[1], 10))\n",
    "b = 1\n",
    "softmaxNet = SingleLayerNetSoftmax()\n",
    "randWts.shape\n",
    "\n",
    "# for test code purposes, use the last 15 images/labels\n",
    "test_imgs, test_labels = stl_imgs[-15:], stl_labels[-15:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the `onehot` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array([2, 2, 0, 1])\n",
    "c_test = 4\n",
    "y_one_hot = softmaxNet.one_hot(y_test, c_test)\n",
    "print(f'Your one hot vectors:\\n{y_one_hot}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your one hot vectors should look like:\n",
    "\n",
    "    [[0. 0. 1. 0.]\n",
    "     [0. 0. 1. 0.]\n",
    "     [1. 0. 0. 0.]\n",
    "     [0. 1. 0. 0.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the `loss`,  `net_in`, softmax `activation` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lossNoReg, lossReg = softmaxNet.test_loss(randWts, b, test_imgs, test_labels)\n",
    "print(f'The loss (without regularization) is {lossNoReg:.2f} and it should approx be 2.28')\n",
    "print(f'The loss (with 0.5 regularization) is {lossReg:.2f} and it should approx be 3.04')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the `gradient` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_wts, grad_b = softmaxNet.test_gradient(randWts, b, test_imgs, test_labels, 10)\n",
    "print(f'1st few Wt gradient values are {grad_wts[:4,0]}\\nand should be                  [0.01  0.015 0.014 0.007] ')\n",
    "print(f'1st few Wt bias values are {grad_b[:4]}\\nand should be              [ 0.103 -0.099  0.101 -0.026]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `fit` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmaxNet = SingleLayerNetSoftmax()\n",
    "loss_history = softmaxNet.fit(x_dev, y_dev, n_epochs=600, mini_batch_sz=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the random mini-batch sampling process, you should get different specific numbers, but the loss should decrease with iterations. You should get something like this:\n",
    "\n",
    "    Starting to train network...There will be 600 epochs and 1200 iterations total, 2 iter/epoch.\n",
    "      Completed iter 0/1200. Training loss: 2.29.\n",
    "      Completed iter 100/1200. Training loss: 2.30.\n",
    "      Completed iter 200/1200. Training loss: 2.27.\n",
    "      Completed iter 300/1200. Training loss: 2.29.\n",
    "      Completed iter 400/1200. Training loss: 2.27.\n",
    "      Completed iter 500/1200. Training loss: 2.23.\n",
    "      Completed iter 600/1200. Training loss: 2.23.\n",
    "      Completed iter 700/1200. Training loss: 2.23.\n",
    "      Completed iter 800/1200. Training loss: 2.21.\n",
    "      Completed iter 900/1200. Training loss: 2.20.\n",
    "      Completed iter 1000/1200. Training loss: 2.20.\n",
    "      Completed iter 1100/1200. Training loss: 2.17.\n",
    "    Finished training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot the loss. \n",
    "\n",
    "It should look noisy, but on average be decreasing and look roughly linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cross_entropy_loss(loss_history):\n",
    "    plt.plot(loss_history)\n",
    "    plt.xlabel('Training iteration')\n",
    "    plt.ylabel('loss (cross-entropy)')\n",
    "    plt.show()\n",
    "    \n",
    "plot_cross_entropy_loss(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**: What do you think the linear drop in loss over the epochs tells us about the state of the training process? How is current training going? What's the future potential like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test regularization with training. The loss should hover around 17-18 for `reg=10`\n",
    "\n",
    "**Question 2**: Play around with the regularization parameter. You can drastically change the magnitude, but it should remain positive. How does this affect the training loss and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmaxNet = SingleLayerNetSoftmax()\n",
    "loss_history = softmaxNet.fit(x_dev, y_dev, reg=10, n_epochs=500)\n",
    "plot_cross_entropy_loss(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test training batch size\n",
    "\n",
    "**Question 3**: Play around with the batch size parameter. **How does this affect the training loss and why?** (Think about the error gradient and how the weights change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmaxNet = SingleLayerNetSoftmax()\n",
    "loss_history = softmaxNet.fit(x_dev, y_dev, mini_batch_sz=1, n_epochs=10, verbose=0);\n",
    "plot_cross_entropy_loss(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Train and optimize STL-10 dataset performance, plot results\n",
    "\n",
    "#### (i) Grid search\n",
    "\n",
    "As you've surely noticed, hyperparameters can drastically affect learning! \n",
    "\n",
    "\n",
    "**TODO:** Implement a grid search for the best hyperparameters\n",
    "\n",
    "- learning rate,\n",
    "- regularization\n",
    "- batch size \n",
    "\n",
    "The grid search process should:\n",
    "\n",
    "1. Fit the model with specific values of hyperparameters that we're testing (using the training set).\n",
    "2. Compute the accuracy on the training set. \n",
    "3. Compute the accuracy on the validation set. \n",
    "4. Print out and record the best parameter combination as you go (that improves the **validation set accuracy**).\n",
    "5. Wipe the weights clean (reinitialize them) every time you try new parameters. It's easiest just to create a new net object on each run.\n",
    "\n",
    "\n",
    "This can take quite a bit of simulation time!\n",
    "\n",
    "I suggest implementing the coarse-to-fine search strategy. First try varying parameters over many orders of magnitude. Use the \"new best\" print outs to refine the ranges that you test. Abort simulations prematurely if you feel there aren't productive (no reason to wait!). This can take however long or short that you want to dedicate. Remember, you are printing out the best parameter values on each run, so you can just proceed with those.\n",
    "\n",
    "You should be able to achieve ~35% accuracy without too much effort (10% is chance performance).\n",
    "\n",
    "Your grade won't depend on the accuracy that you achieve, but if you get roughly the above accuracy, the payoff will be cooler visualizations in 2d!\n",
    "\n",
    "##### Guidelines\n",
    "- Learning rates above 1e-1 don't really make sense.\n",
    "- Your mini-batch sizes should be <= N and >= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_acc = 0\n",
    "best_lr = None\n",
    "best_reg = None\n",
    "best_batch = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ii) Evaluate best model on test set\n",
    "\n",
    "**Question 4:** Now that you have \"good\" parameter values recorded, train a new model with the best learning rate, regularization strength, and batch size values. What accuracy do you get on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace the following with your best values that you find above with grid search\n",
    "best_lr = None\n",
    "best_reg = None\n",
    "best_batch = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestNet = SingleLayerNetSoftmax()\n",
    "best_loss = bestNet.fit(x_train, y_train,\n",
    "            lr=best_lr, reg=best_reg, mini_batch_sz=best_batch, n_epochs=300, verbose=2)\n",
    "y_test_pred = bestNet.predict(x_test)\n",
    "test_acc = bestNet.accuracy(y_test, y_test_pred)\n",
    "\n",
    "print(f'Test accuracy {test_acc}')\n",
    "\n",
    "plt.plot(best_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Visualize learned weights\n",
    "\n",
    "**TODO:** Run the following code that plots the network weights going to each output neuron. If all goes well, you should see something really cool! Include the plot in your submitted project to show me what you got!\n",
    "\n",
    "**Note:** the quality of your visualizations will depend on:\n",
    "- The quality of the hyperparameters that you got via grid search.\n",
    "- How many epochs that you trained the network before plotting the weights\n",
    "\n",
    "One extension idea: is to find the combination of the above that result in the best visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't care about the bias wt\n",
    "wts = bestNet.wts\n",
    "# Reshape the wt vectors into spatial 'image' configurations to visualization\n",
    "wts = wts.reshape(32, 32, 3, 10)\n",
    "\n",
    "# Make a large new empty figure/plot\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "# Loop through each output neuron\n",
    "for i in range(10):\n",
    "  # Make a 2x5 grid of images\n",
    "  plt.subplot(2, 5, i+1)\n",
    "  \n",
    "  # Rescale the weights to be between 0 and 255\n",
    "  currImg = 255.0 * (wts[:, :, :, i].squeeze() - np.min(wts)) / (np.max(wts) - np.min(wts))\n",
    "  \n",
    "  plt.imshow(currImg.astype('uint8'))\n",
    "  plt.axis('off')\n",
    "  plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
